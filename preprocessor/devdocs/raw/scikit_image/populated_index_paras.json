[{"name": "A crash course on NumPy for images", "path": "user_guide/numpy_images", "type": "Guide", "text": ["Images in scikit-image are represented by NumPy ndarrays. Hence, many common operations can be achieved using standard NumPy methods for manipulating arrays:", "Retrieving the geometry of the image and the number of pixels:", "Retrieving statistical information about image intensity values:", "NumPy arrays representing images can be of different integer or float numerical types. See Image data types and what they mean for more information about these types and how scikit-image treats them.", "NumPy indexing can be used both for looking at the pixel values and to modify them:", "Be careful! In NumPy indexing, the first dimension (camera.shape[0]) corresponds to rows, while the second (camera.shape[1]) corresponds to columns, with the origin (camera[0, 0]) at the top-left corner. This matches matrix/linear algebra notation, but is in contrast to Cartesian (x, y) coordinates. See Coordinate conventions below for more details.", "Beyond individual pixels, it is possible to access/modify values of whole sets of pixels using the different indexing capabilities of NumPy.", "Slicing:", "Masking (indexing with masks of booleans):", "Fancy indexing (indexing with sets of indices):", "Masks are very useful when you need to select a set of pixels on which to perform the manipulations. The mask can be any boolean array of the same shape as the image (or a shape broadcastable to the image shape). This can be used to define a region of interest, for example, a disk:", "Boolean operations from NumPy can be used to define even more complex masks:", "All of the above remains true for color images. A color image is a NumPy array with an additional trailing dimension for the channels:", "This shows that cat is a 300-by-451 pixel image with three channels (red, green, and blue). As before, we can get and set the pixel values:", "We can also use 2D boolean masks for 2D multichannel images, as we did with the grayscale image above:", "Using a 2D mask on a 2D color image", "(Source code, png, pdf)", "Because scikit-image represents images using NumPy arrays, the coordinate conventions must match. Two-dimensional (2D) grayscale images (such as camera above) are indexed by rows and columns (abbreviated to either (row, col) or (r, c)), with the lowest element (0, 0) at the top-left corner. In various parts of the library, you will also see rr and cc refer to lists of row and column coordinates. We distinguish this convention from (x, y), which commonly denote standard Cartesian coordinates, where x is the horizontal coordinate, y - the vertical one, and the origin is at the bottom left (Matplotlib axes, for example, use this convention).", "In the case of multichannel images, the last dimension is used for color channels and is denoted by channel or ch.", "Finally, for volumetric (3D) images, such as videos, magnetic resonance imaging (MRI) scans, confocal microscopy, etc. we refer to the leading dimension as plane, abbreviated as pln or p.", "These conventions are summarized below:", "Image type", "Coordinates", "2D grayscale", "(row, col)", "2D multichannel (eg. RGB)", "(row, col, ch)", "3D grayscale", "(pln, row, col)", "3D multichannel", "(pln, row, col, ch)", "Many functions in scikit-image can operate on 3D images directly:", "In many cases, however, the third spatial dimension has lower resolution than the other two. Some scikit-image functions provide a spacing keyword argument to help handle this kind of data:", "Other times, the processing must be done plane-wise. When planes are stacked along the leading dimension (in agreement with our convention), the following syntax can be used:", "Although the labeling of the axes might seem arbitrary, it can have a significant effect on the speed of operations. This is because modern processors never retrieve just one item from memory, but rather a whole chunk of adjacent items (an operation called prefetching). Therefore, processing of elements that are next to each other in memory is faster than processing them when they are scattered, even if the number of operations is the same:", "When the last/rightmost dimension becomes even larger the speedup is even more dramatic. It is worth thinking about data locality when developing algorithms. In particular, scikit-image uses C-contiguous arrays by default. When using nested loops, the last/rightmost dimension of the array should be in the innermost loop of the computation. In the example above, the *= numpy operator iterates over all remaining dimensions.", "Although scikit-image does not currently provide functions to work specifically with time-varying 3D data, its compatibility with NumPy arrays allows us to work quite naturally with a 5D array of the shape (t, pln, row, col, ch):", "We can then supplement the above table as follows:", "Image type", "coordinates", "2D color video", "(t, row, col, ch)", "3D multichannel video", "(t, pln, row, col, ch)"]}, {"name": "color", "path": "api/skimage.color", "type": "color", "text": ["skimage.color.combine_stains(stains, conv_matrix)", "Stain to RGB color space conversion.", "skimage.color.convert_colorspace(arr, \u2026)", "Convert an image array to a new color space.", "skimage.color.deltaE_cie76(lab1, lab2)", "Euclidean distance between two points in Lab color space", "skimage.color.deltaE_ciede2000(lab1, lab2[, \u2026])", "Color difference as given by the CIEDE 2000 standard.", "skimage.color.deltaE_ciede94(lab1, lab2[, \u2026])", "Color difference according to CIEDE 94 standard", "skimage.color.deltaE_cmc(lab1, lab2[, kL, kC])", "Color difference from the CMC l:c standard.", "skimage.color.gray2rgb(image[, alpha])", "Create an RGB representation of a gray-level image.", "skimage.color.gray2rgba(image[, alpha])", "Create a RGBA representation of a gray-level image.", "skimage.color.grey2rgb(image[, alpha])", "Create an RGB representation of a gray-level image.", "skimage.color.hed2rgb(hed)", "Haematoxylin-Eosin-DAB (HED) to RGB color space conversion.", "skimage.color.hsv2rgb(hsv)", "HSV to RGB color space conversion.", "skimage.color.lab2lch(lab)", "CIE-LAB to CIE-LCH color space conversion.", "skimage.color.lab2rgb(lab[, illuminant, \u2026])", "Lab to RGB color space conversion.", "skimage.color.lab2xyz(lab[, illuminant, \u2026])", "CIE-LAB to XYZcolor space conversion.", "skimage.color.label2rgb(label[, image, \u2026])", "Return an RGB image where color-coded labels are painted over the image.", "skimage.color.lch2lab(lch)", "CIE-LCH to CIE-LAB color space conversion.", "skimage.color.rgb2gray(rgb)", "Compute luminance of an RGB image.", "skimage.color.rgb2grey(rgb)", "Compute luminance of an RGB image.", "skimage.color.rgb2hed(rgb)", "RGB to Haematoxylin-Eosin-DAB (HED) color space conversion.", "skimage.color.rgb2hsv(rgb)", "RGB to HSV color space conversion.", "skimage.color.rgb2lab(rgb[, illuminant, \u2026])", "Conversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.", "skimage.color.rgb2rgbcie(rgb)", "RGB to RGB CIE color space conversion.", "skimage.color.rgb2xyz(rgb)", "RGB to XYZ color space conversion.", "skimage.color.rgb2ycbcr(rgb)", "RGB to YCbCr color space conversion.", "skimage.color.rgb2ydbdr(rgb)", "RGB to YDbDr color space conversion.", "skimage.color.rgb2yiq(rgb)", "RGB to YIQ color space conversion.", "skimage.color.rgb2ypbpr(rgb)", "RGB to YPbPr color space conversion.", "skimage.color.rgb2yuv(rgb)", "RGB to YUV color space conversion.", "skimage.color.rgba2rgb(rgba[, background])", "RGBA to RGB conversion using alpha blending [1].", "skimage.color.rgbcie2rgb(rgbcie)", "RGB CIE to RGB color space conversion.", "skimage.color.separate_stains(rgb, conv_matrix)", "RGB to stain color space conversion.", "skimage.color.xyz2lab(xyz[, illuminant, \u2026])", "XYZ to CIE-LAB color space conversion.", "skimage.color.xyz2rgb(xyz)", "XYZ to RGB color space conversion.", "skimage.color.ycbcr2rgb(ycbcr)", "YCbCr to RGB color space conversion.", "skimage.color.ydbdr2rgb(ydbdr)", "YDbDr to RGB color space conversion.", "skimage.color.yiq2rgb(yiq)", "YIQ to RGB color space conversion.", "skimage.color.ypbpr2rgb(ypbpr)", "YPbPr to RGB color space conversion.", "skimage.color.yuv2rgb(yuv)", "YUV to RGB color space conversion.", "Stain to RGB color space conversion.", "The image in stain color space. Final dimension denotes channels.", "The stain separation matrix as described by G. Landini [1].", "The image in RGB format. Same dimensions as input.", "If stains is not at least 2-D with shape (\u2026, 3).", "Stain combination matrices available in the color module and their respective colorspace:", "https://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.", "Convert an image array to a new color space.", "\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019", "The image to convert. Final dimension denotes channels.", "The color space to convert from. Can be specified in lower case.", "The color space to convert to. Can be specified in lower case.", "The converted image. Same dimensions as input.", "If fromspace is not a valid color space", "If tospace is not a valid color space", "Conversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion from XYZ to HSV is implemented as XYZ -> RGB -> HSV instead of directly.", "Euclidean distance between two points in Lab color space", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "distance between colors lab1 and lab2", "https://en.wikipedia.org/wiki/Color_difference", "A. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2, 7-11 (1977).", "Color difference as given by the CIEDE 2000 standard.", "CIEDE 2000 is a major revision of CIDE94. The perceptual calibration is largely based on experience with automotive paint on smooth surfaces.", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "lightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see deltaE_cmc", "chroma scale factor, usually 1", "hue scale factor, usually 1", "The distance between lab1 and lab2", "CIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and hue (kL, kC, kH respectively). These default to 1.", "https://en.wikipedia.org/wiki/Color_difference", "http://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf DOI:10.1364/AO.33.008069", "M. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics tested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33, 8069-8077 (1994).", "Color difference according to CIEDE 94 standard", "Accommodates perceptual non-uniformities through the use of application specific scale factors (kH, kC, kL, k1, and k2).", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "Hue scale", "Chroma scale", "Lightness scale", "first scale parameter", "second scale parameter", "color difference between lab1 and lab2", "deltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently, the first color should be regarded as the \u201creference\u201d color.", "kL, k1, k2 depend on the application and default to the values suggested for graphic arts", "Parameter", "Graphic Arts", "Textiles", "kL", "1.000", "2.000", "k1", "0.045", "0.048", "k2", "0.015", "0.014", "https://en.wikipedia.org/wiki/Color_difference", "http://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html", "Color difference from the CMC l:c standard.", "This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry.", "The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "distance between colors lab1 and lab2", "deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1)", "https://en.wikipedia.org/wiki/Color_difference", "http://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html", "F. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984).", "Create an RGB representation of a gray-level image.", "Input image.", "Ensure that the output image has an alpha layer. If None, alpha layers are passed through but not created.", "RGB image. A new dimension of length 3 is added to input image.", "If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3).", "Tinting gray-scale images", "Create a RGBA representation of a gray-level image.", "Input image.", "Alpha channel of the output image. It may be a scalar or an array that can be broadcast to image. If not specified it is set to the maximum limit corresponding to the image dtype.", "RGBA image. A new dimension of length 4 is added to input image shape.", "Create an RGB representation of a gray-level image.", "Input image.", "Ensure that the output image has an alpha layer. If None, alpha layers are passed through but not created.", "RGB image. A new dimension of length 3 is added to input image.", "If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3).", "Haematoxylin-Eosin-DAB (HED) to RGB color space conversion.", "The image in the HED color space. Final dimension denotes channels.", "The image in RGB. Same dimensions as input.", "If hed is not at least 2-D with shape (\u2026, 3).", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.", "HSV to RGB color space conversion.", "The image in HSV format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If hsv is not at least 2-D with shape (\u2026, 3).", "Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1].", "https://en.wikipedia.org/wiki/HSL_and_HSV", "Tinting gray-scale images", "Flood Fill", "CIE-LAB to CIE-LCH color space conversion.", "LCH is the cylindrical representation of the LAB (Cartesian) colorspace", "The N-D image in CIE-LAB format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.", "The image in LCH format, in a N-D array with same shape as input lab.", "If lch does not have at least 3 color channels (i.e. l, a, b).", "The Hue is expressed as an angle between (0, 2*pi)", "Lab to RGB color space conversion.", "The image in Lab format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in RGB format. Same dimensions as input.", "If lab is not at least 2-D with shape (\u2026, 3).", "This function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants.", "https://en.wikipedia.org/wiki/Standard_illuminant", "CIE-LAB to XYZcolor space conversion.", "The image in Lab format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in XYZ format. Same dimensions as input.", "If lab is not at least 2-D with shape (\u2026, 3).", "If either the illuminant or the observer angle are not supported or unknown.", "If any of the pixels are invalid (Z < 0).", "By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a list of supported illuminants.", "http://www.easyrgb.com/index.php?X=MATH&H=07", "https://en.wikipedia.org/wiki/Lab_color_space", "Return an RGB image where color-coded labels are painted over the image.", "Integer array of labels with the same shape as image.", "Image used as underlay for labels. If the input is an RGB image, it\u2019s converted to grayscale before coloring.", "List of colors. If the number of labels exceeds the number of colors, then the colors are cycled.", "Opacity of colorized labels. Ignored if image is None.", "Label that\u2019s treated as the background. If bg_label is specified, bg_color is None, and kind is overlay, background is not painted by any colors.", "Background color. Must be a name in color_dict or RGB float values between [0, 1].", "Opacity of the image.", "The kind of color image desired. \u2018overlay\u2019 cycles over defined colors and overlays the colored labels over the original image. \u2018avg\u2019 replaces each labeled segment with its average color, for a stained-class or pastel painting appearance.", "The result of blending a cycling colormap (colors) for each distinct value in label with the image, at a certain alpha value.", "Segment human cells (in mitosis)", "CIE-LCH to CIE-LAB color space conversion.", "LCH is the cylindrical representation of the LAB (Cartesian) colorspace", "The N-D image in CIE-LCH format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.", "The image in LAB format, with same shape as input lch.", "If lch does not have at least 3 color channels (i.e. l, c, h).", "Compute luminance of an RGB image.", "The image in RGB format. Final dimension denotes channels.", "The luminance image - an array which is the same size as the input array, but with the channel dimension removed.", "If rgb is not at least 2-D with shape (\u2026, 3).", "The weights used in this conversion are calibrated for contemporary CRT phosphors:", "If there is an alpha channel present, it is ignored.", "http://poynton.ca/PDFs/ColorFAQ.pdf", "Registration using optical flow", "Phase Unwrapping", "Compute luminance of an RGB image.", "The image in RGB format. Final dimension denotes channels.", "The luminance image - an array which is the same size as the input array, but with the channel dimension removed.", "If rgb is not at least 2-D with shape (\u2026, 3).", "The weights used in this conversion are calibrated for contemporary CRT phosphors:", "If there is an alpha channel present, it is ignored.", "http://poynton.ca/PDFs/ColorFAQ.pdf", "RGB to Haematoxylin-Eosin-DAB (HED) color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in HED format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001.", "RGB to HSV color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in HSV format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1].", "https://en.wikipedia.org/wiki/HSL_and_HSV", "Tinting gray-scale images", "Flood Fill", "Conversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.", "The image in RGB format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in Lab format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "RGB is a device-dependent color space so, if you use this function, be sure that the image you are analyzing has been mapped to the sRGB color space.", "This function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants.", "https://en.wikipedia.org/wiki/Standard_illuminant", "RGB to RGB CIE color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in RGB CIE format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/CIE_1931_color_space", "RGB to XYZ color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in XYZ format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts from sRGB.", "https://en.wikipedia.org/wiki/CIE_1931_color_space", "RGB to YCbCr color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YCbCr format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d.", "https://en.wikipedia.org/wiki/YCbCr", "RGB to YDbDr color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YDbDr format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "This is the color space commonly used by video codecs. It is also the reversible color transform in JPEG2000.", "https://en.wikipedia.org/wiki/YDbDr", "RGB to YIQ color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YIQ format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "RGB to YPbPr color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YPbPr format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/YPbPr", "RGB to YUV color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YUV format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Y is between 0 and 1. Use YCbCr instead of YUV for the color space commonly used by video codecs, where Y ranges from 16 to 235.", "https://en.wikipedia.org/wiki/YUV", "RGBA to RGB conversion using alpha blending [1].", "The image in RGBA format. Final dimension denotes channels.", "The color of the background to blend the image with (3 floats between 0 to 1 - the RGB value of the background).", "The image in RGB format. Same dimensions as input.", "If rgba is not at least 2-D with shape (\u2026, 4).", "https://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending", "RGB CIE to RGB color space conversion.", "The image in RGB CIE format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If rgbcie is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/CIE_1931_color_space", "RGB to stain color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The stain separation matrix as described by G. Landini [1].", "The image in stain color space. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Stain separation matrices available in the color module and their respective colorspace:", "This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law.", "https://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html", "https://github.com/DIPlib/diplib/", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001.", "XYZ to CIE-LAB color space conversion.", "The image in XYZ format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in CIE-LAB format. Same dimensions as input.", "If xyz is not at least 2-D with shape (\u2026, 3).", "If either the illuminant or the observer angle is unsupported or unknown.", "By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants.", "http://www.easyrgb.com/index.php?X=MATH&H=07", "https://en.wikipedia.org/wiki/Lab_color_space", "XYZ to RGB color space conversion.", "The image in XYZ format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If xyz is not at least 2-D with shape (\u2026, 3).", "The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts to sRGB.", "https://en.wikipedia.org/wiki/CIE_1931_color_space", "YCbCr to RGB color space conversion.", "The image in YCbCr format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If ycbcr is not at least 2-D with shape (\u2026, 3).", "Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d.", "https://en.wikipedia.org/wiki/YCbCr", "YDbDr to RGB color space conversion.", "The image in YDbDr format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If ydbdr is not at least 2-D with shape (\u2026, 3).", "This is the color space commonly used by video codecs, also called the reversible color transform in JPEG2000.", "https://en.wikipedia.org/wiki/YDbDr", "YIQ to RGB color space conversion.", "The image in YIQ format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If yiq is not at least 2-D with shape (\u2026, 3).", "YPbPr to RGB color space conversion.", "The image in YPbPr format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If ypbpr is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/YPbPr", "YUV to RGB color space conversion.", "The image in YUV format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If yuv is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/YUV"]}, {"name": "color.combine_stains()", "path": "api/skimage.color#skimage.color.combine_stains", "type": "color", "text": ["Stain to RGB color space conversion.", "The image in stain color space. Final dimension denotes channels.", "The stain separation matrix as described by G. Landini [1].", "The image in RGB format. Same dimensions as input.", "If stains is not at least 2-D with shape (\u2026, 3).", "Stain combination matrices available in the color module and their respective colorspace:", "https://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001."]}, {"name": "color.convert_colorspace()", "path": "api/skimage.color#skimage.color.convert_colorspace", "type": "color", "text": ["Convert an image array to a new color space.", "\u2018RGB\u2019, \u2018HSV\u2019, \u2018RGB CIE\u2019, \u2018XYZ\u2019, \u2018YUV\u2019, \u2018YIQ\u2019, \u2018YPbPr\u2019, \u2018YCbCr\u2019, \u2018YDbDr\u2019", "The image to convert. Final dimension denotes channels.", "The color space to convert from. Can be specified in lower case.", "The color space to convert to. Can be specified in lower case.", "The converted image. Same dimensions as input.", "If fromspace is not a valid color space", "If tospace is not a valid color space", "Conversion is performed through the \u201ccentral\u201d RGB color space, i.e. conversion from XYZ to HSV is implemented as XYZ -> RGB -> HSV instead of directly."]}, {"name": "color.deltaE_cie76()", "path": "api/skimage.color#skimage.color.deltaE_cie76", "type": "color", "text": ["Euclidean distance between two points in Lab color space", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "distance between colors lab1 and lab2", "https://en.wikipedia.org/wiki/Color_difference", "A. R. Robertson, \u201cThe CIE 1976 color-difference formulae,\u201d Color Res. Appl. 2, 7-11 (1977)."]}, {"name": "color.deltaE_ciede2000()", "path": "api/skimage.color#skimage.color.deltaE_ciede2000", "type": "color", "text": ["Color difference as given by the CIEDE 2000 standard.", "CIEDE 2000 is a major revision of CIDE94. The perceptual calibration is largely based on experience with automotive paint on smooth surfaces.", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "lightness scale factor, 1 for \u201cacceptably close\u201d; 2 for \u201cimperceptible\u201d see deltaE_cmc", "chroma scale factor, usually 1", "hue scale factor, usually 1", "The distance between lab1 and lab2", "CIEDE 2000 assumes parametric weighting factors for the lightness, chroma, and hue (kL, kC, kH respectively). These default to 1.", "https://en.wikipedia.org/wiki/Color_difference", "http://www.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf DOI:10.1364/AO.33.008069", "M. Melgosa, J. Quesada, and E. Hita, \u201cUniformity of some recent color metrics tested with an accurate color-difference tolerance dataset,\u201d Appl. Opt. 33, 8069-8077 (1994)."]}, {"name": "color.deltaE_ciede94()", "path": "api/skimage.color#skimage.color.deltaE_ciede94", "type": "color", "text": ["Color difference according to CIEDE 94 standard", "Accommodates perceptual non-uniformities through the use of application specific scale factors (kH, kC, kL, k1, and k2).", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "Hue scale", "Chroma scale", "Lightness scale", "first scale parameter", "second scale parameter", "color difference between lab1 and lab2", "deltaE_ciede94 is not symmetric with respect to lab1 and lab2. CIEDE94 defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently, the first color should be regarded as the \u201creference\u201d color.", "kL, k1, k2 depend on the application and default to the values suggested for graphic arts", "Parameter", "Graphic Arts", "Textiles", "kL", "1.000", "2.000", "k1", "0.045", "0.048", "k2", "0.015", "0.014", "https://en.wikipedia.org/wiki/Color_difference", "http://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html"]}, {"name": "color.deltaE_cmc()", "path": "api/skimage.color#skimage.color.deltaE_cmc", "type": "color", "text": ["Color difference from the CMC l:c standard.", "This color difference was developed by the Colour Measurement Committee (CMC) of the Society of Dyers and Colourists (United Kingdom). It is intended for use in the textile industry.", "The scale factors kL, kC set the weight given to differences in lightness and chroma relative to differences in hue. The usual values are kL=2, kC=1 for \u201cacceptability\u201d and kL=1, kC=1 for \u201cimperceptibility\u201d. Colors with dE > 1 are \u201cdifferent\u201d for the given scale factors.", "reference color (Lab colorspace)", "comparison color (Lab colorspace)", "distance between colors lab1 and lab2", "deltaE_cmc the defines the scales for the lightness, hue, and chroma in terms of the first color. Consequently deltaE_cmc(lab1, lab2) != deltaE_cmc(lab2, lab1)", "https://en.wikipedia.org/wiki/Color_difference", "http://www.brucelindbloom.com/index.html?Eqn_DeltaE_CIE94.html", "F. J. J. Clarke, R. McDonald, and B. Rigg, \u201cModification to the JPC79 colour-difference formula,\u201d J. Soc. Dyers Colour. 100, 128-132 (1984)."]}, {"name": "color.gray2rgb()", "path": "api/skimage.color#skimage.color.gray2rgb", "type": "color", "text": ["Create an RGB representation of a gray-level image.", "Input image.", "Ensure that the output image has an alpha layer. If None, alpha layers are passed through but not created.", "RGB image. A new dimension of length 3 is added to input image.", "If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3)."]}, {"name": "color.gray2rgba()", "path": "api/skimage.color#skimage.color.gray2rgba", "type": "color", "text": ["Create a RGBA representation of a gray-level image.", "Input image.", "Alpha channel of the output image. It may be a scalar or an array that can be broadcast to image. If not specified it is set to the maximum limit corresponding to the image dtype.", "RGBA image. A new dimension of length 4 is added to input image shape."]}, {"name": "color.grey2rgb()", "path": "api/skimage.color#skimage.color.grey2rgb", "type": "color", "text": ["Create an RGB representation of a gray-level image.", "Input image.", "Ensure that the output image has an alpha layer. If None, alpha layers are passed through but not created.", "RGB image. A new dimension of length 3 is added to input image.", "If the input is a 1-dimensional image of shape (M, ), the output will be shape (M, 3)."]}, {"name": "color.hed2rgb()", "path": "api/skimage.color#skimage.color.hed2rgb", "type": "color", "text": ["Haematoxylin-Eosin-DAB (HED) to RGB color space conversion.", "The image in the HED color space. Final dimension denotes channels.", "The image in RGB. Same dimensions as input.", "If hed is not at least 2-D with shape (\u2026, 3).", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001."]}, {"name": "color.hsv2rgb()", "path": "api/skimage.color#skimage.color.hsv2rgb", "type": "color", "text": ["HSV to RGB color space conversion.", "The image in HSV format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If hsv is not at least 2-D with shape (\u2026, 3).", "Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1].", "https://en.wikipedia.org/wiki/HSL_and_HSV"]}, {"name": "color.lab2lch()", "path": "api/skimage.color#skimage.color.lab2lch", "type": "color", "text": ["CIE-LAB to CIE-LCH color space conversion.", "LCH is the cylindrical representation of the LAB (Cartesian) colorspace", "The N-D image in CIE-LAB format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.", "The image in LCH format, in a N-D array with same shape as input lab.", "If lch does not have at least 3 color channels (i.e. l, a, b).", "The Hue is expressed as an angle between (0, 2*pi)"]}, {"name": "color.lab2rgb()", "path": "api/skimage.color#skimage.color.lab2rgb", "type": "color", "text": ["Lab to RGB color space conversion.", "The image in Lab format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in RGB format. Same dimensions as input.", "If lab is not at least 2-D with shape (\u2026, 3).", "This function uses lab2xyz and xyz2rgb. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants.", "https://en.wikipedia.org/wiki/Standard_illuminant"]}, {"name": "color.lab2xyz()", "path": "api/skimage.color#skimage.color.lab2xyz", "type": "color", "text": ["CIE-LAB to XYZcolor space conversion.", "The image in Lab format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in XYZ format. Same dimensions as input.", "If lab is not at least 2-D with shape (\u2026, 3).", "If either the illuminant or the observer angle are not supported or unknown.", "If any of the pixels are invalid (Z < 0).", "By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref = 95.047, y_ref = 100., z_ref = 108.883. See function \u2018get_xyz_coords\u2019 for a list of supported illuminants.", "http://www.easyrgb.com/index.php?X=MATH&H=07", "https://en.wikipedia.org/wiki/Lab_color_space"]}, {"name": "color.label2rgb()", "path": "api/skimage.color#skimage.color.label2rgb", "type": "color", "text": ["Return an RGB image where color-coded labels are painted over the image.", "Integer array of labels with the same shape as image.", "Image used as underlay for labels. If the input is an RGB image, it\u2019s converted to grayscale before coloring.", "List of colors. If the number of labels exceeds the number of colors, then the colors are cycled.", "Opacity of colorized labels. Ignored if image is None.", "Label that\u2019s treated as the background. If bg_label is specified, bg_color is None, and kind is overlay, background is not painted by any colors.", "Background color. Must be a name in color_dict or RGB float values between [0, 1].", "Opacity of the image.", "The kind of color image desired. \u2018overlay\u2019 cycles over defined colors and overlays the colored labels over the original image. \u2018avg\u2019 replaces each labeled segment with its average color, for a stained-class or pastel painting appearance.", "The result of blending a cycling colormap (colors) for each distinct value in label with the image, at a certain alpha value."]}, {"name": "color.lch2lab()", "path": "api/skimage.color#skimage.color.lch2lab", "type": "color", "text": ["CIE-LCH to CIE-LAB color space conversion.", "LCH is the cylindrical representation of the LAB (Cartesian) colorspace", "The N-D image in CIE-LCH format. The last (N+1-th) dimension must have at least 3 elements, corresponding to the L, a, and b color channels. Subsequent elements are copied.", "The image in LAB format, with same shape as input lch.", "If lch does not have at least 3 color channels (i.e. l, c, h)."]}, {"name": "color.rgb2gray()", "path": "api/skimage.color#skimage.color.rgb2gray", "type": "color", "text": ["Compute luminance of an RGB image.", "The image in RGB format. Final dimension denotes channels.", "The luminance image - an array which is the same size as the input array, but with the channel dimension removed.", "If rgb is not at least 2-D with shape (\u2026, 3).", "The weights used in this conversion are calibrated for contemporary CRT phosphors:", "If there is an alpha channel present, it is ignored.", "http://poynton.ca/PDFs/ColorFAQ.pdf"]}, {"name": "color.rgb2grey()", "path": "api/skimage.color#skimage.color.rgb2grey", "type": "color", "text": ["Compute luminance of an RGB image.", "The image in RGB format. Final dimension denotes channels.", "The luminance image - an array which is the same size as the input array, but with the channel dimension removed.", "If rgb is not at least 2-D with shape (\u2026, 3).", "The weights used in this conversion are calibrated for contemporary CRT phosphors:", "If there is an alpha channel present, it is ignored.", "http://poynton.ca/PDFs/ColorFAQ.pdf"]}, {"name": "color.rgb2hed()", "path": "api/skimage.color#skimage.color.rgb2hed", "type": "color", "text": ["RGB to Haematoxylin-Eosin-DAB (HED) color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in HED format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution.,\u201d Analytical and quantitative cytology and histology / the International Academy of Cytology [and] American Society of Cytology, vol. 23, no. 4, pp. 291-9, Aug. 2001."]}, {"name": "color.rgb2hsv()", "path": "api/skimage.color#skimage.color.rgb2hsv", "type": "color", "text": ["RGB to HSV color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in HSV format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Conversion between RGB and HSV color spaces results in some loss of precision, due to integer arithmetic and rounding [1].", "https://en.wikipedia.org/wiki/HSL_and_HSV"]}, {"name": "color.rgb2lab()", "path": "api/skimage.color#skimage.color.rgb2lab", "type": "color", "text": ["Conversion from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer.", "The image in RGB format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in Lab format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "RGB is a device-dependent color space so, if you use this function, be sure that the image you are analyzing has been mapped to the sRGB color space.", "This function uses rgb2xyz and xyz2lab. By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants.", "https://en.wikipedia.org/wiki/Standard_illuminant"]}, {"name": "color.rgb2rgbcie()", "path": "api/skimage.color#skimage.color.rgb2rgbcie", "type": "color", "text": ["RGB to RGB CIE color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in RGB CIE format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/CIE_1931_color_space"]}, {"name": "color.rgb2xyz()", "path": "api/skimage.color#skimage.color.rgb2xyz", "type": "color", "text": ["RGB to XYZ color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in XYZ format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts from sRGB.", "https://en.wikipedia.org/wiki/CIE_1931_color_space"]}, {"name": "color.rgb2ycbcr()", "path": "api/skimage.color#skimage.color.rgb2ycbcr", "type": "color", "text": ["RGB to YCbCr color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YCbCr format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d.", "https://en.wikipedia.org/wiki/YCbCr"]}, {"name": "color.rgb2ydbdr()", "path": "api/skimage.color#skimage.color.rgb2ydbdr", "type": "color", "text": ["RGB to YDbDr color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YDbDr format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "This is the color space commonly used by video codecs. It is also the reversible color transform in JPEG2000.", "https://en.wikipedia.org/wiki/YDbDr"]}, {"name": "color.rgb2yiq()", "path": "api/skimage.color#skimage.color.rgb2yiq", "type": "color", "text": ["RGB to YIQ color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YIQ format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3)."]}, {"name": "color.rgb2ypbpr()", "path": "api/skimage.color#skimage.color.rgb2ypbpr", "type": "color", "text": ["RGB to YPbPr color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YPbPr format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/YPbPr"]}, {"name": "color.rgb2yuv()", "path": "api/skimage.color#skimage.color.rgb2yuv", "type": "color", "text": ["RGB to YUV color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The image in YUV format. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Y is between 0 and 1. Use YCbCr instead of YUV for the color space commonly used by video codecs, where Y ranges from 16 to 235.", "https://en.wikipedia.org/wiki/YUV"]}, {"name": "color.rgba2rgb()", "path": "api/skimage.color#skimage.color.rgba2rgb", "type": "color", "text": ["RGBA to RGB conversion using alpha blending [1].", "The image in RGBA format. Final dimension denotes channels.", "The color of the background to blend the image with (3 floats between 0 to 1 - the RGB value of the background).", "The image in RGB format. Same dimensions as input.", "If rgba is not at least 2-D with shape (\u2026, 4).", "https://en.wikipedia.org/wiki/Alpha_compositing#Alpha_blending"]}, {"name": "color.rgbcie2rgb()", "path": "api/skimage.color#skimage.color.rgbcie2rgb", "type": "color", "text": ["RGB CIE to RGB color space conversion.", "The image in RGB CIE format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If rgbcie is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/CIE_1931_color_space"]}, {"name": "color.separate_stains()", "path": "api/skimage.color#skimage.color.separate_stains", "type": "color", "text": ["RGB to stain color space conversion.", "The image in RGB format. Final dimension denotes channels.", "The stain separation matrix as described by G. Landini [1].", "The image in stain color space. Same dimensions as input.", "If rgb is not at least 2-D with shape (\u2026, 3).", "Stain separation matrices available in the color module and their respective colorspace:", "This implementation borrows some ideas from DIPlib [2], e.g. the compensation using a small value to avoid log artifacts when calculating the Beer-Lambert law.", "https://web.archive.org/web/20160624145052/http://www.mecourse.com/landinig/software/cdeconv/cdeconv.html", "https://github.com/DIPlib/diplib/", "A. C. Ruifrok and D. A. Johnston, \u201cQuantification of histochemical staining by color deconvolution,\u201d Anal. Quant. Cytol. Histol., vol. 23, no. 4, pp. 291\u2013299, Aug. 2001."]}, {"name": "color.xyz2lab()", "path": "api/skimage.color#skimage.color.xyz2lab", "type": "color", "text": ["XYZ to CIE-LAB color space conversion.", "The image in XYZ format. Final dimension denotes channels.", "The name of the illuminant (the function is NOT case sensitive).", "The aperture angle of the observer.", "The image in CIE-LAB format. Same dimensions as input.", "If xyz is not at least 2-D with shape (\u2026, 3).", "If either the illuminant or the observer angle is unsupported or unknown.", "By default Observer= 2A, Illuminant= D65. CIE XYZ tristimulus values x_ref=95.047, y_ref=100., z_ref=108.883. See function get_xyz_coords for a list of supported illuminants.", "http://www.easyrgb.com/index.php?X=MATH&H=07", "https://en.wikipedia.org/wiki/Lab_color_space"]}, {"name": "color.xyz2rgb()", "path": "api/skimage.color#skimage.color.xyz2rgb", "type": "color", "text": ["XYZ to RGB color space conversion.", "The image in XYZ format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If xyz is not at least 2-D with shape (\u2026, 3).", "The CIE XYZ color space is derived from the CIE RGB color space. Note however that this function converts to sRGB.", "https://en.wikipedia.org/wiki/CIE_1931_color_space"]}, {"name": "color.ycbcr2rgb()", "path": "api/skimage.color#skimage.color.ycbcr2rgb", "type": "color", "text": ["YCbCr to RGB color space conversion.", "The image in YCbCr format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If ycbcr is not at least 2-D with shape (\u2026, 3).", "Y is between 16 and 235. This is the color space commonly used by video codecs; it is sometimes incorrectly called \u201cYUV\u201d.", "https://en.wikipedia.org/wiki/YCbCr"]}, {"name": "color.ydbdr2rgb()", "path": "api/skimage.color#skimage.color.ydbdr2rgb", "type": "color", "text": ["YDbDr to RGB color space conversion.", "The image in YDbDr format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If ydbdr is not at least 2-D with shape (\u2026, 3).", "This is the color space commonly used by video codecs, also called the reversible color transform in JPEG2000.", "https://en.wikipedia.org/wiki/YDbDr"]}, {"name": "color.yiq2rgb()", "path": "api/skimage.color#skimage.color.yiq2rgb", "type": "color", "text": ["YIQ to RGB color space conversion.", "The image in YIQ format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If yiq is not at least 2-D with shape (\u2026, 3)."]}, {"name": "color.ypbpr2rgb()", "path": "api/skimage.color#skimage.color.ypbpr2rgb", "type": "color", "text": ["YPbPr to RGB color space conversion.", "The image in YPbPr format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If ypbpr is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/YPbPr"]}, {"name": "color.yuv2rgb()", "path": "api/skimage.color#skimage.color.yuv2rgb", "type": "color", "text": ["YUV to RGB color space conversion.", "The image in YUV format. Final dimension denotes channels.", "The image in RGB format. Same dimensions as input.", "If yuv is not at least 2-D with shape (\u2026, 3).", "https://en.wikipedia.org/wiki/YUV"]}, {"name": "data", "path": "api/skimage.data", "type": "data", "text": ["Standard test images.", "For more images, see", "skimage.data.astronaut()", "Color image of the astronaut Eileen Collins.", "skimage.data.binary_blobs([length, \u2026])", "Generate synthetic binary image with several rounded blob-like objects.", "skimage.data.brain()", "Subset of data from the University of North Carolina Volume Rendering Test Data Set.", "skimage.data.brick()", "Brick wall.", "skimage.data.camera()", "Gray-level \u201ccamera\u201d image.", "skimage.data.cat()", "Chelsea the cat.", "skimage.data.cell()", "Cell floating in saline.", "skimage.data.cells3d()", "3D fluorescence microscopy image of cells.", "skimage.data.checkerboard()", "Checkerboard image.", "skimage.data.chelsea()", "Chelsea the cat.", "skimage.data.clock()", "Motion blurred clock.", "skimage.data.coffee()", "Coffee cup.", "skimage.data.coins()", "Greek coins from Pompeii.", "skimage.data.colorwheel()", "Color Wheel.", "skimage.data.download_all([directory])", "Download all datasets for use with scikit-image offline.", "skimage.data.eagle()", "A golden eagle.", "skimage.data.grass()", "Grass.", "skimage.data.gravel()", "Gravel", "skimage.data.horse()", "Black and white silhouette of a horse.", "skimage.data.hubble_deep_field()", "Hubble eXtreme Deep Field.", "skimage.data.human_mitosis()", "Image of human cells undergoing mitosis.", "skimage.data.immunohistochemistry()", "Immunohistochemical (IHC) staining with hematoxylin counterstaining.", "skimage.data.kidney()", "Mouse kidney tissue.", "skimage.data.lbp_frontal_face_cascade_filename()", "Return the path to the XML file containing the weak classifier cascade.", "skimage.data.lfw_subset()", "Subset of data from the LFW dataset.", "skimage.data.lily()", "Lily of the valley plant stem.", "skimage.data.logo()", "Scikit-image logo, a RGBA image.", "skimage.data.microaneurysms()", "Gray-level \u201cmicroaneurysms\u201d image.", "skimage.data.moon()", "Surface of the moon.", "skimage.data.page()", "Scanned page.", "skimage.data.retina()", "Human retina.", "skimage.data.rocket()", "Launch photo of DSCOVR on Falcon 9 by SpaceX.", "skimage.data.shepp_logan_phantom()", "Shepp Logan Phantom.", "skimage.data.skin()", "Microscopy image of dermis and epidermis (skin layers).", "skimage.data.stereo_motorcycle()", "Rectified stereo image pair with ground-truth disparities.", "skimage.data.text()", "Gray-level \u201ctext\u201d image used for corner detection.", "Color image of the astronaut Eileen Collins.", "Photograph of Eileen Collins, an American astronaut. She was selected as an astronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She retired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in outer space.", "This image was downloaded from the NASA Great Images database <https://flic.kr/p/r9qvLn>`__.", "No known copyright restrictions, released into the public domain.", "Astronaut image.", "Flood Fill", "Generate synthetic binary image with several rounded blob-like objects.", "Linear size of output image.", "Typical linear size of blob, as a fraction of length, should be smaller than 1.", "Number of dimensions of output image.", "Fraction of image pixels covered by the blobs (where the output is 1). Should be in [0, 1].", "Seed to initialize the random number generator. If None, a random seed from the operating system is used.", "Output binary image", "Subset of data from the University of North Carolina Volume Rendering Test Data Set.", "The full dataset is available at [1].", "The 3D volume consists of 10 layers from the larger volume.", "https://graphics.stanford.edu/data/voldata/", "Local Histogram Equalization", "Rank filters", "Brick wall.", "A small section of a brick wall.", "The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License.", "A perspective transform was then applied to the image, prior to rotating it by 90 degrees, cropping and scaling it to obtain the final image.", "Gray-level \u201ccamera\u201d image.", "Can be used for segmentation and denoising examples.", "Camera image.", "No copyright restrictions. CC0 by the photographer (Lav Varshney).", "Changed in version 0.18: This image was replaced due to copyright restrictions. For more information, please see [1].", "https://github.com/scikit-image/scikit-image/issues/3927", "Tinting gray-scale images", "Masked Normalized Cross-Correlation", "Entropy", "GLCM Texture Features", "Multi-Otsu Thresholding", "Flood Fill", "Rank filters", "Chelsea the cat.", "An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.", "Chelsea image.", "No copyright restrictions. CC0 by the photographer (Stefan van der Walt).", "Cell floating in saline.", "This is a quantitative phase image retrieved from a digital hologram using the Python library qpformat. The image shows a cell with high phase value, above the background phase.", "Because of a banding pattern artifact in the background, this image is a good test of thresholding algorithms. The pixel spacing is 0.107 \u00b5m.", "These data were part of a comparison between several refractive index retrieval techniques for spherical objects as part of [1].", "This image is CC0, dedicated to the public domain. You may copy, modify, or distribute it without asking permission.", "Image of a cell.", "Paul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen Guck. \u201cAccurate evaluation of size and refractive index for spherical objects in quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018). DOI:10.1364/OE.26.010729", "3D fluorescence microscopy image of cells.", "The returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.", "The volumetric images of cells taken with an optical microscope.", "The data for this was provided by the Allen Institute for Cell Science.", "It has been downsampled by a factor of 4 in the row and column dimensions to reduce computational time.", "The microscope reports the following voxel spacing in microns:", "3D adaptive histogram equalization", "Use rolling-ball algorithm for estimating background intensity", "Explore 3D images (of cells)", "Checkerboard image.", "Checkerboards are often used in image calibration, since the corner-points are easy to locate. Because of the many parallel edges, they also visualise distortions particularly well.", "Checkerboard image.", "Flood Fill", "Chelsea the cat.", "An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.", "Chelsea image.", "No copyright restrictions. CC0 by the photographer (Stefan van der Walt).", "Phase Unwrapping", "Flood Fill", "Motion blurred clock.", "This photograph of a wall clock was taken while moving the camera in an aproximately horizontal direction. It may be used to illustrate inverse filters and deconvolution.", "Released into the public domain by the photographer (Stefan van der Walt).", "Clock image.", "Coffee cup.", "This photograph is courtesy of Pikolo Espresso Bar. It contains several elliptical shapes as well as varying texture (smooth porcelain to course wood grain).", "Coffee image.", "No copyright restrictions. CC0 by the photographer (Rachel Michetti).", "Greek coins from Pompeii.", "This image shows several coins outlined against a gray background. It is especially useful in, e.g. segmentation tests, where individual objects need to be identified against a background. The background shares enough grey levels with the coins that a simple segmentation is not sufficient.", "Coins image.", "This image was downloaded from the Brooklyn Museum Collection.", "No known copyright restrictions.", "Finding local maxima", "Measure region properties", "Use rolling-ball algorithm for estimating background intensity", "Color Wheel.", "A colorwheel.", "Download all datasets for use with scikit-image offline.", "Scikit-image datasets are no longer shipped with the library by default. This allows us to use higher quality datasets, while keeping the library download size small.", "This function requires the installation of an optional dependency, pooch, to download the full dataset. Follow installation instruction found at", "https://scikit-image.org/docs/stable/install.html", "Call this function to download all sample images making them available offline on your machine.", "The directory where the dataset should be stored.", "If pooch is not install, this error will be raised.", "scikit-image will only search for images stored in the default directory. Only specify the directory if you wish to download the images to your own folder for a particular reason. You can access the location of the default data directory by inspecting the variable skimage.data.data_dir.", "A golden eagle.", "Suitable for examples on segmentation, Hough transforms, and corner detection.", "Eagle image.", "No copyright restrictions. CC0 by the photographer (Dayane Machado).", "Markers for watershed transform", "Grass.", "Some grass.", "The original image was downloaded from DeviantArt and licensed underthe Creative Commons CC0 License.", "The downloaded image was cropped to include a region of (512, 512) pixels around the top left corner, converted to grayscale, then to uint8 prior to saving the result in PNG format.", "Gravel", "Grayscale gravel sample.", "The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License.", "The downloaded image was then rescaled to (1024, 1024), then the top left (512, 512) pixel region was cropped prior to converting the image to grayscale and uint8 data type. The result was saved using the PNG format.", "Black and white silhouette of a horse.", "This image was downloaded from openclipart", "No copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).", "Horse image.", "Hubble eXtreme Deep Field.", "This photograph contains the Hubble Telescope\u2019s farthest ever view of the universe. It can be useful as an example for multi-scale detection.", "Hubble deep field image.", "This image was downloaded from HubbleSite.", "The image was captured by NASA and may be freely used in the public domain.", "Image of human cells undergoing mitosis.", "Data of human cells undergoing mitosis taken during the preperation of the manuscript in [1].", "Copyright David Root. Licensed under CC-0 [2].", "Moffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B, Eisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell BR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral RNAi library for human and mouse genes applied to an arrayed viral high-content screen. Cell, 124(6):1283-98 / :DOI: 10.1016/j.cell.2006.01.040 PMID 16564017", "GitHub licensing discussion https://github.com/CellProfiler/examples/issues/41", "Segment human cells (in mitosis)", "Immunohistochemical (IHC) staining with hematoxylin counterstaining.", "This picture shows colonic glands where the IHC expression of FHL2 protein is revealed with DAB. Hematoxylin counterstaining is applied to enhance the negative parts of the tissue.", "This image was acquired at the Center for Microscopy And Molecular Imaging (CMMI).", "No known copyright restrictions.", "Immunohistochemistry image.", "Mouse kidney tissue.", "This biological tissue on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16, 512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color channels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-space voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is unsigned 16-bit integers.", "Kidney 3D multichannel image.", "This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0", "Return the path to the XML file containing the weak classifier cascade.", "These classifiers were trained using LBP features. The file is part of the OpenCV repository [1].", "OpenCV lbpcascade trained files https://github.com/opencv/opencv/tree/master/data/lbpcascades", "Subset of data from the LFW dataset.", "This database is a subset of the LFW database containing:", "The full dataset is available at [2].", "100 first images are faces and subsequent 100 are non-faces.", "The faces were randomly selected from the LFW dataset and the non-faces were extracted from the background of the same dataset. The cropped ROIs have been resized to a 25 x 25 pixels.", "Huang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to align from scratch. In Advances in Neural Information Processing Systems (pp. 764-772).", "http://vis-www.cs.umass.edu/lfw/", "Specific images", "Lily of the valley plant stem.", "This plant stem on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That is 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24 microns in X-Y. Data type is unsigned 16-bit integers.", "Lily 2D multichannel image.", "This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0", "Scikit-image logo, a RGBA image.", "Logo image.", "Gray-level \u201cmicroaneurysms\u201d image.", "Detail from an image of the retina (green channel). The image is a crop of image 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database: https://www5.cs.fau.de/research/data/fundus-images/", "Retina image with lesions.", "No copyright restrictions. CC0 given by owner (Andreas Maier).", "Budai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust Vessel Segmentation in Fundus Images. International Journal of Biomedical Imaging, vol. 2013, 2013. DOI:10.1155/2013/154860", "Surface of the moon.", "This low-contrast image of the surface of the moon is useful for illustrating histogram equalization and contrast stretching.", "Moon image.", "Local Histogram Equalization", "Scanned page.", "This image of printed text is useful for demonstrations requiring uneven background illumination.", "Page image.", "Use rolling-ball algorithm for estimating background intensity", "Rank filters", "Human retina.", "This image of a retina is useful for demonstrations requiring circular images.", "Retina image in RGB.", "This image was downloaded from wikimedia. This file is made available under the Creative Commons CC0 1.0 Universal Public Domain Dedication.", "H\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d. WikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436. Public Domain", "Launch photo of DSCOVR on Falcon 9 by SpaceX.", "This is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s Launch Complex 40 at Cape Canaveral Air Force Station, FL.", "Rocket image.", "This image was downloaded from SpaceX Photos.", "The image was captured by SpaceX and released in the public domain.", "Shepp Logan Phantom.", "Image of the Shepp-Logan phantom in grayscale.", "L. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d in IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974. DOI:10.1109/TNS.1974.6499235", "Microscopy image of dermis and epidermis (skin layers).", "Hematoxylin and eosin stained slide at 10x of normal epidermis and dermis with a benign intradermal nevus.", "This image requires an Internet connection the first time it is called, and to have the pooch package installed, in order to fetch the image file from the scikit-image datasets repository.", "The source of this image is https://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG", "The image was released in the public domain by its author Kilbad.", "Trainable segmentation using local features and random forests", "Rectified stereo image pair with ground-truth disparities.", "The two images are rectified such that every pixel in the left image has its corresponding pixel on the same scanline in the right image. That means that both images are warped such that they have the same orientation but a horizontal spatial offset (baseline). The ground-truth pixel offset in column direction is specified by the included disparity map.", "The two images are part of the Middlebury 2014 stereo benchmark. The dataset was created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg Krathwohl, and Daniel Scharstein at Middlebury College. A detailed description of the acquisition process can be found in [1].", "The images included here are down-sampled versions of the default exposure images in the benchmark. The images are down-sampled by a factor of 4 using the function skimage.transform.downscale_local_mean. The calibration data in the following and the included ground-truth disparity map are valid for the down-sampled images:", "Left stereo image.", "Right stereo image.", "Ground-truth disparity map, where each value describes the offset in column direction between corresponding pixels in the left and the right stereo images. E.g. the corresponding pixel of img_left[10, 10 + disp[10, 10]] is img_right[10, 10]. NaNs denote pixels in the left image that do not have ground-truth.", "The original resolution images, images with different exposure and lighting, and ground-truth depth maps can be found at the Middlebury website [2].", "D. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang, and P. Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German Conference on Pattern Recognition (GCPR 2014), Muenster, Germany, September 2014.", "http://vision.middlebury.edu/stereo/data/scenes2014/", "Specific images", "Registration using optical flow", "Gray-level \u201ctext\u201d image used for corner detection.", "Text image.", "This image was downloaded from Wikipedia <https://en.wikipedia.org/wiki/File:Corner.png>`__.", "No known copyright restrictions, released into the public domain."]}, {"name": "Data visualization", "path": "user_guide/visualization", "type": "Guide", "text": ["Data visualization takes an important place in image processing. Data can be a simple unique 2D image or a more complex with multidimensional aspects: 3D in space, timeslapse, multiple channels.", "Therefore, the visualization strategy will depend on the data complexity and a range of tools external to scikit-image can be used for this purpose. Historically, scikit-image provided viewer tools but powerful packages are now available and must be preferred.", "Matplotlib is a library able to generate static plots, which includes image visualization.", "Plotly is a plotting library relying on web technologies with interaction capabilities.", "Mayavi can be used to visualize 3D images.", "Napari is a multi-dimensional image viewer. It\u2019s designed for browsing, annotating, and analyzing large multi-dimensional images."]}, {"name": "data.astronaut()", "path": "api/skimage.data#skimage.data.astronaut", "type": "data", "text": ["Color image of the astronaut Eileen Collins.", "Photograph of Eileen Collins, an American astronaut. She was selected as an astronaut in 1992 and first piloted the space shuttle STS-63 in 1995. She retired in 2006 after spending a total of 38 days, 8 hours and 10 minutes in outer space.", "This image was downloaded from the NASA Great Images database <https://flic.kr/p/r9qvLn>`__.", "No known copyright restrictions, released into the public domain.", "Astronaut image."]}, {"name": "data.binary_blobs()", "path": "api/skimage.data#skimage.data.binary_blobs", "type": "data", "text": ["Generate synthetic binary image with several rounded blob-like objects.", "Linear size of output image.", "Typical linear size of blob, as a fraction of length, should be smaller than 1.", "Number of dimensions of output image.", "Fraction of image pixels covered by the blobs (where the output is 1). Should be in [0, 1].", "Seed to initialize the random number generator. If None, a random seed from the operating system is used.", "Output binary image"]}, {"name": "data.brain()", "path": "api/skimage.data#skimage.data.brain", "type": "data", "text": ["Subset of data from the University of North Carolina Volume Rendering Test Data Set.", "The full dataset is available at [1].", "The 3D volume consists of 10 layers from the larger volume.", "https://graphics.stanford.edu/data/voldata/"]}, {"name": "data.brick()", "path": "api/skimage.data#skimage.data.brick", "type": "data", "text": ["Brick wall.", "A small section of a brick wall.", "The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License.", "A perspective transform was then applied to the image, prior to rotating it by 90 degrees, cropping and scaling it to obtain the final image."]}, {"name": "data.camera()", "path": "api/skimage.data#skimage.data.camera", "type": "data", "text": ["Gray-level \u201ccamera\u201d image.", "Can be used for segmentation and denoising examples.", "Camera image.", "No copyright restrictions. CC0 by the photographer (Lav Varshney).", "Changed in version 0.18: This image was replaced due to copyright restrictions. For more information, please see [1].", "https://github.com/scikit-image/scikit-image/issues/3927"]}, {"name": "data.cat()", "path": "api/skimage.data#skimage.data.cat", "type": "data", "text": ["Chelsea the cat.", "An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.", "Chelsea image.", "No copyright restrictions. CC0 by the photographer (Stefan van der Walt)."]}, {"name": "data.cell()", "path": "api/skimage.data#skimage.data.cell", "type": "data", "text": ["Cell floating in saline.", "This is a quantitative phase image retrieved from a digital hologram using the Python library qpformat. The image shows a cell with high phase value, above the background phase.", "Because of a banding pattern artifact in the background, this image is a good test of thresholding algorithms. The pixel spacing is 0.107 \u00b5m.", "These data were part of a comparison between several refractive index retrieval techniques for spherical objects as part of [1].", "This image is CC0, dedicated to the public domain. You may copy, modify, or distribute it without asking permission.", "Image of a cell.", "Paul M\u00fcller, Mirjam Sch\u00fcrmann, Salvatore Girardo, Gheorghe Cojoc, and Jochen Guck. \u201cAccurate evaluation of size and refractive index for spherical objects in quantitative phase imaging.\u201d Optics Express 26(8): 10729-10743 (2018). DOI:10.1364/OE.26.010729"]}, {"name": "data.cells3d()", "path": "api/skimage.data#skimage.data.cells3d", "type": "data", "text": ["3D fluorescence microscopy image of cells.", "The returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.", "The volumetric images of cells taken with an optical microscope.", "The data for this was provided by the Allen Institute for Cell Science.", "It has been downsampled by a factor of 4 in the row and column dimensions to reduce computational time.", "The microscope reports the following voxel spacing in microns:"]}, {"name": "data.checkerboard()", "path": "api/skimage.data#skimage.data.checkerboard", "type": "data", "text": ["Checkerboard image.", "Checkerboards are often used in image calibration, since the corner-points are easy to locate. Because of the many parallel edges, they also visualise distortions particularly well.", "Checkerboard image."]}, {"name": "data.chelsea()", "path": "api/skimage.data#skimage.data.chelsea", "type": "data", "text": ["Chelsea the cat.", "An example with texture, prominent edges in horizontal and diagonal directions, as well as features of differing scales.", "Chelsea image.", "No copyright restrictions. CC0 by the photographer (Stefan van der Walt)."]}, {"name": "data.clock()", "path": "api/skimage.data#skimage.data.clock", "type": "data", "text": ["Motion blurred clock.", "This photograph of a wall clock was taken while moving the camera in an aproximately horizontal direction. It may be used to illustrate inverse filters and deconvolution.", "Released into the public domain by the photographer (Stefan van der Walt).", "Clock image."]}, {"name": "data.coffee()", "path": "api/skimage.data#skimage.data.coffee", "type": "data", "text": ["Coffee cup.", "This photograph is courtesy of Pikolo Espresso Bar. It contains several elliptical shapes as well as varying texture (smooth porcelain to course wood grain).", "Coffee image.", "No copyright restrictions. CC0 by the photographer (Rachel Michetti)."]}, {"name": "data.coins()", "path": "api/skimage.data#skimage.data.coins", "type": "data", "text": ["Greek coins from Pompeii.", "This image shows several coins outlined against a gray background. It is especially useful in, e.g. segmentation tests, where individual objects need to be identified against a background. The background shares enough grey levels with the coins that a simple segmentation is not sufficient.", "Coins image.", "This image was downloaded from the Brooklyn Museum Collection.", "No known copyright restrictions."]}, {"name": "data.colorwheel()", "path": "api/skimage.data#skimage.data.colorwheel", "type": "data", "text": ["Color Wheel.", "A colorwheel."]}, {"name": "data.download_all()", "path": "api/skimage.data#skimage.data.download_all", "type": "data", "text": ["Download all datasets for use with scikit-image offline.", "Scikit-image datasets are no longer shipped with the library by default. This allows us to use higher quality datasets, while keeping the library download size small.", "This function requires the installation of an optional dependency, pooch, to download the full dataset. Follow installation instruction found at", "https://scikit-image.org/docs/stable/install.html", "Call this function to download all sample images making them available offline on your machine.", "The directory where the dataset should be stored.", "If pooch is not install, this error will be raised.", "scikit-image will only search for images stored in the default directory. Only specify the directory if you wish to download the images to your own folder for a particular reason. You can access the location of the default data directory by inspecting the variable skimage.data.data_dir."]}, {"name": "data.eagle()", "path": "api/skimage.data#skimage.data.eagle", "type": "data", "text": ["A golden eagle.", "Suitable for examples on segmentation, Hough transforms, and corner detection.", "Eagle image.", "No copyright restrictions. CC0 by the photographer (Dayane Machado)."]}, {"name": "data.grass()", "path": "api/skimage.data#skimage.data.grass", "type": "data", "text": ["Grass.", "Some grass.", "The original image was downloaded from DeviantArt and licensed underthe Creative Commons CC0 License.", "The downloaded image was cropped to include a region of (512, 512) pixels around the top left corner, converted to grayscale, then to uint8 prior to saving the result in PNG format."]}, {"name": "data.gravel()", "path": "api/skimage.data#skimage.data.gravel", "type": "data", "text": ["Gravel", "Grayscale gravel sample.", "The original image was downloaded from CC0Textures and licensed under the Creative Commons CC0 License.", "The downloaded image was then rescaled to (1024, 1024), then the top left (512, 512) pixel region was cropped prior to converting the image to grayscale and uint8 data type. The result was saved using the PNG format."]}, {"name": "data.horse()", "path": "api/skimage.data#skimage.data.horse", "type": "data", "text": ["Black and white silhouette of a horse.", "This image was downloaded from openclipart", "No copyright restrictions. CC0 given by owner (Andreas Preuss (marauder)).", "Horse image."]}, {"name": "data.hubble_deep_field()", "path": "api/skimage.data#skimage.data.hubble_deep_field", "type": "data", "text": ["Hubble eXtreme Deep Field.", "This photograph contains the Hubble Telescope\u2019s farthest ever view of the universe. It can be useful as an example for multi-scale detection.", "Hubble deep field image.", "This image was downloaded from HubbleSite.", "The image was captured by NASA and may be freely used in the public domain."]}, {"name": "data.human_mitosis()", "path": "api/skimage.data#skimage.data.human_mitosis", "type": "data", "text": ["Image of human cells undergoing mitosis.", "Data of human cells undergoing mitosis taken during the preperation of the manuscript in [1].", "Copyright David Root. Licensed under CC-0 [2].", "Moffat J, Grueneberg DA, Yang X, Kim SY, Kloepfer AM, Hinkle G, Piqani B, Eisenhaure TM, Luo B, Grenier JK, Carpenter AE, Foo SY, Stewart SA, Stockwell BR, Hacohen N, Hahn WC, Lander ES, Sabatini DM, Root DE (2006) A lentiviral RNAi library for human and mouse genes applied to an arrayed viral high-content screen. Cell, 124(6):1283-98 / :DOI: 10.1016/j.cell.2006.01.040 PMID 16564017", "GitHub licensing discussion https://github.com/CellProfiler/examples/issues/41"]}, {"name": "data.immunohistochemistry()", "path": "api/skimage.data#skimage.data.immunohistochemistry", "type": "data", "text": ["Immunohistochemical (IHC) staining with hematoxylin counterstaining.", "This picture shows colonic glands where the IHC expression of FHL2 protein is revealed with DAB. Hematoxylin counterstaining is applied to enhance the negative parts of the tissue.", "This image was acquired at the Center for Microscopy And Molecular Imaging (CMMI).", "No known copyright restrictions.", "Immunohistochemistry image."]}, {"name": "data.kidney()", "path": "api/skimage.data#skimage.data.kidney", "type": "data", "text": ["Mouse kidney tissue.", "This biological tissue on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (16, 512, 512, 3). That is 512x512 pixels in X-Y, 16 image slices in Z, and 3 color channels (emission wavelengths 450nm, 515nm, and 605nm, respectively). Real-space voxel size is 1.24 microns in X-Y, and 1.25 microns in Z. Data type is unsigned 16-bit integers.", "Kidney 3D multichannel image.", "This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0"]}, {"name": "data.lbp_frontal_face_cascade_filename()", "path": "api/skimage.data#skimage.data.lbp_frontal_face_cascade_filename", "type": "data", "text": ["Return the path to the XML file containing the weak classifier cascade.", "These classifiers were trained using LBP features. The file is part of the OpenCV repository [1].", "OpenCV lbpcascade trained files https://github.com/opencv/opencv/tree/master/data/lbpcascades"]}, {"name": "data.lfw_subset()", "path": "api/skimage.data#skimage.data.lfw_subset", "type": "data", "text": ["Subset of data from the LFW dataset.", "This database is a subset of the LFW database containing:", "The full dataset is available at [2].", "100 first images are faces and subsequent 100 are non-faces.", "The faces were randomly selected from the LFW dataset and the non-faces were extracted from the background of the same dataset. The cropped ROIs have been resized to a 25 x 25 pixels.", "Huang, G., Mattar, M., Lee, H., & Learned-Miller, E. G. (2012). Learning to align from scratch. In Advances in Neural Information Processing Systems (pp. 764-772).", "http://vis-www.cs.umass.edu/lfw/"]}, {"name": "data.lily()", "path": "api/skimage.data#skimage.data.lily", "type": "data", "text": ["Lily of the valley plant stem.", "This plant stem on a pre-prepared slide was imaged with confocal fluorescence microscopy (Nikon C1 inverted microscope). Image shape is (922, 922, 4). That is 922x922 pixels in X-Y, with 4 color channels. Real-space voxel size is 1.24 microns in X-Y. Data type is unsigned 16-bit integers.", "Lily 2D multichannel image.", "This image was acquired by Genevieve Buckley at Monasoh Micro Imaging in 2018. License: CC0"]}, {"name": "data.logo()", "path": "api/skimage.data#skimage.data.logo", "type": "data", "text": ["Scikit-image logo, a RGBA image.", "Logo image."]}, {"name": "data.microaneurysms()", "path": "api/skimage.data#skimage.data.microaneurysms", "type": "data", "text": ["Gray-level \u201cmicroaneurysms\u201d image.", "Detail from an image of the retina (green channel). The image is a crop of image 07_dr.JPG from the High-Resolution Fundus (HRF) Image Database: https://www5.cs.fau.de/research/data/fundus-images/", "Retina image with lesions.", "No copyright restrictions. CC0 given by owner (Andreas Maier).", "Budai, A., Bock, R, Maier, A., Hornegger, J., Michelson, G. (2013). Robust Vessel Segmentation in Fundus Images. International Journal of Biomedical Imaging, vol. 2013, 2013. DOI:10.1155/2013/154860"]}, {"name": "data.moon()", "path": "api/skimage.data#skimage.data.moon", "type": "data", "text": ["Surface of the moon.", "This low-contrast image of the surface of the moon is useful for illustrating histogram equalization and contrast stretching.", "Moon image."]}, {"name": "data.page()", "path": "api/skimage.data#skimage.data.page", "type": "data", "text": ["Scanned page.", "This image of printed text is useful for demonstrations requiring uneven background illumination.", "Page image."]}, {"name": "data.retina()", "path": "api/skimage.data#skimage.data.retina", "type": "data", "text": ["Human retina.", "This image of a retina is useful for demonstrations requiring circular images.", "Retina image in RGB.", "This image was downloaded from wikimedia. This file is made available under the Creative Commons CC0 1.0 Universal Public Domain Dedication.", "H\u00e4ggstr\u00f6m, Mikael (2014). \u201cMedical gallery of Mikael H\u00e4ggstr\u00f6m 2014\u201d. WikiJournal of Medicine 1 (2). DOI:10.15347/wjm/2014.008. ISSN 2002-4436. Public Domain"]}, {"name": "data.rocket()", "path": "api/skimage.data#skimage.data.rocket", "type": "data", "text": ["Launch photo of DSCOVR on Falcon 9 by SpaceX.", "This is the launch photo of Falcon 9 carrying DSCOVR lifted off from SpaceX\u2019s Launch Complex 40 at Cape Canaveral Air Force Station, FL.", "Rocket image.", "This image was downloaded from SpaceX Photos.", "The image was captured by SpaceX and released in the public domain."]}, {"name": "data.shepp_logan_phantom()", "path": "api/skimage.data#skimage.data.shepp_logan_phantom", "type": "data", "text": ["Shepp Logan Phantom.", "Image of the Shepp-Logan phantom in grayscale.", "L. A. Shepp and B. F. Logan, \u201cThe Fourier reconstruction of a head section,\u201d in IEEE Transactions on Nuclear Science, vol. 21, no. 3, pp. 21-43, June 1974. DOI:10.1109/TNS.1974.6499235"]}, {"name": "data.skin()", "path": "api/skimage.data#skimage.data.skin", "type": "data", "text": ["Microscopy image of dermis and epidermis (skin layers).", "Hematoxylin and eosin stained slide at 10x of normal epidermis and dermis with a benign intradermal nevus.", "This image requires an Internet connection the first time it is called, and to have the pooch package installed, in order to fetch the image file from the scikit-image datasets repository.", "The source of this image is https://en.wikipedia.org/wiki/File:Normal_Epidermis_and_Dermis_with_Intradermal_Nevus_10x.JPG", "The image was released in the public domain by its author Kilbad."]}, {"name": "data.stereo_motorcycle()", "path": "api/skimage.data#skimage.data.stereo_motorcycle", "type": "data", "text": ["Rectified stereo image pair with ground-truth disparities.", "The two images are rectified such that every pixel in the left image has its corresponding pixel on the same scanline in the right image. That means that both images are warped such that they have the same orientation but a horizontal spatial offset (baseline). The ground-truth pixel offset in column direction is specified by the included disparity map.", "The two images are part of the Middlebury 2014 stereo benchmark. The dataset was created by Nera Nesic, Porter Westling, Xi Wang, York Kitajima, Greg Krathwohl, and Daniel Scharstein at Middlebury College. A detailed description of the acquisition process can be found in [1].", "The images included here are down-sampled versions of the default exposure images in the benchmark. The images are down-sampled by a factor of 4 using the function skimage.transform.downscale_local_mean. The calibration data in the following and the included ground-truth disparity map are valid for the down-sampled images:", "Left stereo image.", "Right stereo image.", "Ground-truth disparity map, where each value describes the offset in column direction between corresponding pixels in the left and the right stereo images. E.g. the corresponding pixel of img_left[10, 10 + disp[10, 10]] is img_right[10, 10]. NaNs denote pixels in the left image that do not have ground-truth.", "The original resolution images, images with different exposure and lighting, and ground-truth depth maps can be found at the Middlebury website [2].", "D. Scharstein, H. Hirschmueller, Y. Kitajima, G. Krathwohl, N. Nesic, X. Wang, and P. Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German Conference on Pattern Recognition (GCPR 2014), Muenster, Germany, September 2014.", "http://vision.middlebury.edu/stereo/data/scenes2014/"]}, {"name": "data.text()", "path": "api/skimage.data#skimage.data.text", "type": "data", "text": ["Gray-level \u201ctext\u201d image used for corner detection.", "Text image.", "This image was downloaded from Wikipedia <https://en.wikipedia.org/wiki/File:Corner.png>`__.", "No known copyright restrictions, released into the public domain."]}, {"name": "draw", "path": "api/skimage.draw", "type": "draw", "text": ["skimage.draw.bezier_curve(r0, c0, r1, c1, \u2026)", "Generate Bezier curve coordinates.", "skimage.draw.circle(r, c, radius[, shape])", "Generate coordinates of pixels within circle.", "skimage.draw.circle_perimeter(r, c, radius)", "Generate circle perimeter coordinates.", "skimage.draw.circle_perimeter_aa(r, c, radius)", "Generate anti-aliased circle perimeter coordinates.", "skimage.draw.disk(center, radius, *[, shape])", "Generate coordinates of pixels within circle.", "skimage.draw.ellipse(r, c, r_radius, c_radius)", "Generate coordinates of pixels within ellipse.", "skimage.draw.ellipse_perimeter(r, c, \u2026[, \u2026])", "Generate ellipse perimeter coordinates.", "skimage.draw.ellipsoid(a, b, c[, spacing, \u2026])", "Generates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.", "skimage.draw.ellipsoid_stats(a, b, c)", "Calculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.", "skimage.draw.line(r0, c0, r1, c1)", "Generate line pixel coordinates.", "skimage.draw.line_aa(r0, c0, r1, c1)", "Generate anti-aliased line pixel coordinates.", "skimage.draw.line_nd(start, stop, *[, \u2026])", "Draw a single-pixel thick line in n dimensions.", "skimage.draw.polygon(r, c[, shape])", "Generate coordinates of pixels within polygon.", "skimage.draw.polygon2mask(image_shape, polygon)", "Compute a mask from polygon.", "skimage.draw.polygon_perimeter(r, c[, \u2026])", "Generate polygon perimeter coordinates.", "skimage.draw.random_shapes(image_shape, \u2026)", "Generate an image with random shapes, labeled with bounding boxes.", "skimage.draw.rectangle(start[, end, extent, \u2026])", "Generate coordinates of pixels within a rectangle.", "skimage.draw.rectangle_perimeter(start[, \u2026])", "Generate coordinates of pixels that are exactly around a rectangle.", "skimage.draw.set_color(image, coords, color)", "Set pixel color in the image at the given coordinates.", "Generate Bezier curve coordinates.", "Coordinates of the first control point.", "Coordinates of the middle control point.", "Coordinates of the last control point.", "Middle control point weight, it describes the line tension.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for curves that exceed the image size. If None, the full extent of the curve is used.", "Indices of pixels that belong to the Bezier curve. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "The algorithm is the rational quadratic algorithm presented in reference [1].", "A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf", "Generate coordinates of pixels within circle.", "Center coordinate of disk.", "Radius of disk.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Pixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk instead.", "Generate circle perimeter coordinates.", "Centre coordinate of circle.", "Radius of circle.", "bresenham : Bresenham method (default) andres : Andres method", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Bresenham and Andres\u2019 method: Indices of pixels that belong to the circle perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Andres method presents the advantage that concentric circles create a disc whereas Bresenham can make holes. There is also less distortions when Andres circles are rotated. Bresenham method is also known as midpoint circle algorithm. Anti-aliased circle generator is available with circle_perimeter_aa.", "J.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM Systems journal, 4 (1965) 25-30.", "E. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18 (1994) 695-706.", "Generate anti-aliased circle perimeter coordinates.", "Centre coordinate of circle.", "Radius of circle.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Indices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.", "Wu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup table optimization.", "Use the function draw.set_color to apply circle_perimeter_aa results to color images.", "X. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer Graphics, 25 (1991) 143-152.", "Generate coordinates of pixels within circle.", "Center coordinate of disk.", "Radius of disk.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Pixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Generate coordinates of pixels within ellipse.", "Centre coordinate of ellipse.", "Minor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses which exceed the image size. By default the full extent of the ellipse are used. Must be at least length 2. Only the first two values are used to determine the extent.", "Set the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise direction, so PI/2 degree means swap ellipse axis", "Pixel coordinates of ellipse. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "The ellipse equation:", "Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1]", "Masked Normalized Cross-Correlation", "Measure region properties", "Generate ellipse perimeter coordinates.", "Centre coordinate of ellipse.", "Minor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.", "Major axis orientation in clockwise direction as radians.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses that exceed the image size. If None, the full extent of the ellipse is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Indices of pixels that belong to the ellipse perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf", "Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1]", "Generates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.", "Length of semimajor axis aligned with x-axis.", "Length of semimajor axis aligned with y-axis.", "Length of semimajor axis aligned with z-axis.", "Spacing in (x, y, z) spatial dimensions.", "If True, returns the level set for this ellipsoid (signed level set about zero, with positive denoting interior) as np.float64. False returns a binarized version of said level set.", "Ellipsoid centered in a correctly sized array for given spacing. Boolean dtype unless levelset=True, in which case a float array is returned with the level set above 0.0 representing the ellipsoid.", "Calculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.", "Length of semimajor axis aligned with x-axis.", "Length of semimajor axis aligned with y-axis.", "Length of semimajor axis aligned with z-axis.", "Calculated volume of ellipsoid.", "Calculated surface area of ellipsoid.", "Generate line pixel coordinates.", "Starting position (row, column).", "End position (row, column).", "Indices of pixels that belong to the line. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Anti-aliased line generator is available with line_aa.", "Generate anti-aliased line pixel coordinates.", "Starting position (row, column).", "End position (row, column).", "Indices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.", "A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf", "Draw a single-pixel thick line in n dimensions.", "The line produced will be ndim-connected. That is, two subsequent pixels in the line will be either direct or diagonal neighbours in n dimensions.", "The start coordinates of the line.", "The end coordinates of the line.", "Whether to include the endpoint in the returned line. Defaults to False, which allows for easy drawing of multi-point paths.", "Whether to round the coordinates to integer. If True (default), the returned coordinates can be used to directly index into an array. False could be used for e.g. vector drawing.", "The coordinates of points on the line.", "Generate coordinates of pixels within polygon.", "Row coordinates of vertices of polygon.", "Column coordinates of vertices of polygon.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extent of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Pixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Compute a mask from polygon.", "The shape of the mask.", "The polygon coordinates of shape (N, 2) where N is the number of points.", "The mask that corresponds to the input polygon.", "This function does not do any border checking, so that all the vertices need to be within the given shape.", "Generate polygon perimeter coordinates.", "Row coordinates of vertices of polygon.", "Column coordinates of vertices of polygon.", "Image shape which is used to determine maximum extents of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extents of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Whether to clip the polygon to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.", "Pixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Generate an image with random shapes, labeled with bounding boxes.", "The image is populated with random shapes with random sizes, random locations, and random colors, with or without overlap.", "Shapes have random (row, col) starting coordinates and random sizes bounded by min_size and max_size. It can occur that a randomly generated shape will not fit the image at all. In that case, the algorithm will try again with new starting coordinates a certain number of times. However, it also means that some shapes may be skipped altogether. In that case, this function will generate fewer shapes than requested.", "The number of rows and columns of the image to generate.", "The maximum number of shapes to (attempt to) fit into the shape.", "The minimum number of shapes to (attempt to) fit into the shape.", "The minimum dimension of each shape to fit into the image.", "The maximum dimension of each shape to fit into the image.", "If True, the generated image has num_channels color channels, otherwise generates grayscale image.", "Number of channels in the generated image. If 1, generate monochrome images, else color images with multiple channels. Ignored if multichannel is set to False.", "The name of the shape to generate or None to pick random ones.", "The range of values to sample pixel values from. For grayscale images the format is (min, max). For multichannel - ((min, max),) if the ranges are equal across the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As the function supports generation of uint8 arrays only, the maximum range is (0, 255). If None, set to (0, 254) for each channel reserving color of intensity = 255 for background.", "If True, allow shapes to overlap.", "How often to attempt to fit a shape into the image before skipping it.", "Seed to initialize the random number generator. If None, a random seed from the operating system is used.", "An image with the fitted shapes.", "A list of labels, one per shape in the image. Each label is a (category, ((r0, r1), (c0, c1))) tuple specifying the category and bounding box coordinates of the shape.", "Generate coordinates of pixels within a rectangle.", "Origin point of the rectangle, e.g., ([plane,] row, column).", "End point of the rectangle ([plane,] row, column). For a 2D matrix, the slice defined by the rectangle is [start:(end+1)]. Either end or extent must be specified.", "The extent (size) of the drawn rectangle. E.g., ([num_planes,] num_rows, num_cols). Either end or extent must be specified. A negative extent is valid, and will result in a rectangle going along the opposite direction. If extent is negative, the start point is not included.", "Image shape used to determine the maximum bounds of the output coordinates. This is useful for clipping rectangles that exceed the image size. By default, no clipping is done.", "The coordinates of all pixels in the rectangle.", "This function can be applied to N-dimensional images, by passing start and end or extent as tuples of length N.", "Generate coordinates of pixels that are exactly around a rectangle.", "Origin point of the inner rectangle, e.g., (row, column).", "End point of the inner rectangle (row, column). For a 2D matrix, the slice defined by inner the rectangle is [start:(end+1)]. Either end or extent must be specified.", "The extent (size) of the inner rectangle. E.g., (num_rows, num_cols). Either end or extent must be specified. Negative extents are permitted. See rectangle to better understand how they behave.", "Image shape used to determine the maximum bounds of the output coordinates. This is useful for clipping perimeters that exceed the image size. By default, no clipping is done. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Whether to clip the perimeter to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.", "The coordinates of all pixels in the rectangle.", "Set pixel color in the image at the given coordinates.", "Note that this function modifies the color of the image in-place. Coordinates that exceed the shape of the image will be ignored.", "Image", "Row and column coordinates of pixels to be colored.", "Color to be assigned to coordinates in the image.", "Alpha values used to blend color with image. 0 is transparent, 1 is opaque."]}, {"name": "draw.bezier_curve()", "path": "api/skimage.draw#skimage.draw.bezier_curve", "type": "draw", "text": ["Generate Bezier curve coordinates.", "Coordinates of the first control point.", "Coordinates of the middle control point.", "Coordinates of the last control point.", "Middle control point weight, it describes the line tension.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for curves that exceed the image size. If None, the full extent of the curve is used.", "Indices of pixels that belong to the Bezier curve. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "The algorithm is the rational quadratic algorithm presented in reference [1].", "A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf"]}, {"name": "draw.circle()", "path": "api/skimage.draw#skimage.draw.circle", "type": "draw", "text": ["Generate coordinates of pixels within circle.", "Center coordinate of disk.", "Radius of disk.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Pixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk instead."]}, {"name": "draw.circle_perimeter()", "path": "api/skimage.draw#skimage.draw.circle_perimeter", "type": "draw", "text": ["Generate circle perimeter coordinates.", "Centre coordinate of circle.", "Radius of circle.", "bresenham : Bresenham method (default) andres : Andres method", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Bresenham and Andres\u2019 method: Indices of pixels that belong to the circle perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Andres method presents the advantage that concentric circles create a disc whereas Bresenham can make holes. There is also less distortions when Andres circles are rotated. Bresenham method is also known as midpoint circle algorithm. Anti-aliased circle generator is available with circle_perimeter_aa.", "J.E. Bresenham, \u201cAlgorithm for computer control of a digital plotter\u201d, IBM Systems journal, 4 (1965) 25-30.", "E. Andres, \u201cDiscrete circles, rings and spheres\u201d, Computers & Graphics, 18 (1994) 695-706."]}, {"name": "draw.circle_perimeter_aa()", "path": "api/skimage.draw#skimage.draw.circle_perimeter_aa", "type": "draw", "text": ["Generate anti-aliased circle perimeter coordinates.", "Centre coordinate of circle.", "Radius of circle.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for circles that exceed the image size. If None, the full extent of the circle is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Indices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.", "Wu\u2019s method draws anti-aliased circle. This implementation doesn\u2019t use lookup table optimization.", "Use the function draw.set_color to apply circle_perimeter_aa results to color images.", "X. Wu, \u201cAn efficient antialiasing technique\u201d, In ACM SIGGRAPH Computer Graphics, 25 (1991) 143-152."]}, {"name": "draw.disk()", "path": "api/skimage.draw#skimage.draw.disk", "type": "draw", "text": ["Generate coordinates of pixels within circle.", "Center coordinate of disk.", "Radius of disk.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for disks that exceed the image size. If None, the full extent of the disk is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Pixel coordinates of disk. May be used to directly index into an array, e.g. img[rr, cc] = 1."]}, {"name": "draw.ellipse()", "path": "api/skimage.draw#skimage.draw.ellipse", "type": "draw", "text": ["Generate coordinates of pixels within ellipse.", "Centre coordinate of ellipse.", "Minor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses which exceed the image size. By default the full extent of the ellipse are used. Must be at least length 2. Only the first two values are used to determine the extent.", "Set the ellipse rotation (rotation) in range (-PI, PI) in contra clock wise direction, so PI/2 degree means swap ellipse axis", "Pixel coordinates of ellipse. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "The ellipse equation:", "Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1]"]}, {"name": "draw.ellipse_perimeter()", "path": "api/skimage.draw#skimage.draw.ellipse_perimeter", "type": "draw", "text": ["Generate ellipse perimeter coordinates.", "Centre coordinate of ellipse.", "Minor and major semi-axes. (r/r_radius)**2 + (c/c_radius)**2 = 1.", "Major axis orientation in clockwise direction as radians.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for ellipses that exceed the image size. If None, the full extent of the ellipse is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Indices of pixels that belong to the ellipse perimeter. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf", "Note that the positions of ellipse without specified shape can have also, negative values, as this is correct on the plane. On the other hand using these ellipse positions for an image afterwards may lead to appearing on the other side of image, because image[-1, -1] = image[end-1, end-1]"]}, {"name": "draw.ellipsoid()", "path": "api/skimage.draw#skimage.draw.ellipsoid", "type": "draw", "text": ["Generates ellipsoid with semimajor axes aligned with grid dimensions on grid with specified spacing.", "Length of semimajor axis aligned with x-axis.", "Length of semimajor axis aligned with y-axis.", "Length of semimajor axis aligned with z-axis.", "Spacing in (x, y, z) spatial dimensions.", "If True, returns the level set for this ellipsoid (signed level set about zero, with positive denoting interior) as np.float64. False returns a binarized version of said level set.", "Ellipsoid centered in a correctly sized array for given spacing. Boolean dtype unless levelset=True, in which case a float array is returned with the level set above 0.0 representing the ellipsoid."]}, {"name": "draw.ellipsoid_stats()", "path": "api/skimage.draw#skimage.draw.ellipsoid_stats", "type": "draw", "text": ["Calculates analytical surface area and volume for ellipsoid with semimajor axes aligned with grid dimensions of specified spacing.", "Length of semimajor axis aligned with x-axis.", "Length of semimajor axis aligned with y-axis.", "Length of semimajor axis aligned with z-axis.", "Calculated volume of ellipsoid.", "Calculated surface area of ellipsoid."]}, {"name": "draw.line()", "path": "api/skimage.draw#skimage.draw.line", "type": "draw", "text": ["Generate line pixel coordinates.", "Starting position (row, column).", "End position (row, column).", "Indices of pixels that belong to the line. May be used to directly index into an array, e.g. img[rr, cc] = 1.", "Anti-aliased line generator is available with line_aa."]}, {"name": "draw.line_aa()", "path": "api/skimage.draw#skimage.draw.line_aa", "type": "draw", "text": ["Generate anti-aliased line pixel coordinates.", "Starting position (row, column).", "End position (row, column).", "Indices of pixels (rr, cc) and intensity values (val). img[rr, cc] = val.", "A Rasterizing Algorithm for Drawing Curves, A. Zingl, 2012 http://members.chello.at/easyfilter/Bresenham.pdf"]}, {"name": "draw.line_nd()", "path": "api/skimage.draw#skimage.draw.line_nd", "type": "draw", "text": ["Draw a single-pixel thick line in n dimensions.", "The line produced will be ndim-connected. That is, two subsequent pixels in the line will be either direct or diagonal neighbours in n dimensions.", "The start coordinates of the line.", "The end coordinates of the line.", "Whether to include the endpoint in the returned line. Defaults to False, which allows for easy drawing of multi-point paths.", "Whether to round the coordinates to integer. If True (default), the returned coordinates can be used to directly index into an array. False could be used for e.g. vector drawing.", "The coordinates of points on the line."]}, {"name": "draw.polygon()", "path": "api/skimage.draw#skimage.draw.polygon", "type": "draw", "text": ["Generate coordinates of pixels within polygon.", "Row coordinates of vertices of polygon.", "Column coordinates of vertices of polygon.", "Image shape which is used to determine the maximum extent of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extent of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Pixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1."]}, {"name": "draw.polygon2mask()", "path": "api/skimage.draw#skimage.draw.polygon2mask", "type": "draw", "text": ["Compute a mask from polygon.", "The shape of the mask.", "The polygon coordinates of shape (N, 2) where N is the number of points.", "The mask that corresponds to the input polygon.", "This function does not do any border checking, so that all the vertices need to be within the given shape."]}, {"name": "draw.polygon_perimeter()", "path": "api/skimage.draw#skimage.draw.polygon_perimeter", "type": "draw", "text": ["Generate polygon perimeter coordinates.", "Row coordinates of vertices of polygon.", "Column coordinates of vertices of polygon.", "Image shape which is used to determine maximum extents of output pixel coordinates. This is useful for polygons that exceed the image size. If None, the full extents of the polygon is used. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Whether to clip the polygon to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.", "Pixel coordinates of polygon. May be used to directly index into an array, e.g. img[rr, cc] = 1."]}, {"name": "draw.random_shapes()", "path": "api/skimage.draw#skimage.draw.random_shapes", "type": "draw", "text": ["Generate an image with random shapes, labeled with bounding boxes.", "The image is populated with random shapes with random sizes, random locations, and random colors, with or without overlap.", "Shapes have random (row, col) starting coordinates and random sizes bounded by min_size and max_size. It can occur that a randomly generated shape will not fit the image at all. In that case, the algorithm will try again with new starting coordinates a certain number of times. However, it also means that some shapes may be skipped altogether. In that case, this function will generate fewer shapes than requested.", "The number of rows and columns of the image to generate.", "The maximum number of shapes to (attempt to) fit into the shape.", "The minimum number of shapes to (attempt to) fit into the shape.", "The minimum dimension of each shape to fit into the image.", "The maximum dimension of each shape to fit into the image.", "If True, the generated image has num_channels color channels, otherwise generates grayscale image.", "Number of channels in the generated image. If 1, generate monochrome images, else color images with multiple channels. Ignored if multichannel is set to False.", "The name of the shape to generate or None to pick random ones.", "The range of values to sample pixel values from. For grayscale images the format is (min, max). For multichannel - ((min, max),) if the ranges are equal across the channels, and ((min_0, max_0), \u2026 (min_N, max_N)) if they differ. As the function supports generation of uint8 arrays only, the maximum range is (0, 255). If None, set to (0, 254) for each channel reserving color of intensity = 255 for background.", "If True, allow shapes to overlap.", "How often to attempt to fit a shape into the image before skipping it.", "Seed to initialize the random number generator. If None, a random seed from the operating system is used.", "An image with the fitted shapes.", "A list of labels, one per shape in the image. Each label is a (category, ((r0, r1), (c0, c1))) tuple specifying the category and bounding box coordinates of the shape."]}, {"name": "draw.rectangle()", "path": "api/skimage.draw#skimage.draw.rectangle", "type": "draw", "text": ["Generate coordinates of pixels within a rectangle.", "Origin point of the rectangle, e.g., ([plane,] row, column).", "End point of the rectangle ([plane,] row, column). For a 2D matrix, the slice defined by the rectangle is [start:(end+1)]. Either end or extent must be specified.", "The extent (size) of the drawn rectangle. E.g., ([num_planes,] num_rows, num_cols). Either end or extent must be specified. A negative extent is valid, and will result in a rectangle going along the opposite direction. If extent is negative, the start point is not included.", "Image shape used to determine the maximum bounds of the output coordinates. This is useful for clipping rectangles that exceed the image size. By default, no clipping is done.", "The coordinates of all pixels in the rectangle.", "This function can be applied to N-dimensional images, by passing start and end or extent as tuples of length N."]}, {"name": "draw.rectangle_perimeter()", "path": "api/skimage.draw#skimage.draw.rectangle_perimeter", "type": "draw", "text": ["Generate coordinates of pixels that are exactly around a rectangle.", "Origin point of the inner rectangle, e.g., (row, column).", "End point of the inner rectangle (row, column). For a 2D matrix, the slice defined by inner the rectangle is [start:(end+1)]. Either end or extent must be specified.", "The extent (size) of the inner rectangle. E.g., (num_rows, num_cols). Either end or extent must be specified. Negative extents are permitted. See rectangle to better understand how they behave.", "Image shape used to determine the maximum bounds of the output coordinates. This is useful for clipping perimeters that exceed the image size. By default, no clipping is done. Must be at least length 2. Only the first two values are used to determine the extent of the input image.", "Whether to clip the perimeter to the provided shape. If this is set to True, the drawn figure will always be a closed polygon with all edges visible.", "The coordinates of all pixels in the rectangle."]}, {"name": "draw.set_color()", "path": "api/skimage.draw#skimage.draw.set_color", "type": "draw", "text": ["Set pixel color in the image at the given coordinates.", "Note that this function modifies the color of the image in-place. Coordinates that exceed the shape of the image will be ignored.", "Image", "Row and column coordinates of pixels to be colored.", "Color to be assigned to coordinates in the image.", "Alpha values used to blend color with image. 0 is transparent, 1 is opaque."]}, {"name": "dtype_limits()", "path": "api/skimage#skimage.dtype_limits", "type": "skimage", "text": ["Return intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.", "Input image.", "If True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.", "Lower and upper intensity limits."]}, {"name": "ensure_python_version()", "path": "api/skimage#skimage.ensure_python_version", "type": "skimage", "text": []}, {"name": "exposure", "path": "api/skimage.exposure", "type": "exposure", "text": ["skimage.exposure.adjust_gamma(image[, \u2026])", "Performs Gamma Correction on the input image.", "skimage.exposure.adjust_log(image[, gain, inv])", "Performs Logarithmic correction on the input image.", "skimage.exposure.adjust_sigmoid(image[, \u2026])", "Performs Sigmoid Correction on the input image.", "skimage.exposure.cumulative_distribution(image)", "Return cumulative distribution function (cdf) for the given image.", "skimage.exposure.equalize_adapthist(image[, \u2026])", "Contrast Limited Adaptive Histogram Equalization (CLAHE).", "skimage.exposure.equalize_hist(image[, \u2026])", "Return image after histogram equalization.", "skimage.exposure.histogram(image[, nbins, \u2026])", "Return histogram of image.", "skimage.exposure.is_low_contrast(image[, \u2026])", "Determine if an image is low contrast.", "skimage.exposure.match_histograms(image, \u2026)", "Adjust an image so that its cumulative histogram matches that of another.", "skimage.exposure.rescale_intensity(image[, \u2026])", "Return image after stretching or shrinking its intensity levels.", "Performs Gamma Correction on the input image.", "Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.", "Input image.", "Non negative real number. Default value is 1.", "The constant multiplier. Default value is 1.", "Gamma corrected output image.", "See also", "For gamma greater than 1, the histogram will shift towards left and the output image will be darker than the input image.", "For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image.", "https://en.wikipedia.org/wiki/Gamma_correction", "Explore 3D images (of cells)", "Performs Logarithmic correction on the input image.", "This function transforms the input image pixelwise according to the equation O = gain*log(1 + I) after scaling each pixel to the range 0 to 1. For inverse logarithmic correction, the equation is O = gain*(2**I - 1).", "Input image.", "The constant multiplier. Default value is 1.", "If True, it performs inverse logarithmic correction, else correction will be logarithmic. Defaults to False.", "Logarithm corrected output image.", "See also", "http://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf", "Performs Sigmoid Correction on the input image.", "Also known as Contrast Adjustment. This function transforms the input image pixelwise according to the equation O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each pixel to the range 0 to 1.", "Input image.", "Cutoff of the sigmoid function that shifts the characteristic curve in horizontal direction. Default value is 0.5.", "The constant multiplier in exponential\u2019s power of sigmoid function. Default value is 10.", "If True, returns the negative sigmoid correction. Defaults to False.", "Sigmoid corrected output image.", "See also", "Gustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf", "Return cumulative distribution function (cdf) for the given image.", "Image array.", "Number of bins for image histogram.", "Values of cumulative distribution function.", "Centers of bins.", "See also", "https://en.wikipedia.org/wiki/Cumulative_distribution_function", "Local Histogram Equalization", "Explore 3D images (of cells)", "Contrast Limited Adaptive Histogram Equalization (CLAHE).", "An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.", "Input image.", "Defines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.", "Clipping limit, normalized between 0 and 1 (higher values give more contrast).", "Number of gray bins for histogram (\u201cdata range\u201d).", "Equalized image with float64 dtype.", "See also", "Changed in version 0.17: The values returned by this function are slightly shifted upwards because of an internal change in rounding behavior.", "http://tog.acm.org/resources/GraphicsGems/", "https://en.wikipedia.org/wiki/CLAHE#CLAHE", "3D adaptive histogram equalization", "Return image after histogram equalization.", "Image array.", "Number of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.", "Array of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.", "Image array after histogram equalization.", "This function is adapted from [1] with the author\u2019s permission.", "http://www.janeriksolem.net/histogram-equalization-with-python-and.html", "https://en.wikipedia.org/wiki/Histogram_equalization", "Local Histogram Equalization", "3D adaptive histogram equalization", "Explore 3D images (of cells)", "Rank filters", "Return histogram of image.", "Unlike numpy.histogram, this function returns the centers of bins and does not rebin integer arrays. For integer arrays, each integer value has its own bin, which improves speed and intensity-resolution.", "The histogram is computed on the flattened image: for color images, the function should be used separately on each channel to obtain a histogram for each color channel.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019 determines the range from the expected range of the images of that data type.", "If True, normalize the histogram by the sum of its values.", "The values of the histogram.", "The values at the center of the bins.", "See also", "Rank filters", "Determine if an image is low contrast.", "The image under test.", "The low contrast fraction threshold. An image is considered low- contrast when its range of brightness spans less than this fraction of its data type\u2019s full range. [1]", "Disregard values below this percentile when computing image contrast.", "Disregard values above this percentile when computing image contrast.", "The contrast determination method. Right now the only available option is \u201clinear\u201d.", "True when the image is determined to be low contrast.", "https://scikit-image.org/docs/dev/user_guide/data_types.html", "Adjust an image so that its cumulative histogram matches that of another.", "The adjustment is applied separately for each channel.", "Input image. Can be gray-scale or in color.", "Image to match histogram of. Must have the same number of channels as image.", "Apply the matching separately for each channel.", "Transformed input image.", "Thrown when the number of channels in the input image and the reference differ.", "http://paulbourke.net/miscellaneous/equalisation/", "Return image after stretching or shrinking its intensity levels.", "The desired intensity range of the input and output, in_range and out_range respectively, are used to stretch or shrink the intensity range of the input image. See examples below.", "Image array.", "Min and max intensity values of input and output image. The possible values for this parameter are enumerated below.", "Use image min/max as the intensity range.", "Use min/max of the image\u2019s dtype as the intensity range.", "Use intensity range based on desired dtype. Must be valid key in DTYPE_RANGE.", "Use range_values as explicit min/max intensities.", "Image array after rescaling its intensity. This image is the same dtype as the input image.", "See also", "Changed in version 0.17: The dtype of the output array has changed to match the output dtype, or float if the output range is specified by a pair of floats.", "By default, the min/max intensities of the input image are stretched to the limits allowed by the image\u2019s dtype, since in_range defaults to \u2018image\u2019 and out_range defaults to \u2018dtype\u2019:", "It\u2019s easy to accidentally convert an image dtype from uint8 to float:", "Use rescale_intensity to rescale to the proper range for float dtypes:", "To maintain the low contrast of the original, use the in_range parameter:", "If the min/max value of in_range is more/less than the min/max image intensity, then the intensity levels are clipped:", "If you have an image with signed integers but want to rescale the image to just the positive range, use the out_range parameter. In that case, the output dtype will be float:", "To get the desired range with a specific dtype, use .astype():", "If the input image is constant, the output will be clipped directly to the output range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127, 127], dtype=int32)", "Phase Unwrapping", "Explore 3D images (of cells)", "Rank filters"]}, {"name": "exposure.adjust_gamma()", "path": "api/skimage.exposure#skimage.exposure.adjust_gamma", "type": "exposure", "text": ["Performs Gamma Correction on the input image.", "Also known as Power Law Transform. This function transforms the input image pixelwise according to the equation O = I**gamma after scaling each pixel to the range 0 to 1.", "Input image.", "Non negative real number. Default value is 1.", "The constant multiplier. Default value is 1.", "Gamma corrected output image.", "See also", "For gamma greater than 1, the histogram will shift towards left and the output image will be darker than the input image.", "For gamma less than 1, the histogram will shift towards right and the output image will be brighter than the input image.", "https://en.wikipedia.org/wiki/Gamma_correction"]}, {"name": "exposure.adjust_log()", "path": "api/skimage.exposure#skimage.exposure.adjust_log", "type": "exposure", "text": ["Performs Logarithmic correction on the input image.", "This function transforms the input image pixelwise according to the equation O = gain*log(1 + I) after scaling each pixel to the range 0 to 1. For inverse logarithmic correction, the equation is O = gain*(2**I - 1).", "Input image.", "The constant multiplier. Default value is 1.", "If True, it performs inverse logarithmic correction, else correction will be logarithmic. Defaults to False.", "Logarithm corrected output image.", "See also", "http://www.ece.ucsb.edu/Faculty/Manjunath/courses/ece178W03/EnhancePart1.pdf"]}, {"name": "exposure.adjust_sigmoid()", "path": "api/skimage.exposure#skimage.exposure.adjust_sigmoid", "type": "exposure", "text": ["Performs Sigmoid Correction on the input image.", "Also known as Contrast Adjustment. This function transforms the input image pixelwise according to the equation O = 1/(1 + exp*(gain*(cutoff - I))) after scaling each pixel to the range 0 to 1.", "Input image.", "Cutoff of the sigmoid function that shifts the characteristic curve in horizontal direction. Default value is 0.5.", "The constant multiplier in exponential\u2019s power of sigmoid function. Default value is 10.", "If True, returns the negative sigmoid correction. Defaults to False.", "Sigmoid corrected output image.", "See also", "Gustav J. Braun, \u201cImage Lightness Rescaling Using Sigmoidal Contrast Enhancement Functions\u201d, http://www.cis.rit.edu/fairchild/PDFs/PAP07.pdf"]}, {"name": "exposure.cumulative_distribution()", "path": "api/skimage.exposure#skimage.exposure.cumulative_distribution", "type": "exposure", "text": ["Return cumulative distribution function (cdf) for the given image.", "Image array.", "Number of bins for image histogram.", "Values of cumulative distribution function.", "Centers of bins.", "See also", "https://en.wikipedia.org/wiki/Cumulative_distribution_function"]}, {"name": "exposure.equalize_adapthist()", "path": "api/skimage.exposure#skimage.exposure.equalize_adapthist", "type": "exposure", "text": ["Contrast Limited Adaptive Histogram Equalization (CLAHE).", "An algorithm for local contrast enhancement, that uses histograms computed over different tile regions of the image. Local details can therefore be enhanced even in regions that are darker or lighter than most of the image.", "Input image.", "Defines the shape of contextual regions used in the algorithm. If iterable is passed, it must have the same number of elements as image.ndim (without color channel). If integer, it is broadcasted to each image dimension. By default, kernel_size is 1/8 of image height by 1/8 of its width.", "Clipping limit, normalized between 0 and 1 (higher values give more contrast).", "Number of gray bins for histogram (\u201cdata range\u201d).", "Equalized image with float64 dtype.", "See also", "Changed in version 0.17: The values returned by this function are slightly shifted upwards because of an internal change in rounding behavior.", "http://tog.acm.org/resources/GraphicsGems/", "https://en.wikipedia.org/wiki/CLAHE#CLAHE"]}, {"name": "exposure.equalize_hist()", "path": "api/skimage.exposure#skimage.exposure.equalize_hist", "type": "exposure", "text": ["Return image after histogram equalization.", "Image array.", "Number of bins for image histogram. Note: this argument is ignored for integer images, for which each integer is its own bin.", "Array of same shape as image. Only points at which mask == True are used for the equalization, which is applied to the whole image.", "Image array after histogram equalization.", "This function is adapted from [1] with the author\u2019s permission.", "http://www.janeriksolem.net/histogram-equalization-with-python-and.html", "https://en.wikipedia.org/wiki/Histogram_equalization"]}, {"name": "exposure.histogram()", "path": "api/skimage.exposure#skimage.exposure.histogram", "type": "exposure", "text": ["Return histogram of image.", "Unlike numpy.histogram, this function returns the centers of bins and does not rebin integer arrays. For integer arrays, each integer value has its own bin, which improves speed and intensity-resolution.", "The histogram is computed on the flattened image: for color images, the function should be used separately on each channel to obtain a histogram for each color channel.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "\u2018image\u2019 (default) determines the range from the input image. \u2018dtype\u2019 determines the range from the expected range of the images of that data type.", "If True, normalize the histogram by the sum of its values.", "The values of the histogram.", "The values at the center of the bins.", "See also"]}, {"name": "exposure.is_low_contrast()", "path": "api/skimage.exposure#skimage.exposure.is_low_contrast", "type": "exposure", "text": ["Determine if an image is low contrast.", "The image under test.", "The low contrast fraction threshold. An image is considered low- contrast when its range of brightness spans less than this fraction of its data type\u2019s full range. [1]", "Disregard values below this percentile when computing image contrast.", "Disregard values above this percentile when computing image contrast.", "The contrast determination method. Right now the only available option is \u201clinear\u201d.", "True when the image is determined to be low contrast.", "https://scikit-image.org/docs/dev/user_guide/data_types.html"]}, {"name": "exposure.match_histograms()", "path": "api/skimage.exposure#skimage.exposure.match_histograms", "type": "exposure", "text": ["Adjust an image so that its cumulative histogram matches that of another.", "The adjustment is applied separately for each channel.", "Input image. Can be gray-scale or in color.", "Image to match histogram of. Must have the same number of channels as image.", "Apply the matching separately for each channel.", "Transformed input image.", "Thrown when the number of channels in the input image and the reference differ.", "http://paulbourke.net/miscellaneous/equalisation/"]}, {"name": "exposure.rescale_intensity()", "path": "api/skimage.exposure#skimage.exposure.rescale_intensity", "type": "exposure", "text": ["Return image after stretching or shrinking its intensity levels.", "The desired intensity range of the input and output, in_range and out_range respectively, are used to stretch or shrink the intensity range of the input image. See examples below.", "Image array.", "Min and max intensity values of input and output image. The possible values for this parameter are enumerated below.", "Use image min/max as the intensity range.", "Use min/max of the image\u2019s dtype as the intensity range.", "Use intensity range based on desired dtype. Must be valid key in DTYPE_RANGE.", "Use range_values as explicit min/max intensities.", "Image array after rescaling its intensity. This image is the same dtype as the input image.", "See also", "Changed in version 0.17: The dtype of the output array has changed to match the output dtype, or float if the output range is specified by a pair of floats.", "By default, the min/max intensities of the input image are stretched to the limits allowed by the image\u2019s dtype, since in_range defaults to \u2018image\u2019 and out_range defaults to \u2018dtype\u2019:", "It\u2019s easy to accidentally convert an image dtype from uint8 to float:", "Use rescale_intensity to rescale to the proper range for float dtypes:", "To maintain the low contrast of the original, use the in_range parameter:", "If the min/max value of in_range is more/less than the min/max image intensity, then the intensity levels are clipped:", "If you have an image with signed integers but want to rescale the image to just the positive range, use the out_range parameter. In that case, the output dtype will be float:", "To get the desired range with a specific dtype, use .astype():", "If the input image is constant, the output will be clipped directly to the output range: >>> image = np.array([130, 130, 130], dtype=np.int32) >>> rescale_intensity(image, out_range=(0, 127)).astype(np.int32) array([127, 127, 127], dtype=int32)"]}, {"name": "feature", "path": "api/skimage.feature", "type": "feature", "text": ["skimage.feature.blob_dog(image[, min_sigma, \u2026])", "Finds blobs in the given grayscale image.", "skimage.feature.blob_doh(image[, min_sigma, \u2026])", "Finds blobs in the given grayscale image.", "skimage.feature.blob_log(image[, min_sigma, \u2026])", "Finds blobs in the given grayscale image.", "skimage.feature.canny(image[, sigma, \u2026])", "Edge filter an image using the Canny algorithm.", "skimage.feature.corner_fast(image[, n, \u2026])", "Extract FAST corners for a given image.", "skimage.feature.corner_foerstner(image[, sigma])", "Compute Foerstner corner measure response image.", "skimage.feature.corner_harris(image[, \u2026])", "Compute Harris corner measure response image.", "skimage.feature.corner_kitchen_rosenfeld(image)", "Compute Kitchen and Rosenfeld corner measure response image.", "skimage.feature.corner_moravec(image[, \u2026])", "Compute Moravec corner measure response image.", "skimage.feature.corner_orientations(image, \u2026)", "Compute the orientation of corners.", "skimage.feature.corner_peaks(image[, \u2026])", "Find peaks in corner measure response image.", "skimage.feature.corner_shi_tomasi(image[, sigma])", "Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.", "skimage.feature.corner_subpix(image, corners)", "Determine subpixel position of corners.", "skimage.feature.daisy(image[, step, radius, \u2026])", "Extract DAISY feature descriptors densely for the given image.", "skimage.feature.draw_haar_like_feature(\u2026)", "Visualization of Haar-like features.", "skimage.feature.draw_multiblock_lbp(image, \u2026)", "Multi-block local binary pattern visualization.", "skimage.feature.greycomatrix(image, \u2026[, \u2026])", "Calculate the grey-level co-occurrence matrix.", "skimage.feature.greycoprops(P[, prop])", "Calculate texture properties of a GLCM.", "skimage.feature.haar_like_feature(int_image, \u2026)", "Compute the Haar-like features for a region of interest (ROI) of an integral image.", "skimage.feature.haar_like_feature_coord(\u2026)", "Compute the coordinates of Haar-like features.", "skimage.feature.hessian_matrix(image[, \u2026])", "Compute Hessian matrix.", "skimage.feature.hessian_matrix_det(image[, \u2026])", "Compute the approximate Hessian Determinant over an image.", "skimage.feature.hessian_matrix_eigvals(H_elems)", "Compute eigenvalues of Hessian matrix.", "skimage.feature.hog(image[, orientations, \u2026])", "Extract Histogram of Oriented Gradients (HOG) for a given image.", "skimage.feature.local_binary_pattern(image, P, R)", "Gray scale and rotation invariant LBP (Local Binary Patterns).", "skimage.feature.masked_register_translation(\u2026)", "Deprecated function.", "skimage.feature.match_descriptors(\u2026[, \u2026])", "Brute-force matching of descriptors.", "skimage.feature.match_template(image, template)", "Match a template to a 2-D or 3-D image using normalized correlation.", "skimage.feature.multiblock_lbp(int_image, r, \u2026)", "Multi-block local binary pattern (MB-LBP).", "skimage.feature.multiscale_basic_features(image)", "Local features for a single- or multi-channel nd image.", "skimage.feature.peak_local_max(image[, \u2026])", "Find peaks in an image as coordinate list or boolean mask.", "skimage.feature.plot_matches(ax, image1, \u2026)", "Plot matched features.", "skimage.feature.register_translation(\u2026[, \u2026])", "Deprecated function.", "skimage.feature.shape_index(image[, sigma, \u2026])", "Compute the shape index.", "skimage.feature.structure_tensor(image[, \u2026])", "Compute structure tensor using sum of squared differences.", "skimage.feature.structure_tensor_eigenvalues(A_elems)", "Compute eigenvalues of structure tensor.", "skimage.feature.structure_tensor_eigvals(\u2026)", "Compute eigenvalues of structure tensor.", "skimage.feature.BRIEF([descriptor_size, \u2026])", "BRIEF binary descriptor extractor.", "skimage.feature.CENSURE([min_scale, \u2026])", "CENSURE keypoint detector.", "skimage.feature.Cascade", "Class for cascade of classifiers that is used for object detection.", "skimage.feature.ORB([downscale, n_scales, \u2026])", "Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.", "Finds blobs in the given grayscale image.", "Blobs are found using the Difference of Gaussian (DoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.", "Input grayscale image, blobs are assumed to be light on dark background (white on black).", "The minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The ratio between the standard deviation of Gaussian Kernels used for computing the Difference of Gaussians", "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.", "A value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.", "If tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.", "A 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.", "See also", "The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image.", "https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach", "Finds blobs in the given grayscale image.", "Blobs are found using the Determinant of Hessian method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian Kernel used for the Hessian matrix whose determinant detected the blob. Determinant of Hessians is approximated using [2].", "Input grayscale image.Blobs can either be light on dark or vice versa.", "The minimum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this low to detect smaller blobs.", "The maximum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this high to detect larger blobs.", "The number of intermediate values of standard deviations to consider between min_sigma and max_sigma.", "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect less prominent blobs.", "A value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.", "If set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.", "A 2d array with each row representing 3 values, (y,x,sigma) where (y,x) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel of the Hessian Matrix whose determinant detected the blob.", "The radius of each blob is approximately sigma. Computation of Determinant of Hessians is independent of the standard deviation. Therefore detecting larger blobs won\u2019t take more time. In methods line blob_dog() and blob_log() the computation of Gaussians for larger sigma takes more time. The downside is that this method can\u2019t be used for detecting blobs of radius less than 3px due to the box filters used in the approximation of Hessian Determinant.", "https://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian", "Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf", "Finds blobs in the given grayscale image.", "Blobs are found using the Laplacian of Gaussian (LoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.", "Input grayscale image, blobs are assumed to be light on dark background (white on black).", "the minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The number of intermediate values of standard deviations to consider between min_sigma and max_sigma.", "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.", "A value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.", "If set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.", "If tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.", "A 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.", "The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image.", "https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian", "Edge filter an image using the Canny algorithm.", "Grayscale input image to detect edges on; can be of any dtype.", "Standard deviation of the Gaussian filter.", "Lower bound for hysteresis thresholding (linking edges). If None, low_threshold is set to 10% of dtype\u2019s max.", "Upper bound for hysteresis thresholding (linking edges). If None, high_threshold is set to 20% of dtype\u2019s max.", "Mask to limit the application of Canny to a certain area.", "If True then treat low_threshold and high_threshold as quantiles of the edge magnitude image, rather than absolute edge magnitude values. If True then the thresholds must be in the range [0, 1].", "The binary edge map.", "See also", "The steps of the algorithm are as follows:", "Canny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679-714, 1986 DOI:10.1109/TPAMI.1986.4767851", "William Green\u2019s Canny tutorial https://en.wikipedia.org/wiki/Canny_edge_detector", "Extract FAST corners for a given image.", "Input image.", "Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t testpixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.", "Threshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.", "FAST corner response image.", "Rosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner detection. In European conference on computer vision (pp. 430-443). Springer, Berlin, Heidelberg. DOI:10.1007/11744023_34 http://www.edwardrosten.com/work/rosten_2006_machine.pdf", "Wikipedia, \u201cFeatures from accelerated segment test\u201d, https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test", "Compute Foerstner corner measure response image.", "This corner detector uses information from the auto-correlation matrix A:", "Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as:", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "Error ellipse sizes.", "Roundness of error ellipse.", "F\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf", "https://en.wikipedia.org/wiki/Corner_detection", "Compute Harris corner measure response image.", "This corner detector uses information from the auto-correlation matrix A:", "Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as:", "or:", "Input image.", "Method to compute the response image from the auto-correlation matrix.", "Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.", "Normalisation factor (Noble\u2019s corner measure).", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "Harris response image.", "https://en.wikipedia.org/wiki/Corner_detection", "Compute Kitchen and Rosenfeld corner measure response image.", "The corner measure is calculated as follows:", "Where imx and imy are the first and imxx, imxy, imyy the second derivatives.", "Input image.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Kitchen and Rosenfeld response image.", "Kitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern recognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4", "Compute Moravec corner measure response image.", "This is one of the simplest corner detectors and is comparatively fast but has several limitations (e.g. not rotation invariant).", "Input image.", "Window size.", "Moravec response image.", "https://en.wikipedia.org/wiki/Corner_detection", "Compute the orientation of corners.", "The orientation of corners is computed using the first order central moment i.e. the center of mass approach. The corner orientation is the angle of the vector from the corner coordinate to the intensity centroid in the local neighborhood around the corner calculated using first order central moment.", "Input grayscale image.", "Corner coordinates as (row, col).", "Mask defining the local neighborhood of the corner used for the calculation of the central moment.", "Orientations of corners in the range [-pi, pi].", "Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf", "Paul L. Rosin, \u201cMeasuring Corner Properties\u201d http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf", "Find peaks in corner measure response image.", "This differs from skimage.feature.peak_local_max in that it suppresses multiple connected peaks with the same accumulator value.", "Input image.", "The minimal allowed distance separating peaks.", "See skimage.feature.peak_local_max().", "Which Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.", "See also", "Changed in version 0.18: The default value of threshold_rel has changed to None, which corresponds to letting skimage.feature.peak_local_max decide on the default. This is equivalent to threshold_rel=0.", "The num_peaks limit is applied before suppression of connected peaks. To limit the number of peaks after suppression, set num_peaks=np.inf and post-process the output of this function.", "Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.", "This corner detector uses information from the auto-correlation matrix A:", "Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as the smaller eigenvalue of A:", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "Shi-Tomasi response image.", "https://en.wikipedia.org/wiki/Corner_detection", "Determine subpixel position of corners.", "A statistical test decides whether the corner is defined as the intersection of two edges or a single peak. Depending on the classification result, the subpixel corner location is determined based on the local covariance of the grey-values. If the significance level for either statistical test is not sufficient, the corner cannot be classified, and the output subpixel position is set to NaN.", "Input image.", "Corner coordinates (row, col).", "Search window size for subpixel estimation.", "Significance level for corner classification.", "Subpixel corner positions. NaN for \u201cnot classified\u201d corners.", "F\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf", "https://en.wikipedia.org/wiki/Corner_detection", "Extract DAISY feature descriptors densely for the given image.", "DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations.", "The implementation follows Tola et al. [1] but deviate on the following points:", "Input image (grayscale).", "Distance between descriptor sampling points.", "Radius (in pixels) of the outermost ring.", "Number of rings.", "Number of histograms sampled per ring.", "Number of orientations (bins) per histogram.", "How to normalize the descriptors", "Standard deviation of spatial Gaussian smoothing for the center histogram and for each ring of histograms. The array of sigmas should be sorted from the center and out. I.e. the first sigma value defines the spatial smoothing of the center histogram and the last sigma value defines the spatial smoothing of the outermost ring. Specifying sigmas overrides the following parameter.", "rings = len(sigmas) - 1", "Radius (in pixels) for each ring. Specifying ring_radii overrides the following two parameters.", "rings = len(ring_radii) radius = ring_radii[-1]", "If both sigmas and ring_radii are given, they must satisfy the following predicate since no radius is needed for the center histogram.", "len(ring_radii) == len(sigmas) + 1", "Generate a visualization of the DAISY descriptors", "Grid of DAISY descriptors for the given image as an array dimensionality (P, Q, R) where", "P = ceil((M - radius*2) / step) Q = ceil((N - radius*2) / step) R = (rings * histograms + 1) * orientations", "Visualization of the DAISY descriptors.", "Tola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline stereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.", "http://cvlab.epfl.ch/software/daisy", "Visualization of Haar-like features.", "The region of an integral image for which the features need to be computed.", "Row-coordinate of top left corner of the detection window.", "Column-coordinate of top left corner of the detection window.", "Width of the detection window.", "Height of the detection window.", "The array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.", "Floats specifying the color for the positive block. Corresponding values define (R, G, B) values. Default value is red (1, 0, 0).", "Floats specifying the color for the negative block Corresponding values define (R, G, B) values. Default value is blue (0, 1, 0).", "Value in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.", "The maximum number of features to be returned. By default, all features are returned.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used when generating a set of features smaller than the total number of available features.", "An image in which the different features will be added.", "Multi-block local binary pattern visualization.", "Blocks with higher sums are colored with alpha-blended white rectangles, whereas blocks with lower sums are colored alpha-blended cyan. Colors and the alpha parameter can be changed.", "Image on which to visualize the pattern.", "Row-coordinate of top left corner of a rectangle containing feature.", "Column-coordinate of top left corner of a rectangle containing feature.", "Width of one of 9 equal rectangles that will be used to compute a feature.", "Height of one of 9 equal rectangles that will be used to compute a feature.", "The descriptor of feature to visualize. If not provided, the descriptor with 0 value will be used.", "Floats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is white (1, 1, 1).", "Floats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is cyan (0, 0.69, 0.96).", "Value in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.", "Image with MB-LBP visualization.", "Face Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf", "Calculate the grey-level co-occurrence matrix.", "A grey level co-occurrence matrix is a histogram of co-occurring greyscale values at a given offset over an image.", "Integer typed input image. Only positive valued images are supported. If type is other than uint8, the argument levels needs to be set.", "List of pixel pair distance offsets.", "List of pixel pair angles in radians.", "The input image should contain integers in [0, levels-1], where levels indicate the number of grey-levels counted (typically 256 for an 8-bit image). This argument is required for 16-bit images or higher and is typically the maximum of the image. As the output matrix is at least levels x levels, it might be preferable to use binning of the input image rather than large values for levels.", "If True, the output matrix P[:, :, d, theta] is symmetric. This is accomplished by ignoring the order of value pairs, so both (i, j) and (j, i) are accumulated when (i, j) is encountered for a given offset. The default is False.", "If True, normalize each matrix P[:, :, d, theta] by dividing by the total number of accumulated co-occurrences for the given offset. The elements of the resulting matrix sum to 1. The default is False.", "The grey-level co-occurrence histogram. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i. If normed is False, the output is of type uint32, otherwise it is float64. The dimensions are: levels x levels x number of distances x number of angles.", "The GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm", "Haralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d IEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621. DOI:10.1109/TSMC.1973.4309314", "Pattern Recognition Engineering, Morton Nadler & Eric P. Smith", "Wikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix", "Compute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel offset upwards.", "GLCM Texture Features", "Calculate texture properties of a GLCM.", "Compute a feature of a grey level co-occurrence matrix to serve as a compact summary of the matrix. The properties are computed as follows:", "Each GLCM is normalized to have a sum of 1 before the computation of texture properties.", "Input array. P is the grey-level co-occurrence histogram for which to compute the specified property. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i.", "The property of the GLCM to compute. The default is \u2018contrast\u2019.", "2-dimensional array. results[d, a] is the property \u2018prop\u2019 for the d\u2019th distance and the a\u2019th angle.", "The GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm", "Compute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90 degrees]", "GLCM Texture Features", "Compute the Haar-like features for a region of interest (ROI) of an integral image.", "Haar-like features have been successfully used for image classification and object detection [1]. It has been used for real-time face detection algorithm proposed in [2].", "Integral image for which the features need to be computed.", "Row-coordinate of top left corner of the detection window.", "Column-coordinate of top left corner of the detection window.", "Width of the detection window.", "Height of the detection window.", "The type of feature to consider:", "By default all features are extracted.", "If using with feature_coord, it should correspond to the feature type of each associated coordinate feature.", "The array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.", "Resulting Haar-like features. Each value is equal to the subtraction of sums of the positive and negative rectangles. The data type depends of the data type of int_image: int when the data type of int_image is uint or int and float when the data type of int_image is float.", "When extracting those features in parallel, be aware that the choice of the backend (i.e. multiprocessing vs threading) will have an impact on the performance. The rule of thumb is as follows: use multiprocessing when extracting features for all possible ROI in an image; use threading when extracting the feature at specific location for a limited number of ROIs. Refer to the example Face classification using Haar-like feature descriptor for more insights.", "https://en.wikipedia.org/wiki/Haar-like_feature", "Oren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June). Pedestrian detection using wavelet templates. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp. 193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319", "Viola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d International journal of computer vision 57.2 (2004): 137-154. https://www.merl.com/publications/docs/TR2004-043.pdf DOI:10.1109/CVPR.2001.990517", "You can compute the feature for some pre-computed coordinates.", "Compute the coordinates of Haar-like features.", "Width of the detection window.", "Height of the detection window.", "The type of feature to consider:", "By default all features are extracted.", "Coordinates of the rectangles for each feature.", "The corresponding type for each feature.", "Compute Hessian matrix.", "The Hessian matrix is defined as:", "which is computed by convolving the image with the second derivatives of the Gaussian kernel in the respective r- and c-directions.", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Hxx, Hxy, Hyy)", "Element of the Hessian matrix for each pixel in the input image.", "Element of the Hessian matrix for each pixel in the input image.", "Element of the Hessian matrix for each pixel in the input image.", "Compute the approximate Hessian Determinant over an image.", "The 2D approximate method uses box filters over integral images to compute the approximate Hessian Determinant, as described in [1].", "The image over which to compute Hessian Determinant.", "Standard deviation used for the Gaussian kernel, used for the Hessian matrix.", "If True and the image is 2D, use a much faster approximate computation. This argument has no effect on 3D and higher images.", "The array of the Determinant of Hessians.", "For 2D images when approximate=True, the running time of this method only depends on size of the image. It is independent of sigma as one would expect. The downside is that the result for sigma less than 3 is not accurate, i.e., not similar to the result obtained if someone computed the Hessian and took its determinant.", "Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf", "Compute eigenvalues of Hessian matrix.", "The upper-diagonal elements of the Hessian matrix, as returned by hessian_matrix.", "The eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues are the leading dimension. That is, eigs[i, j, k] contains the ith-largest eigenvalue at position (j, k).", "Extract Histogram of Oriented Gradients (HOG) for a given image.", "Compute a Histogram of Oriented Gradients (HOG) by", "Input image.", "Number of orientation bins.", "Size (in pixels) of a cell.", "Number of cells in each block.", "Block normalization method:", "Normalization using L1-norm.", "Normalization using L1-norm, followed by square root.", "Normalization using L2-norm.", "Normalization using L2-norm, followed by limiting the maximum values to 0.2 (Hys stands for hysteresis) and renormalization using L2-norm. (default) For details, see [3], [4].", "Also return an image of the HOG. For each cell and orientation bin, the image contains a line segment that is centered at the cell center, is perpendicular to the midpoint of the range of angles spanned by the orientation bin, and has intensity proportional to the corresponding histogram value.", "Apply power law compression to normalize the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.", "Return the data as a feature vector by calling .ravel() on the result just before returning.", "If True, the last image dimension is considered as a color channel, otherwise as spatial.", "HOG descriptor for the image. If feature_vector is True, a 1D (flattened) array is returned.", "A visualisation of the HOG image. Only provided if visualize is True.", "The presented code implements the HOG extraction method from [2] with the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the paper); (III) L1 block normalization is used (L2-Hys in the paper).", "Power law compression, also known as Gamma correction, is used to reduce the effects of shadowing and illumination variations. The compression makes the dark regions lighter. When the kwarg transform_sqrt is set to True, the function computes the square root of each color channel and then applies the hog algorithm to the image.", "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients", "Dalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf, DOI:10.1109/CVPR.2005.177", "Lowe, D.G., Distinctive image features from scale-invatiant keypoints, International Journal of Computer Vision (2004) 60: 91, http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf, DOI:10.1023/B:VISI.0000029664.99615.94", "Dalal, N, Finding People in Images and Videos, Human-Computer Interaction [cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006, https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf", "Gray scale and rotation invariant LBP (Local Binary Patterns).", "LBP is an invariant descriptor that can be used for texture classification.", "Graylevel image.", "Number of circularly symmetric neighbour set points (quantization of the angular space).", "Radius of circle (spatial resolution of the operator).", "Method to determine the pattern.", "rotation invariant.", "rotation invariant.", "finer quantization of the angular space which is gray scale and rotation invariant.", "which is only gray scale invariant [2].", "image texture which is rotation but not gray scale invariant.", "LBP image.", "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa. http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.", "Face recognition with local binary patterns. Timo Ahonen, Abdenour Hadid, Matti Pietikainen, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004.", "Deprecated function. Use skimage.registration.phase_cross_correlation instead.", "Brute-force matching of descriptors.", "For each descriptor in the first set this matcher finds the closest descriptor in the second set (and vice-versa in the case of enabled cross-checking).", "Descriptors of size P about M keypoints in the first image.", "Descriptors of size P about N keypoints in the second image.", "The metric to compute the distance between two descriptors. See scipy.spatial.distance.cdist for all possible types. The hamming distance should be used for binary descriptors. By default the L2-norm is used for all descriptors of dtype float or double and the Hamming distance is used for binary descriptors automatically.", "The p-norm to apply for metric='minkowski'.", "Maximum allowed distance between descriptors of two keypoints in separate images to be regarded as a match.", "If True, the matched keypoints are returned after cross checking i.e. a matched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match for keypoint1 in second image and keypoint1 is the best match for keypoint2 in first image.", "Maximum ratio of distances between first and second closest descriptor in the second set of descriptors. This threshold is useful to filter ambiguous matches between the two descriptor sets. The choice of this value depends on the statistics of the chosen descriptor, e.g., for SIFT descriptors a value of 0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-Invariant Keypoints\u201d, International Journal of Computer Vision, 2004.", "Indices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.", "Match a template to a 2-D or 3-D image using normalized correlation.", "The output is an array with values between -1.0 and 1.0. The value at a given position corresponds to the correlation coefficient between the image and the template.", "For pad_input=True matches correspond to the center and otherwise to the top-left corner of the template. To find the best match you must search for peaks in the response (output) image.", "2-D or 3-D input image.", "Template to locate. It must be (m <= M, n <= N[, d <= D]).", "If True, pad image so that output is the same size as the image, and output values correspond to the template center. Otherwise, the output is an array with shape (M - m + 1, N - n + 1) for an (M, N) image and an (m, n) template, and matches correspond to origin (top-left corner) of the template.", "Padding mode.", "Constant values used in conjunction with mode='constant'.", "Response image with correlation coefficients.", "Details on the cross-correlation are presented in [1]. This implementation uses FFT convolutions of the image and the template. Reference [2] presents similar derivations but the approximation presented in this reference is not used in our implementation.", "J. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.", "Briechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross Correlation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129", "Multi-block local binary pattern (MB-LBP).", "The features are calculated similarly to local binary patterns (LBPs), (See local_binary_pattern()) except that summed blocks are used instead of individual pixel values.", "MB-LBP is an extension of LBP that can be computed on multiple scales in constant time using the integral image. Nine equally-sized rectangles are used to compute a feature. For each rectangle, the sum of the pixel intensities is computed. Comparisons of these sums to that of the central rectangle determine the feature, similarly to LBP.", "Integral image.", "Row-coordinate of top left corner of a rectangle containing feature.", "Column-coordinate of top left corner of a rectangle containing feature.", "Width of one of the 9 equal rectangles that will be used to compute a feature.", "Height of one of the 9 equal rectangles that will be used to compute a feature.", "8-bit MB-LBP feature descriptor.", "Face Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf", "Local features for a single- or multi-channel nd image.", "Intensity, gradient intensity and local structure are computed at different scales thanks to Gaussian blurring.", "Input image, which can be grayscale or multichannel.", "True if the last dimension corresponds to color channels.", "If True, pixel intensities averaged over the different scales are added to the feature set.", "If True, intensities of local gradients averaged over the different scales are added to the feature set.", "If True, eigenvalues of the Hessian matrix after Gaussian blurring at different scales are added to the feature set.", "Smallest value of the Gaussian kernel used to average local neighbourhoods before extracting features.", "Largest value of the Gaussian kernel used to average local neighbourhoods before extracting features.", "Number of values of the Gaussian kernel between sigma_min and sigma_max. If None, sigma_min multiplied by powers of 2 are used.", "The number of parallel threads to use. If set to None, the full set of available cores are used.", "Array of shape image.shape + (n_features,)", "Trainable segmentation using local features and random forests", "Find peaks in an image as coordinate list or boolean mask.", "Peaks are the local maxima in a region of 2 * min_distance + 1 (i.e. peaks are separated by at least min_distance).", "If both threshold_abs and threshold_rel are provided, the maximum of the two is chosen as the minimum intensity threshold of peaks.", "Changed in version 0.18: Prior to version 0.18, peaks of the same height within a radius of min_distance were all returned, but this could cause unexpected behaviour. From 0.18 onwards, an arbitrary peak within the region is returned. See issue gh-2592.", "Input image.", "The minimal allowed distance separating peaks. To find the maximum number of peaks, use min_distance=1.", "Minimum intensity of peaks. By default, the absolute threshold is the minimum intensity of the image.", "Minimum intensity of peaks, calculated as max(image) * threshold_rel.", "If positive integer, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If tuple of non-negative ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If True, takes the min_distance parameter as value. If zero or False, peaks are identified regardless of their distance from the border.", "If True, the output will be an array representing peak coordinates. The coordinates are sorted according to peaks values (Larger first). If False, the output will be a boolean array shaped as image.shape with peaks present at True elements. indices is deprecated and will be removed in version 0.20. Default behavior will be to always return peak coordinates. You can obtain a mask as shown in the example below.", "Maximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks peaks based on highest peak intensity.", "If provided, footprint == 1 represents the local region within which to search for peaks at every point in image.", "If provided, each unique region labels == value represents a unique region to search for peaks. Zero is reserved for background.", "Maximum number of peaks for each label.", "Which Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.", "See also", "The peak local maximum function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image. After comparison of the dilated and original image, this function returns the coordinates or a mask of the peaks where the dilated image equals the original image.", "Finding local maxima", "Watershed segmentation", "Segment human cells (in mitosis)", "Plot matched features.", "Matches and image are drawn in this ax.", "First grayscale or color image.", "Second grayscale or color image.", "First keypoint coordinates as (row, col).", "Second keypoint coordinates as (row, col).", "Indices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.", "Color for keypoint locations.", "Color for lines which connect keypoint matches. By default the color is chosen randomly.", "Whether to only plot matches and not plot the keypoint locations.", "Whether to show images side by side, 'horizontal', or one above the other, 'vertical'.", "Deprecated function. Use skimage.registration.phase_cross_correlation instead.", "Compute the shape index.", "The shape index, as defined by Koenderink & van Doorn [1], is a single valued measure of local curvature, assuming the image as a 3D plane with intensities representing heights.", "It is derived from the eigen values of the Hessian, and its value ranges from -1 to 1 (and is undefined (=NaN) in flat regions), with following ranges representing following shapes:", "Interval (s in \u2026)", "Shape", "[ -1, -7/8)", "Spherical cup", "[-7/8, -5/8)", "Through", "[-5/8, -3/8)", "Rut", "[-3/8, -1/8)", "Saddle rut", "[-1/8, +1/8)", "Saddle", "[+1/8, +3/8)", "Saddle ridge", "[+3/8, +5/8)", "Ridge", "[+5/8, +7/8)", "Dome", "[+7/8, +1]", "Spherical cap", "Input image.", "Standard deviation used for the Gaussian kernel, which is used for smoothing the input data before Hessian eigen value calculation.", "How to handle values outside the image borders", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Shape index", "Koenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d, Image and Vision Computing, 1992, 10, 557-564. DOI:10.1016/0262-8856(92)90076-F", "Compute structure tensor using sum of squared differences.", "The (2-dimensional) structure tensor A is defined as:", "which is approximated by the weighted sum of squared differences in a local window around each pixel in the image. This formula can be extended to a larger number of dimensions (see [1]).", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as a weighting function for the local summation of squared differences.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "NOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr, Arc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx, Axy, Ayy).", "Upper-diagonal elements of the structure tensor for each pixel in the input image.", "See also", "https://en.wikipedia.org/wiki/Structure_tensor", "Compute eigenvalues of structure tensor.", "The upper-diagonal elements of the structure tensor, as returned by structure_tensor.", "The eigenvalues of the structure tensor, in decreasing order. The eigenvalues are the leading dimension. That is, the coordinate [i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).", "See also", "Compute eigenvalues of structure tensor.", "Element of the structure tensor for each pixel in the input image.", "Element of the structure tensor for each pixel in the input image.", "Element of the structure tensor for each pixel in the input image.", "Larger eigen value for each input matrix.", "Smaller eigen value for each input matrix.", "Bases: skimage.feature.util.DescriptorExtractor", "BRIEF binary descriptor extractor.", "BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests.", "For each keypoint, intensity comparisons are carried out for a specifically distributed number N of pixel-pairs resulting in a binary descriptor of length N. For binary descriptors the Hamming distance can be used for feature matching, which leads to lower computational cost in comparison to the L2 norm.", "Size of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended by the authors. Default is 256.", "Length of the two dimensional square patch sampling region around the keypoints. Default is 49.", "Probability distribution for sampling location of decision pixel-pairs around keypoints.", "Seed for the random sampling of the decision pixel-pairs. From a square window with length patch_size, pixel pairs are sampled using the mode parameter to build the descriptors using intensity comparison. The value of sample_seed must be the same for the images to be matched while building the descriptors.", "Standard deviation of the Gaussian low-pass filter applied to the image to alleviate noise sensitivity, which is strongly recommended to obtain discriminative and good descriptors.", "2D ndarray of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).", "Mask indicating whether a keypoint has been filtered out (False) or is described in the descriptors array (True).", "Initialize self. See help(type(self)) for accurate signature.", "Extract BRIEF binary descriptors for given keypoints in image.", "Input image.", "Keypoint coordinates as (row, col).", "Bases: skimage.feature.util.FeatureDetector", "CENSURE keypoint detector.", "Minimum scale to extract keypoints from.", "Maximum scale to extract keypoints from. The keypoints will be extracted from all the scales except the first and the last i.e. from the scales in the range [min_scale + 1, max_scale - 1]. The filter sizes for different scales is such that the two adjacent scales comprise of an octave.", "Type of bi-level filter used to get the scales of the input image. Possible values are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of the bi-level filters i.e. box(square), octagon and star respectively. For instance, a bi-level octagon filter consists of a smaller inner octagon and a larger outer octagon with the filter weights being uniformly negative in both the inner octagon while uniformly positive in the difference region. Use STAR and Octagon for better features and DoB for better performance.", "Threshold value used to suppress maximas and minimas with a weak magnitude response obtained after Non-Maximal Suppression.", "Threshold for rejecting interest points which have ratio of principal curvatures greater than this value.", "Motilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround Extremas for Realtime Feature Detection and Matching\u201d, https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8 DOI:10.1007/978-3-540-88693-8_8", "Adam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative Assessment of Point Feature Detectors and Descriptors in the Context of Robot Navigation\u201d http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf DOI:10.1.1.465.1117", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Initialize self. See help(type(self)) for accurate signature.", "Detect CENSURE keypoints along with the corresponding scale.", "Input image.", "Bases: object", "Class for cascade of classifiers that is used for object detection.", "The main idea behind cascade of classifiers is to create classifiers of medium accuracy and ensemble them into one strong classifier instead of just creating a strong one. The second advantage of cascade classifier is that easy examples can be classified only by evaluating some of the classifiers in the cascade, making the process much faster than the process of evaluating a one strong classifier.", "Accuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.", "Amount of stages in a cascade. Each cascade consists of stumps i.e. trained features.", "The overall amount of stumps in all the stages of cascade.", "The overall amount of different features used by cascade. Two stumps can use the same features but has different trained values.", "The width of a detection window that is used. Objects smaller than this window can\u2019t be detected.", "The height of a detection window.", "A link to the c array that stores stages information using Stage struct.", "Link to the c array that stores MBLBP features using MBLBP struct.", "The ling to the array with look-up tables that are used by trained MBLBP features (MBLBPStumps) to evaluate a particular region.", "Initialize cascade classifier.", "A file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.", "Accuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.", "Search for the object on multiple scales of input image.", "The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.", "Ndarray that represents the input image.", "The scale by which searching window is multiplied on each step.", "The ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.", "Minimum size of the search window.", "Maximum size of the search window.", "Minimum amount of intersecting detections in order for detection to be approved by the function.", "The minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.", "Dict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window.", "Bases: skimage.feature.util.FeatureDetector, skimage.feature.util.DescriptorExtractor", "Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.", "Number of keypoints to be returned. The function will return the best n_keypoints according to the Harris corner response if more than n_keypoints are detected. If not, then all the detected keypoints are returned.", "The n parameter in skimage.feature.corner_fast. Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.", "The threshold parameter in feature.corner_fast. Threshold used to decide whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.", "The k parameter in skimage.feature.corner_harris. Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.", "Downscale factor for the image pyramid. Default value 1.2 is chosen so that there are more dense scales which enable robust scale invariance for a subsequent feature description.", "Maximum number of scales from the bottom of the image pyramid to extract the features from.", "Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Corresponding orientations in radians.", "Corresponding Harris corner responses.", "2D array of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).", "Initialize self. See help(type(self)) for accurate signature.", "Detect oriented FAST keypoints along with the corresponding scale.", "Input image.", "Detect oriented FAST keypoints and extract rBRIEF descriptors.", "Note that this is faster than first calling detect and then extract.", "Input image.", "Extract rBRIEF binary descriptors for given keypoints in image.", "Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.", "Input image.", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Corresponding orientations in radians."]}, {"name": "feature.blob_dog()", "path": "api/skimage.feature#skimage.feature.blob_dog", "type": "feature", "text": ["Finds blobs in the given grayscale image.", "Blobs are found using the Difference of Gaussian (DoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.", "Input grayscale image, blobs are assumed to be light on dark background (white on black).", "The minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The ratio between the standard deviation of Gaussian Kernels used for computing the Difference of Gaussians", "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.", "A value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.", "If tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.", "A 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.", "See also", "The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image.", "https://en.wikipedia.org/wiki/Blob_detection#The_difference_of_Gaussians_approach"]}, {"name": "feature.blob_doh()", "path": "api/skimage.feature#skimage.feature.blob_doh", "type": "feature", "text": ["Finds blobs in the given grayscale image.", "Blobs are found using the Determinant of Hessian method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian Kernel used for the Hessian matrix whose determinant detected the blob. Determinant of Hessians is approximated using [2].", "Input grayscale image.Blobs can either be light on dark or vice versa.", "The minimum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this low to detect smaller blobs.", "The maximum standard deviation for Gaussian Kernel used to compute Hessian matrix. Keep this high to detect larger blobs.", "The number of intermediate values of standard deviations to consider between min_sigma and max_sigma.", "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect less prominent blobs.", "A value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.", "If set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.", "A 2d array with each row representing 3 values, (y,x,sigma) where (y,x) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel of the Hessian Matrix whose determinant detected the blob.", "The radius of each blob is approximately sigma. Computation of Determinant of Hessians is independent of the standard deviation. Therefore detecting larger blobs won\u2019t take more time. In methods line blob_dog() and blob_log() the computation of Gaussians for larger sigma takes more time. The downside is that this method can\u2019t be used for detecting blobs of radius less than 3px due to the box filters used in the approximation of Hessian Determinant.", "https://en.wikipedia.org/wiki/Blob_detection#The_determinant_of_the_Hessian", "Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf"]}, {"name": "feature.blob_log()", "path": "api/skimage.feature#skimage.feature.blob_log", "type": "feature", "text": ["Finds blobs in the given grayscale image.", "Blobs are found using the Laplacian of Gaussian (LoG) method [1]. For each blob found, the method returns its coordinates and the standard deviation of the Gaussian kernel that detected the blob.", "Input grayscale image, blobs are assumed to be light on dark background (white on black).", "the minimum standard deviation for Gaussian kernel. Keep this low to detect smaller blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The maximum standard deviation for Gaussian kernel. Keep this high to detect larger blobs. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The number of intermediate values of standard deviations to consider between min_sigma and max_sigma.", "The absolute lower bound for scale space maxima. Local maxima smaller than thresh are ignored. Reduce this to detect blobs with less intensities.", "A value between 0 and 1. If the area of two blobs overlaps by a fraction greater than threshold, the smaller blob is eliminated.", "If set intermediate values of standard deviations are interpolated using a logarithmic scale to the base 10. If not, linear interpolation is used.", "If tuple of ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If nonzero int, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If zero or False, peaks are identified regardless of their distance from the border.", "A 2d array with each row representing 2 coordinate values for a 2D image, and 3 coordinate values for a 3D image, plus the sigma(s) used. When a single sigma is passed, outputs are: (r, c, sigma) or (p, r, c, sigma) where (r, c) or (p, r, c) are coordinates of the blob and sigma is the standard deviation of the Gaussian kernel which detected the blob. When an anisotropic gaussian is used (sigmas per dimension), the detected sigma is returned for each dimension.", "The radius of each blob is approximately \\(\\sqrt{2}\\sigma\\) for a 2-D image and \\(\\sqrt{3}\\sigma\\) for a 3-D image.", "https://en.wikipedia.org/wiki/Blob_detection#The_Laplacian_of_Gaussian"]}, {"name": "feature.BRIEF", "path": "api/skimage.feature#skimage.feature.BRIEF", "type": "feature", "text": ["Bases: skimage.feature.util.DescriptorExtractor", "BRIEF binary descriptor extractor.", "BRIEF (Binary Robust Independent Elementary Features) is an efficient feature point descriptor. It is highly discriminative even when using relatively few bits and is computed using simple intensity difference tests.", "For each keypoint, intensity comparisons are carried out for a specifically distributed number N of pixel-pairs resulting in a binary descriptor of length N. For binary descriptors the Hamming distance can be used for feature matching, which leads to lower computational cost in comparison to the L2 norm.", "Size of BRIEF descriptor for each keypoint. Sizes 128, 256 and 512 recommended by the authors. Default is 256.", "Length of the two dimensional square patch sampling region around the keypoints. Default is 49.", "Probability distribution for sampling location of decision pixel-pairs around keypoints.", "Seed for the random sampling of the decision pixel-pairs. From a square window with length patch_size, pixel pairs are sampled using the mode parameter to build the descriptors using intensity comparison. The value of sample_seed must be the same for the images to be matched while building the descriptors.", "Standard deviation of the Gaussian low-pass filter applied to the image to alleviate noise sensitivity, which is strongly recommended to obtain discriminative and good descriptors.", "2D ndarray of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).", "Mask indicating whether a keypoint has been filtered out (False) or is described in the descriptors array (True).", "Initialize self. See help(type(self)) for accurate signature.", "Extract BRIEF binary descriptors for given keypoints in image.", "Input image.", "Keypoint coordinates as (row, col)."]}, {"name": "feature.BRIEF.extract()", "path": "api/skimage.feature#skimage.feature.BRIEF.extract", "type": "feature", "text": ["Extract BRIEF binary descriptors for given keypoints in image.", "Input image.", "Keypoint coordinates as (row, col)."]}, {"name": "feature.BRIEF.__init__()", "path": "api/skimage.feature#skimage.feature.BRIEF.__init__", "type": "feature", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "feature.canny()", "path": "api/skimage.feature#skimage.feature.canny", "type": "feature", "text": ["Edge filter an image using the Canny algorithm.", "Grayscale input image to detect edges on; can be of any dtype.", "Standard deviation of the Gaussian filter.", "Lower bound for hysteresis thresholding (linking edges). If None, low_threshold is set to 10% of dtype\u2019s max.", "Upper bound for hysteresis thresholding (linking edges). If None, high_threshold is set to 20% of dtype\u2019s max.", "Mask to limit the application of Canny to a certain area.", "If True then treat low_threshold and high_threshold as quantiles of the edge magnitude image, rather than absolute edge magnitude values. If True then the thresholds must be in the range [0, 1].", "The binary edge map.", "See also", "The steps of the algorithm are as follows:", "Canny, J., A Computational Approach To Edge Detection, IEEE Trans. Pattern Analysis and Machine Intelligence, 8:679-714, 1986 DOI:10.1109/TPAMI.1986.4767851", "William Green\u2019s Canny tutorial https://en.wikipedia.org/wiki/Canny_edge_detector"]}, {"name": "feature.Cascade", "path": "api/skimage.feature#skimage.feature.Cascade", "type": "feature", "text": ["Bases: object", "Class for cascade of classifiers that is used for object detection.", "The main idea behind cascade of classifiers is to create classifiers of medium accuracy and ensemble them into one strong classifier instead of just creating a strong one. The second advantage of cascade classifier is that easy examples can be classified only by evaluating some of the classifiers in the cascade, making the process much faster than the process of evaluating a one strong classifier.", "Accuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.", "Amount of stages in a cascade. Each cascade consists of stumps i.e. trained features.", "The overall amount of stumps in all the stages of cascade.", "The overall amount of different features used by cascade. Two stumps can use the same features but has different trained values.", "The width of a detection window that is used. Objects smaller than this window can\u2019t be detected.", "The height of a detection window.", "A link to the c array that stores stages information using Stage struct.", "Link to the c array that stores MBLBP features using MBLBP struct.", "The ling to the array with look-up tables that are used by trained MBLBP features (MBLBPStumps) to evaluate a particular region.", "Initialize cascade classifier.", "A file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.", "Accuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases.", "Search for the object on multiple scales of input image.", "The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.", "Ndarray that represents the input image.", "The scale by which searching window is multiplied on each step.", "The ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.", "Minimum size of the search window.", "Maximum size of the search window.", "Minimum amount of intersecting detections in order for detection to be approved by the function.", "The minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.", "Dict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window."]}, {"name": "feature.Cascade.detect_multi_scale()", "path": "api/skimage.feature#skimage.feature.Cascade.detect_multi_scale", "type": "feature", "text": ["Search for the object on multiple scales of input image.", "The function takes the input image, the scale factor by which the searching window is multiplied on each step, minimum window size and maximum window size that specify the interval for the search windows that are applied to the input image to detect objects.", "Ndarray that represents the input image.", "The scale by which searching window is multiplied on each step.", "The ratio by which the search step in multiplied on each scale of the image. 1 represents the exaustive search and usually is slow. By setting this parameter to higher values the results will be worse but the computation will be much faster. Usually, values in the interval [1, 1.5] give good results.", "Minimum size of the search window.", "Maximum size of the search window.", "Minimum amount of intersecting detections in order for detection to be approved by the function.", "The minimum value of value of ratio (intersection area) / (small rectangle ratio) in order to merge two detections into one.", "Dict have form {\u2018r\u2019: int, \u2018c\u2019: int, \u2018width\u2019: int, \u2018height\u2019: int}, where \u2018r\u2019 represents row position of top left corner of detected window, \u2018c\u2019 - col position, \u2018width\u2019 - width of detected window, \u2018height\u2019 - height of detected window."]}, {"name": "feature.Cascade.eps", "path": "api/skimage.feature#skimage.feature.Cascade.eps", "type": "feature", "text": []}, {"name": "feature.Cascade.features_number", "path": "api/skimage.feature#skimage.feature.Cascade.features_number", "type": "feature", "text": []}, {"name": "feature.Cascade.stages_number", "path": "api/skimage.feature#skimage.feature.Cascade.stages_number", "type": "feature", "text": []}, {"name": "feature.Cascade.stumps_number", "path": "api/skimage.feature#skimage.feature.Cascade.stumps_number", "type": "feature", "text": []}, {"name": "feature.Cascade.window_height", "path": "api/skimage.feature#skimage.feature.Cascade.window_height", "type": "feature", "text": []}, {"name": "feature.Cascade.window_width", "path": "api/skimage.feature#skimage.feature.Cascade.window_width", "type": "feature", "text": []}, {"name": "feature.Cascade.__init__()", "path": "api/skimage.feature#skimage.feature.Cascade.__init__", "type": "feature", "text": ["Initialize cascade classifier.", "A file in a OpenCv format from which all the cascade classifier\u2019s parameters are loaded.", "Accuracy parameter. Increasing it, makes the classifier detect less false positives but at the same time the false negative score increases."]}, {"name": "feature.CENSURE", "path": "api/skimage.feature#skimage.feature.CENSURE", "type": "feature", "text": ["Bases: skimage.feature.util.FeatureDetector", "CENSURE keypoint detector.", "Minimum scale to extract keypoints from.", "Maximum scale to extract keypoints from. The keypoints will be extracted from all the scales except the first and the last i.e. from the scales in the range [min_scale + 1, max_scale - 1]. The filter sizes for different scales is such that the two adjacent scales comprise of an octave.", "Type of bi-level filter used to get the scales of the input image. Possible values are \u2018DoB\u2019, \u2018Octagon\u2019 and \u2018STAR\u2019. The three modes represent the shape of the bi-level filters i.e. box(square), octagon and star respectively. For instance, a bi-level octagon filter consists of a smaller inner octagon and a larger outer octagon with the filter weights being uniformly negative in both the inner octagon while uniformly positive in the difference region. Use STAR and Octagon for better features and DoB for better performance.", "Threshold value used to suppress maximas and minimas with a weak magnitude response obtained after Non-Maximal Suppression.", "Threshold for rejecting interest points which have ratio of principal curvatures greater than this value.", "Motilal Agrawal, Kurt Konolige and Morten Rufus Blas \u201cCENSURE: Center Surround Extremas for Realtime Feature Detection and Matching\u201d, https://link.springer.com/chapter/10.1007/978-3-540-88693-8_8 DOI:10.1007/978-3-540-88693-8_8", "Adam Schmidt, Marek Kraft, Michal Fularz and Zuzanna Domagala \u201cComparative Assessment of Point Feature Detectors and Descriptors in the Context of Robot Navigation\u201d http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.baztech-268aaf28-0faf-4872-a4df-7e2e61cb364c/c/Schmidt_comparative.pdf DOI:10.1.1.465.1117", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Initialize self. See help(type(self)) for accurate signature.", "Detect CENSURE keypoints along with the corresponding scale.", "Input image."]}, {"name": "feature.CENSURE.detect()", "path": "api/skimage.feature#skimage.feature.CENSURE.detect", "type": "feature", "text": ["Detect CENSURE keypoints along with the corresponding scale.", "Input image."]}, {"name": "feature.CENSURE.__init__()", "path": "api/skimage.feature#skimage.feature.CENSURE.__init__", "type": "feature", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "feature.corner_fast()", "path": "api/skimage.feature#skimage.feature.corner_fast", "type": "feature", "text": ["Extract FAST corners for a given image.", "Input image.", "Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t testpixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.", "Threshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.", "FAST corner response image.", "Rosten, E., & Drummond, T. (2006, May). Machine learning for high-speed corner detection. In European conference on computer vision (pp. 430-443). Springer, Berlin, Heidelberg. DOI:10.1007/11744023_34 http://www.edwardrosten.com/work/rosten_2006_machine.pdf", "Wikipedia, \u201cFeatures from accelerated segment test\u201d, https://en.wikipedia.org/wiki/Features_from_accelerated_segment_test"]}, {"name": "feature.corner_foerstner()", "path": "api/skimage.feature#skimage.feature.corner_foerstner", "type": "feature", "text": ["Compute Foerstner corner measure response image.", "This corner detector uses information from the auto-correlation matrix A:", "Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as:", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "Error ellipse sizes.", "Roundness of error ellipse.", "F\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf", "https://en.wikipedia.org/wiki/Corner_detection"]}, {"name": "feature.corner_harris()", "path": "api/skimage.feature#skimage.feature.corner_harris", "type": "feature", "text": ["Compute Harris corner measure response image.", "This corner detector uses information from the auto-correlation matrix A:", "Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as:", "or:", "Input image.", "Method to compute the response image from the auto-correlation matrix.", "Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.", "Normalisation factor (Noble\u2019s corner measure).", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "Harris response image.", "https://en.wikipedia.org/wiki/Corner_detection"]}, {"name": "feature.corner_kitchen_rosenfeld()", "path": "api/skimage.feature#skimage.feature.corner_kitchen_rosenfeld", "type": "feature", "text": ["Compute Kitchen and Rosenfeld corner measure response image.", "The corner measure is calculated as follows:", "Where imx and imy are the first and imxx, imxy, imyy the second derivatives.", "Input image.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Kitchen and Rosenfeld response image.", "Kitchen, L., & Rosenfeld, A. (1982). Gray-level corner detection. Pattern recognition letters, 1(2), 95-102. DOI:10.1016/0167-8655(82)90020-4"]}, {"name": "feature.corner_moravec()", "path": "api/skimage.feature#skimage.feature.corner_moravec", "type": "feature", "text": ["Compute Moravec corner measure response image.", "This is one of the simplest corner detectors and is comparatively fast but has several limitations (e.g. not rotation invariant).", "Input image.", "Window size.", "Moravec response image.", "https://en.wikipedia.org/wiki/Corner_detection"]}, {"name": "feature.corner_orientations()", "path": "api/skimage.feature#skimage.feature.corner_orientations", "type": "feature", "text": ["Compute the orientation of corners.", "The orientation of corners is computed using the first order central moment i.e. the center of mass approach. The corner orientation is the angle of the vector from the corner coordinate to the intensity centroid in the local neighborhood around the corner calculated using first order central moment.", "Input grayscale image.", "Corner coordinates as (row, col).", "Mask defining the local neighborhood of the corner used for the calculation of the central moment.", "Orientations of corners in the range [-pi, pi].", "Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB : An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf", "Paul L. Rosin, \u201cMeasuring Corner Properties\u201d http://users.cs.cf.ac.uk/Paul.Rosin/corner2.pdf"]}, {"name": "feature.corner_peaks()", "path": "api/skimage.feature#skimage.feature.corner_peaks", "type": "feature", "text": ["Find peaks in corner measure response image.", "This differs from skimage.feature.peak_local_max in that it suppresses multiple connected peaks with the same accumulator value.", "Input image.", "The minimal allowed distance separating peaks.", "See skimage.feature.peak_local_max().", "Which Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.", "See also", "Changed in version 0.18: The default value of threshold_rel has changed to None, which corresponds to letting skimage.feature.peak_local_max decide on the default. This is equivalent to threshold_rel=0.", "The num_peaks limit is applied before suppression of connected peaks. To limit the number of peaks after suppression, set num_peaks=np.inf and post-process the output of this function."]}, {"name": "feature.corner_shi_tomasi()", "path": "api/skimage.feature#skimage.feature.corner_shi_tomasi", "type": "feature", "text": ["Compute Shi-Tomasi (Kanade-Tomasi) corner measure response image.", "This corner detector uses information from the auto-correlation matrix A:", "Where imx and imy are first derivatives, averaged with a gaussian filter. The corner measure is then defined as the smaller eigenvalue of A:", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "Shi-Tomasi response image.", "https://en.wikipedia.org/wiki/Corner_detection"]}, {"name": "feature.corner_subpix()", "path": "api/skimage.feature#skimage.feature.corner_subpix", "type": "feature", "text": ["Determine subpixel position of corners.", "A statistical test decides whether the corner is defined as the intersection of two edges or a single peak. Depending on the classification result, the subpixel corner location is determined based on the local covariance of the grey-values. If the significance level for either statistical test is not sufficient, the corner cannot be classified, and the output subpixel position is set to NaN.", "Input image.", "Corner coordinates (row, col).", "Search window size for subpixel estimation.", "Significance level for corner classification.", "Subpixel corner positions. NaN for \u201cnot classified\u201d corners.", "F\u00f6rstner, W., & G\u00fclch, E. (1987, June). A fast operator for detection and precise location of distinct points, corners and centres of circular features. In Proc. ISPRS intercommission conference on fast processing of photogrammetric data (pp. 281-305). https://cseweb.ucsd.edu/classes/sp02/cse252/foerstner/foerstner.pdf", "https://en.wikipedia.org/wiki/Corner_detection"]}, {"name": "feature.daisy()", "path": "api/skimage.feature#skimage.feature.daisy", "type": "feature", "text": ["Extract DAISY feature descriptors densely for the given image.", "DAISY is a feature descriptor similar to SIFT formulated in a way that allows for fast dense extraction. Typically, this is practical for bag-of-features image representations.", "The implementation follows Tola et al. [1] but deviate on the following points:", "Input image (grayscale).", "Distance between descriptor sampling points.", "Radius (in pixels) of the outermost ring.", "Number of rings.", "Number of histograms sampled per ring.", "Number of orientations (bins) per histogram.", "How to normalize the descriptors", "Standard deviation of spatial Gaussian smoothing for the center histogram and for each ring of histograms. The array of sigmas should be sorted from the center and out. I.e. the first sigma value defines the spatial smoothing of the center histogram and the last sigma value defines the spatial smoothing of the outermost ring. Specifying sigmas overrides the following parameter.", "rings = len(sigmas) - 1", "Radius (in pixels) for each ring. Specifying ring_radii overrides the following two parameters.", "rings = len(ring_radii) radius = ring_radii[-1]", "If both sigmas and ring_radii are given, they must satisfy the following predicate since no radius is needed for the center histogram.", "len(ring_radii) == len(sigmas) + 1", "Generate a visualization of the DAISY descriptors", "Grid of DAISY descriptors for the given image as an array dimensionality (P, Q, R) where", "P = ceil((M - radius*2) / step) Q = ceil((N - radius*2) / step) R = (rings * histograms + 1) * orientations", "Visualization of the DAISY descriptors.", "Tola et al. \u201cDaisy: An efficient dense descriptor applied to wide- baseline stereo.\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.5 (2010): 815-830.", "http://cvlab.epfl.ch/software/daisy"]}, {"name": "feature.draw_haar_like_feature()", "path": "api/skimage.feature#skimage.feature.draw_haar_like_feature", "type": "feature", "text": ["Visualization of Haar-like features.", "The region of an integral image for which the features need to be computed.", "Row-coordinate of top left corner of the detection window.", "Column-coordinate of top left corner of the detection window.", "Width of the detection window.", "Height of the detection window.", "The array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.", "Floats specifying the color for the positive block. Corresponding values define (R, G, B) values. Default value is red (1, 0, 0).", "Floats specifying the color for the negative block Corresponding values define (R, G, B) values. Default value is blue (0, 1, 0).", "Value in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.", "The maximum number of features to be returned. By default, all features are returned.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used when generating a set of features smaller than the total number of available features.", "An image in which the different features will be added."]}, {"name": "feature.draw_multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.draw_multiblock_lbp", "type": "feature", "text": ["Multi-block local binary pattern visualization.", "Blocks with higher sums are colored with alpha-blended white rectangles, whereas blocks with lower sums are colored alpha-blended cyan. Colors and the alpha parameter can be changed.", "Image on which to visualize the pattern.", "Row-coordinate of top left corner of a rectangle containing feature.", "Column-coordinate of top left corner of a rectangle containing feature.", "Width of one of 9 equal rectangles that will be used to compute a feature.", "Height of one of 9 equal rectangles that will be used to compute a feature.", "The descriptor of feature to visualize. If not provided, the descriptor with 0 value will be used.", "Floats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is white (1, 1, 1).", "Floats specifying the color for the block that has greater intensity value. They should be in the range [0, 1]. Corresponding values define (R, G, B) values. Default value is cyan (0, 0.69, 0.96).", "Value in the range [0, 1] that specifies opacity of visualization. 1 - fully transparent, 0 - opaque.", "Image with MB-LBP visualization.", "Face Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf"]}, {"name": "feature.greycomatrix()", "path": "api/skimage.feature#skimage.feature.greycomatrix", "type": "feature", "text": ["Calculate the grey-level co-occurrence matrix.", "A grey level co-occurrence matrix is a histogram of co-occurring greyscale values at a given offset over an image.", "Integer typed input image. Only positive valued images are supported. If type is other than uint8, the argument levels needs to be set.", "List of pixel pair distance offsets.", "List of pixel pair angles in radians.", "The input image should contain integers in [0, levels-1], where levels indicate the number of grey-levels counted (typically 256 for an 8-bit image). This argument is required for 16-bit images or higher and is typically the maximum of the image. As the output matrix is at least levels x levels, it might be preferable to use binning of the input image rather than large values for levels.", "If True, the output matrix P[:, :, d, theta] is symmetric. This is accomplished by ignoring the order of value pairs, so both (i, j) and (j, i) are accumulated when (i, j) is encountered for a given offset. The default is False.", "If True, normalize each matrix P[:, :, d, theta] by dividing by the total number of accumulated co-occurrences for the given offset. The elements of the resulting matrix sum to 1. The default is False.", "The grey-level co-occurrence histogram. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i. If normed is False, the output is of type uint32, otherwise it is float64. The dimensions are: levels x levels x number of distances x number of angles.", "The GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm", "Haralick, RM.; Shanmugam, K., \u201cTextural features for image classification\u201d IEEE Transactions on systems, man, and cybernetics 6 (1973): 610-621. DOI:10.1109/TSMC.1973.4309314", "Pattern Recognition Engineering, Morton Nadler & Eric P. Smith", "Wikipedia, https://en.wikipedia.org/wiki/Co-occurrence_matrix", "Compute 2 GLCMs: One for a 1-pixel offset to the right, and one for a 1-pixel offset upwards."]}, {"name": "feature.greycoprops()", "path": "api/skimage.feature#skimage.feature.greycoprops", "type": "feature", "text": ["Calculate texture properties of a GLCM.", "Compute a feature of a grey level co-occurrence matrix to serve as a compact summary of the matrix. The properties are computed as follows:", "Each GLCM is normalized to have a sum of 1 before the computation of texture properties.", "Input array. P is the grey-level co-occurrence histogram for which to compute the specified property. The value P[i,j,d,theta] is the number of times that grey-level j occurs at a distance d and at an angle theta from grey-level i.", "The property of the GLCM to compute. The default is \u2018contrast\u2019.", "2-dimensional array. results[d, a] is the property \u2018prop\u2019 for the d\u2019th distance and the a\u2019th angle.", "The GLCM Tutorial Home Page, http://www.fp.ucalgary.ca/mhallbey/tutorial.htm", "Compute the contrast for GLCMs with distances [1, 2] and angles [0 degrees, 90 degrees]"]}, {"name": "feature.haar_like_feature()", "path": "api/skimage.feature#skimage.feature.haar_like_feature", "type": "feature", "text": ["Compute the Haar-like features for a region of interest (ROI) of an integral image.", "Haar-like features have been successfully used for image classification and object detection [1]. It has been used for real-time face detection algorithm proposed in [2].", "Integral image for which the features need to be computed.", "Row-coordinate of top left corner of the detection window.", "Column-coordinate of top left corner of the detection window.", "Width of the detection window.", "Height of the detection window.", "The type of feature to consider:", "By default all features are extracted.", "If using with feature_coord, it should correspond to the feature type of each associated coordinate feature.", "The array of coordinates to be extracted. This is useful when you want to recompute only a subset of features. In this case feature_type needs to be an array containing the type of each feature, as returned by haar_like_feature_coord(). By default, all coordinates are computed.", "Resulting Haar-like features. Each value is equal to the subtraction of sums of the positive and negative rectangles. The data type depends of the data type of int_image: int when the data type of int_image is uint or int and float when the data type of int_image is float.", "When extracting those features in parallel, be aware that the choice of the backend (i.e. multiprocessing vs threading) will have an impact on the performance. The rule of thumb is as follows: use multiprocessing when extracting features for all possible ROI in an image; use threading when extracting the feature at specific location for a limited number of ROIs. Refer to the example Face classification using Haar-like feature descriptor for more insights.", "https://en.wikipedia.org/wiki/Haar-like_feature", "Oren, M., Papageorgiou, C., Sinha, P., Osuna, E., & Poggio, T. (1997, June). Pedestrian detection using wavelet templates. In Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on (pp. 193-199). IEEE. http://tinyurl.com/y6ulxfta DOI:10.1109/CVPR.1997.609319", "Viola, Paul, and Michael J. Jones. \u201cRobust real-time face detection.\u201d International journal of computer vision 57.2 (2004): 137-154. https://www.merl.com/publications/docs/TR2004-043.pdf DOI:10.1109/CVPR.2001.990517", "You can compute the feature for some pre-computed coordinates."]}, {"name": "feature.haar_like_feature_coord()", "path": "api/skimage.feature#skimage.feature.haar_like_feature_coord", "type": "feature", "text": ["Compute the coordinates of Haar-like features.", "Width of the detection window.", "Height of the detection window.", "The type of feature to consider:", "By default all features are extracted.", "Coordinates of the rectangles for each feature.", "The corresponding type for each feature."]}, {"name": "feature.hessian_matrix()", "path": "api/skimage.feature#skimage.feature.hessian_matrix", "type": "feature", "text": ["Compute Hessian matrix.", "The Hessian matrix is defined as:", "which is computed by convolving the image with the second derivatives of the Gaussian kernel in the respective r- and c-directions.", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as weighting function for the auto-correlation matrix.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Hrr, Hrc, Hcc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Hxx, Hxy, Hyy)", "Element of the Hessian matrix for each pixel in the input image.", "Element of the Hessian matrix for each pixel in the input image.", "Element of the Hessian matrix for each pixel in the input image."]}, {"name": "feature.hessian_matrix_det()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_det", "type": "feature", "text": ["Compute the approximate Hessian Determinant over an image.", "The 2D approximate method uses box filters over integral images to compute the approximate Hessian Determinant, as described in [1].", "The image over which to compute Hessian Determinant.", "Standard deviation used for the Gaussian kernel, used for the Hessian matrix.", "If True and the image is 2D, use a much faster approximate computation. This argument has no effect on 3D and higher images.", "The array of the Determinant of Hessians.", "For 2D images when approximate=True, the running time of this method only depends on size of the image. It is independent of sigma as one would expect. The downside is that the result for sigma less than 3 is not accurate, i.e., not similar to the result obtained if someone computed the Hessian and took its determinant.", "Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \u201cSURF: Speeded Up Robust Features\u201d ftp://ftp.vision.ee.ethz.ch/publications/articles/eth_biwi_00517.pdf"]}, {"name": "feature.hessian_matrix_eigvals()", "path": "api/skimage.feature#skimage.feature.hessian_matrix_eigvals", "type": "feature", "text": ["Compute eigenvalues of Hessian matrix.", "The upper-diagonal elements of the Hessian matrix, as returned by hessian_matrix.", "The eigenvalues of the Hessian matrix, in decreasing order. The eigenvalues are the leading dimension. That is, eigs[i, j, k] contains the ith-largest eigenvalue at position (j, k)."]}, {"name": "feature.hog()", "path": "api/skimage.feature#skimage.feature.hog", "type": "feature", "text": ["Extract Histogram of Oriented Gradients (HOG) for a given image.", "Compute a Histogram of Oriented Gradients (HOG) by", "Input image.", "Number of orientation bins.", "Size (in pixels) of a cell.", "Number of cells in each block.", "Block normalization method:", "Normalization using L1-norm.", "Normalization using L1-norm, followed by square root.", "Normalization using L2-norm.", "Normalization using L2-norm, followed by limiting the maximum values to 0.2 (Hys stands for hysteresis) and renormalization using L2-norm. (default) For details, see [3], [4].", "Also return an image of the HOG. For each cell and orientation bin, the image contains a line segment that is centered at the cell center, is perpendicular to the midpoint of the range of angles spanned by the orientation bin, and has intensity proportional to the corresponding histogram value.", "Apply power law compression to normalize the image before processing. DO NOT use this if the image contains negative values. Also see notes section below.", "Return the data as a feature vector by calling .ravel() on the result just before returning.", "If True, the last image dimension is considered as a color channel, otherwise as spatial.", "HOG descriptor for the image. If feature_vector is True, a 1D (flattened) array is returned.", "A visualisation of the HOG image. Only provided if visualize is True.", "The presented code implements the HOG extraction method from [2] with the following changes: (I) blocks of (3, 3) cells are used ((2, 2) in the paper); (II) no smoothing within cells (Gaussian spatial window with sigma=8pix in the paper); (III) L1 block normalization is used (L2-Hys in the paper).", "Power law compression, also known as Gamma correction, is used to reduce the effects of shadowing and illumination variations. The compression makes the dark regions lighter. When the kwarg transform_sqrt is set to True, the function computes the square root of each color channel and then applies the hog algorithm to the image.", "https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients", "Dalal, N and Triggs, B, Histograms of Oriented Gradients for Human Detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2005 San Diego, CA, USA, https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf, DOI:10.1109/CVPR.2005.177", "Lowe, D.G., Distinctive image features from scale-invatiant keypoints, International Journal of Computer Vision (2004) 60: 91, http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf, DOI:10.1023/B:VISI.0000029664.99615.94", "Dalal, N, Finding People in Images and Videos, Human-Computer Interaction [cs.HC], Institut National Polytechnique de Grenoble - INPG, 2006, https://tel.archives-ouvertes.fr/tel-00390303/file/NavneetDalalThesis.pdf"]}, {"name": "feature.local_binary_pattern()", "path": "api/skimage.feature#skimage.feature.local_binary_pattern", "type": "feature", "text": ["Gray scale and rotation invariant LBP (Local Binary Patterns).", "LBP is an invariant descriptor that can be used for texture classification.", "Graylevel image.", "Number of circularly symmetric neighbour set points (quantization of the angular space).", "Radius of circle (spatial resolution of the operator).", "Method to determine the pattern.", "rotation invariant.", "rotation invariant.", "finer quantization of the angular space which is gray scale and rotation invariant.", "which is only gray scale invariant [2].", "image texture which is rotation but not gray scale invariant.", "LBP image.", "Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns. Timo Ojala, Matti Pietikainen, Topi Maenpaa. http://www.ee.oulu.fi/research/mvmp/mvg/files/pdf/pdf_94.pdf, 2002.", "Face recognition with local binary patterns. Timo Ahonen, Abdenour Hadid, Matti Pietikainen, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.214.6851, 2004."]}, {"name": "feature.masked_register_translation()", "path": "api/skimage.feature#skimage.feature.masked_register_translation", "type": "feature", "text": ["Deprecated function. Use skimage.registration.phase_cross_correlation instead."]}, {"name": "feature.match_descriptors()", "path": "api/skimage.feature#skimage.feature.match_descriptors", "type": "feature", "text": ["Brute-force matching of descriptors.", "For each descriptor in the first set this matcher finds the closest descriptor in the second set (and vice-versa in the case of enabled cross-checking).", "Descriptors of size P about M keypoints in the first image.", "Descriptors of size P about N keypoints in the second image.", "The metric to compute the distance between two descriptors. See scipy.spatial.distance.cdist for all possible types. The hamming distance should be used for binary descriptors. By default the L2-norm is used for all descriptors of dtype float or double and the Hamming distance is used for binary descriptors automatically.", "The p-norm to apply for metric='minkowski'.", "Maximum allowed distance between descriptors of two keypoints in separate images to be regarded as a match.", "If True, the matched keypoints are returned after cross checking i.e. a matched pair (keypoint1, keypoint2) is returned if keypoint2 is the best match for keypoint1 in second image and keypoint1 is the best match for keypoint2 in first image.", "Maximum ratio of distances between first and second closest descriptor in the second set of descriptors. This threshold is useful to filter ambiguous matches between the two descriptor sets. The choice of this value depends on the statistics of the chosen descriptor, e.g., for SIFT descriptors a value of 0.8 is usually chosen, see D.G. Lowe, \u201cDistinctive Image Features from Scale-Invariant Keypoints\u201d, International Journal of Computer Vision, 2004.", "Indices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors."]}, {"name": "feature.match_template()", "path": "api/skimage.feature#skimage.feature.match_template", "type": "feature", "text": ["Match a template to a 2-D or 3-D image using normalized correlation.", "The output is an array with values between -1.0 and 1.0. The value at a given position corresponds to the correlation coefficient between the image and the template.", "For pad_input=True matches correspond to the center and otherwise to the top-left corner of the template. To find the best match you must search for peaks in the response (output) image.", "2-D or 3-D input image.", "Template to locate. It must be (m <= M, n <= N[, d <= D]).", "If True, pad image so that output is the same size as the image, and output values correspond to the template center. Otherwise, the output is an array with shape (M - m + 1, N - n + 1) for an (M, N) image and an (m, n) template, and matches correspond to origin (top-left corner) of the template.", "Padding mode.", "Constant values used in conjunction with mode='constant'.", "Response image with correlation coefficients.", "Details on the cross-correlation are presented in [1]. This implementation uses FFT convolutions of the image and the template. Reference [2] presents similar derivations but the approximation presented in this reference is not used in our implementation.", "J. P. Lewis, \u201cFast Normalized Cross-Correlation\u201d, Industrial Light and Magic.", "Briechle and Hanebeck, \u201cTemplate Matching using Fast Normalized Cross Correlation\u201d, Proceedings of the SPIE (2001). DOI:10.1117/12.421129"]}, {"name": "feature.multiblock_lbp()", "path": "api/skimage.feature#skimage.feature.multiblock_lbp", "type": "feature", "text": ["Multi-block local binary pattern (MB-LBP).", "The features are calculated similarly to local binary patterns (LBPs), (See local_binary_pattern()) except that summed blocks are used instead of individual pixel values.", "MB-LBP is an extension of LBP that can be computed on multiple scales in constant time using the integral image. Nine equally-sized rectangles are used to compute a feature. For each rectangle, the sum of the pixel intensities is computed. Comparisons of these sums to that of the central rectangle determine the feature, similarly to LBP.", "Integral image.", "Row-coordinate of top left corner of a rectangle containing feature.", "Column-coordinate of top left corner of a rectangle containing feature.", "Width of one of the 9 equal rectangles that will be used to compute a feature.", "Height of one of the 9 equal rectangles that will be used to compute a feature.", "8-bit MB-LBP feature descriptor.", "Face Detection Based on Multi-Block LBP Representation. Lun Zhang, Rufeng Chu, Shiming Xiang, Shengcai Liao, Stan Z. Li http://www.cbsr.ia.ac.cn/users/scliao/papers/Zhang-ICB07-MBLBP.pdf"]}, {"name": "feature.multiscale_basic_features()", "path": "api/skimage.feature#skimage.feature.multiscale_basic_features", "type": "feature", "text": ["Local features for a single- or multi-channel nd image.", "Intensity, gradient intensity and local structure are computed at different scales thanks to Gaussian blurring.", "Input image, which can be grayscale or multichannel.", "True if the last dimension corresponds to color channels.", "If True, pixel intensities averaged over the different scales are added to the feature set.", "If True, intensities of local gradients averaged over the different scales are added to the feature set.", "If True, eigenvalues of the Hessian matrix after Gaussian blurring at different scales are added to the feature set.", "Smallest value of the Gaussian kernel used to average local neighbourhoods before extracting features.", "Largest value of the Gaussian kernel used to average local neighbourhoods before extracting features.", "Number of values of the Gaussian kernel between sigma_min and sigma_max. If None, sigma_min multiplied by powers of 2 are used.", "The number of parallel threads to use. If set to None, the full set of available cores are used.", "Array of shape image.shape + (n_features,)"]}, {"name": "feature.ORB", "path": "api/skimage.feature#skimage.feature.ORB", "type": "feature", "text": ["Bases: skimage.feature.util.FeatureDetector, skimage.feature.util.DescriptorExtractor", "Oriented FAST and rotated BRIEF feature detector and binary descriptor extractor.", "Number of keypoints to be returned. The function will return the best n_keypoints according to the Harris corner response if more than n_keypoints are detected. If not, then all the detected keypoints are returned.", "The n parameter in skimage.feature.corner_fast. Minimum number of consecutive pixels out of 16 pixels on the circle that should all be either brighter or darker w.r.t test-pixel. A point c on the circle is darker w.r.t test pixel p if Ic < Ip - threshold and brighter if Ic > Ip + threshold. Also stands for the n in FAST-n corner detector.", "The threshold parameter in feature.corner_fast. Threshold used to decide whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.", "The k parameter in skimage.feature.corner_harris. Sensitivity factor to separate corners from edges, typically in range [0, 0.2]. Small values of k result in detection of sharp corners.", "Downscale factor for the image pyramid. Default value 1.2 is chosen so that there are more dense scales which enable robust scale invariance for a subsequent feature description.", "Maximum number of scales from the bottom of the image pyramid to extract the features from.", "Ethan Rublee, Vincent Rabaud, Kurt Konolige and Gary Bradski \u201cORB: An efficient alternative to SIFT and SURF\u201d http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Corresponding orientations in radians.", "Corresponding Harris corner responses.", "2D array of binary descriptors of size descriptor_size for Q keypoints after filtering out border keypoints with value at an index (i, j) either being True or False representing the outcome of the intensity comparison for i-th keypoint on j-th decision pixel-pair. It is Q == np.sum(mask).", "Initialize self. See help(type(self)) for accurate signature.", "Detect oriented FAST keypoints along with the corresponding scale.", "Input image.", "Detect oriented FAST keypoints and extract rBRIEF descriptors.", "Note that this is faster than first calling detect and then extract.", "Input image.", "Extract rBRIEF binary descriptors for given keypoints in image.", "Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.", "Input image.", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Corresponding orientations in radians."]}, {"name": "feature.ORB.detect()", "path": "api/skimage.feature#skimage.feature.ORB.detect", "type": "feature", "text": ["Detect oriented FAST keypoints along with the corresponding scale.", "Input image."]}, {"name": "feature.ORB.detect_and_extract()", "path": "api/skimage.feature#skimage.feature.ORB.detect_and_extract", "type": "feature", "text": ["Detect oriented FAST keypoints and extract rBRIEF descriptors.", "Note that this is faster than first calling detect and then extract.", "Input image."]}, {"name": "feature.ORB.extract()", "path": "api/skimage.feature#skimage.feature.ORB.extract", "type": "feature", "text": ["Extract rBRIEF binary descriptors for given keypoints in image.", "Note that the keypoints must be extracted using the same downscale and n_scales parameters. Additionally, if you want to extract both keypoints and descriptors you should use the faster detect_and_extract.", "Input image.", "Keypoint coordinates as (row, col).", "Corresponding scales.", "Corresponding orientations in radians."]}, {"name": "feature.ORB.__init__()", "path": "api/skimage.feature#skimage.feature.ORB.__init__", "type": "feature", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "feature.peak_local_max()", "path": "api/skimage.feature#skimage.feature.peak_local_max", "type": "feature", "text": ["Find peaks in an image as coordinate list or boolean mask.", "Peaks are the local maxima in a region of 2 * min_distance + 1 (i.e. peaks are separated by at least min_distance).", "If both threshold_abs and threshold_rel are provided, the maximum of the two is chosen as the minimum intensity threshold of peaks.", "Changed in version 0.18: Prior to version 0.18, peaks of the same height within a radius of min_distance were all returned, but this could cause unexpected behaviour. From 0.18 onwards, an arbitrary peak within the region is returned. See issue gh-2592.", "Input image.", "The minimal allowed distance separating peaks. To find the maximum number of peaks, use min_distance=1.", "Minimum intensity of peaks. By default, the absolute threshold is the minimum intensity of the image.", "Minimum intensity of peaks, calculated as max(image) * threshold_rel.", "If positive integer, exclude_border excludes peaks from within exclude_border-pixels of the border of the image. If tuple of non-negative ints, the length of the tuple must match the input array\u2019s dimensionality. Each element of the tuple will exclude peaks from within exclude_border-pixels of the border of the image along that dimension. If True, takes the min_distance parameter as value. If zero or False, peaks are identified regardless of their distance from the border.", "If True, the output will be an array representing peak coordinates. The coordinates are sorted according to peaks values (Larger first). If False, the output will be a boolean array shaped as image.shape with peaks present at True elements. indices is deprecated and will be removed in version 0.20. Default behavior will be to always return peak coordinates. You can obtain a mask as shown in the example below.", "Maximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks peaks based on highest peak intensity.", "If provided, footprint == 1 represents the local region within which to search for peaks at every point in image.", "If provided, each unique region labels == value represents a unique region to search for peaks. Zero is reserved for background.", "Maximum number of peaks for each label.", "Which Minkowski p-norm to use. Should be in the range [1, inf]. A finite large p may cause a ValueError if overflow can occur. inf corresponds to the Chebyshev distance and 2 to the Euclidean distance.", "See also", "The peak local maximum function returns the coordinates of local peaks (maxima) in an image. Internally, a maximum filter is used for finding local maxima. This operation dilates the original image. After comparison of the dilated and original image, this function returns the coordinates or a mask of the peaks where the dilated image equals the original image."]}, {"name": "feature.plot_matches()", "path": "api/skimage.feature#skimage.feature.plot_matches", "type": "feature", "text": ["Plot matched features.", "Matches and image are drawn in this ax.", "First grayscale or color image.", "Second grayscale or color image.", "First keypoint coordinates as (row, col).", "Second keypoint coordinates as (row, col).", "Indices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in the first and matches[:, 1] the indices in the second set of descriptors.", "Color for keypoint locations.", "Color for lines which connect keypoint matches. By default the color is chosen randomly.", "Whether to only plot matches and not plot the keypoint locations.", "Whether to show images side by side, 'horizontal', or one above the other, 'vertical'."]}, {"name": "feature.register_translation()", "path": "api/skimage.feature#skimage.feature.register_translation", "type": "feature", "text": ["Deprecated function. Use skimage.registration.phase_cross_correlation instead."]}, {"name": "feature.shape_index()", "path": "api/skimage.feature#skimage.feature.shape_index", "type": "feature", "text": ["Compute the shape index.", "The shape index, as defined by Koenderink & van Doorn [1], is a single valued measure of local curvature, assuming the image as a 3D plane with intensities representing heights.", "It is derived from the eigen values of the Hessian, and its value ranges from -1 to 1 (and is undefined (=NaN) in flat regions), with following ranges representing following shapes:", "Interval (s in \u2026)", "Shape", "[ -1, -7/8)", "Spherical cup", "[-7/8, -5/8)", "Through", "[-5/8, -3/8)", "Rut", "[-3/8, -1/8)", "Saddle rut", "[-1/8, +1/8)", "Saddle", "[+1/8, +3/8)", "Saddle ridge", "[+3/8, +5/8)", "Ridge", "[+5/8, +7/8)", "Dome", "[+7/8, +1]", "Spherical cap", "Input image.", "Standard deviation used for the Gaussian kernel, which is used for smoothing the input data before Hessian eigen value calculation.", "How to handle values outside the image borders", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Shape index", "Koenderink, J. J. & van Doorn, A. J., \u201cSurface shape and curvature scales\u201d, Image and Vision Computing, 1992, 10, 557-564. DOI:10.1016/0262-8856(92)90076-F"]}, {"name": "feature.structure_tensor()", "path": "api/skimage.feature#skimage.feature.structure_tensor", "type": "feature", "text": ["Compute structure tensor using sum of squared differences.", "The (2-dimensional) structure tensor A is defined as:", "which is approximated by the weighted sum of squared differences in a local window around each pixel in the image. This formula can be extended to a larger number of dimensions (see [1]).", "Input image.", "Standard deviation used for the Gaussian kernel, which is used as a weighting function for the local summation of squared differences.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "NOTE: Only applies in 2D. Higher dimensions must always use \u2018rc\u2019 order. This parameter allows for the use of reverse or forward order of the image axes in gradient computation. \u2018rc\u2019 indicates the use of the first axis initially (Arr, Arc, Acc), whilst \u2018xy\u2019 indicates the usage of the last axis initially (Axx, Axy, Ayy).", "Upper-diagonal elements of the structure tensor for each pixel in the input image.", "See also", "https://en.wikipedia.org/wiki/Structure_tensor"]}, {"name": "feature.structure_tensor_eigenvalues()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigenvalues", "type": "feature", "text": ["Compute eigenvalues of structure tensor.", "The upper-diagonal elements of the structure tensor, as returned by structure_tensor.", "The eigenvalues of the structure tensor, in decreasing order. The eigenvalues are the leading dimension. That is, the coordinate [i, j, k] corresponds to the ith-largest eigenvalue at position (j, k).", "See also"]}, {"name": "feature.structure_tensor_eigvals()", "path": "api/skimage.feature#skimage.feature.structure_tensor_eigvals", "type": "feature", "text": ["Compute eigenvalues of structure tensor.", "Element of the structure tensor for each pixel in the input image.", "Element of the structure tensor for each pixel in the input image.", "Element of the structure tensor for each pixel in the input image.", "Larger eigen value for each input matrix.", "Smaller eigen value for each input matrix."]}, {"name": "filters", "path": "api/skimage.filters", "type": "filters", "text": ["skimage.filters.apply_hysteresis_threshold(\u2026)", "Apply hysteresis thresholding to image.", "skimage.filters.correlate_sparse(image, kernel)", "Compute valid cross-correlation of padded_array and kernel.", "skimage.filters.difference_of_gaussians(\u2026)", "Find features between low_sigma and high_sigma in size.", "skimage.filters.farid(image, *[, mask])", "Find the edge magnitude using the Farid transform.", "skimage.filters.farid_h(image, *[, mask])", "Find the horizontal edges of an image using the Farid transform.", "skimage.filters.farid_v(image, *[, mask])", "Find the vertical edges of an image using the Farid transform.", "skimage.filters.frangi(image[, sigmas, \u2026])", "Filter an image with the Frangi vesselness filter.", "skimage.filters.gabor(image, frequency[, \u2026])", "Return real and imaginary responses to Gabor filter.", "skimage.filters.gabor_kernel(frequency[, \u2026])", "Return complex 2D Gabor filter kernel.", "skimage.filters.gaussian(image[, sigma, \u2026])", "Multi-dimensional Gaussian filter.", "skimage.filters.hessian(image[, sigmas, \u2026])", "Filter an image with the Hybrid Hessian filter.", "skimage.filters.inverse(data[, \u2026])", "Apply the filter in reverse to the given data.", "skimage.filters.laplace(image[, ksize, mask])", "Find the edges of an image using the Laplace operator.", "skimage.filters.median(image[, selem, out, \u2026])", "Return local median of an image.", "skimage.filters.meijering(image[, sigmas, \u2026])", "Filter an image with the Meijering neuriteness filter.", "skimage.filters.prewitt(image[, mask, axis, \u2026])", "Find the edge magnitude using the Prewitt transform.", "skimage.filters.prewitt_h(image[, mask])", "Find the horizontal edges of an image using the Prewitt transform.", "skimage.filters.prewitt_v(image[, mask])", "Find the vertical edges of an image using the Prewitt transform.", "skimage.filters.rank_order(image)", "Return an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.", "skimage.filters.roberts(image[, mask])", "Find the edge magnitude using Roberts\u2019 cross operator.", "skimage.filters.roberts_neg_diag(image[, mask])", "Find the cross edges of an image using the Roberts\u2019 Cross operator.", "skimage.filters.roberts_pos_diag(image[, mask])", "Find the cross edges of an image using Roberts\u2019 cross operator.", "skimage.filters.sato(image[, sigmas, \u2026])", "Filter an image with the Sato tubeness filter.", "skimage.filters.scharr(image[, mask, axis, \u2026])", "Find the edge magnitude using the Scharr transform.", "skimage.filters.scharr_h(image[, mask])", "Find the horizontal edges of an image using the Scharr transform.", "skimage.filters.scharr_v(image[, mask])", "Find the vertical edges of an image using the Scharr transform.", "skimage.filters.sobel(image[, mask, axis, \u2026])", "Find edges in an image using the Sobel filter.", "skimage.filters.sobel_h(image[, mask])", "Find the horizontal edges of an image using the Sobel transform.", "skimage.filters.sobel_v(image[, mask])", "Find the vertical edges of an image using the Sobel transform.", "skimage.filters.threshold_isodata([image, \u2026])", "Return threshold value(s) based on ISODATA method.", "skimage.filters.threshold_li(image, *[, \u2026])", "Compute threshold value by Li\u2019s iterative Minimum Cross Entropy method.", "skimage.filters.threshold_local(image, \u2026)", "Compute a threshold mask image based on local pixel neighborhood.", "skimage.filters.threshold_mean(image)", "Return threshold value based on the mean of grayscale values.", "skimage.filters.threshold_minimum([image, \u2026])", "Return threshold value based on minimum method.", "skimage.filters.threshold_multiotsu(image[, \u2026])", "Generate classes-1 threshold values to divide gray levels in image.", "skimage.filters.threshold_niblack(image[, \u2026])", "Applies Niblack local threshold to an array.", "skimage.filters.threshold_otsu([image, \u2026])", "Return threshold value based on Otsu\u2019s method.", "skimage.filters.threshold_sauvola(image[, \u2026])", "Applies Sauvola local threshold to an array.", "skimage.filters.threshold_triangle(image[, \u2026])", "Return threshold value based on the triangle algorithm.", "skimage.filters.threshold_yen([image, \u2026])", "Return threshold value based on Yen\u2019s method.", "skimage.filters.try_all_threshold(image[, \u2026])", "Returns a figure comparing the outputs of different thresholding methods.", "skimage.filters.unsharp_mask(image[, \u2026])", "Unsharp masking filter.", "skimage.filters.wiener(data[, \u2026])", "Minimum Mean Square Error (Wiener) inverse filter.", "skimage.filters.window(window_type, shape[, \u2026])", "Return an n-dimensional window of a given size and dimensionality.", "skimage.filters.LPIFilter2D(\u2026)", "Linear Position-Invariant Filter (2-dimensional)", "skimage.filters.rank", "Apply hysteresis thresholding to image.", "This algorithm finds regions where image is greater than high OR image is greater than low and that region is connected to a region greater than high.", "Grayscale input image.", "Lower threshold.", "Higher threshold.", "Array in which True indicates the locations where image was above the hysteresis threshold.", "J. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698. DOI:10.1109/TPAMI.1986.4767851", "Compute valid cross-correlation of padded_array and kernel.", "This function is fast when kernel is large with many zeros.", "See scipy.ndimage.correlate for a description of cross-correlation.", "The input array. If mode is \u2018valid\u2019, this array should already be padded, as a margin of the same shape as kernel will be stripped off.", "The kernel to be correlated. Must have the same number of dimensions as padded_array. For high performance, it should be sparse (few nonzero entries).", "See scipy.ndimage.correlate for valid modes. Additionally, mode \u2018valid\u2019 is accepted, in which case no padding is applied and the result is the result for the smaller image for which the kernel is entirely inside the original data.", "The result of cross-correlating image with kernel. If mode \u2018valid\u2019 is used, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1).", "Find features between low_sigma and high_sigma in size.", "This function uses the Difference of Gaussians method for applying band-pass filters to multi-dimensional arrays. The input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.", "Input array to filter.", "Standard deviation(s) for the Gaussian kernel with the smaller sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes.", "Standard deviation(s) for the Gaussian kernel with the larger sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes. If None is given (default), sigmas for all axes are calculated as 1.6 * low_sigma.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0", "Whether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together).", "Truncate the filter at this many standard deviations.", "the filtered array.", "See also", "This function will subtract an array filtered with a Gaussian kernel with sigmas given by high_sigma from an array filtered with a Gaussian kernel with sigmas provided by low_sigma. The values for high_sigma must always be greater than or equal to the corresponding values in low_sigma, or a ValueError will be raised.", "When high_sigma is none, the values for high_sigma will be calculated as 1.6x the corresponding values in low_sigma. This ratio was originally proposed by Marr and Hildreth (1980) [1] and is commonly used when approximating the inverted Laplacian of Gaussian, which is used in edge and blob detection.", "Input image is converted according to the conventions of img_as_float.", "Except for sigma values, all parameters are used for both filters.", "Marr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series B 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020", "Apply a simple Difference of Gaussians filter to a color image:", "Apply a Laplacian of Gaussian filter as approximated by the Difference of Gaussians filter:", "Apply a Difference of Gaussians filter to a grayscale image using different sigma values for each axis:", "Find the edge magnitude using the Farid transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Farid edge map.", "See also", "Take the square root of the sum of the squares of the horizontal and vertical derivatives to get a magnitude that is somewhat insensitive to direction. Similar to the Scharr operator, this operator is designed with a rotation invariance constraint.", "Farid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819", "Wikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at: <https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>", "Find the horizontal edges of an image using the Farid transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Farid edge map.", "The kernel was constructed using the 5-tap weights from [1].", "Farid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819", "Farid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional derivative kernels\u201d, In: 7th International Conference on Computer Analysis of Images and Patterns, Kiel, Germany. Sep, 1997.", "Find the vertical edges of an image using the Farid transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Farid edge map.", "The kernel was constructed using the 5-tap weights from [1].", "Farid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819", "Filter an image with the Frangi vesselness filter.", "This filter can be used to detect continuous ridges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to vessels, according to the method described in [1].", "Array with input image data.", "Sigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)", "The range of sigmas used.", "Step size between sigmas.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Written by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University of Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar 20017, [3]", "Frangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,). Multiscale vessel enhancement filtering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 130-137). Springer Berlin Heidelberg. DOI:10.1007/BFb0056195", "Kroon, D. J.: Hessian based Frangi vesselness filter.", "Ellis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi", "Return real and imaginary responses to Gabor filter.", "The real and imaginary parts of the Gabor filter kernel are applied to the image and the response is returned as a pair of arrays.", "Gabor filter is a linear filter with a Gaussian kernel which is modulated by a sinusoidal plane wave. Frequency and orientation representations of the Gabor filter are similar to those of the human visual system. Gabor filter banks are commonly used in computer vision and image processing. They are especially suitable for edge detection and texture classification.", "Input image.", "Spatial frequency of the harmonic function. Specified in pixels.", "Orientation in radians. If 0, the harmonic is in the x-direction.", "The bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.", "Standard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.", "The linear size of the kernel is n_stds (3 by default) standard deviations.", "Phase offset of harmonic function in radians.", "Mode used to convolve image with a kernel, passed to ndi.convolve", "Value to fill past edges of input if mode of convolution is \u2018constant\u2019. The parameter is passed to ndi.convolve.", "Filtered images using the real and imaginary parts of the Gabor filter kernel. Images are of the same dimensions as the input one.", "https://en.wikipedia.org/wiki/Gabor_filter", "https://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf", "Return complex 2D Gabor filter kernel.", "Gabor kernel is a Gaussian kernel modulated by a complex harmonic function. Harmonic function consists of an imaginary sine function and a real cosine function. Spatial frequency is inversely proportional to the wavelength of the harmonic and to the standard deviation of a Gaussian kernel. The bandwidth is also inversely proportional to the standard deviation.", "Spatial frequency of the harmonic function. Specified in pixels.", "Orientation in radians. If 0, the harmonic is in the x-direction.", "The bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.", "Standard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.", "The linear size of the kernel is n_stds (3 by default) standard deviations", "Phase offset of harmonic function in radians.", "Complex filter kernel.", "https://en.wikipedia.org/wiki/Gabor_filter", "https://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf", "Multi-dimensional Gaussian filter.", "Input image (grayscale or color) to filter.", "Standard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The output parameter passes an array in which to store the filter output.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0", "Whether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together). Only 3 channels are supported. If None, the function will attempt to guess this, and raise a warning if ambiguous, when the array has shape (M, N, 3).", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Truncate the filter at this many standard deviations.", "the filtered array", "This function is a wrapper around scipy.ndi.gaussian_filter().", "Integer arrays are converted to float.", "The output should be floating point data type since gaussian converts to float provided image. If output is not provided, another array will be allocated and returned as the result.", "The multi-dimensional filter is implemented as a sequence of one-dimensional convolution filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a limited precision, the results may be imprecise because intermediate results may be stored with insufficient precision.", "Filter an image with the Hybrid Hessian filter.", "This filter can be used to detect continuous edges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Defined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses alternative method of smoothing. Refer to [1] to find the differences between Frangi and Hessian filters.", "Array with input image data.", "Sigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)", "The range of sigmas used.", "Step size between sigmas.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Written by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University of Twente (May 2009) [2]", "Ng, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle detection using hybrid Hessian filter. In Asian Conference on Computer Vision (pp. 609-622). Springer International Publishing. DOI:10.1007/978-3-319-16811-1_40", "Kroon, D. J.: Hessian based Frangi vesselness filter.", "Apply the filter in reverse to the given data.", "Input data.", "Impulse response of the filter. See LPIFilter2D.__init__.", "Additional keyword parameters to the impulse_response function.", "Limit the filter gain. Often, the filter contains zeros, which would cause the inverse filter to have infinite gain. High gain causes amplification of artefacts, so a conservative limit is recommended.", "If you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.", "Find the edges of an image using the Laplace operator.", "Image to process.", "Define the size of the discrete Laplacian operator such that it will have a size of (ksize,) * image.ndim.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Laplace edge map.", "The Laplacian operator is generated using the function skimage.restoration.uft.laplacian().", "Return local median of an image.", "Input image.", "If behavior=='rank', selem is a 2-D array of 1\u2019s and 0\u2019s. If behavior=='ndimage', selem is a N-D array of 1\u2019s and 0\u2019s with the same number of dimension than image. If None, selem will be a N-D array with 3 elements for each dimension (e.g., vector, square, cube, etc.)", "If None, a new array is allocated.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.", "New in version 0.15: mode is used when behavior='ndimage'.", "Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0", "New in version 0.15: cval was added in 0.15 is used when behavior='ndimage'.", "Either to use the old behavior (i.e., < 0.15) or the new behavior. The old behavior will call the skimage.filters.rank.median(). The new behavior will call the scipy.ndimage.median_filter(). Default is \u2018ndimage\u2019.", "New in version 0.15: behavior is introduced in 0.15", "Changed in version 0.16: Default behavior has been changed from \u2018rank\u2019 to \u2018ndimage\u2019", "Output image.", "See also", "Rank-based implementation of the median filtering offering more flexibility with additional parameters but dedicated for unsigned integer images.", "Filter an image with the Meijering neuriteness filter.", "This filter can be used to detect continuous ridges, e.g. neurites, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Calculates the eigenvectors of the Hessian to compute the similarity of an image region to neurites, according to the method described in [1].", "Array with input image data.", "Sigmas used as scales of filter", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Meijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M. (2004). Design and validation of a tool for neurite tracing and analysis in fluorescence microscopy images. Cytometry Part A, 58(2), 167-176. DOI:10.1002/cyto.a.20022", "Find the edge magnitude using the Prewitt transform.", "The input image.", "Clip the output image to this mask. (Values where mask=0 will be set to 0.)", "Compute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as:", "The magnitude is also computed if axis is a sequence.", "The boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.", "When mode is 'constant', this is the constant used in values outside the boundary of the image data.", "The Prewitt edge map.", "See also", "The edge magnitude depends slightly on edge directions, since the approximation of the gradient operator by the Prewitt operator is not completely rotation invariant. For a better rotation invariance, the Scharr operator should be used. The Sobel operator has a better rotation invariance than the Prewitt operator, but a worse rotation invariance than the Scharr operator.", "Find the horizontal edges of an image using the Prewitt transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Prewitt edge map.", "We use the following kernel:", "Find the vertical edges of an image using the Prewitt transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Prewitt edge map.", "We use the following kernel:", "Return an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.", "New array where each pixel has the rank-order value of the corresponding pixel in image. Pixel values are between 0 and n - 1, where n is the number of distinct unique values in image.", "Unique original values of image", "Find the edge magnitude using Roberts\u2019 cross operator.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Roberts\u2019 Cross edge map.", "See also", "Find the cross edges of an image using the Roberts\u2019 Cross operator.", "The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Robert\u2019s edge map.", "We use the following kernel:", "Find the cross edges of an image using Roberts\u2019 cross operator.", "The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Robert\u2019s edge map.", "We use the following kernel:", "Filter an image with the Sato tubeness filter.", "This filter can be used to detect continuous ridges, e.g. tubes, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to tubes, according to the method described in [1].", "Array with input image data.", "Sigmas used as scales of filter.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Sato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026, Kikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1", "Find the edge magnitude using the Scharr transform.", "The input image.", "Clip the output image to this mask. (Values where mask=0 will be set to 0.)", "Compute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as:", "The magnitude is also computed if axis is a sequence.", "The boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.", "When mode is 'constant', this is the constant used in values outside the boundary of the image data.", "The Scharr edge map.", "See also", "The Scharr operator has a better rotation invariance than other edge filters such as the Sobel or the Prewitt operators.", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.", "https://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators", "Find the horizontal edges of an image using the Scharr transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Scharr edge map.", "We use the following kernel:", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.", "Find the vertical edges of an image using the Scharr transform.", "Image to process", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Scharr edge map.", "We use the following kernel:", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.", "Find edges in an image using the Sobel filter.", "The input image.", "Clip the output image to this mask. (Values where mask=0 will be set to 0.)", "Compute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as:", "The magnitude is also computed if axis is a sequence.", "The boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.", "When mode is 'constant', this is the constant used in values outside the boundary of the image data.", "The Sobel edge map.", "See also", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.", "https://en.wikipedia.org/wiki/Sobel_operator", "Flood Fill", "Find the horizontal edges of an image using the Sobel transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Sobel edge map.", "We use the following kernel:", "Find the vertical edges of an image using the Sobel transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Sobel edge map.", "We use the following kernel:", "Return threshold value(s) based on ISODATA method.", "Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality:", "That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups.", "For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width.", "Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "If False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.", "Histogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.", "Threshold value(s).", "Ridler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109/TSMC.1978.4310039", "Sezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf DOI:10.1117/1.1631315", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold", "Compute threshold value by Li\u2019s iterative Minimum Cross Entropy method.", "Input image.", "Finish the computation when the change in the threshold in an iteration is less than this value. By default, this is half the smallest difference between intensity values in image.", "Li\u2019s iterative method uses gradient descent to find the optimal threshold. If the image intensity histogram contains more than two modes (peaks), the gradient descent could get stuck in a local optimum. An initial guess for the iteration can help the algorithm find the globally-optimal threshold. A float value defines a specific start point, while a callable should take in an array of image intensities and return a float value. Example valid callables include numpy.mean (default), lambda arr: numpy.quantile(arr, 0.95), or even skimage.filters.threshold_otsu().", "A function that will be called on the threshold at every iteration of the algorithm.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "Li C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern Recognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D", "Li C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross Entropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776 DOI:10.1016/S0167-8655(98)00057-9", "Sezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165 DOI:10.1117/1.1631315", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold", "Compute a threshold mask image based on local pixel neighborhood.", "Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the \u2018generic\u2019 method.", "Input image.", "Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, \u2026, 21, \u2026).", "Method used to determine adaptive threshold for local neighbourhood in weighted mean image.", "By default the \u2018gaussian\u2019 method is used.", "Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.", "Either specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019 method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Threshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground.", "https://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold", "Return threshold value based on the mean of grayscale values.", "Grayscale input image.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "C. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993. DOI:10.1006/cgip.1993.1040", "Return threshold value based on minimum method.", "The histogram of the input image is computed if not provided and smoothed until there are only two maxima. Then the minimum in between is the threshold value.", "Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Maximum number of iterations to smooth the histogram.", "Histogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "If unable to find two local maxima in the histogram or if the smoothing takes more than 1e4 iterations.", "C. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.", "Prewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of the New York Academy of Sciences 128: 1035-1053 DOI:10.1111/j.1749-6632.1965.tb11715.x", "Generate classes-1 threshold values to divide gray levels in image.", "The threshold values are chosen to maximize the total sum of pairwise variances between the thresholded graylevel classes. See Notes and [1] for more details.", "Grayscale input image.", "Number of classes to be thresholded, i.e. the number of resulting regions.", "Number of bins used to calculate the histogram. This value is ignored for integer arrays.", "Array containing the threshold values for the desired classes.", "If image contains less grayscale value then the desired number of classes.", "This implementation relies on a Cython function whose complexity is \\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\), where \\(h\\) is the number of histogram bins and \\(C\\) is the number of classes desired.", "The input image must be grayscale.", "Liao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel thresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727, 2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf> DOI:10.6688/JISE.2001.17.5.1", "Tosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at: <http://imagej.net/plugins/download/Multi_OtsuThreshold.java>", "Multi-Otsu Thresholding", "Segment human cells (in mitosis)", "Applies Niblack local threshold to an array.", "A threshold T is calculated for every pixel in the image using the following formula:", "where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation.", "Input image.", "Window size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).", "Value of parameter k in threshold formula.", "Threshold mask. All pixels with an intensity higher than this value are assumed to be foreground.", "This algorithm is originally designed for text recognition.", "The Bradley threshold is a particular case of the Niblack one, being equivalent to", "for some value q. By default, Bradley and Roth use q=1.", "W. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.", "D. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal of Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236", "Return threshold value based on Otsu\u2019s method.", "Either image or hist must be provided. If hist is provided, the actual histogram of the image is ignored.", "Grayscale input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Histogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "The input image must be grayscale.", "Wikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method", "Measure region properties", "Rank filters", "Applies Sauvola local threshold to an array. Sauvola is a modification of Niblack technique.", "In the original method a threshold T is calculated for every pixel in the image using the following formula:", "where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation. R is the maximum standard deviation of a greyscale image.", "Input image.", "Window size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).", "Value of the positive parameter k.", "Value of R, the dynamic range of standard deviation. If None, set to the half of the image dtype range.", "Threshold mask. All pixels with an intensity higher than this value are assumed to be foreground.", "This algorithm is originally designed for text recognition.", "J. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern Recognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2", "Return threshold value based on the triangle algorithm.", "Grayscale input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "Zack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of Sister Chromatid Exchange Frequency, Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold", "Return threshold value based on Yen\u2019s method. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Histogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "Yen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic Multilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378. DOI:10.1109/83.366472", "Sezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315 http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold", "Returns a figure comparing the outputs of different thresholding methods.", "Input image.", "Figure size (in inches).", "Print function name for each method.", "Matplotlib figure and axes.", "The following algorithms are used:", "Unsharp masking filter.", "The sharp details are identified as the difference between the original image and its blurred version. These details are then scaled, and added back to the original image.", "Input image.", "If a scalar is given, then its value is used for all dimensions. If sequence is given, then there must be exactly one radius for each dimension except the last dimension for multichannel images. Note that 0 radius means no blurring, and negative values are not allowed.", "The details will be amplified with this factor. The factor could be 0 or negative. Typically, it is a small positive number, e.g. 1.0.", "If True, the last image dimension is considered as a color channel, otherwise as spatial. Color channels are processed individually.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Image with unsharp mask applied.", "Unsharp masking is an image sharpening technique. It is a linear image operation, and numerically stable, unlike deconvolution which is an ill-posed problem. Because of this stability, it is often preferred over deconvolution.", "The main idea is as follows: sharp details are identified as the difference between the original image and its blurred version. These details are added back to the original image after a scaling step:", "enhanced image = original + amount * (original - blurred)", "When applying this filter to several color layers independently, color bleeding may occur. More visually pleasing result can be achieved by processing only the brightness/lightness/intensity channel in a suitable color space such as HSV, HSL, YUV, or YCbCr.", "Unsharp masking is described in most introductory digital image processing books. This implementation is based on [1].", "Maria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed ii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398", "Wikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking", "Minimum Mean Square Error (Wiener) inverse filter.", "Input data.", "Ratio between power spectrum of noise and undegraded image.", "Impulse response of the filter. See LPIFilter2D.__init__.", "Additional keyword parameters to the impulse_response function.", "If you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here.", "Return an n-dimensional window of a given size and dimensionality.", "The type of window to be created. Any window type supported by scipy.signal.get_window is allowed here. See notes below for a current list, or the SciPy documentation for the version of SciPy on your machine.", "The shape of the window along each axis. If an integer is provided, a 1D window is generated.", "Keyword arguments passed to skimage.transform.warp (e.g., warp_kwargs={'order':3} to change interpolation method).", "A window of the specified shape. dtype is np.double.", "This function is based on scipy.signal.get_window and thus can access all of the window types available to that function (e.g., \"hann\", \"boxcar\"). Note that certain window types require parameters that have to be supplied with the window name as a tuple (e.g., (\"tukey\", 0.8)). If only a float is supplied, it is interpreted as the beta parameter of the Kaiser window.", "See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html for more details.", "Note that this function generates a double precision array of the specified shape and can thus generate very large arrays that consume a large amount of available memory.", "The approach taken here to create nD windows is to first calculate the Euclidean distance from the center of the intended nD window to each position in the array. That distance is used to sample, with interpolation, from a 1D window returned from scipy.signal.get_window. The method of interpolation can be changed with the order keyword argument passed to skimage.transform.warp.", "Some coordinates in the output window will be outside of the original signal; these will be filled in with zeros.", "Window types: - boxcar - triang - blackman - hamming - hann - bartlett - flattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser (needs beta) - gaussian (needs standard deviation) - general_gaussian (needs power, width) - slepian (needs width) - dpss (needs normalized half-bandwidth) - chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs taper fraction)", "Two-dimensional window design, Wikipedia, https://en.wikipedia.org/wiki/Two_dimensional_window_design", "Return a Hann window with shape (512, 512):", "Return a Kaiser window with beta parameter of 16 and shape (256, 256, 35):", "Return a Tukey window with an alpha parameter of 0.8 and shape (100, 300):", "Bases: object", "Linear Position-Invariant Filter (2-dimensional)", "Function that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through.", "In other words, impulse_response would be called like this:", "Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients."]}, {"name": "filters.apply_hysteresis_threshold()", "path": "api/skimage.filters#skimage.filters.apply_hysteresis_threshold", "type": "filters", "text": ["Apply hysteresis thresholding to image.", "This algorithm finds regions where image is greater than high OR image is greater than low and that region is connected to a region greater than high.", "Grayscale input image.", "Lower threshold.", "Higher threshold.", "Array in which True indicates the locations where image was above the hysteresis threshold.", "J. Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1986; vol. 8, pp.679-698. DOI:10.1109/TPAMI.1986.4767851"]}, {"name": "filters.correlate_sparse()", "path": "api/skimage.filters#skimage.filters.correlate_sparse", "type": "filters", "text": ["Compute valid cross-correlation of padded_array and kernel.", "This function is fast when kernel is large with many zeros.", "See scipy.ndimage.correlate for a description of cross-correlation.", "The input array. If mode is \u2018valid\u2019, this array should already be padded, as a margin of the same shape as kernel will be stripped off.", "The kernel to be correlated. Must have the same number of dimensions as padded_array. For high performance, it should be sparse (few nonzero entries).", "See scipy.ndimage.correlate for valid modes. Additionally, mode \u2018valid\u2019 is accepted, in which case no padding is applied and the result is the result for the smaller image for which the kernel is entirely inside the original data.", "The result of cross-correlating image with kernel. If mode \u2018valid\u2019 is used, the resulting shape is (M-Q+1, N-R+1,[ \u2026,] P-S+1)."]}, {"name": "filters.difference_of_gaussians()", "path": "api/skimage.filters#skimage.filters.difference_of_gaussians", "type": "filters", "text": ["Find features between low_sigma and high_sigma in size.", "This function uses the Difference of Gaussians method for applying band-pass filters to multi-dimensional arrays. The input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.", "Input array to filter.", "Standard deviation(s) for the Gaussian kernel with the smaller sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes.", "Standard deviation(s) for the Gaussian kernel with the larger sigmas across all axes. The standard deviations are given for each axis as a sequence, or as a single number, in which case the single number is used as the standard deviation value for all axes. If None is given (default), sigmas for all axes are calculated as 1.6 * low_sigma.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0", "Whether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together).", "Truncate the filter at this many standard deviations.", "the filtered array.", "See also", "This function will subtract an array filtered with a Gaussian kernel with sigmas given by high_sigma from an array filtered with a Gaussian kernel with sigmas provided by low_sigma. The values for high_sigma must always be greater than or equal to the corresponding values in low_sigma, or a ValueError will be raised.", "When high_sigma is none, the values for high_sigma will be calculated as 1.6x the corresponding values in low_sigma. This ratio was originally proposed by Marr and Hildreth (1980) [1] and is commonly used when approximating the inverted Laplacian of Gaussian, which is used in edge and blob detection.", "Input image is converted according to the conventions of img_as_float.", "Except for sigma values, all parameters are used for both filters.", "Marr, D. and Hildreth, E. Theory of Edge Detection. Proc. R. Soc. Lond. Series B 207, 187-217 (1980). https://doi.org/10.1098/rspb.1980.0020", "Apply a simple Difference of Gaussians filter to a color image:", "Apply a Laplacian of Gaussian filter as approximated by the Difference of Gaussians filter:", "Apply a Difference of Gaussians filter to a grayscale image using different sigma values for each axis:"]}, {"name": "filters.farid()", "path": "api/skimage.filters#skimage.filters.farid", "type": "filters", "text": ["Find the edge magnitude using the Farid transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Farid edge map.", "See also", "Take the square root of the sum of the squares of the horizontal and vertical derivatives to get a magnitude that is somewhat insensitive to direction. Similar to the Scharr operator, this operator is designed with a rotation invariance constraint.", "Farid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819", "Wikipedia, \u201cFarid and Simoncelli Derivatives.\u201d Available at: <https://en.wikipedia.org/wiki/Image_derivatives#Farid_and_Simoncelli_Derivatives>"]}, {"name": "filters.farid_h()", "path": "api/skimage.filters#skimage.filters.farid_h", "type": "filters", "text": ["Find the horizontal edges of an image using the Farid transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Farid edge map.", "The kernel was constructed using the 5-tap weights from [1].", "Farid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819", "Farid, H. and Simoncelli, E. P. \u201cOptimally rotation-equivariant directional derivative kernels\u201d, In: 7th International Conference on Computer Analysis of Images and Patterns, Kiel, Germany. Sep, 1997."]}, {"name": "filters.farid_v()", "path": "api/skimage.filters#skimage.filters.farid_v", "type": "filters", "text": ["Find the vertical edges of an image using the Farid transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Farid edge map.", "The kernel was constructed using the 5-tap weights from [1].", "Farid, H. and Simoncelli, E. P., \u201cDifferentiation of discrete multidimensional signals\u201d, IEEE Transactions on Image Processing 13(4): 496-508, 2004. DOI:10.1109/TIP.2004.823819"]}, {"name": "filters.frangi()", "path": "api/skimage.filters#skimage.filters.frangi", "type": "filters", "text": ["Filter an image with the Frangi vesselness filter.", "This filter can be used to detect continuous ridges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to vessels, according to the method described in [1].", "Array with input image data.", "Sigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)", "The range of sigmas used.", "Step size between sigmas.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Written by Marc Schrijver, November 2001 Re-Written by D. J. Kroon, University of Twente, May 2009, [2] Adoption of 3D version from D. G. Ellis, Januar 20017, [3]", "Frangi, A. F., Niessen, W. J., Vincken, K. L., & Viergever, M. A. (1998,). Multiscale vessel enhancement filtering. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 130-137). Springer Berlin Heidelberg. DOI:10.1007/BFb0056195", "Kroon, D. J.: Hessian based Frangi vesselness filter.", "Ellis, D. G.: https://github.com/ellisdg/frangi3d/tree/master/frangi"]}, {"name": "filters.gabor()", "path": "api/skimage.filters#skimage.filters.gabor", "type": "filters", "text": ["Return real and imaginary responses to Gabor filter.", "The real and imaginary parts of the Gabor filter kernel are applied to the image and the response is returned as a pair of arrays.", "Gabor filter is a linear filter with a Gaussian kernel which is modulated by a sinusoidal plane wave. Frequency and orientation representations of the Gabor filter are similar to those of the human visual system. Gabor filter banks are commonly used in computer vision and image processing. They are especially suitable for edge detection and texture classification.", "Input image.", "Spatial frequency of the harmonic function. Specified in pixels.", "Orientation in radians. If 0, the harmonic is in the x-direction.", "The bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.", "Standard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.", "The linear size of the kernel is n_stds (3 by default) standard deviations.", "Phase offset of harmonic function in radians.", "Mode used to convolve image with a kernel, passed to ndi.convolve", "Value to fill past edges of input if mode of convolution is \u2018constant\u2019. The parameter is passed to ndi.convolve.", "Filtered images using the real and imaginary parts of the Gabor filter kernel. Images are of the same dimensions as the input one.", "https://en.wikipedia.org/wiki/Gabor_filter", "https://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf"]}, {"name": "filters.gabor_kernel()", "path": "api/skimage.filters#skimage.filters.gabor_kernel", "type": "filters", "text": ["Return complex 2D Gabor filter kernel.", "Gabor kernel is a Gaussian kernel modulated by a complex harmonic function. Harmonic function consists of an imaginary sine function and a real cosine function. Spatial frequency is inversely proportional to the wavelength of the harmonic and to the standard deviation of a Gaussian kernel. The bandwidth is also inversely proportional to the standard deviation.", "Spatial frequency of the harmonic function. Specified in pixels.", "Orientation in radians. If 0, the harmonic is in the x-direction.", "The bandwidth captured by the filter. For fixed bandwidth, sigma_x and sigma_y will decrease with increasing frequency. This value is ignored if sigma_x and sigma_y are set by the user.", "Standard deviation in x- and y-directions. These directions apply to the kernel before rotation. If theta = pi/2, then the kernel is rotated 90 degrees so that sigma_x controls the vertical direction.", "The linear size of the kernel is n_stds (3 by default) standard deviations", "Phase offset of harmonic function in radians.", "Complex filter kernel.", "https://en.wikipedia.org/wiki/Gabor_filter", "https://web.archive.org/web/20180127125930/http://mplab.ucsd.edu/tutorials/gabor.pdf"]}, {"name": "filters.gaussian()", "path": "api/skimage.filters#skimage.filters.gaussian", "type": "filters", "text": ["Multi-dimensional Gaussian filter.", "Input image (grayscale or color) to filter.", "Standard deviation for Gaussian kernel. The standard deviations of the Gaussian filter are given for each axis as a sequence, or as a single number, in which case it is equal for all axes.", "The output parameter passes an array in which to store the filter output.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0", "Whether the last axis of the image is to be interpreted as multiple channels. If True, each channel is filtered separately (channels are not mixed together). Only 3 channels are supported. If None, the function will attempt to guess this, and raise a warning if ambiguous, when the array has shape (M, N, 3).", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Truncate the filter at this many standard deviations.", "the filtered array", "This function is a wrapper around scipy.ndi.gaussian_filter().", "Integer arrays are converted to float.", "The output should be floating point data type since gaussian converts to float provided image. If output is not provided, another array will be allocated and returned as the result.", "The multi-dimensional filter is implemented as a sequence of one-dimensional convolution filters. The intermediate arrays are stored in the same data type as the output. Therefore, for output types with a limited precision, the results may be imprecise because intermediate results may be stored with insufficient precision."]}, {"name": "filters.hessian()", "path": "api/skimage.filters#skimage.filters.hessian", "type": "filters", "text": ["Filter an image with the Hybrid Hessian filter.", "This filter can be used to detect continuous edges, e.g. vessels, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Defined only for 2-D and 3-D images. Almost equal to Frangi filter, but uses alternative method of smoothing. Refer to [1] to find the differences between Frangi and Hessian filters.", "Array with input image data.", "Sigmas used as scales of filter, i.e., np.arange(scale_range[0], scale_range[1], scale_step)", "The range of sigmas used.", "Step size between sigmas.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a blob-like structure.", "Frangi correction constant that adjusts the filter\u2019s sensitivity to areas of high variance/texture/structure.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Written by Marc Schrijver (November 2001) Re-Written by D. J. Kroon University of Twente (May 2009) [2]", "Ng, C. C., Yap, M. H., Costen, N., & Li, B. (2014,). Automatic wrinkle detection using hybrid Hessian filter. In Asian Conference on Computer Vision (pp. 609-622). Springer International Publishing. DOI:10.1007/978-3-319-16811-1_40", "Kroon, D. J.: Hessian based Frangi vesselness filter."]}, {"name": "filters.inverse()", "path": "api/skimage.filters#skimage.filters.inverse", "type": "filters", "text": ["Apply the filter in reverse to the given data.", "Input data.", "Impulse response of the filter. See LPIFilter2D.__init__.", "Additional keyword parameters to the impulse_response function.", "Limit the filter gain. Often, the filter contains zeros, which would cause the inverse filter to have infinite gain. High gain causes amplification of artefacts, so a conservative limit is recommended.", "If you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here."]}, {"name": "filters.laplace()", "path": "api/skimage.filters#skimage.filters.laplace", "type": "filters", "text": ["Find the edges of an image using the Laplace operator.", "Image to process.", "Define the size of the discrete Laplacian operator such that it will have a size of (ksize,) * image.ndim.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Laplace edge map.", "The Laplacian operator is generated using the function skimage.restoration.uft.laplacian()."]}, {"name": "filters.LPIFilter2D", "path": "api/skimage.filters#skimage.filters.LPIFilter2D", "type": "filters", "text": ["Bases: object", "Linear Position-Invariant Filter (2-dimensional)", "Function that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through.", "In other words, impulse_response would be called like this:", "Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients."]}, {"name": "filters.LPIFilter2D.__init__()", "path": "api/skimage.filters#skimage.filters.LPIFilter2D.__init__", "type": "filters", "text": ["Function that yields the impulse response. r and c are 1-dimensional vectors that represent row and column positions, in other words coordinates are (r[0],c[0]),(r[0],c[1]) etc. **filter_params are passed through.", "In other words, impulse_response would be called like this:", "Gaussian filter: Use a 1-D gaussian in each direction without normalization coefficients."]}, {"name": "filters.median()", "path": "api/skimage.filters#skimage.filters.median", "type": "filters", "text": ["Return local median of an image.", "Input image.", "If behavior=='rank', selem is a 2-D array of 1\u2019s and 0\u2019s. If behavior=='ndimage', selem is a N-D array of 1\u2019s and 0\u2019s with the same number of dimension than image. If None, selem will be a N-D array with 3 elements for each dimension (e.g., vector, square, cube, etc.)", "If None, a new array is allocated.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018nearest\u2019.", "New in version 0.15: mode is used when behavior='ndimage'.", "Value to fill past edges of input if mode is \u2018constant\u2019. Default is 0.0", "New in version 0.15: cval was added in 0.15 is used when behavior='ndimage'.", "Either to use the old behavior (i.e., < 0.15) or the new behavior. The old behavior will call the skimage.filters.rank.median(). The new behavior will call the scipy.ndimage.median_filter(). Default is \u2018ndimage\u2019.", "New in version 0.15: behavior is introduced in 0.15", "Changed in version 0.16: Default behavior has been changed from \u2018rank\u2019 to \u2018ndimage\u2019", "Output image.", "See also", "Rank-based implementation of the median filtering offering more flexibility with additional parameters but dedicated for unsigned integer images."]}, {"name": "filters.meijering()", "path": "api/skimage.filters#skimage.filters.meijering", "type": "filters", "text": ["Filter an image with the Meijering neuriteness filter.", "This filter can be used to detect continuous ridges, e.g. neurites, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Calculates the eigenvectors of the Hessian to compute the similarity of an image region to neurites, according to the method described in [1].", "Array with input image data.", "Sigmas used as scales of filter", "Frangi correction constant that adjusts the filter\u2019s sensitivity to deviation from a plate-like structure.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Meijering, E., Jacob, M., Sarria, J. C., Steiner, P., Hirling, H., Unser, M. (2004). Design and validation of a tool for neurite tracing and analysis in fluorescence microscopy images. Cytometry Part A, 58(2), 167-176. DOI:10.1002/cyto.a.20022"]}, {"name": "filters.prewitt()", "path": "api/skimage.filters#skimage.filters.prewitt", "type": "filters", "text": ["Find the edge magnitude using the Prewitt transform.", "The input image.", "Clip the output image to this mask. (Values where mask=0 will be set to 0.)", "Compute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as:", "The magnitude is also computed if axis is a sequence.", "The boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.", "When mode is 'constant', this is the constant used in values outside the boundary of the image data.", "The Prewitt edge map.", "See also", "The edge magnitude depends slightly on edge directions, since the approximation of the gradient operator by the Prewitt operator is not completely rotation invariant. For a better rotation invariance, the Scharr operator should be used. The Sobel operator has a better rotation invariance than the Prewitt operator, but a worse rotation invariance than the Scharr operator."]}, {"name": "filters.prewitt_h()", "path": "api/skimage.filters#skimage.filters.prewitt_h", "type": "filters", "text": ["Find the horizontal edges of an image using the Prewitt transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Prewitt edge map.", "We use the following kernel:"]}, {"name": "filters.prewitt_v()", "path": "api/skimage.filters#skimage.filters.prewitt_v", "type": "filters", "text": ["Find the vertical edges of an image using the Prewitt transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Prewitt edge map.", "We use the following kernel:"]}, {"name": "filters.rank", "path": "api/skimage.filters.rank", "type": "filters", "text": ["skimage.filters.rank.autolevel(image, selem)", "Auto-level image using local histogram.", "skimage.filters.rank.autolevel_percentile(\u2026)", "Return greyscale local autolevel of an image.", "skimage.filters.rank.bottomhat(image, selem)", "Local bottom-hat of an image.", "skimage.filters.rank.enhance_contrast(image, \u2026)", "Enhance contrast of an image.", "skimage.filters.rank.enhance_contrast_percentile(\u2026)", "Enhance contrast of an image.", "skimage.filters.rank.entropy(image, selem[, \u2026])", "Local entropy.", "skimage.filters.rank.equalize(image, selem)", "Equalize image using local histogram.", "skimage.filters.rank.geometric_mean(image, selem)", "Return local geometric mean of an image.", "skimage.filters.rank.gradient(image, selem)", "Return local gradient of an image (i.e.", "skimage.filters.rank.gradient_percentile(\u2026)", "Return local gradient of an image (i.e.", "skimage.filters.rank.majority(image, selem, *)", "Majority filter assign to each pixel the most occuring value within its neighborhood.", "skimage.filters.rank.maximum(image, selem[, \u2026])", "Return local maximum of an image.", "skimage.filters.rank.mean(image, selem[, \u2026])", "Return local mean of an image.", "skimage.filters.rank.mean_bilateral(image, selem)", "Apply a flat kernel bilateral filter.", "skimage.filters.rank.mean_percentile(image, \u2026)", "Return local mean of an image.", "skimage.filters.rank.median(image[, selem, \u2026])", "Return local median of an image.", "skimage.filters.rank.minimum(image, selem[, \u2026])", "Return local minimum of an image.", "skimage.filters.rank.modal(image, selem[, \u2026])", "Return local mode of an image.", "skimage.filters.rank.noise_filter(image, selem)", "Noise feature.", "skimage.filters.rank.otsu(image, selem[, \u2026])", "Local Otsu\u2019s threshold value for each pixel.", "skimage.filters.rank.percentile(image, selem)", "Return local percentile of an image.", "skimage.filters.rank.pop(image, selem[, \u2026])", "Return the local number (population) of pixels.", "skimage.filters.rank.pop_bilateral(image, selem)", "Return the local number (population) of pixels.", "skimage.filters.rank.pop_percentile(image, selem)", "Return the local number (population) of pixels.", "skimage.filters.rank.subtract_mean(image, selem)", "Return image subtracted from its local mean.", "skimage.filters.rank.subtract_mean_percentile(\u2026)", "Return image subtracted from its local mean.", "skimage.filters.rank.sum(image, selem[, \u2026])", "Return the local sum of pixels.", "skimage.filters.rank.sum_bilateral(image, selem)", "Apply a flat kernel bilateral filter.", "skimage.filters.rank.sum_percentile(image, selem)", "Return the local sum of pixels.", "skimage.filters.rank.threshold(image, selem)", "Local threshold of an image.", "skimage.filters.rank.threshold_percentile(\u2026)", "Local threshold of an image.", "skimage.filters.rank.tophat(image, selem[, \u2026])", "Local top-hat of an image.", "skimage.filters.rank.windowed_histogram(\u2026)", "Normalized sliding window histogram", "Auto-level image using local histogram.", "This filter locally stretches the histogram of gray values to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Rank filters", "Return greyscale local autolevel of an image.", "This filter locally stretches the histogram of greyvalues to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Rank filters", "Local bottom-hat of an image.", "This filter computes the morphological closing of the image and then subtracts the result from the original image.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "New in version 0.17.", "This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.", "Enhance contrast of an image.", "This replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image", "Rank filters", "Enhance contrast of an image.", "This replaces each pixel by the local maximum if the pixel greyvalue is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Rank filters", "Local entropy.", "The entropy is computed using base 2 logarithm i.e. the filter returns the minimum number of bits needed to encode the local gray level distribution.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "Tinting gray-scale images", "Entropy", "Rank filters", "Equalize image using local histogram.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Local Histogram Equalization", "Rank filters", "Return local geometric mean of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Gonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d Prentice-Hall Inc, 2006.", "Return local gradient of an image (i.e. local maximum - local minimum).", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Markers for watershed transform", "Rank filters", "Return local gradient of an image (i.e. local maximum - local minimum).", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Majority filter assign to each pixel the most occuring value within its neighborhood.", "Image array (uint8, uint16 array).", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array will be allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Return local maximum of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "See also", "The lower algorithm complexity makes skimage.filters.rank.maximum more efficient for larger images and structuring elements.", "Rank filters", "Return local mean of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Segment human cells (in mitosis)", "Rank filters", "Apply a flat kernel bilateral filter.", "This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity.", "Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element.", "Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel.", "Only pixels belonging to the structuring element and having a greylevel inside this interval are averaged.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.", "Output image.", "See also", "Rank filters", "Return local mean of an image.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Return local median of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full square of size 3 is used.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "See also", "Implementation of a median filtering which handles images with floating precision.", "Markers for watershed transform", "Rank filters", "Return local minimum of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "See also", "The lower algorithm complexity makes skimage.filters.rank.minimum more efficient for larger images and structuring elements.", "Rank filters", "Return local mode of an image.", "The mode is the value that appears most often in the local histogram.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Noise feature.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "N. Hashimoto et al. Referenceless image quality evaluation for whole slide imaging. J Pathol Inform 2012;3:9.", "Local Otsu\u2019s threshold value for each pixel.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "https://en.wikipedia.org/wiki/Otsu\u2019s_method", "Rank filters", "Return local percentile of an image.", "Returns the value of the p0 lower percentile of the local greyvalue distribution.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Set the percentile value.", "Output image.", "Return the local number (population) of pixels.", "The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Return the local number (population) of pixels.", "The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Additionally pixels must have a greylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the center pixel.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.", "Output image.", "Return the local number (population) of pixels.", "The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Return image subtracted from its local mean.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Subtracting the mean value may introduce underflow. To compensate this potential underflow, the obtained difference is downscaled by a factor of 2 and shifted by n_bins / 2 - 1, the median value of the local histogram (n_bins = max(3, image.max()) +1 for 16-bits images and 256 otherwise).", "Return image subtracted from its local mean.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Return the local sum of pixels.", "Note that the sum may overflow depending on the data type of the input array.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Apply a flat kernel bilateral filter.", "This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity.", "Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element (selem).", "Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel.", "Only pixels belonging to the structuring element AND having a greylevel inside this interval are summed.", "Note that the sum may overflow depending on the data type of the input array.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.", "Output image.", "See also", "Return the local sum of pixels.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Note that the sum may overflow depending on the data type of the input array.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image.", "Local threshold of an image.", "The resulting binary mask is True if the gray value of the center pixel is greater than the local mean.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Local threshold of an image.", "The resulting binary mask is True if the greyvalue of the center pixel is greater than the local mean.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Set the percentile value.", "Output image.", "Local top-hat of an image.", "This filter computes the morphological opening of the image and then subtracts the result from the original image.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "New in version 0.17.", "This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow.", "Normalized sliding window histogram", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "The number of histogram bins. Will default to image.max() + 1 if None is passed.", "Array of dimensions (H,W,N), where (H,W) are the dimensions of the input image and N is n_bins or image.max() + 1 if no value is provided as a parameter. Effectively, each pixel is a N-D feature vector that is the histogram. The sum of the elements in the feature vector will be 1, unless no pixels in the window were covered by both selem and mask, in which case all elements will be 0."]}, {"name": "filters.rank.autolevel()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel", "type": "filters", "text": ["Auto-level image using local histogram.", "This filter locally stretches the histogram of gray values to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.autolevel_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.autolevel_percentile", "type": "filters", "text": ["Return greyscale local autolevel of an image.", "This filter locally stretches the histogram of greyvalues to cover the entire range of values from \u201cwhite\u201d to \u201cblack\u201d.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.bottomhat()", "path": "api/skimage.filters.rank#skimage.filters.rank.bottomhat", "type": "filters", "text": ["Local bottom-hat of an image.", "This filter computes the morphological closing of the image and then subtracts the result from the original image.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "New in version 0.17.", "This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow."]}, {"name": "filters.rank.enhance_contrast()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast", "type": "filters", "text": ["Enhance contrast of an image.", "This replaces each pixel by the local maximum if the pixel gray value is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image"]}, {"name": "filters.rank.enhance_contrast_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.enhance_contrast_percentile", "type": "filters", "text": ["Enhance contrast of an image.", "This replaces each pixel by the local maximum if the pixel greyvalue is closer to the local maximum than the local minimum. Otherwise it is replaced by the local minimum.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.entropy()", "path": "api/skimage.filters.rank#skimage.filters.rank.entropy", "type": "filters", "text": ["Local entropy.", "The entropy is computed using base 2 logarithm i.e. the filter returns the minimum number of bits needed to encode the local gray level distribution.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "https://en.wikipedia.org/wiki/Entropy_(information_theory)"]}, {"name": "filters.rank.equalize()", "path": "api/skimage.filters.rank#skimage.filters.rank.equalize", "type": "filters", "text": ["Equalize image using local histogram.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.geometric_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.geometric_mean", "type": "filters", "text": ["Return local geometric mean of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Gonzalez, R. C. and Wood, R. E. \u201cDigital Image Processing (3rd Edition).\u201d Prentice-Hall Inc, 2006."]}, {"name": "filters.rank.gradient()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient", "type": "filters", "text": ["Return local gradient of an image (i.e. local maximum - local minimum).", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.gradient_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.gradient_percentile", "type": "filters", "text": ["Return local gradient of an image (i.e. local maximum - local minimum).", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.majority()", "path": "api/skimage.filters.rank#skimage.filters.rank.majority", "type": "filters", "text": ["Majority filter assign to each pixel the most occuring value within its neighborhood.", "Image array (uint8, uint16 array).", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array will be allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.maximum()", "path": "api/skimage.filters.rank#skimage.filters.rank.maximum", "type": "filters", "text": ["Return local maximum of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "See also", "The lower algorithm complexity makes skimage.filters.rank.maximum more efficient for larger images and structuring elements."]}, {"name": "filters.rank.mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean", "type": "filters", "text": ["Return local mean of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.mean_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_bilateral", "type": "filters", "text": ["Apply a flat kernel bilateral filter.", "This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity.", "Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element.", "Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel.", "Only pixels belonging to the structuring element and having a greylevel inside this interval are averaged.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.", "Output image.", "See also"]}, {"name": "filters.rank.mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.mean_percentile", "type": "filters", "text": ["Return local mean of an image.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.median()", "path": "api/skimage.filters.rank#skimage.filters.rank.median", "type": "filters", "text": ["Return local median of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s. If None, a full square of size 3 is used.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "See also", "Implementation of a median filtering which handles images with floating precision."]}, {"name": "filters.rank.minimum()", "path": "api/skimage.filters.rank#skimage.filters.rank.minimum", "type": "filters", "text": ["Return local minimum of an image.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "See also", "The lower algorithm complexity makes skimage.filters.rank.minimum more efficient for larger images and structuring elements."]}, {"name": "filters.rank.modal()", "path": "api/skimage.filters.rank#skimage.filters.rank.modal", "type": "filters", "text": ["Return local mode of an image.", "The mode is the value that appears most often in the local histogram.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.noise_filter()", "path": "api/skimage.filters.rank#skimage.filters.rank.noise_filter", "type": "filters", "text": ["Noise feature.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "N. Hashimoto et al. Referenceless image quality evaluation for whole slide imaging. J Pathol Inform 2012;3:9."]}, {"name": "filters.rank.otsu()", "path": "api/skimage.filters.rank#skimage.filters.rank.otsu", "type": "filters", "text": ["Local Otsu\u2019s threshold value for each pixel.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "https://en.wikipedia.org/wiki/Otsu\u2019s_method"]}, {"name": "filters.rank.percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.percentile", "type": "filters", "text": ["Return local percentile of an image.", "Returns the value of the p0 lower percentile of the local greyvalue distribution.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Set the percentile value.", "Output image."]}, {"name": "filters.rank.pop()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop", "type": "filters", "text": ["Return the local number (population) of pixels.", "The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.pop_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_bilateral", "type": "filters", "text": ["Return the local number (population) of pixels.", "The number of pixels is defined as the number of pixels which are included in the structuring element and the mask. Additionally pixels must have a greylevel inside the interval [g-s0, g+s1] where g is the greyvalue of the center pixel.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.pop_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.pop_percentile", "type": "filters", "text": ["Return the local number (population) of pixels.", "The number of pixels is defined as the number of pixels which are included in the structuring element and the mask.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.subtract_mean()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean", "type": "filters", "text": ["Return image subtracted from its local mean.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "Subtracting the mean value may introduce underflow. To compensate this potential underflow, the obtained difference is downscaled by a factor of 2 and shifted by n_bins / 2 - 1, the median value of the local histogram (n_bins = max(3, image.max()) +1 for 16-bits images and 256 otherwise)."]}, {"name": "filters.rank.subtract_mean_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.subtract_mean_percentile", "type": "filters", "text": ["Return image subtracted from its local mean.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.sum()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum", "type": "filters", "text": ["Return the local sum of pixels.", "Note that the sum may overflow depending on the data type of the input array.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.sum_bilateral()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_bilateral", "type": "filters", "text": ["Apply a flat kernel bilateral filter.", "This is an edge-preserving and noise reducing denoising filter. It averages pixels based on their spatial closeness and radiometric similarity.", "Spatial closeness is measured by considering only the local pixel neighborhood given by a structuring element (selem).", "Radiometric similarity is defined by the greylevel interval [g-s0, g+s1] where g is the current pixel greylevel.", "Only pixels belonging to the structuring element AND having a greylevel inside this interval are summed.", "Note that the sum may overflow depending on the data type of the input array.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [s0, s1] interval around the greyvalue of the center pixel to be considered for computing the value.", "Output image.", "See also"]}, {"name": "filters.rank.sum_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.sum_percentile", "type": "filters", "text": ["Return the local sum of pixels.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Note that the sum may overflow depending on the data type of the input array.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Define the [p0, p1] percentile interval to be considered for computing the value.", "Output image."]}, {"name": "filters.rank.threshold()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold", "type": "filters", "text": ["Local threshold of an image.", "The resulting binary mask is True if the gray value of the center pixel is greater than the local mean.", "Input image.", "The neighborhood expressed as an ndarray of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image."]}, {"name": "filters.rank.threshold_percentile()", "path": "api/skimage.filters.rank#skimage.filters.rank.threshold_percentile", "type": "filters", "text": ["Local threshold of an image.", "The resulting binary mask is True if the greyvalue of the center pixel is greater than the local mean.", "Only greyvalues between percentiles [p0, p1] are considered in the filter.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Set the percentile value.", "Output image."]}, {"name": "filters.rank.tophat()", "path": "api/skimage.filters.rank#skimage.filters.rank.tophat", "type": "filters", "text": ["Local top-hat of an image.", "This filter computes the morphological opening of the image and then subtracts the result from the original image.", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "Output image.", "New in version 0.17.", "This function is deprecated and will be removed in scikit-image 0.19. This filter was misnamed and we believe that the usefulness is narrow."]}, {"name": "filters.rank.windowed_histogram()", "path": "api/skimage.filters.rank#skimage.filters.rank.windowed_histogram", "type": "filters", "text": ["Normalized sliding window histogram", "Input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s.", "If None, a new array is allocated.", "Mask array that defines (>0) area of the image included in the local neighborhood. If None, the complete image is used (default).", "Offset added to the structuring element center point. Shift is bounded to the structuring element sizes (center must be inside the given structuring element).", "The number of histogram bins. Will default to image.max() + 1 if None is passed.", "Array of dimensions (H,W,N), where (H,W) are the dimensions of the input image and N is n_bins or image.max() + 1 if no value is provided as a parameter. Effectively, each pixel is a N-D feature vector that is the histogram. The sum of the elements in the feature vector will be 1, unless no pixels in the window were covered by both selem and mask, in which case all elements will be 0."]}, {"name": "filters.rank_order()", "path": "api/skimage.filters#skimage.filters.rank_order", "type": "filters", "text": ["Return an image of the same shape where each pixel is the index of the pixel value in the ascending order of the unique values of image, aka the rank-order value.", "New array where each pixel has the rank-order value of the corresponding pixel in image. Pixel values are between 0 and n - 1, where n is the number of distinct unique values in image.", "Unique original values of image"]}, {"name": "filters.roberts()", "path": "api/skimage.filters#skimage.filters.roberts", "type": "filters", "text": ["Find the edge magnitude using Roberts\u2019 cross operator.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Roberts\u2019 Cross edge map.", "See also"]}, {"name": "filters.roberts_neg_diag()", "path": "api/skimage.filters#skimage.filters.roberts_neg_diag", "type": "filters", "text": ["Find the cross edges of an image using the Roberts\u2019 Cross operator.", "The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Robert\u2019s edge map.", "We use the following kernel:"]}, {"name": "filters.roberts_pos_diag()", "path": "api/skimage.filters#skimage.filters.roberts_pos_diag", "type": "filters", "text": ["Find the cross edges of an image using Roberts\u2019 cross operator.", "The kernel is applied to the input image to produce separate measurements of the gradient component one orientation.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Robert\u2019s edge map.", "We use the following kernel:"]}, {"name": "filters.sato()", "path": "api/skimage.filters#skimage.filters.sato", "type": "filters", "text": ["Filter an image with the Sato tubeness filter.", "This filter can be used to detect continuous ridges, e.g. tubes, wrinkles, rivers. It can be used to calculate the fraction of the whole image containing such objects.", "Defined only for 2-D and 3-D images. Calculates the eigenvectors of the Hessian to compute the similarity of an image region to tubes, according to the method described in [1].", "Array with input image data.", "Sigmas used as scales of filter.", "When True (the default), the filter detects black ridges; when False, it detects white ridges.", "How to handle values outside the image borders.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Filtered image (maximum of pixels across all scales).", "See also", "Sato, Y., Nakajima, S., Shiraga, N., Atsumi, H., Yoshida, S., Koller, T., \u2026, Kikinis, R. (1998). Three-dimensional multi-scale line filter for segmentation and visualization of curvilinear structures in medical images. Medical image analysis, 2(2), 143-168. DOI:10.1016/S1361-8415(98)80009-1"]}, {"name": "filters.scharr()", "path": "api/skimage.filters#skimage.filters.scharr", "type": "filters", "text": ["Find the edge magnitude using the Scharr transform.", "The input image.", "Clip the output image to this mask. (Values where mask=0 will be set to 0.)", "Compute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as:", "The magnitude is also computed if axis is a sequence.", "The boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.", "When mode is 'constant', this is the constant used in values outside the boundary of the image data.", "The Scharr edge map.", "See also", "The Scharr operator has a better rotation invariance than other edge filters such as the Sobel or the Prewitt operators.", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.", "https://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators"]}, {"name": "filters.scharr_h()", "path": "api/skimage.filters#skimage.filters.scharr_h", "type": "filters", "text": ["Find the horizontal edges of an image using the Scharr transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Scharr edge map.", "We use the following kernel:", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives."]}, {"name": "filters.scharr_v()", "path": "api/skimage.filters#skimage.filters.scharr_v", "type": "filters", "text": ["Find the vertical edges of an image using the Scharr transform.", "Image to process", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Scharr edge map.", "We use the following kernel:", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives."]}, {"name": "filters.sobel()", "path": "api/skimage.filters#skimage.filters.sobel", "type": "filters", "text": ["Find edges in an image using the Sobel filter.", "The input image.", "Clip the output image to this mask. (Values where mask=0 will be set to 0.)", "Compute the edge filter along this axis. If not provided, the edge magnitude is computed. This is defined as:", "The magnitude is also computed if axis is a sequence.", "The boundary mode for the convolution. See scipy.ndimage.convolve for a description of the modes. This can be either a single boundary mode or one boundary mode per axis.", "When mode is 'constant', this is the constant used in values outside the boundary of the image data.", "The Sobel edge map.", "See also", "D. Kroon, 2009, Short Paper University Twente, Numerical Optimization of Kernel Based Image Derivatives.", "https://en.wikipedia.org/wiki/Sobel_operator"]}, {"name": "filters.sobel_h()", "path": "api/skimage.filters#skimage.filters.sobel_h", "type": "filters", "text": ["Find the horizontal edges of an image using the Sobel transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Sobel edge map.", "We use the following kernel:"]}, {"name": "filters.sobel_v()", "path": "api/skimage.filters#skimage.filters.sobel_v", "type": "filters", "text": ["Find the vertical edges of an image using the Sobel transform.", "Image to process.", "An optional mask to limit the application to a certain area. Note that pixels surrounding masked regions are also masked to prevent masked regions from affecting the result.", "The Sobel edge map.", "We use the following kernel:"]}, {"name": "filters.threshold_isodata()", "path": "api/skimage.filters#skimage.filters.threshold_isodata", "type": "filters", "text": ["Return threshold value(s) based on ISODATA method.", "Histogram-based threshold, known as Ridler-Calvard method or inter-means. Threshold values returned satisfy the following equality:", "That is, returned thresholds are intensities that separate the image into two groups of pixels, where the threshold intensity is midway between the mean intensities of these groups.", "For integer images, the above equality holds to within one; for floating- point images, the equality holds to within the histogram bin-width.", "Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "If False (default), return only the lowest threshold that satisfies the above equality. If True, return all valid thresholds.", "Histogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.", "Threshold value(s).", "Ridler, TW & Calvard, S (1978), \u201cPicture thresholding using an iterative selection method\u201d IEEE Transactions on Systems, Man and Cybernetics 8: 630-632, DOI:10.1109/TSMC.1978.4310039", "Sezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf DOI:10.1117/1.1631315", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold"]}, {"name": "filters.threshold_li()", "path": "api/skimage.filters#skimage.filters.threshold_li", "type": "filters", "text": ["Compute threshold value by Li\u2019s iterative Minimum Cross Entropy method.", "Input image.", "Finish the computation when the change in the threshold in an iteration is less than this value. By default, this is half the smallest difference between intensity values in image.", "Li\u2019s iterative method uses gradient descent to find the optimal threshold. If the image intensity histogram contains more than two modes (peaks), the gradient descent could get stuck in a local optimum. An initial guess for the iteration can help the algorithm find the globally-optimal threshold. A float value defines a specific start point, while a callable should take in an array of image intensities and return a float value. Example valid callables include numpy.mean (default), lambda arr: numpy.quantile(arr, 0.95), or even skimage.filters.threshold_otsu().", "A function that will be called on the threshold at every iteration of the algorithm.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "Li C.H. and Lee C.K. (1993) \u201cMinimum Cross Entropy Thresholding\u201d Pattern Recognition, 26(4): 617-625 DOI:10.1016/0031-3203(93)90115-D", "Li C.H. and Tam P.K.S. (1998) \u201cAn Iterative Algorithm for Minimum Cross Entropy Thresholding\u201d Pattern Recognition Letters, 18(8): 771-776 DOI:10.1016/S0167-8655(98)00057-9", "Sezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165 DOI:10.1117/1.1631315", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold"]}, {"name": "filters.threshold_local()", "path": "api/skimage.filters#skimage.filters.threshold_local", "type": "filters", "text": ["Compute a threshold mask image based on local pixel neighborhood.", "Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Alternatively the threshold can be determined dynamically by a given function, using the \u2018generic\u2019 method.", "Input image.", "Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, \u2026, 21, \u2026).", "Method used to determine adaptive threshold for local neighbourhood in weighted mean image.", "By default the \u2018gaussian\u2019 method is used.", "Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019. Default is \u2018reflect\u2019.", "Either specify sigma for \u2018gaussian\u2019 method or function object for \u2018generic\u2019 method. This functions takes the flat array of local neighbourhood as a single argument and returns the calculated threshold for the centre pixel.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Threshold image. All pixels in the input image higher than the corresponding pixel in the threshold image are considered foreground.", "https://docs.opencv.org/modules/imgproc/doc/miscellaneous_transformations.html?highlight=threshold#adaptivethreshold"]}, {"name": "filters.threshold_mean()", "path": "api/skimage.filters#skimage.filters.threshold_mean", "type": "filters", "text": ["Return threshold value based on the mean of grayscale values.", "Grayscale input image.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "C. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993. DOI:10.1006/cgip.1993.1040"]}, {"name": "filters.threshold_minimum()", "path": "api/skimage.filters#skimage.filters.threshold_minimum", "type": "filters", "text": ["Return threshold value based on minimum method.", "The histogram of the input image is computed if not provided and smoothed until there are only two maxima. Then the minimum in between is the threshold value.", "Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Maximum number of iterations to smooth the histogram.", "Histogram to determine the threshold from and a corresponding array of bin center intensities. Alternatively, only the histogram can be passed.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "If unable to find two local maxima in the histogram or if the smoothing takes more than 1e4 iterations.", "C. A. Glasbey, \u201cAn analysis of histogram-based thresholding algorithms,\u201d CVGIP: Graphical Models and Image Processing, vol. 55, pp. 532-537, 1993.", "Prewitt, JMS & Mendelsohn, ML (1966), \u201cThe analysis of cell images\u201d, Annals of the New York Academy of Sciences 128: 1035-1053 DOI:10.1111/j.1749-6632.1965.tb11715.x"]}, {"name": "filters.threshold_multiotsu()", "path": "api/skimage.filters#skimage.filters.threshold_multiotsu", "type": "filters", "text": ["Generate classes-1 threshold values to divide gray levels in image.", "The threshold values are chosen to maximize the total sum of pairwise variances between the thresholded graylevel classes. See Notes and [1] for more details.", "Grayscale input image.", "Number of classes to be thresholded, i.e. the number of resulting regions.", "Number of bins used to calculate the histogram. This value is ignored for integer arrays.", "Array containing the threshold values for the desired classes.", "If image contains less grayscale value then the desired number of classes.", "This implementation relies on a Cython function whose complexity is \\(O\\left(\\frac{Ch^{C-1}}{(C-1)!}\\right)\\), where \\(h\\) is the number of histogram bins and \\(C\\) is the number of classes desired.", "The input image must be grayscale.", "Liao, P-S., Chen, T-S. and Chung, P-C., \u201cA fast algorithm for multilevel thresholding\u201d, Journal of Information Science and Engineering 17 (5): 713-727, 2001. Available at: <https://ftp.iis.sinica.edu.tw/JISE/2001/200109_01.pdf> DOI:10.6688/JISE.2001.17.5.1", "Tosa, Y., \u201cMulti-Otsu Threshold\u201d, a java plugin for ImageJ. Available at: <http://imagej.net/plugins/download/Multi_OtsuThreshold.java>"]}, {"name": "filters.threshold_niblack()", "path": "api/skimage.filters#skimage.filters.threshold_niblack", "type": "filters", "text": ["Applies Niblack local threshold to an array.", "A threshold T is calculated for every pixel in the image using the following formula:", "where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation.", "Input image.", "Window size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).", "Value of parameter k in threshold formula.", "Threshold mask. All pixels with an intensity higher than this value are assumed to be foreground.", "This algorithm is originally designed for text recognition.", "The Bradley threshold is a particular case of the Niblack one, being equivalent to", "for some value q. By default, Bradley and Roth use q=1.", "W. Niblack, An introduction to Digital Image Processing, Prentice-Hall, 1986.", "D. Bradley and G. Roth, \u201cAdaptive thresholding using Integral Image\u201d, Journal of Graphics Tools 12(2), pp. 13-21, 2007. DOI:10.1080/2151237X.2007.10129236"]}, {"name": "filters.threshold_otsu()", "path": "api/skimage.filters#skimage.filters.threshold_otsu", "type": "filters", "text": ["Return threshold value based on Otsu\u2019s method.", "Either image or hist must be provided. If hist is provided, the actual histogram of the image is ignored.", "Grayscale input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Histogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "The input image must be grayscale.", "Wikipedia, https://en.wikipedia.org/wiki/Otsu\u2019s_Method"]}, {"name": "filters.threshold_sauvola()", "path": "api/skimage.filters#skimage.filters.threshold_sauvola", "type": "filters", "text": ["Applies Sauvola local threshold to an array. Sauvola is a modification of Niblack technique.", "In the original method a threshold T is calculated for every pixel in the image using the following formula:", "where m(x,y) and s(x,y) are the mean and standard deviation of pixel (x,y) neighborhood defined by a rectangular window with size w times w centered around the pixel. k is a configurable parameter that weights the effect of standard deviation. R is the maximum standard deviation of a greyscale image.", "Input image.", "Window size specified as a single odd integer (3, 5, 7, \u2026), or an iterable of length image.ndim containing only odd integers (e.g. (1, 5, 5)).", "Value of the positive parameter k.", "Value of R, the dynamic range of standard deviation. If None, set to the half of the image dtype range.", "Threshold mask. All pixels with an intensity higher than this value are assumed to be foreground.", "This algorithm is originally designed for text recognition.", "J. Sauvola and M. Pietikainen, \u201cAdaptive document image binarization,\u201d Pattern Recognition 33(2), pp. 225-236, 2000. DOI:10.1016/S0031-3203(99)00055-2"]}, {"name": "filters.threshold_triangle()", "path": "api/skimage.filters#skimage.filters.threshold_triangle", "type": "filters", "text": ["Return threshold value based on the triangle algorithm.", "Grayscale input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "Zack, G. W., Rogers, W. E. and Latt, S. A., 1977, Automatic Measurement of Sister Chromatid Exchange Frequency, Journal of Histochemistry and Cytochemistry 25 (7), pp. 741-753 DOI:10.1177/25.7.70454", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold"]}, {"name": "filters.threshold_yen()", "path": "api/skimage.filters#skimage.filters.threshold_yen", "type": "filters", "text": ["Return threshold value based on Yen\u2019s method. Either image or hist must be provided. In case hist is given, the actual histogram of the image is ignored.", "Input image.", "Number of bins used to calculate histogram. This value is ignored for integer arrays.", "Histogram from which to determine the threshold, and optionally a corresponding array of bin center intensities. An alternative use of this function is to pass it only hist.", "Upper threshold value. All pixels with an intensity higher than this value are assumed to be foreground.", "Yen J.C., Chang F.J., and Chang S. (1995) \u201cA New Criterion for Automatic Multilevel Thresholding\u201d IEEE Trans. on Image Processing, 4(3): 370-378. DOI:10.1109/83.366472", "Sezgin M. and Sankur B. (2004) \u201cSurvey over Image Thresholding Techniques and Quantitative Performance Evaluation\u201d Journal of Electronic Imaging, 13(1): 146-165, DOI:10.1117/1.1631315 http://www.busim.ee.boun.edu.tr/~sankur/SankurFolder/Threshold_survey.pdf", "ImageJ AutoThresholder code, http://fiji.sc/wiki/index.php/Auto_Threshold"]}, {"name": "filters.try_all_threshold()", "path": "api/skimage.filters#skimage.filters.try_all_threshold", "type": "filters", "text": ["Returns a figure comparing the outputs of different thresholding methods.", "Input image.", "Figure size (in inches).", "Print function name for each method.", "Matplotlib figure and axes.", "The following algorithms are used:"]}, {"name": "filters.unsharp_mask()", "path": "api/skimage.filters#skimage.filters.unsharp_mask", "type": "filters", "text": ["Unsharp masking filter.", "The sharp details are identified as the difference between the original image and its blurred version. These details are then scaled, and added back to the original image.", "Input image.", "If a scalar is given, then its value is used for all dimensions. If sequence is given, then there must be exactly one radius for each dimension except the last dimension for multichannel images. Note that 0 radius means no blurring, and negative values are not allowed.", "The details will be amplified with this factor. The factor could be 0 or negative. Typically, it is a small positive number, e.g. 1.0.", "If True, the last image dimension is considered as a color channel, otherwise as spatial. Color channels are processed individually.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Image with unsharp mask applied.", "Unsharp masking is an image sharpening technique. It is a linear image operation, and numerically stable, unlike deconvolution which is an ill-posed problem. Because of this stability, it is often preferred over deconvolution.", "The main idea is as follows: sharp details are identified as the difference between the original image and its blurred version. These details are added back to the original image after a scaling step:", "enhanced image = original + amount * (original - blurred)", "When applying this filter to several color layers independently, color bleeding may occur. More visually pleasing result can be achieved by processing only the brightness/lightness/intensity channel in a suitable color space such as HSV, HSL, YUV, or YCbCr.", "Unsharp masking is described in most introductory digital image processing books. This implementation is based on [1].", "Maria Petrou, Costas Petrou \u201cImage Processing: The Fundamentals\u201d, (2010), ed ii., page 357, ISBN 13: 9781119994398 DOI:10.1002/9781119994398", "Wikipedia. Unsharp masking https://en.wikipedia.org/wiki/Unsharp_masking"]}, {"name": "filters.wiener()", "path": "api/skimage.filters#skimage.filters.wiener", "type": "filters", "text": ["Minimum Mean Square Error (Wiener) inverse filter.", "Input data.", "Ratio between power spectrum of noise and undegraded image.", "Impulse response of the filter. See LPIFilter2D.__init__.", "Additional keyword parameters to the impulse_response function.", "If you need to apply the same filter multiple times over different images, construct the LPIFilter2D and specify it here."]}, {"name": "filters.window()", "path": "api/skimage.filters#skimage.filters.window", "type": "filters", "text": ["Return an n-dimensional window of a given size and dimensionality.", "The type of window to be created. Any window type supported by scipy.signal.get_window is allowed here. See notes below for a current list, or the SciPy documentation for the version of SciPy on your machine.", "The shape of the window along each axis. If an integer is provided, a 1D window is generated.", "Keyword arguments passed to skimage.transform.warp (e.g., warp_kwargs={'order':3} to change interpolation method).", "A window of the specified shape. dtype is np.double.", "This function is based on scipy.signal.get_window and thus can access all of the window types available to that function (e.g., \"hann\", \"boxcar\"). Note that certain window types require parameters that have to be supplied with the window name as a tuple (e.g., (\"tukey\", 0.8)). If only a float is supplied, it is interpreted as the beta parameter of the Kaiser window.", "See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.get_window.html for more details.", "Note that this function generates a double precision array of the specified shape and can thus generate very large arrays that consume a large amount of available memory.", "The approach taken here to create nD windows is to first calculate the Euclidean distance from the center of the intended nD window to each position in the array. That distance is used to sample, with interpolation, from a 1D window returned from scipy.signal.get_window. The method of interpolation can be changed with the order keyword argument passed to skimage.transform.warp.", "Some coordinates in the output window will be outside of the original signal; these will be filled in with zeros.", "Window types: - boxcar - triang - blackman - hamming - hann - bartlett - flattop - parzen - bohman - blackmanharris - nuttall - barthann - kaiser (needs beta) - gaussian (needs standard deviation) - general_gaussian (needs power, width) - slepian (needs width) - dpss (needs normalized half-bandwidth) - chebwin (needs attenuation) - exponential (needs decay scale) - tukey (needs taper fraction)", "Two-dimensional window design, Wikipedia, https://en.wikipedia.org/wiki/Two_dimensional_window_design", "Return a Hann window with shape (512, 512):", "Return a Kaiser window with beta parameter of 16 and shape (256, 256, 35):", "Return a Tukey window with an alpha parameter of 0.8 and shape (100, 300):"]}, {"name": "future", "path": "api/skimage.future", "type": "future", "text": ["Functionality with an experimental API. Although you can count on the functions in this package being around in the future, the API may change with any version update and will not follow the skimage two-version deprecation path. Therefore, use the functions herein with care, and do not use them in production code that will depend on updated skimage versions.", "skimage.future.fit_segmenter(labels, \u2026)", "Segmentation using labeled parts of the image and a classifier.", "skimage.future.manual_lasso_segmentation(image)", "Return a label image based on freeform selections made with the mouse.", "skimage.future.manual_polygon_segmentation(image)", "Return a label image based on polygon selections made with the mouse.", "skimage.future.predict_segmenter(features, clf)", "Segmentation of images using a pretrained classifier.", "skimage.future.TrainableSegmenter([clf, \u2026])", "Estimator for classifying pixels.", "skimage.future.graph", "Segmentation using labeled parts of the image and a classifier.", "Image of labels. Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.", "Array of features, with the first dimension corresponding to the number of features, and the other dimensions correspond to labels.shape.", "classifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.", "classifier trained on labels", "Trainable segmentation using local features and random forests", "Return a label image based on freeform selections made with the mouse.", "Grayscale or RGB image.", "Transparency value for polygons drawn over the image.", "If True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.", "The segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.", "Press and hold the left mouse button to draw around each object.", "Return a label image based on polygon selections made with the mouse.", "Grayscale or RGB image.", "Transparency value for polygons drawn over the image.", "If True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.", "The segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.", "Use left click to select the vertices of the polygon and right click to confirm the selection once all vertices are selected.", "Segmentation of images using a pretrained classifier.", "Array of features, with the last dimension corresponding to the number of features, and the other dimensions are compatible with the shape of the image to segment, or a flattened image.", "trained classifier object, exposing a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier. The classifier must be already trained, for example with skimage.segmentation.fit_segmenter().", "Labeled array, built from the prediction of the classifier.", "Trainable segmentation using local features and random forests", "Bases: object", "Estimator for classifying pixels.", "classifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.", "function computing features on all pixels of the image, to be passed to the classifier. The output should be of shape (m_features, *labels.shape). If None, skimage.segmentation.multiscale_basic_features() is used.", "fit(image, labels)", "Train classifier using partially labeled (annotated) image.", "predict(image)", "Segment new image using trained internal classifier.", "compute_features", "Initialize self. See help(type(self)) for accurate signature.", "Train classifier using partially labeled (annotated) image.", "Input image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.", "Labeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.", "Segment new image using trained internal classifier.", "Input image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func."]}, {"name": "future.fit_segmenter()", "path": "api/skimage.future#skimage.future.fit_segmenter", "type": "future", "text": ["Segmentation using labeled parts of the image and a classifier.", "Image of labels. Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.", "Array of features, with the first dimension corresponding to the number of features, and the other dimensions correspond to labels.shape.", "classifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.", "classifier trained on labels"]}, {"name": "future.graph", "path": "api/skimage.future.graph", "type": "future", "text": ["skimage.future.graph.cut_normalized(labels, rag)", "Perform Normalized Graph cut on the Region Adjacency Graph.", "skimage.future.graph.cut_threshold(labels, \u2026)", "Combine regions separated by weight less than threshold.", "skimage.future.graph.merge_hierarchical(\u2026)", "Perform hierarchical merging of a RAG.", "skimage.future.graph.ncut(labels, rag[, \u2026])", "Perform Normalized Graph cut on the Region Adjacency Graph.", "skimage.future.graph.rag_boundary(labels, \u2026)", "Comouter RAG based on region boundaries", "skimage.future.graph.rag_mean_color(image, \u2026)", "Compute the Region Adjacency Graph using mean colors.", "skimage.future.graph.show_rag(labels, rag, image)", "Show a Region Adjacency Graph on an image.", "skimage.future.graph.RAG([label_image, \u2026])", "The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph", "Perform Normalized Graph cut on the Region Adjacency Graph.", "Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.", "The array of labels.", "The region adjacency graph.", "The threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.", "The number or N-cuts to perform before determining the optimal one.", "If set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].", "The maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.", "The new labeled array.", "Shi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.", "Combine regions separated by weight less than threshold.", "Given an image\u2019s labels and its RAG, output new labels by combining regions whose nodes are separated by a weight less than the given threshold.", "The array of labels.", "The region adjacency graph.", "The threshold. Regions connected by edges with smaller weights are combined.", "If set, modifies rag in place. The function will remove the edges with weights less that thresh. If set to False the function makes a copy of rag before proceeding.", "The new labelled array.", "Alain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950", "Perform hierarchical merging of a RAG.", "Greedily merges the most similar pair of nodes until no edges lower than thresh remain.", "The array of labels.", "The Region Adjacency Graph.", "Regions connected by an edge with weight smaller than thresh are merged.", "If set, the RAG copied before modifying.", "If set, the nodes are merged in place. Otherwise, a new node is created for each merge..", "This function is called before merging two nodes. For the RAG graph while merging src and dst, it is called as follows merge_func(graph, src, dst).", "The function to compute the new weights of the nodes adjacent to the merged node. This is directly supplied as the argument weight_func to merge_nodes.", "The new labeled array.", "Perform Normalized Graph cut on the Region Adjacency Graph.", "Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.", "The array of labels.", "The region adjacency graph.", "The threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.", "The number or N-cuts to perform before determining the optimal one.", "If set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].", "The maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.", "The new labeled array.", "Shi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000.", "Comouter RAG based on region boundaries", "Given an image\u2019s initial segmentation and its edge map this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within the image with the same label in labels. The weight between two adjacent regions is the average value in edge_map along their boundary.", "The labelled image.", "This should have the same shape as that of labels. For all pixels along the boundary between 2 adjacent regions, the average value of the corresponding pixels in edge_map is the edge weight between them.", "Pixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.filters.generate_binary_structure.", "Compute the Region Adjacency Graph using mean colors.", "Given an image and its initial segmentation, this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within image with the same label in labels. The weight between two adjacent regions represents how similar or dissimilar two regions are depending on the mode parameter.", "Input image.", "The labelled image. This should have one dimension less than image. If image has dimensions (M, N, 3) labels should have dimensions (M, N).", "Pixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.generate_binary_structure.", "The strategy to assign edge weights.", "\u2018distance\u2019 : The weight between two adjacent regions is the \\(|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents the Euclidean distance in their average color.", "\u2018similarity\u2019 : The weight between two adjacent is \\(e^{-d^2/sigma}\\) where \\(d=|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents how similar two regions are.", "Used for computation when mode is \u201csimilarity\u201d. It governs how close to each other two colors should be, for their corresponding edge weight to be significant. A very large value of sigma could make any two colors behave as though they were similar.", "The region adjacency graph.", "Alain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950", "Show a Region Adjacency Graph on an image.", "Given a labelled image and its corresponding RAG, show the nodes and edges of the RAG on the image with the specified colors. Edges are displayed between the centroid of the 2 adjacent regions in the image.", "The labelled image.", "The Region Adjacency Graph.", "Input image. If colormap is None, the image should be in RGB format.", "Color with which the borders between regions are drawn.", "The thickness with which the RAG edges are drawn.", "Any matplotlib colormap with which the edges are drawn.", "Any matplotlib colormap with which the image is draw. If set to None the image is drawn as it is.", "If set, the RAG is modified in place. For each node n the function will set a new attribute rag.nodes[n]['centroid'].", "The axes to draw on. If not specified, new axes are created and drawn on.", "A colection of lines that represent the edges of the graph. It can be passed to the matplotlib.figure.Figure.colorbar() function.", "Bases: networkx.classes.graph.Graph", "The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph", "An initial segmentation, with each region labeled as a different integer. Every unique value in label_image will correspond to a node in the graph.", "The connectivity between pixels in label_image. For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right, while a connectivity of 2 also includes diagonal neighbors. See scipy.ndimage.generate_binary_structure.", "Initial or additional edges to pass to the NetworkX Graph constructor. See networkx.Graph. Valid edge specifications include edge list (list of tuples), NumPy arrays, and SciPy sparse matrices.", "Additional attributes to add to the graph.", "Initialize a graph with edges, name, or graph attributes.", "Data to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.", "Attributes to add to graph as key=value pairs.", "See also", "Arbitrary graph attribute pairs (key=value) may be assigned", "Add an edge between u and v while updating max node id.", "See also", "networkx.Graph.add_edge().", "Add node n while updating the maximum node id.", "See also", "networkx.Graph.add_node().", "Copy the graph with its max node id.", "See also", "networkx.Graph.copy().", "Return a fresh copy graph with the same data structure.", "A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph.", "This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document:", "Merge node src and dst.", "The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.", "Nodes to be merged.", "Function to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.", "If set to True, the merged node has the id dst, else merged node has a new id which is returned.", "The sequence of extra positional arguments passed to weight_func.", "The dict of keyword arguments passed to the weight_func.", "The id of the new node.", "If in_place is False the resulting node has a new id, rather than dst.", "Returns the id for the new node to be inserted.", "The current implementation returns one more than the maximum id.", "The id of the new node to be inserted."]}, {"name": "future.graph.cut_normalized()", "path": "api/skimage.future.graph#skimage.future.graph.cut_normalized", "type": "future", "text": ["Perform Normalized Graph cut on the Region Adjacency Graph.", "Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.", "The array of labels.", "The region adjacency graph.", "The threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.", "The number or N-cuts to perform before determining the optimal one.", "If set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].", "The maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.", "The new labeled array.", "Shi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000."]}, {"name": "future.graph.cut_threshold()", "path": "api/skimage.future.graph#skimage.future.graph.cut_threshold", "type": "future", "text": ["Combine regions separated by weight less than threshold.", "Given an image\u2019s labels and its RAG, output new labels by combining regions whose nodes are separated by a weight less than the given threshold.", "The array of labels.", "The region adjacency graph.", "The threshold. Regions connected by edges with smaller weights are combined.", "If set, modifies rag in place. The function will remove the edges with weights less that thresh. If set to False the function makes a copy of rag before proceeding.", "The new labelled array.", "Alain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950"]}, {"name": "future.graph.merge_hierarchical()", "path": "api/skimage.future.graph#skimage.future.graph.merge_hierarchical", "type": "future", "text": ["Perform hierarchical merging of a RAG.", "Greedily merges the most similar pair of nodes until no edges lower than thresh remain.", "The array of labels.", "The Region Adjacency Graph.", "Regions connected by an edge with weight smaller than thresh are merged.", "If set, the RAG copied before modifying.", "If set, the nodes are merged in place. Otherwise, a new node is created for each merge..", "This function is called before merging two nodes. For the RAG graph while merging src and dst, it is called as follows merge_func(graph, src, dst).", "The function to compute the new weights of the nodes adjacent to the merged node. This is directly supplied as the argument weight_func to merge_nodes.", "The new labeled array."]}, {"name": "future.graph.ncut()", "path": "api/skimage.future.graph#skimage.future.graph.ncut", "type": "future", "text": ["Perform Normalized Graph cut on the Region Adjacency Graph.", "Given an image\u2019s labels and its similarity RAG, recursively perform a 2-way normalized cut on it. All nodes belonging to a subgraph that cannot be cut further are assigned a unique label in the output.", "The array of labels.", "The region adjacency graph.", "The threshold. A subgraph won\u2019t be further subdivided if the value of the N-cut exceeds thresh.", "The number or N-cuts to perform before determining the optimal one.", "If set, modifies rag in place. For each node n the function will set a new attribute rag.nodes[n]['ncut label'].", "The maximum possible value of an edge in the RAG. This corresponds to an edge between identical regions. This is used to put self edges in the RAG.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. The random state is used for the starting point of scipy.sparse.linalg.eigsh.", "The new labeled array.", "Shi, J.; Malik, J., \u201cNormalized cuts and image segmentation\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 22, no. 8, pp. 888-905, August 2000."]}, {"name": "future.graph.RAG", "path": "api/skimage.future.graph#skimage.future.graph.RAG", "type": "future", "text": ["Bases: networkx.classes.graph.Graph", "The Region Adjacency Graph (RAG) of an image, subclasses networx.Graph", "An initial segmentation, with each region labeled as a different integer. Every unique value in label_image will correspond to a node in the graph.", "The connectivity between pixels in label_image. For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right, while a connectivity of 2 also includes diagonal neighbors. See scipy.ndimage.generate_binary_structure.", "Initial or additional edges to pass to the NetworkX Graph constructor. See networkx.Graph. Valid edge specifications include edge list (list of tuples), NumPy arrays, and SciPy sparse matrices.", "Additional attributes to add to the graph.", "Initialize a graph with edges, name, or graph attributes.", "Data to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.", "Attributes to add to graph as key=value pairs.", "See also", "Arbitrary graph attribute pairs (key=value) may be assigned", "Add an edge between u and v while updating max node id.", "See also", "networkx.Graph.add_edge().", "Add node n while updating the maximum node id.", "See also", "networkx.Graph.add_node().", "Copy the graph with its max node id.", "See also", "networkx.Graph.copy().", "Return a fresh copy graph with the same data structure.", "A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph.", "This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document:", "Merge node src and dst.", "The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.", "Nodes to be merged.", "Function to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.", "If set to True, the merged node has the id dst, else merged node has a new id which is returned.", "The sequence of extra positional arguments passed to weight_func.", "The dict of keyword arguments passed to the weight_func.", "The id of the new node.", "If in_place is False the resulting node has a new id, rather than dst.", "Returns the id for the new node to be inserted.", "The current implementation returns one more than the maximum id.", "The id of the new node to be inserted."]}, {"name": "future.graph.RAG.add_edge()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_edge", "type": "future", "text": ["Add an edge between u and v while updating max node id.", "See also", "networkx.Graph.add_edge()."]}, {"name": "future.graph.RAG.add_node()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.add_node", "type": "future", "text": ["Add node n while updating the maximum node id.", "See also", "networkx.Graph.add_node()."]}, {"name": "future.graph.RAG.copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.copy", "type": "future", "text": ["Copy the graph with its max node id.", "See also", "networkx.Graph.copy()."]}, {"name": "future.graph.RAG.fresh_copy()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.fresh_copy", "type": "future", "text": ["Return a fresh copy graph with the same data structure.", "A fresh copy has no nodes, edges or graph attributes. It is the same data structure as the current graph. This method is typically used to create an empty version of the graph.", "This is required when subclassing Graph with networkx v2 and does not cause problems for v1. Here is more detail from the network migrating from 1.x to 2.x document:"]}, {"name": "future.graph.RAG.merge_nodes()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.merge_nodes", "type": "future", "text": ["Merge node src and dst.", "The new combined node is adjacent to all the neighbors of src and dst. weight_func is called to decide the weight of edges incident on the new node.", "Nodes to be merged.", "Function to decide the attributes of edges incident on the new node. For each neighbor n for src and `dst, weight_func will be called as follows: weight_func(src, dst, n, *extra_arguments, **extra_keywords). src, dst and n are IDs of vertices in the RAG object which is in turn a subclass of networkx.Graph. It is expected to return a dict of attributes of the resulting edge.", "If set to True, the merged node has the id dst, else merged node has a new id which is returned.", "The sequence of extra positional arguments passed to weight_func.", "The dict of keyword arguments passed to the weight_func.", "The id of the new node.", "If in_place is False the resulting node has a new id, rather than dst."]}, {"name": "future.graph.RAG.next_id()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.next_id", "type": "future", "text": ["Returns the id for the new node to be inserted.", "The current implementation returns one more than the maximum id.", "The id of the new node to be inserted."]}, {"name": "future.graph.RAG.__init__()", "path": "api/skimage.future.graph#skimage.future.graph.RAG.__init__", "type": "future", "text": ["Initialize a graph with edges, name, or graph attributes.", "Data to initialize graph. If None (default) an empty graph is created. The data can be an edge list, or any NetworkX graph object. If the corresponding optional Python packages are installed the data can also be a NumPy matrix or 2d ndarray, a SciPy sparse matrix, or a PyGraphviz graph.", "Attributes to add to graph as key=value pairs.", "See also", "Arbitrary graph attribute pairs (key=value) may be assigned"]}, {"name": "future.graph.rag_boundary()", "path": "api/skimage.future.graph#skimage.future.graph.rag_boundary", "type": "future", "text": ["Comouter RAG based on region boundaries", "Given an image\u2019s initial segmentation and its edge map this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within the image with the same label in labels. The weight between two adjacent regions is the average value in edge_map along their boundary.", "The labelled image.", "This should have the same shape as that of labels. For all pixels along the boundary between 2 adjacent regions, the average value of the corresponding pixels in edge_map is the edge weight between them.", "Pixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.filters.generate_binary_structure."]}, {"name": "future.graph.rag_mean_color()", "path": "api/skimage.future.graph#skimage.future.graph.rag_mean_color", "type": "future", "text": ["Compute the Region Adjacency Graph using mean colors.", "Given an image and its initial segmentation, this method constructs the corresponding Region Adjacency Graph (RAG). Each node in the RAG represents a set of pixels within image with the same label in labels. The weight between two adjacent regions represents how similar or dissimilar two regions are depending on the mode parameter.", "Input image.", "The labelled image. This should have one dimension less than image. If image has dimensions (M, N, 3) labels should have dimensions (M, N).", "Pixels with a squared distance less than connectivity from each other are considered adjacent. It can range from 1 to labels.ndim. Its behavior is the same as connectivity parameter in scipy.ndimage.generate_binary_structure.", "The strategy to assign edge weights.", "\u2018distance\u2019 : The weight between two adjacent regions is the \\(|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents the Euclidean distance in their average color.", "\u2018similarity\u2019 : The weight between two adjacent is \\(e^{-d^2/sigma}\\) where \\(d=|c_1 - c_2|\\), where \\(c_1\\) and \\(c_2\\) are the mean colors of the two regions. It represents how similar two regions are.", "Used for computation when mode is \u201csimilarity\u201d. It governs how close to each other two colors should be, for their corresponding edge weight to be significant. A very large value of sigma could make any two colors behave as though they were similar.", "The region adjacency graph.", "Alain Tremeau and Philippe Colantoni \u201cRegions Adjacency Graph Applied To Color Image Segmentation\u201d DOI:10.1109/83.841950"]}, {"name": "future.graph.show_rag()", "path": "api/skimage.future.graph#skimage.future.graph.show_rag", "type": "future", "text": ["Show a Region Adjacency Graph on an image.", "Given a labelled image and its corresponding RAG, show the nodes and edges of the RAG on the image with the specified colors. Edges are displayed between the centroid of the 2 adjacent regions in the image.", "The labelled image.", "The Region Adjacency Graph.", "Input image. If colormap is None, the image should be in RGB format.", "Color with which the borders between regions are drawn.", "The thickness with which the RAG edges are drawn.", "Any matplotlib colormap with which the edges are drawn.", "Any matplotlib colormap with which the image is draw. If set to None the image is drawn as it is.", "If set, the RAG is modified in place. For each node n the function will set a new attribute rag.nodes[n]['centroid'].", "The axes to draw on. If not specified, new axes are created and drawn on.", "A colection of lines that represent the edges of the graph. It can be passed to the matplotlib.figure.Figure.colorbar() function."]}, {"name": "future.manual_lasso_segmentation()", "path": "api/skimage.future#skimage.future.manual_lasso_segmentation", "type": "future", "text": ["Return a label image based on freeform selections made with the mouse.", "Grayscale or RGB image.", "Transparency value for polygons drawn over the image.", "If True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.", "The segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.", "Press and hold the left mouse button to draw around each object."]}, {"name": "future.manual_polygon_segmentation()", "path": "api/skimage.future#skimage.future.manual_polygon_segmentation", "type": "future", "text": ["Return a label image based on polygon selections made with the mouse.", "Grayscale or RGB image.", "Transparency value for polygons drawn over the image.", "If True, an array containing each separate polygon drawn is returned. (The polygons may overlap.) If False (default), latter polygons \u201coverwrite\u201d earlier ones where they overlap.", "The segmented regions. If mode is \u2018separate\u2019, the leading dimension of the array corresponds to the number of regions that the user drew.", "Use left click to select the vertices of the polygon and right click to confirm the selection once all vertices are selected."]}, {"name": "future.predict_segmenter()", "path": "api/skimage.future#skimage.future.predict_segmenter", "type": "future", "text": ["Segmentation of images using a pretrained classifier.", "Array of features, with the last dimension corresponding to the number of features, and the other dimensions are compatible with the shape of the image to segment, or a flattened image.", "trained classifier object, exposing a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier. The classifier must be already trained, for example with skimage.segmentation.fit_segmenter().", "Labeled array, built from the prediction of the classifier."]}, {"name": "future.TrainableSegmenter", "path": "api/skimage.future#skimage.future.TrainableSegmenter", "type": "future", "text": ["Bases: object", "Estimator for classifying pixels.", "classifier object, exposing a fit and a predict method as in scikit-learn\u2019s API, for example an instance of RandomForestClassifier or LogisticRegression classifier.", "function computing features on all pixels of the image, to be passed to the classifier. The output should be of shape (m_features, *labels.shape). If None, skimage.segmentation.multiscale_basic_features() is used.", "fit(image, labels)", "Train classifier using partially labeled (annotated) image.", "predict(image)", "Segment new image using trained internal classifier.", "compute_features", "Initialize self. See help(type(self)) for accurate signature.", "Train classifier using partially labeled (annotated) image.", "Input image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.", "Labeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented.", "Segment new image using trained internal classifier.", "Input image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func."]}, {"name": "future.TrainableSegmenter.compute_features()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.compute_features", "type": "future", "text": []}, {"name": "future.TrainableSegmenter.fit()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.fit", "type": "future", "text": ["Train classifier using partially labeled (annotated) image.", "Input image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func.", "Labeled array of shape compatible with image (same shape for a single-channel image). Labels >= 1 correspond to the training set and label 0 to unlabeled pixels to be segmented."]}, {"name": "future.TrainableSegmenter.predict()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.predict", "type": "future", "text": ["Segment new image using trained internal classifier.", "Input image, which can be grayscale or multichannel, and must have a number of dimensions compatible with self.features_func."]}, {"name": "future.TrainableSegmenter.__init__()", "path": "api/skimage.future#skimage.future.TrainableSegmenter.__init__", "type": "future", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "Geometrical transformations of images", "path": "user_guide/geometrical_transform", "type": "Guide", "text": ["Images being NumPy arrays (as described in the A crash course on NumPy for images section), cropping an image can be done with simple slicing operations. Below we crop a 100x100 square corresponding to the top-left corner of the astronaut image. Note that this operation is done for all color channels (the color dimension is the last, third dimension):", "In order to change the shape of the image, skimage.color provides several functions described in Rescale, resize, and downscale .", "Homographies are transformations of a Euclidean space that preserve the alignment of points. Specific cases of homographies correspond to the conservation of more properties, such as parallelism (affine transformation), shape (similar transformation) or distances (Euclidean transformation). The different types of homographies available in scikit-image are presented in Types of homographies.", "Projective transformations can either be created using the explicit parameters (e.g. scale, shear, rotation and translation):", "or the full transformation matrix:", "The transformation matrix of a transform is available as its tform.params attribute. Transformations can be composed by multiplying matrices with the @ matrix multiplication operator.", "Transformation matrices use Homogeneous coordinates, which are the extension of Cartesian coordinates used in Euclidean geometry to the more general projective geometry. In particular, points at infinity can be represented with finite coordinates.", "Transformations can be applied to images using skimage.transform.warp():", "The different transformations in skimage.transform have a estimate method in order to estimate the parameters of the transformation from two sets of points (the source and the destination), as explained in the Using geometric transformations tutorial:", "The estimate method uses least-squares optimization to minimize the distance between source and optimization. Source and destination points can be determined manually, or using the different methods for feature detection available in skimage.feature, such as", "and matching points using skimage.feature.match_descriptors() before estimating transformation parameters. However, spurious matches are often made, and it is advisable to use the RANSAC algorithm (instead of simple least-squares optimization) to improve the robustness to outliers, as explained in Robust matching using RANSAC.", "Examples showing applications of transformation estimation are", "The estimate method is point-based, that is, it uses only a set of points from the source and destination images. For estimating translations (shifts), it is also possible to use a full-field method using all pixels, based on Fourier-space cross-correlation. This method is implemented by skimage.registration.register_translation() and explained in the Image Registration tutorial.", "The Using Polar and Log-Polar Transformations for Registration tutorial explains a variant of this full-field method for estimating a rotation, by using first a log-polar transformation."]}, {"name": "Getting help on using skimage", "path": "user_guide/getting_help", "type": "Guide", "text": ["Besides the user guide, there exist other opportunities to get help on using skimage.", "The General examples gallery provides graphical examples of typical image processing tasks. By a quick glance at the different thumbnails, the user may find an example close to a typical use case of interest. Each graphical example page displays an introductory paragraph, a figure, and the source code that generated the figure. Downloading the Python source code enables one to modify quickly the example into a case closer to one\u2019s image processing applications.", "Users are warmly encouraged to report on their use of skimage on the Mailing-list, in order to propose more examples in the future. Contributing examples to the gallery can be done on github (see How to contribute to scikit-image).", "The quick search field located in the navigation bar of the html documentation can be used to search for specific keywords (segmentation, rescaling, denoising, etc.).", "NumPy provides a lookfor function to search API functions. By default lookfor will search the NumPy API. NumPy lookfor example: `np.lookfor('eigenvector') `", "But it can be used to search in modules, by passing in the module name as a string:", "` np.lookfor('boundaries', 'skimage') `", "or the module itself. `\n> import skimage\n> np.lookfor('boundaries', skimage)\n`", "Docstrings of skimage functions are formatted using Numpy\u2019s documentation standard, starting with a Parameters section for the arguments and a Returns section for the objects returned by the function. Also, most functions include one or more examples.", "The scikit-image mailing-list is scikit-image@python.org (users should join before posting). This mailing-list is shared by users and developers, and it is the right place to ask any question about skimage, or in general, image processing using Python. Posting snippets of code with minimal examples ensures to get more relevant and focused answers.", "We would love to hear from how you use skimage for your work on the mailing-list!"]}, {"name": "Getting started", "path": "user_guide/getting_started", "type": "Guide", "text": ["scikit-image is an image processing Python package that works with numpy arrays. The package is imported as skimage:", "Most functions of skimage are found within submodules:", "A list of submodules and functions is found on the API reference webpage.", "Within scikit-image, images are represented as NumPy arrays, for example 2-D arrays for grayscale 2-D images", "The skimage.data submodule provides a set of functions returning example images, that can be used to get started quickly on using scikit-image\u2019s functions:", "Of course, it is also possible to load your own images as NumPy arrays from image files, using skimage.io.imread():", "Use natsort to load multiple images"]}, {"name": "graph", "path": "api/skimage.graph", "type": "graph", "text": ["skimage.graph.route_through_array(array, \u2026)", "Simple example of how to use the MCP and MCP_Geometric classes.", "skimage.graph.shortest_path(arr[, reach, \u2026])", "Find the shortest path through an n-d array from one side to another.", "skimage.graph.MCP(costs[, offsets, \u2026])", "A class for finding the minimum cost path through a given n-d costs array.", "skimage.graph.MCP_Connect(costs[, offsets, \u2026])", "Connect source points using the distance-weighted minimum cost function.", "skimage.graph.MCP_Flexible(costs[, offsets, \u2026])", "Find minimum cost paths through an N-d costs array.", "skimage.graph.MCP_Geometric(costs[, \u2026])", "Find distance-weighted minimum cost paths through an n-d costs array.", "Simple example of how to use the MCP and MCP_Geometric classes.", "See the MCP and MCP_Geometric class documentation for explanation of the path-finding algorithm.", "Array of costs.", "n-d index into array defining the starting point", "n-d index into array defining the end point", "If True, diagonal moves are permitted, if False, only axial moves.", "If True, the MCP_Geometric class is used to calculate costs, if False, the MCP base class is used. See the class documentation for an explanation of the differences between MCP and MCP_Geometric.", "List of n-d index tuples defining the path from start to end.", "Cost of the path. If geometric is False, the cost of the path is the sum of the values of array along the path. If geometric is True, a finer computation is made (see the documentation of the MCP_Geometric class).", "See also", "Find the shortest path through an n-d array from one side to another.", "By default (reach = 1), the shortest path can only move one row up or down for every step it moves forward (i.e., the path gradient is limited to 1). reach defines the number of elements that can be skipped along each non-axis dimension at each step.", "The axis along which the path must always move forward (default -1)", "See return value p for explanation.", "For each step along axis, the coordinate of the shortest path. If output_indexlist is True, then the path is returned as a list of n-d tuples that index into arr. If False, then the path is returned as an array listing the coordinates of the path along the non-axis dimensions for each step along the axis dimension. That is, p.shape == (arr.shape[axis], arr.ndim-1) except that p is squeezed before returning so if arr.ndim == 2, then p.shape == (arr.shape[axis],)", "Cost of path. This is the absolute sum of all the differences along the path.", "Bases: object", "A class for finding the minimum cost path through a given n-d costs array.", "Given an n-d costs array, this class can be used to find the minimum-cost path through that array from any set of points to any other set of points. Basic usage is to initialize the class and call find_costs() with a one or more starting indices (and an optional list of end indices). After that, call traceback() one or more times to find the path from any given end-position to the closest starting index. New paths through the same costs array can be found by calling find_costs() repeatedly.", "The cost of a path is calculated simply as the sum of the values of the costs array at each point on the path. The class MCP_Geometric, on the other hand, accounts for the fact that diagonal vs. axial moves are of different lengths, and weights the path cost accordingly.", "Array elements with infinite or negative costs will simply be ignored, as will paths whose cumulative cost overflows to infinite.", "A list of offset tuples: each offset specifies a valid move from a given n-d position. If not provided, offsets corresponding to a singly- or fully-connected n-d neighborhood will be constructed with make_offsets(), using the fully_connected parameter value.", "If no offsets are provided, this determines the connectivity of the generated neighborhood. If true, the path may go along diagonals between elements of the costs array; otherwise only axial moves are permitted.", "For each dimension, specifies the distance between two cells/voxels. If not given or None, the distance is assumed unit.", "Equivalent to the offsets provided to the constructor, or if none were so provided, the offsets created for the requested n-d neighborhood. These are useful for interpreting the traceback array returned by the find_costs() method.", "See class documentation.", "Find the minimum-cost path from the given starting points.", "This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.", "A list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.", "A list of n-d ending indices.", "If \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)", "Same shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)", "Same shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.", "int goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours.", "This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point.", "This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done.", "Trace a minimum cost path through the pre-calculated traceback array.", "This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.", "An n-d index into the costs array.", "A list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)", "Bases: skimage.graph._mcp.MCP", "Connect source points using the distance-weighted minimum cost function.", "A front is grown from each seed point simultaneously, while the origin of the front is tracked as well. When two fronts meet, create_connection() is called. This method must be overloaded to deal with the found edges in a way that is appropriate for the application.", "Initialize self. See help(type(self)) for accurate signature.", "create_connection id1, id2, pos1, pos2, cost1, cost2)", "Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs).", "At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.", "The seed point id where the first neighbor originated from.", "The seed point id where the second neighbor originated from.", "The index of of the first neighbour in the connection.", "The index of of the second neighbour in the connection.", "The cumulative cost at pos1.", "The cumulative costs at pos2.", "Bases: skimage.graph._mcp.MCP", "Find minimum cost paths through an N-d costs array.", "See the documentation for MCP for full details. This class differs from MCP in that several methods can be overloaded (from pure Python) to modify the behavior of the algorithm and/or create custom algorithms based on MCP. Note that goal_reached can also be overloaded in the MCP class.", "See class documentation.", "This method is called once for every pair of neighboring nodes, as soon as both nodes are frozen.", "This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook.", "This method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm.", "This method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated.", "This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map.", "Bases: skimage.graph._mcp.MCP", "Find distance-weighted minimum cost paths through an n-d costs array.", "See the documentation for MCP for full details. This class differs from MCP in that the cost of a path is not simply the sum of the costs along that path.", "This class instead assumes that the costs array contains at each position the \u201ccost\u201d of a unit distance of travel through that position. For example, a move (in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the pixel (1, 1) and terminate in the center of (1, 2). The entire move is of distance 1, half through (1, 1) and half through (1, 2); thus the cost of that move is (1/2)*costs[1,1] + (1/2)*costs[1,2].", "On the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is sqrt(2) in length. Half of this move is within the pixel (1, 1) and the other half in (2, 2), so the cost of this move is calculated as (sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2].", "These calculations don\u2019t make a lot of sense with offsets of magnitude greater than 1. Use the sampling argument in order to deal with anisotropic data.", "See class documentation."]}, {"name": "graph.MCP", "path": "api/skimage.graph#skimage.graph.MCP", "type": "graph", "text": ["Bases: object", "A class for finding the minimum cost path through a given n-d costs array.", "Given an n-d costs array, this class can be used to find the minimum-cost path through that array from any set of points to any other set of points. Basic usage is to initialize the class and call find_costs() with a one or more starting indices (and an optional list of end indices). After that, call traceback() one or more times to find the path from any given end-position to the closest starting index. New paths through the same costs array can be found by calling find_costs() repeatedly.", "The cost of a path is calculated simply as the sum of the values of the costs array at each point on the path. The class MCP_Geometric, on the other hand, accounts for the fact that diagonal vs. axial moves are of different lengths, and weights the path cost accordingly.", "Array elements with infinite or negative costs will simply be ignored, as will paths whose cumulative cost overflows to infinite.", "A list of offset tuples: each offset specifies a valid move from a given n-d position. If not provided, offsets corresponding to a singly- or fully-connected n-d neighborhood will be constructed with make_offsets(), using the fully_connected parameter value.", "If no offsets are provided, this determines the connectivity of the generated neighborhood. If true, the path may go along diagonals between elements of the costs array; otherwise only axial moves are permitted.", "For each dimension, specifies the distance between two cells/voxels. If not given or None, the distance is assumed unit.", "Equivalent to the offsets provided to the constructor, or if none were so provided, the offsets created for the requested n-d neighborhood. These are useful for interpreting the traceback array returned by the find_costs() method.", "See class documentation.", "Find the minimum-cost path from the given starting points.", "This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.", "A list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.", "A list of n-d ending indices.", "If \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)", "Same shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)", "Same shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered.", "int goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours.", "This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point.", "This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done.", "Trace a minimum cost path through the pre-calculated traceback array.", "This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.", "An n-d index into the costs array.", "A list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)"]}, {"name": "graph.MCP.find_costs()", "path": "api/skimage.graph#skimage.graph.MCP.find_costs", "type": "graph", "text": ["Find the minimum-cost path from the given starting points.", "This method finds the minimum-cost path to the specified ending indices from any one of the specified starting indices. If no end positions are given, then the minimum-cost path to every position in the costs array will be found.", "A list of n-d starting indices (where n is the dimension of the costs array). The minimum cost path to the closest/cheapest starting point will be found.", "A list of n-d ending indices.", "If \u2018True\u2019 (default), the minimum-cost-path to every specified end-position will be found; otherwise the algorithm will stop when a a path is found to any end-position. (If no ends were specified, then this parameter has no effect.)", "Same shape as the costs array; this array records the minimum cost path from the nearest/cheapest starting index to each index considered. (If ends were specified, not all elements in the array will necessarily be considered: positions not evaluated will have a cumulative cost of inf. If find_all_ends is \u2018False\u2019, only one of the specified end-positions will have a finite cumulative cost.)", "Same shape as the costs array; this array contains the offset to any given index from its predecessor index. The offset indices index into the offsets attribute, which is a array of n-d offsets. In the 2-d case, if offsets[traceback[x, y]] is (-1, -1), that means that the predecessor of [x, y] in the minimum cost path to some start position is [x+1, y+1]. Note that if the offset_index is -1, then the given index was not considered."]}, {"name": "graph.MCP.goal_reached()", "path": "api/skimage.graph#skimage.graph.MCP.goal_reached", "type": "graph", "text": ["int goal_reached(int index, float cumcost) This method is called each iteration after popping an index from the heap, before examining the neighbours.", "This method can be overloaded to modify the behavior of the MCP algorithm. An example might be to stop the algorithm when a certain cumulative cost is reached, or when the front is a certain distance away from the seed point.", "This method should return 1 if the algorithm should not check the current point\u2019s neighbours and 2 if the algorithm is now done."]}, {"name": "graph.MCP.traceback()", "path": "api/skimage.graph#skimage.graph.MCP.traceback", "type": "graph", "text": ["Trace a minimum cost path through the pre-calculated traceback array.", "This convenience function reconstructs the the minimum cost path to a given end position from one of the starting indices provided to find_costs(), which must have been called previously. This function can be called as many times as desired after find_costs() has been run.", "An n-d index into the costs array.", "A list of indices into the costs array, starting with one of the start positions passed to find_costs(), and ending with the given end index. These indices specify the minimum-cost path from any given start index to the end index. (The total cost of that path can be read out from the cumulative_costs array returned by find_costs().)"]}, {"name": "graph.MCP.__init__()", "path": "api/skimage.graph#skimage.graph.MCP.__init__", "type": "graph", "text": ["See class documentation."]}, {"name": "graph.MCP_Connect", "path": "api/skimage.graph#skimage.graph.MCP_Connect", "type": "graph", "text": ["Bases: skimage.graph._mcp.MCP", "Connect source points using the distance-weighted minimum cost function.", "A front is grown from each seed point simultaneously, while the origin of the front is tracked as well. When two fronts meet, create_connection() is called. This method must be overloaded to deal with the found edges in a way that is appropriate for the application.", "Initialize self. See help(type(self)) for accurate signature.", "create_connection id1, id2, pos1, pos2, cost1, cost2)", "Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs).", "At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.", "The seed point id where the first neighbor originated from.", "The seed point id where the second neighbor originated from.", "The index of of the first neighbour in the connection.", "The index of of the second neighbour in the connection.", "The cumulative cost at pos1.", "The cumulative costs at pos2."]}, {"name": "graph.MCP_Connect.create_connection()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.create_connection", "type": "graph", "text": ["create_connection id1, id2, pos1, pos2, cost1, cost2)", "Overload this method to keep track of the connections that are found during MCP processing. Note that a connection with the same ids can be found multiple times (but with different positions and costs).", "At the time that this method is called, both points are \u201cfrozen\u201d and will not be visited again by the MCP algorithm.", "The seed point id where the first neighbor originated from.", "The seed point id where the second neighbor originated from.", "The index of of the first neighbour in the connection.", "The index of of the second neighbour in the connection.", "The cumulative cost at pos1.", "The cumulative costs at pos2."]}, {"name": "graph.MCP_Connect.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Connect.__init__", "type": "graph", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "graph.MCP_Flexible", "path": "api/skimage.graph#skimage.graph.MCP_Flexible", "type": "graph", "text": ["Bases: skimage.graph._mcp.MCP", "Find minimum cost paths through an N-d costs array.", "See the documentation for MCP for full details. This class differs from MCP in that several methods can be overloaded (from pure Python) to modify the behavior of the algorithm and/or create custom algorithms based on MCP. Note that goal_reached can also be overloaded in the MCP class.", "See class documentation.", "This method is called once for every pair of neighboring nodes, as soon as both nodes are frozen.", "This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook.", "This method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm.", "This method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated.", "This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map."]}, {"name": "graph.MCP_Flexible.examine_neighbor()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.examine_neighbor", "type": "graph", "text": ["This method is called once for every pair of neighboring nodes, as soon as both nodes are frozen.", "This method can be overloaded to obtain information about neightboring nodes, and/or to modify the behavior of the MCP algorithm. One example is the MCP_Connect class, which checks for meeting fronts using this hook."]}, {"name": "graph.MCP_Flexible.travel_cost()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.travel_cost", "type": "graph", "text": ["This method calculates the travel cost for going from the current node to the next. The default implementation returns new_cost. Overload this method to adapt the behaviour of the algorithm."]}, {"name": "graph.MCP_Flexible.update_node()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.update_node", "type": "graph", "text": ["This method is called when a node is updated, right after new_index is pushed onto the heap and the traceback map is updated.", "This method can be overloaded to keep track of other arrays that are used by a specific implementation of the algorithm. For instance the MCP_Connect class uses it to update an id map."]}, {"name": "graph.MCP_Flexible.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Flexible.__init__", "type": "graph", "text": ["See class documentation."]}, {"name": "graph.MCP_Geometric", "path": "api/skimage.graph#skimage.graph.MCP_Geometric", "type": "graph", "text": ["Bases: skimage.graph._mcp.MCP", "Find distance-weighted minimum cost paths through an n-d costs array.", "See the documentation for MCP for full details. This class differs from MCP in that the cost of a path is not simply the sum of the costs along that path.", "This class instead assumes that the costs array contains at each position the \u201ccost\u201d of a unit distance of travel through that position. For example, a move (in 2-d) from (1, 1) to (1, 2) is assumed to originate in the center of the pixel (1, 1) and terminate in the center of (1, 2). The entire move is of distance 1, half through (1, 1) and half through (1, 2); thus the cost of that move is (1/2)*costs[1,1] + (1/2)*costs[1,2].", "On the other hand, a move from (1, 1) to (2, 2) is along the diagonal and is sqrt(2) in length. Half of this move is within the pixel (1, 1) and the other half in (2, 2), so the cost of this move is calculated as (sqrt(2)/2)*costs[1,1] + (sqrt(2)/2)*costs[2,2].", "These calculations don\u2019t make a lot of sense with offsets of magnitude greater than 1. Use the sampling argument in order to deal with anisotropic data.", "See class documentation."]}, {"name": "graph.MCP_Geometric.__init__()", "path": "api/skimage.graph#skimage.graph.MCP_Geometric.__init__", "type": "graph", "text": ["See class documentation."]}, {"name": "graph.route_through_array()", "path": "api/skimage.graph#skimage.graph.route_through_array", "type": "graph", "text": ["Simple example of how to use the MCP and MCP_Geometric classes.", "See the MCP and MCP_Geometric class documentation for explanation of the path-finding algorithm.", "Array of costs.", "n-d index into array defining the starting point", "n-d index into array defining the end point", "If True, diagonal moves are permitted, if False, only axial moves.", "If True, the MCP_Geometric class is used to calculate costs, if False, the MCP base class is used. See the class documentation for an explanation of the differences between MCP and MCP_Geometric.", "List of n-d index tuples defining the path from start to end.", "Cost of the path. If geometric is False, the cost of the path is the sum of the values of array along the path. If geometric is True, a finer computation is made (see the documentation of the MCP_Geometric class).", "See also"]}, {"name": "graph.shortest_path()", "path": "api/skimage.graph#skimage.graph.shortest_path", "type": "graph", "text": ["Find the shortest path through an n-d array from one side to another.", "By default (reach = 1), the shortest path can only move one row up or down for every step it moves forward (i.e., the path gradient is limited to 1). reach defines the number of elements that can be skipped along each non-axis dimension at each step.", "The axis along which the path must always move forward (default -1)", "See return value p for explanation.", "For each step along axis, the coordinate of the shortest path. If output_indexlist is True, then the path is returned as a list of n-d tuples that index into arr. If False, then the path is returned as an array listing the coordinates of the path along the non-axis dimensions for each step along the axis dimension. That is, p.shape == (arr.shape[axis], arr.ndim-1) except that p is squeezed before returning so if arr.ndim == 2, then p.shape == (arr.shape[axis],)", "Cost of path. This is the absolute sum of all the differences along the path."]}, {"name": "Handling Video Files", "path": "user_guide/video", "type": "Guide", "text": ["Sometimes it is necessary to read a sequence of images from a standard video file, such as .avi and .mov files.", "In a scientific context, it is usually better to avoid these formats in favor of a simple directory of images or a multi-dimensional TIF. Video formats are more difficult to read piecemeal, typically do not support random frame access or research-minded meta data, and use lossy compression if not carefully configured. But video files are in widespread use, and they are easy to share, so it is convenient to be equipped to read and write them when necessary.", "Tools for reading video files vary in their ease of installation and use, their disk and memory usage, and their cross-platform compatibility. This is a practical guide.", "For a one-off solution, the simplest, surest route is to convert the video to a collection of sequentially-numbered image files, often called an image sequence. Then the images files can be read into an ImageCollection by skimage.io.imread_collection. Converting the video to frames can be done easily in ImageJ, a cross-platform, GUI-based program from the bio-imaging community, or FFmpeg, a powerful command-line utility for manipulating video files.", "In FFmpeg, the following command generates an image file from each frame in a video. The files are numbered with five digits, padded on the left with zeros.", "More information is available in an FFmpeg tutorial on image sequences.", "Generating an image sequence has disadvantages: they can be large and unwieldy, and generating them can take some time. It is generally preferable to work directly with the original video file. For a more direct solution, we need to execute FFmpeg or LibAV from Python to read frames from the video. FFmpeg and LibAV are two large open-source projects that decode video from the sprawling variety of formats used in the wild. There are several ways to use them from Python. Each, unfortunately, has some disadvantages.", "PyAV uses FFmpeg\u2019s (or LibAV\u2019s) libraries to read image data directly from the video file. It invokes them using Cython bindings, so it is very fast.", "PyAV\u2019s API reflects the way frames are stored in a video file.", "The Video class in PIMS invokes PyAV and adds additional functionality to solve a common problem in scientific applications, accessing a video by frame number. Video file formats are designed to be searched in an approximate way, by time, and they do not support an efficient means of seeking a specific frame number. PIMS adds this missing functionality by decoding (but not reading) the entire video at and producing an internal table of contents that supports indexing by frame.", "Moviepy invokes FFmpeg through a subprocess, pipes the decoded video from FFmpeg into RAM, and reads it out. This approach is straightforward, but it can be brittle, and it\u2019s not workable for large videos that exceed available RAM. It works on all platforms if FFmpeg is installed.", "Since it does not link to FFmpeg\u2019s underlying libraries, it is easier to install but about half as fast.", "Imageio takes the same approach as MoviePy. It supports a wide range of other image file formats as well.", "Finally, another solution is the VideoReader class in OpenCV, which has bindings to FFmpeg. If you need OpenCV for other reasons, then this may be the best approach."]}, {"name": "How to parallelize loops", "path": "user_guide/tutorial_parallelization", "type": "Guide", "text": ["In image processing, we frequently apply the same algorithm on a large batch of images. In this paragraph, we propose to use joblib to parallelize loops. Here is an example of such repetitive tasks:", "To call the function task on each element of the list pics, it is usual to write a for loop. To measure the execution time of this loop, you can use ipython and measure the execution time with %timeit.", "Another equivalent way to code this loop is to use a comprehension list which has the same efficiency.", "joblib is a library providing an easy way to parallelize for loops once we have a comprehension list. The number of jobs can be specified."]}, {"name": "I/O Plugin Infrastructure", "path": "user_guide/plugins", "type": "Guide", "text": ["A plugin consists of two files, the source and the descriptor .ini. Let\u2019s say we\u2019d like to provide a plugin for imshow using matplotlib. We\u2019ll call our plugin mpl:", "The name of the .py and .ini files must correspond. Inside the .ini file, we give the plugin meta-data:", "The \u201cprovides\u201d-line lists all the functions provided by the plugin. Since our plugin provides imshow, we have to define it inside mpl.py:", "Note that, by default, imshow is non-blocking, so a special function _app_show must be provided to block the GUI. We can modify our plugin to provide it as follows:", "Any plugin in the _plugins directory is automatically examined by skimage.io upon import. You may list all the plugins on your system:", "or only those already loaded:", "A plugin is loaded using the use_plugin command:", "or", "Note that, if more than one plugin provides certain functionality, the last plugin loaded is used.", "To query a plugin\u2019s capabilities, use plugin_info:"]}, {"name": "Image adjustment: transforming image content", "path": "user_guide/transforming_image_data", "type": "Guide", "text": ["Most functions for manipulating color channels are found in the submodule skimage.color.", "Color images can be represented using different color spaces. One of the most common color spaces is the RGB space, where an image has red, green and blue channels. However, other color models are widely used, such as the HSV color model, where hue, saturation and value are independent channels, or the CMYK model used for printing.", "skimage.color provides utility functions to convert images to and from different color spaces. Integer-type arrays can be transformed to floating-point type by the conversion operation:", "Converting an RGBA image to an RGB image by alpha blending it with a background is realized with rgba2rgb()", "Converting an RGB image to a grayscale image is realized with rgb2gray()", "rgb2gray() uses a non-uniform weighting of color channels, because of the different sensitivity of the human eye to different colors. Therefore, such a weighting ensures luminance preservation from RGB to grayscale:", "Converting a grayscale image to RGB with gray2rgb() simply duplicates the gray values over the three color channels.", "An inverted image is also called complementary image. For binary images, True values become False and conversely. For grayscale images, pixel values are replaced by the difference of the maximum value of the data type and the actual value. For RGB images, the same operation is done for each channel. This operation can be achieved with skimage.util.invert():", "label2rgb() can be used to superimpose colors on a grayscale image using an array of labels to encode the regions to be represented with the same color.", "Examples:", "Image pixels can take values determined by the dtype of the image (see Image data types and what they mean), such as 0 to 255 for uint8 images or [0,\n1] for floating-point images. However, most images either have a narrower range of values (because of poor contrast), or have most pixel values concentrated in a subrange of the accessible values. skimage.exposure provides functions that spread the intensity values over a larger range.", "A first class of methods compute a nonlinear function of the intensity, that is independent of the pixel values of a specific image. Such methods are often used for correcting a known non-linearity of sensors, or receptors such as the human eye. A well-known example is Gamma correction, implemented in adjust_gamma().", "Other methods re-distribute pixel values according to the histogram of the image. The histogram of pixel values is computed with skimage.exposure.histogram():", "histogram() returns the number of pixels for each value bin, and the centers of the bins. The behavior of histogram() is therefore slightly different from the one of numpy.histogram(), which returns the boundaries of the bins.", "The simplest contrast enhancement rescale_intensity() consists in stretching pixel values to the whole allowed range, using a linear transformation:", "Even if an image uses the whole value range, sometimes there is very little weight at the ends of the value range. In such a case, clipping pixel values using percentiles of the image improves the contrast (at the expense of some loss of information, because some pixels are saturated by this operation):", "The function equalize_hist() maps the cumulative distribution function (cdf) of pixel values onto a linear cdf, ensuring that all parts of the value range are equally represented in the image. As a result, details are enhanced in large regions with poor contrast. As a further refinement, histogram equalization can be performed in subregions of the image with equalize_adapthist(), in order to correct for exposure gradients across the image. See the example Histogram Equalization.", "Examples:"]}, {"name": "Image data types and what they mean", "path": "user_guide/data_types", "type": "Guide", "text": ["In skimage, images are simply numpy arrays, which support a variety of data types 1, i.e. \u201cdtypes\u201d. To avoid distorting image intensities (see Rescaling intensity values), we assume that images use the following dtype ranges:", "Data type", "Range", "uint8", "0 to 255", "uint16", "0 to 65535", "uint32", "0 to 232 - 1", "float", "-1 to 1 or 0 to 1", "int8", "-128 to 127", "int16", "-32768 to 32767", "int32", "-231 to 231 - 1", "Note that float images should be restricted to the range -1 to 1 even though the data type itself can exceed this range; all integer dtypes, on the other hand, have pixel intensities that can span the entire data type range. With a few exceptions, 64-bit (u)int images are not supported.", "Functions in skimage are designed so that they accept any of these dtypes, but, for efficiency, may return an image of a different dtype (see Output types). If you need a particular dtype, skimage provides utility functions that convert dtypes and properly rescale image intensities (see Input types). You should never use astype on an image, because it violates these assumptions about the dtype range:", "Although we aim to preserve the data range and type of input images, functions may support only a subset of these data-types. In such a case, the input will be converted to the required type (if possible), and a warning message printed to the log if a memory copy is needed. Type requirements should be noted in the docstrings.", "The following utility functions in the main package are available to developers and users:", "Function name", "Description", "img_as_float", "Convert to 64-bit floating point.", "img_as_ubyte", "Convert to 8-bit uint.", "img_as_uint", "Convert to 16-bit uint.", "img_as_int", "Convert to 16-bit int.", "These functions convert images to the desired dtype and properly rescale their values:", "Be careful! These conversions can result in a loss of precision, since 8 bits cannot hold the same amount of information as 64 bits:", "Additionally, some functions take a preserve_range argument where a range conversion is convenient but not necessary. For example, interpolation in transform.warp requires an image of type float, which should have a range in [0, 1]. So, by default, input images will be rescaled to this range. However, in some cases, the image values represent physical measurements, such as temperature or rainfall values, that the user does not want rescaled. With preserve_range=True, the original range of the data will be preserved, even though the output is a float image. Users must then ensure this non-standard image is properly processed by downstream functions, which may expect an image in [0, 1].", "The output type of a function is determined by the function author and is documented for the benefit of the user. While this requires the user to explicitly convert the output to whichever format is needed, it ensures that no unnecessary data copies take place.", "A user that requires a specific type of output (e.g., for display purposes), may write:", "It is possible that you may need to use an image created using skimage with OpenCV or vice versa. OpenCV image data can be accessed (without copying) in NumPy (and, thus, in scikit-image). OpenCV uses BGR (instead of scikit-image\u2019s RGB) for color images, and its dtype is uint8 by default (See Image data types and what they mean). BGR stands for Blue Green Red.", "The color images in skimage and OpenCV have 3 dimensions: width, height and color. RGB and BGR use the same color space, except the order of colors is reversed.", "Note that in scikit-image we usually refer to rows and columns instead of width and height (see Coordinate conventions).", "The following instruction effectively reverses the order of the colors, leaving the rows and columns unaffected.", "If cv_image is an array of unsigned bytes, skimage will understand it by default. If you prefer working with floating point images, img_as_float() can be used to convert the image:", "The reverse can be achieved with img_as_ubyte():", "This dtype behavior allows you to string together any skimage function without worrying about the image dtype. On the other hand, if you want to use a custom function that requires a particular dtype, you should call one of the dtype conversion functions (here, func1 and func2 are skimage functions):", "Better yet, you can convert the image internally and use a simplified processing pipeline:", "When possible, functions should avoid blindly stretching image intensities (e.g. rescaling a float image so that the min and max intensities are 0 and 1), since this can heavily distort an image. For example, if you\u2019re looking for bright markers in dark images, there may be an image where no markers are present; stretching its input intensity to span the full range would make background noise look like markers.", "Sometimes, however, you have images that should span the entire intensity range but do not. For example, some cameras store images with 10-, 12-, or 14-bit depth per pixel. If these images are stored in an array with dtype uint16, then the image won\u2019t extend over the full intensity range, and thus, would appear dimmer than it should. To correct for this, you can use the rescale_intensity function to rescale the image so that it uses the full dtype range:", "Here, the in_range argument is set to the maximum range for a 10-bit image. By default, rescale_intensity stretches the values of in_range to match the range of the dtype. rescale_intensity also accepts strings as inputs to in_range and out_range, so the example above could also be written as:", "People very often represent images in signed dtypes, even though they only manipulate the positive values of the image (e.g., using only 0-127 in an int8 image). For this reason, conversion functions only spread the positive values of a signed dtype over the entire range of an unsigned dtype. In other words, negative values are clipped to 0 when converting from signed to unsigned dtypes. (Negative values are preserved when converting between signed dtypes.) To prevent this clipping behavior, you should rescale your image beforehand:", "This behavior is symmetric: The values in an unsigned dtype are spread over just the positive range of a signed dtype.", "https://docs.scipy.org/doc/numpy/user/basics.types.html"]}, {"name": "Image Segmentation", "path": "user_guide/tutorial_segmentation", "type": "Guide", "text": ["Image segmentation is the task of labeling the pixels of objects of interest in an image.", "In this tutorial, we will see how to segment objects from a background. We use the coins image from skimage.data. This image shows several coins outlined against a darker background. The segmentation of the coins cannot be done directly from the histogram of grey values, because the background shares enough grey levels with the coins that a thresholding segmentation is not sufficient.", "Simply thresholding the image leads either to missing significant parts of the coins, or to merging parts of the background with the coins. This is due to the inhomogeneous lighting of the image.", "A first idea is to take advantage of the local contrast, that is, to use the gradients rather than the grey values.", "Let us first try to detect edges that enclose the coins. For edge detection, we use the Canny detector of skimage.feature.canny", "As the background is very smooth, almost all edges are found at the boundary of the coins, or inside the coins.", "Now that we have contours that delineate the outer boundary of the coins, we fill the inner part of the coins using the ndi.binary_fill_holes function, which uses mathematical morphology to fill the holes.", "Most coins are well segmented out of the background. Small objects from the background can be easily removed using the ndi.label function to remove objects smaller than a small threshold.", "However, the segmentation is not very satisfying, since one of the coins has not been segmented correctly at all. The reason is that the contour that we got from the Canny detector was not completely closed, therefore the filling function did not fill the inner part of the coin.", "Therefore, this segmentation method is not very robust: if we miss a single pixel of the contour of the object, we will not be able to fill it. Of course, we could try to dilate the contours in order to close them. However, it is preferable to try a more robust method.", "Let us first determine markers of the coins and the background. These markers are pixels that we can label unambiguously as either object or background. Here, the markers are found at the two extreme parts of the histogram of grey values:", "We will use these markers in a watershed segmentation. The name watershed comes from an analogy with hydrology. The watershed transform floods an image of elevation starting from markers, in order to determine the catchment basins of these markers. Watershed lines separate these catchment basins, and correspond to the desired segmentation.", "The choice of the elevation map is critical for good segmentation. Here, the amplitude of the gradient provides a good elevation map. We use the Sobel operator for computing the amplitude of the gradient:", "From the 3-D surface plot shown below, we see that high barriers effectively separate the coins from the background.", "and here is the corresponding 2-D plot:", "The next step is to find markers of the background and the coins based on the extreme parts of the histogram of grey values:", "Let us now compute the watershed transform:", "With this method, the result is satisfying for all coins. Even if the markers for the background were not well distributed, the barriers in the elevation map were high enough for these markers to flood the entire background.", "We remove a few small holes with mathematical morphology:", "We can now label all the coins one by one using ndi.label:"]}, {"name": "Image Viewer", "path": "user_guide/viewer", "type": "Guide", "text": ["Warning", "The scikit-image viewer is deprecated since 0.18 and will be removed in 0.20. Please, refer to the visualization software page for alternatives.", "skimage.viewer provides a matplotlib-based canvas for displaying images and a Qt-based GUI-toolkit, with the goal of making it easy to create interactive image editors. You can simply use it to display an image:", "Of course, you could just as easily use imshow from matplotlib (or alternatively, skimage.io.imshow which adds support for multiple io-plugins) to display images. The advantage of ImageViewer is that you can easily add plugins for manipulating images. Currently, only a few plugins are implemented, but it is easy to write your own. Before going into the details, let\u2019s see an example of how a pre-defined plugin is added to the viewer:", "The viewer\u2019s show() method returns a list of tuples, one for each attached plugin. Each tuple contains two elements: an overlay of the same shape as the input image, and a data field (which may be None). A plugin class documents its return value in its output method.", "In this example, only one plugin is attached, so the list returned by show will have length 1. We extract the single tuple and bind its overlay and data elements to individual variables. Here, overlay contains an image of the line drawn on the viewer, and data contains the 1-dimensional intensity profile along that line.", "At the moment, there are not many plugins pre-defined, but there is a really simple interface for creating your own plugin. First, let us create a plugin to call the total-variation denoising function, denoise_tv_bregman:", "Note", "The Plugin assumes the first argument given to the image filter is the image from the image viewer. In the future, this should be changed so you can pass the image to a different argument of the filter function.", "To actually interact with the filter, you have to add widgets that adjust the parameters of the function. Typically, that means adding a slider widget and connecting it to the filter parameter and the minimum and maximum values of the slider:", "Here, we connect a slider widget to the filter\u2019s \u2018weight\u2019 argument. We also added some buttons for saving the image to file or to the scikit-image image stack (see skimage.io.push and skimage.io.pop).", "All that\u2019s left is to create an image viewer and add the plugin to that viewer.", "Here, we access only the overlay returned by the plugin, which contains the filtered image for the last used setting of weight."]}, {"name": "img_as_bool()", "path": "api/skimage#skimage.img_as_bool", "type": "skimage", "text": ["Convert an image to boolean format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False."]}, {"name": "img_as_float()", "path": "api/skimage#skimage.img_as_float", "type": "skimage", "text": ["Convert an image to floating point format.", "This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]."]}, {"name": "img_as_float32()", "path": "api/skimage#skimage.img_as_float32", "type": "skimage", "text": ["Convert an image to single-precision (32-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]."]}, {"name": "img_as_float64()", "path": "api/skimage#skimage.img_as_float64", "type": "skimage", "text": ["Convert an image to double-precision (64-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]."]}, {"name": "img_as_int()", "path": "api/skimage#skimage.img_as_int", "type": "skimage", "text": ["Convert an image to 16-bit signed integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values."]}, {"name": "img_as_ubyte()", "path": "api/skimage#skimage.img_as_ubyte", "type": "skimage", "text": ["Convert an image to 8-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 255."]}, {"name": "img_as_uint()", "path": "api/skimage#skimage.img_as_uint", "type": "skimage", "text": ["Convert an image to 16-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 65535."]}, {"name": "io", "path": "api/skimage.io", "type": "io", "text": ["Utilities to read and write images in various formats.", "The following plug-ins are available:", "Plugin", "Description", "qt", "Fast image display using the Qt library. Deprecated since 0.18. Will be removed in 0.20.", "imread", "Image reading and writing via imread", "gdal", "Image reading via the GDAL Library (www.gdal.org)", "simpleitk", "Image reading and writing via SimpleITK", "gtk", "Fast image display using the GTK library", "pil", "Image reading via the Python Imaging Library", "fits", "FITS image reading via PyFITS", "matplotlib", "Display or save images using Matplotlib", "tifffile", "Load and save TIFF and TIFF-based images using tifffile.py", "imageio", "Image reading via the ImageIO Library", "skimage.io.call_plugin(kind, *args, **kwargs)", "Find the appropriate plugin of \u2018kind\u2019 and execute it.", "skimage.io.concatenate_images(ic)", "Concatenate all images in the image collection into an array.", "skimage.io.find_available_plugins([loaded])", "List available plugins.", "skimage.io.imread(fname[, as_gray, plugin])", "Load an image from file.", "skimage.io.imread_collection(load_pattern[, \u2026])", "Load a collection of images.", "skimage.io.imread_collection_wrapper(imread)", "skimage.io.imsave(fname, arr[, plugin, \u2026])", "Save an image to file.", "skimage.io.imshow(arr[, plugin])", "Display an image.", "skimage.io.imshow_collection(ic[, plugin])", "Display a collection of images.", "skimage.io.load_sift(f)", "Read SIFT or SURF features from externally generated file.", "skimage.io.load_surf(f)", "Read SIFT or SURF features from externally generated file.", "skimage.io.plugin_info(plugin)", "Return plugin meta-data.", "skimage.io.plugin_order()", "Return the currently preferred plugin order.", "skimage.io.pop()", "Pop an image from the shared image stack.", "skimage.io.push(img)", "Push an image onto the shared image stack.", "skimage.io.reset_plugins()", "skimage.io.show()", "Display pending images.", "skimage.io.use_plugin(name[, kind])", "Set the default plugin for a specified operation.", "skimage.io.ImageCollection(load_pattern[, \u2026])", "Load and manage a collection of image files.", "skimage.io.MultiImage(filename[, \u2026])", "A class containing all frames from multi-frame images.", "skimage.io.collection", "Data structures to hold collections of images, with optional caching.", "skimage.io.manage_plugins", "Handle image reading, writing and plotting plugins.", "skimage.io.sift", "skimage.io.util", "Find the appropriate plugin of \u2018kind\u2019 and execute it.", "Function to look up.", "Plugin to load. Defaults to None, in which case the first matching plugin is used.", "Passed to the plugin function.", "Concatenate all images in the image collection into an array.", "The images to be concatenated.", "An array having one more dimension than the images in ic.", "If images in ic don\u2019t have identical shapes.", "See also", "concatenate_images receives any iterable object containing images, including ImageCollection and MultiImage, and returns a NumPy array.", "List available plugins.", "If True, show only those plugins currently loaded. By default, all plugins are shown.", "Dictionary with plugin names as keys and exposed functions as values.", "Load an image from file.", "Image file name, e.g. test.jpg or URL.", "If True, convert color images to gray-scale (64-bit floats). Images that are already in gray-scale format are not converted.", "Name of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.", "The different color bands/channels are stored in the third dimension, such that a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.", "Passed to the given plugin.", "Load a collection of images.", "List of objects to load. These are usually filenames, but may vary depending on the currently active plugin. See the docstring for ImageCollection for the default behaviour of this parameter.", "If True, never keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.", "Collection of images.", "Passed to the given plugin.", "Save an image to file.", "Target filename.", "Image data.", "Name of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.", "Check for low contrast and print warning (default: True).", "Passed to the given plugin.", "When saving a JPEG, the compression ratio may be controlled using the quality keyword argument which is an integer with values in [1, 100] where 1 is worst quality and smallest file size, and 100 is best quality and largest file size (default 75). This is only available when using the PIL and imageio plugins.", "Display an image.", "Image data or name of image file.", "Name of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found.", "Passed to the given plugin.", "Explore 3D images (of cells)", "Display a collection of images.", "Collection to display.", "Name of plugin to use. By default, the different plugins are tried until a suitable candidate is found.", "Passed to the given plugin.", "Read SIFT or SURF features from externally generated file.", "This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/.", "This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.", "Input file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .", "Kind of descriptor used to generate filelike.", "row position of feature", "column position of feature", "feature scale", "feature orientation", "feature values", "Read SIFT or SURF features from externally generated file.", "This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/.", "This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.", "Input file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .", "Kind of descriptor used to generate filelike.", "row position of feature", "column position of feature", "feature scale", "feature orientation", "feature values", "Return plugin meta-data.", "Name of plugin.", "Meta data as specified in plugin .ini.", "Return the currently preferred plugin order.", "Dictionary of preferred plugin order, with function name as key and plugins (in order of preference) as value.", "Pop an image from the shared image stack.", "Image popped from the stack.", "Push an image onto the shared image stack.", "Image to push.", "Display pending images.", "Launch the event loop of the current gui plugin, and display all pending images, queued via imshow. This is required when using imshow from non-interactive scripts.", "A call to show will block execution of code until all windows have been closed.", "Set the default plugin for a specified operation. The plugin will be loaded if it hasn\u2019t been already.", "Name of plugin.", "Set the plugin for this function. By default, the plugin is set for all functions.", "See also", "List of available plugins", "To use Matplotlib as the default image reader, you would write:", "To see a list of available plugins run io.available_plugins. Note that this lists plugins that are defined, but the full list may not be usable if your system does not have the required libraries installed.", "Bases: object", "Load and manage a collection of image files.", "Pattern string or list of strings to load. The filename path can be absolute or relative.", "If True, ImageCollection does not keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.", "imread by default. See notes below.", "Note that files are always returned in alphanumerical order. Also note that slicing returns a new ImageCollection, not a view into the data.", "ImageCollection can be modified to load images from an arbitrary source by specifying a combination of load_pattern and load_func. For an ImageCollection ic, ic[5] uses load_func(load_pattern[5]) to load the image.", "Imagine, for example, an ImageCollection that loads every third frame from a video file:", "Another use of load_func would be to convert all images to uint8:", "If a pattern string is given for load_pattern, this attribute stores the expanded file list. Otherwise, this is equal to load_pattern.", "Load and manage a collection of images.", "Concatenate all images in the collection into an array.", "An array having one more dimension than the images in self.", "If images in the ImageCollection don\u2019t have identical shapes.", "See also", "Clear the image cache.", "Clear the cache for this image only. By default, the entire cache is erased.", "Bases: skimage.io.collection.ImageCollection", "A class containing all frames from multi-frame images.", "Pattern glob or filenames to load. The path can be absolute or relative.", "Whether to conserve memory by only caching a single frame. Default is True.", "imread by default. See notes below.", "If conserve_memory=True the memory footprint can be reduced, however the performance can be affected because frames have to be read from file more often.", "The last accessed frame is cached, all other frames will have to be read from file.", "The current implementation makes use of tifffile for Tiff files and PIL otherwise.", "Load a multi-img."]}, {"name": "io.call_plugin()", "path": "api/skimage.io#skimage.io.call_plugin", "type": "io", "text": ["Find the appropriate plugin of \u2018kind\u2019 and execute it.", "Function to look up.", "Plugin to load. Defaults to None, in which case the first matching plugin is used.", "Passed to the plugin function."]}, {"name": "io.concatenate_images()", "path": "api/skimage.io#skimage.io.concatenate_images", "type": "io", "text": ["Concatenate all images in the image collection into an array.", "The images to be concatenated.", "An array having one more dimension than the images in ic.", "If images in ic don\u2019t have identical shapes.", "See also", "concatenate_images receives any iterable object containing images, including ImageCollection and MultiImage, and returns a NumPy array."]}, {"name": "io.find_available_plugins()", "path": "api/skimage.io#skimage.io.find_available_plugins", "type": "io", "text": ["List available plugins.", "If True, show only those plugins currently loaded. By default, all plugins are shown.", "Dictionary with plugin names as keys and exposed functions as values."]}, {"name": "io.ImageCollection", "path": "api/skimage.io#skimage.io.ImageCollection", "type": "io", "text": ["Bases: object", "Load and manage a collection of image files.", "Pattern string or list of strings to load. The filename path can be absolute or relative.", "If True, ImageCollection does not keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.", "imread by default. See notes below.", "Note that files are always returned in alphanumerical order. Also note that slicing returns a new ImageCollection, not a view into the data.", "ImageCollection can be modified to load images from an arbitrary source by specifying a combination of load_pattern and load_func. For an ImageCollection ic, ic[5] uses load_func(load_pattern[5]) to load the image.", "Imagine, for example, an ImageCollection that loads every third frame from a video file:", "Another use of load_func would be to convert all images to uint8:", "If a pattern string is given for load_pattern, this attribute stores the expanded file list. Otherwise, this is equal to load_pattern.", "Load and manage a collection of images.", "Concatenate all images in the collection into an array.", "An array having one more dimension than the images in self.", "If images in the ImageCollection don\u2019t have identical shapes.", "See also", "Clear the image cache.", "Clear the cache for this image only. By default, the entire cache is erased."]}, {"name": "io.ImageCollection.concatenate()", "path": "api/skimage.io#skimage.io.ImageCollection.concatenate", "type": "io", "text": ["Concatenate all images in the collection into an array.", "An array having one more dimension than the images in self.", "If images in the ImageCollection don\u2019t have identical shapes.", "See also"]}, {"name": "io.ImageCollection.conserve_memory()", "path": "api/skimage.io#skimage.io.ImageCollection.conserve_memory", "type": "io", "text": []}, {"name": "io.ImageCollection.files()", "path": "api/skimage.io#skimage.io.ImageCollection.files", "type": "io", "text": []}, {"name": "io.ImageCollection.reload()", "path": "api/skimage.io#skimage.io.ImageCollection.reload", "type": "io", "text": ["Clear the image cache.", "Clear the cache for this image only. By default, the entire cache is erased."]}, {"name": "io.ImageCollection.__init__()", "path": "api/skimage.io#skimage.io.ImageCollection.__init__", "type": "io", "text": ["Load and manage a collection of images."]}, {"name": "io.imread()", "path": "api/skimage.io#skimage.io.imread", "type": "io", "text": ["Load an image from file.", "Image file name, e.g. test.jpg or URL.", "If True, convert color images to gray-scale (64-bit floats). Images that are already in gray-scale format are not converted.", "Name of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.", "The different color bands/channels are stored in the third dimension, such that a gray-image is MxN, an RGB-image MxNx3 and an RGBA-image MxNx4.", "Passed to the given plugin."]}, {"name": "io.imread_collection()", "path": "api/skimage.io#skimage.io.imread_collection", "type": "io", "text": ["Load a collection of images.", "List of objects to load. These are usually filenames, but may vary depending on the currently active plugin. See the docstring for ImageCollection for the default behaviour of this parameter.", "If True, never keep more than one in memory at a specific time. Otherwise, images will be cached once they are loaded.", "Collection of images.", "Passed to the given plugin."]}, {"name": "io.imread_collection_wrapper()", "path": "api/skimage.io#skimage.io.imread_collection_wrapper", "type": "io", "text": []}, {"name": "io.imsave()", "path": "api/skimage.io#skimage.io.imsave", "type": "io", "text": ["Save an image to file.", "Target filename.", "Image data.", "Name of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found. If not given and fname is a tiff file, the tifffile plugin will be used.", "Check for low contrast and print warning (default: True).", "Passed to the given plugin.", "When saving a JPEG, the compression ratio may be controlled using the quality keyword argument which is an integer with values in [1, 100] where 1 is worst quality and smallest file size, and 100 is best quality and largest file size (default 75). This is only available when using the PIL and imageio plugins."]}, {"name": "io.imshow()", "path": "api/skimage.io#skimage.io.imshow", "type": "io", "text": ["Display an image.", "Image data or name of image file.", "Name of plugin to use. By default, the different plugins are tried (starting with imageio) until a suitable candidate is found.", "Passed to the given plugin."]}, {"name": "io.imshow_collection()", "path": "api/skimage.io#skimage.io.imshow_collection", "type": "io", "text": ["Display a collection of images.", "Collection to display.", "Name of plugin to use. By default, the different plugins are tried until a suitable candidate is found.", "Passed to the given plugin."]}, {"name": "io.load_sift()", "path": "api/skimage.io#skimage.io.load_sift", "type": "io", "text": ["Read SIFT or SURF features from externally generated file.", "This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/.", "This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.", "Input file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .", "Kind of descriptor used to generate filelike.", "row position of feature", "column position of feature", "feature scale", "feature orientation", "feature values"]}, {"name": "io.load_surf()", "path": "api/skimage.io#skimage.io.load_surf", "type": "io", "text": ["Read SIFT or SURF features from externally generated file.", "This routine reads SIFT or SURF files generated by binary utilities from http://people.cs.ubc.ca/~lowe/keypoints/ and http://www.vision.ee.ethz.ch/~surf/.", "This routine does not generate SIFT/SURF features from an image. These algorithms are patent encumbered. Please use skimage.feature.CENSURE instead.", "Input file generated by the feature detectors from http://people.cs.ubc.ca/~lowe/keypoints/ or http://www.vision.ee.ethz.ch/~surf/ .", "Kind of descriptor used to generate filelike.", "row position of feature", "column position of feature", "feature scale", "feature orientation", "feature values"]}, {"name": "io.MultiImage", "path": "api/skimage.io#skimage.io.MultiImage", "type": "io", "text": ["Bases: skimage.io.collection.ImageCollection", "A class containing all frames from multi-frame images.", "Pattern glob or filenames to load. The path can be absolute or relative.", "Whether to conserve memory by only caching a single frame. Default is True.", "imread by default. See notes below.", "If conserve_memory=True the memory footprint can be reduced, however the performance can be affected because frames have to be read from file more often.", "The last accessed frame is cached, all other frames will have to be read from file.", "The current implementation makes use of tifffile for Tiff files and PIL otherwise.", "Load a multi-img."]}, {"name": "io.MultiImage.filename()", "path": "api/skimage.io#skimage.io.MultiImage.filename", "type": "io", "text": []}, {"name": "io.MultiImage.__init__()", "path": "api/skimage.io#skimage.io.MultiImage.__init__", "type": "io", "text": ["Load a multi-img."]}, {"name": "io.plugin_info()", "path": "api/skimage.io#skimage.io.plugin_info", "type": "io", "text": ["Return plugin meta-data.", "Name of plugin.", "Meta data as specified in plugin .ini."]}, {"name": "io.plugin_order()", "path": "api/skimage.io#skimage.io.plugin_order", "type": "io", "text": ["Return the currently preferred plugin order.", "Dictionary of preferred plugin order, with function name as key and plugins (in order of preference) as value."]}, {"name": "io.pop()", "path": "api/skimage.io#skimage.io.pop", "type": "io", "text": ["Pop an image from the shared image stack.", "Image popped from the stack."]}, {"name": "io.push()", "path": "api/skimage.io#skimage.io.push", "type": "io", "text": ["Push an image onto the shared image stack.", "Image to push."]}, {"name": "io.reset_plugins()", "path": "api/skimage.io#skimage.io.reset_plugins", "type": "io", "text": []}, {"name": "io.show()", "path": "api/skimage.io#skimage.io.show", "type": "io", "text": ["Display pending images.", "Launch the event loop of the current gui plugin, and display all pending images, queued via imshow. This is required when using imshow from non-interactive scripts.", "A call to show will block execution of code until all windows have been closed."]}, {"name": "io.use_plugin()", "path": "api/skimage.io#skimage.io.use_plugin", "type": "io", "text": ["Set the default plugin for a specified operation. The plugin will be loaded if it hasn\u2019t been already.", "Name of plugin.", "Set the plugin for this function. By default, the plugin is set for all functions.", "See also", "List of available plugins", "To use Matplotlib as the default image reader, you would write:", "To see a list of available plugins run io.available_plugins. Note that this lists plugins that are defined, but the full list may not be usable if your system does not have the required libraries installed."]}, {"name": "lookfor()", "path": "api/skimage#skimage.lookfor", "type": "skimage", "text": ["Do a keyword search on scikit-image docstrings.", "Words to look for."]}, {"name": "measure", "path": "api/skimage.measure", "type": "measure", "text": ["skimage.measure.approximate_polygon(coords, \u2026)", "Approximate a polygonal chain with the specified tolerance.", "skimage.measure.block_reduce(image, block_size)", "Downsample image by applying function func to local blocks.", "skimage.measure.euler_number(image[, \u2026])", "Calculate the Euler characteristic in binary image.", "skimage.measure.find_contours(image[, \u2026])", "Find iso-valued contours in a 2D array for a given level value.", "skimage.measure.grid_points_in_poly(shape, verts)", "Test whether points on a specified grid are inside a polygon.", "skimage.measure.inertia_tensor(image[, mu])", "Compute the inertia tensor of the input image.", "skimage.measure.inertia_tensor_eigvals(image)", "Compute the eigenvalues of the inertia tensor of the image.", "skimage.measure.label(input[, background, \u2026])", "Label connected regions of an integer array.", "skimage.measure.marching_cubes(volume[, \u2026])", "Marching cubes algorithm to find surfaces in 3d volumetric data.", "skimage.measure.marching_cubes_classic(volume)", "Classic marching cubes algorithm to find surfaces in 3d volumetric data.", "skimage.measure.marching_cubes_lewiner(volume)", "Lewiner marching cubes algorithm to find surfaces in 3d volumetric data.", "skimage.measure.mesh_surface_area(verts, faces)", "Compute surface area, given vertices & triangular faces", "skimage.measure.moments(image[, order])", "Calculate all raw image moments up to a certain order.", "skimage.measure.moments_central(image[, \u2026])", "Calculate all central image moments up to a certain order.", "skimage.measure.moments_coords(coords[, order])", "Calculate all raw image moments up to a certain order.", "skimage.measure.moments_coords_central(coords)", "Calculate all central image moments up to a certain order.", "skimage.measure.moments_hu(nu)", "Calculate Hu\u2019s set of image moments (2D-only).", "skimage.measure.moments_normalized(mu[, order])", "Calculate all normalized central image moments up to a certain order.", "skimage.measure.perimeter(image[, neighbourhood])", "Calculate total perimeter of all objects in binary image.", "skimage.measure.perimeter_crofton(image[, \u2026])", "Calculate total Crofton perimeter of all objects in binary image.", "skimage.measure.points_in_poly(points, verts)", "Test whether points lie inside a polygon.", "skimage.measure.profile_line(image, src, dst)", "Return the intensity profile of an image measured along a scan line.", "skimage.measure.ransac(data, model_class, \u2026)", "Fit a model to data with the RANSAC (random sample consensus) algorithm.", "skimage.measure.regionprops(label_image[, \u2026])", "Measure properties of labeled image regions.", "skimage.measure.regionprops_table(label_image)", "Compute image properties and return them as a pandas-compatible table.", "skimage.measure.shannon_entropy(image[, base])", "Calculate the Shannon entropy of an image.", "skimage.measure.subdivide_polygon(coords[, \u2026])", "Subdivision of polygonal curves using B-Splines.", "skimage.measure.CircleModel()", "Total least squares estimator for 2D circles.", "skimage.measure.EllipseModel()", "Total least squares estimator for 2D ellipses.", "skimage.measure.LineModelND()", "Total least squares estimator for N-dimensional lines.", "Approximate a polygonal chain with the specified tolerance.", "It is based on the Douglas-Peucker algorithm.", "Note that the approximated polygon is always within the convex hull of the original polygon.", "Coordinate array.", "Maximum distance from original points of polygon to approximated polygonal chain. If tolerance is 0, the original coordinate array is returned.", "Approximated polygonal chain where M <= N.", "https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm", "Downsample image by applying function func to local blocks.", "This function is useful for max and mean pooling, for example.", "N-dimensional input image.", "Array containing down-sampling integer factor along each axis.", "Function object which is used to calculate the return value for each local block. This function must implement an axis parameter. Primary functions are numpy.sum, numpy.min, numpy.max, numpy.mean and numpy.median. See also func_kwargs.", "Constant padding value if image is not perfectly divisible by the block size.", "Keyword arguments passed to func. Notably useful for passing dtype argument to np.mean. Takes dictionary of inputs, e.g.: func_kwargs={'dtype': np.float16}).", "Down-sampled image with same number of dimensions as input image.", "Calculate the Euler characteristic in binary image.", "For 2D objects, the Euler number is the number of objects minus the number of holes. For 3D objects, the Euler number is obtained as the number of objects plus the number of holes, minus the number of tunnels, or loops.", "2D or 3D images. If image is not binary, all values strictly greater than zero are considered as the object.", "Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used. 4 or 8 neighborhoods are defined for 2D images (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined for 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not defined.", "Euler characteristic of the set of all objects in the image.", "The Euler characteristic is an integer number that describes the topology of the set of all objects in the input image. If object is 4-connected, then background is 8-connected, and conversely.", "The computation of the Euler characteristic is based on an integral geometry formula in discretized space. In practice, a neighbourhood configuration is constructed, and a LUT is applied for each configuration. The coefficients used are the ones of Ohser et al.", "It can be useful to compute the Euler characteristic for several connectivities. A large relative difference between results for different connectivities suggests that the image resolution (with respect to the size of objects and holes) is too low.", "S. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838", "Ohser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets - On the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D. (eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600. Springer, Berlin, Heidelberg.", "Euler number", "Find iso-valued contours in a 2D array for a given level value.", "Uses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the input 2D array for a particular level value. Array values are linearly interpolated to provide better precision for the output contours.", "Input image in which to find contours.", "Value along which to find contours in the array. By default, the level is set to (max(image) + min(image)) / 2", "Changed in version 0.18: This parameter is now optional.", "Indicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)", "Indicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If \u2018low\u2019 then contours will wind counter- clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour. (See below for details.)", "A boolean mask, True where we want to draw contours. Note that NaN values are always excluded from the considered region (mask is set to False wherever array is NaN).", "Each contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.", "See also", "The marching squares algorithm is a special case of the marching cubes algorithm [1]. A simple explanation is available here:", "http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html", "There is a single ambiguous case in the marching squares algorithm: when a given 2 x 2-element square has two high-valued and two low-valued elements, each pair diagonally adjacent. (Where high- and low-valued is with respect to the contour value sought.) In this case, either the high-valued elements can be \u2018connected together\u2019 via a thin isthmus that separates the low-valued elements, or vice-versa. When elements are connected together across a diagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-connected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be fully-connected, the other set will be considered as \u2018face-connected\u2019 or \u20184-connected\u2019. By default, low-valued elements are considered fully-connected; this can be altered with the \u2018fully_connected\u2019 parameter.", "Output contours are not guaranteed to be closed: contours which intersect the array edge or a masked-off region (either where mask is False or where array is NaN) will be left open. All other contours will be closed. (The closed-ness of a contours can be tested by checking whether the beginning point is the same as the end point.)", "Contours are oriented. By default, array values lower than the contour value are to the left of the contour and values greater than the contour value are to the right. This means that contours will wind counter-clockwise (i.e. in \u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can be altered with the \u2018positive_orientation\u2019 parameter.", "The order of the contours in the output list is determined by the position of the smallest x,y (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.", "Warning", "Array coordinates/values are assumed to refer to the center of the array element. Take a simple example input: [0, 1]. The interpolated position of 0.5 in this array is midway between the 0-element (at x=0) and the 1-element (at x=1), and thus would fall at x=0.5.", "This means that to find reasonable contours, it is best to find contours midway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a binarized array, do not choose to find contours at the low or high value of the array. This will often yield degenerate contours, especially around structures that are a single array element wide. Instead choose a middle value, as above.", "Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422", "Contour finding", "Measure region properties", "Test whether points on a specified grid are inside a polygon.", "For each (r, c) coordinate on a grid, i.e. (0, 0), (0, 1) etc., test whether that point lies inside a polygon.", "Shape of the grid.", "Specify the V vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.", "True where the grid falls inside the polygon.", "See also", "Compute the inertia tensor of the input image.", "The input image.", "The pre-computed central moments of image. The inertia tensor computation requires the central moments of the image. If an application requires both the central moments and the inertia tensor (for example, skimage.measure.regionprops), then it is more efficient to pre-compute them and pass them to the inertia tensor call.", "The inertia tensor of the input image. \\(T_{i, j}\\) contains the covariance of image intensity along axes \\(i\\) and \\(j\\).", "https://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor", "Bernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific Applications. (Chapter 8: Tensor Methods) Springer, 1993.", "Compute the eigenvalues of the inertia tensor of the image.", "The inertia tensor measures covariance of the image intensity along the image axes. (See inertia_tensor.) The relative magnitude of the eigenvalues of the tensor is thus a measure of the elongation of a (bright) object in the image.", "The input image.", "The pre-computed central moments of image.", "The pre-computed inertia tensor. If T is given, mu and image are ignored.", "The eigenvalues of the inertia tensor of image, in descending order.", "Computing the eigenvalues requires the inertia tensor of the input image. This is much faster if the central moments (mu) are provided, or, alternatively, one can provide the inertia tensor (T) directly.", "Label connected regions of an integer array.", "Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor:", "Image to label.", "Consider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.", "Whether to return the number of assigned labels.", "Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.", "Labeled array, where all connected regions are assigned the same integer value.", "Number of labels, which equals the maximum label index and is only returned if return_num is True.", "See also", "Christophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.", "Kensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864", "Measure region properties", "Euler number", "Segment human cells (in mitosis)", "Marching cubes algorithm to find surfaces in 3d volumetric data.", "In contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice.", "Input data volume to find isosurfaces. Will internally be converted to float32 if necessary.", "Contour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.", "Voxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.", "Controls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object", "Step size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.", "Whether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.", "One of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al. or Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an old implementation that will be deprecated in version 0.19.", "Boolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.", "Spatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.", "Define triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.", "The normal direction at each vertex, as calculated from the data.", "Gives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.", "See also", "The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation.", "To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area.", "Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package:", "Similarly using the visvis package:", "To reduce the number of triangles in the mesh for better performance, see this example using the mayavi package.", "Thomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582", "Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422", "Classic marching cubes algorithm to find surfaces in 3d volumetric data.", "Note that the marching_cubes() algorithm is recommended over this algorithm, because it\u2019s faster and produces better results.", "Input data volume to find isosurfaces. Will be cast to np.float64.", "Contour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.", "Voxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.", "Controls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object", "Spatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.", "Define triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.", "See also", "The marching cubes algorithm is implemented as described in [1]. A simple explanation is available here:", "There are several known ambiguous cases in the marching cubes algorithm. Using point labeling as in [1], Figure 4, as shown:", "Most notably, if v4, v8, v2, and v6 are all >= level (or any generalization of this case) two parallel planes are generated by this algorithm, separating v4 and v8 from v2 and v6. An equally valid interpretation would be a single connected thin surface enclosing all four points. This is the best known ambiguity, though there are others.", "This algorithm does not attempt to resolve such ambiguities; it is a naive implementation of marching cubes as in [1], but may be a good beginning for work with more recent techniques (Dual Marching Cubes, Extended Marching Cubes, Cubic Marching Squares, etc.).", "Because of interactions between neighboring cubes, the isosurface(s) generated by this algorithm are NOT guaranteed to be closed, particularly for complicated contours. Furthermore, this algorithm does not guarantee a single contour will be returned. Indeed, ALL isosurfaces which cross level will be found, regardless of connectivity.", "The output is a triangular mesh consisting of a set of unique vertices and connecting triangles. The order of these vertices and triangles in the output list is determined by the position of the smallest x,y,z (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.", "The generated mesh guarantees coherent orientation as of version 0.12.", "To quantify the area of an isosurface generated by this algorithm, pass outputs directly into skimage.measure.mesh_surface_area.", "Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422", "Lewiner marching cubes algorithm to find surfaces in 3d volumetric data.", "In contrast to marching_cubes_classic(), this algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice, unless there is a specific need for the classic algorithm.", "Input data volume to find isosurfaces. Will internally be converted to float32 if necessary.", "Contour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.", "Voxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.", "Controls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object", "Step size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.", "Whether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.", "If given and True, the classic marching cubes by Lorensen (1987) is used. This option is included for reference purposes. Note that this algorithm has ambiguities and is not guaranteed to produce a topologically correct result. The results with using this option are not generally the same as the marching_cubes_classic() function.", "Boolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.", "Spatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.", "Define triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.", "The normal direction at each vertex, as calculated from the data.", "Gives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.", "See also", "The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation.", "To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area.", "Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package:", "Similarly using the visvis package:", "Thomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582", "Compute surface area, given vertices & triangular faces", "Array containing (x, y, z) coordinates for V unique mesh vertices.", "List of length-3 lists of integers, referencing vertex coordinates as provided in verts", "Surface area of mesh. Units now [coordinate units] ** 2.", "See also", "The arguments expected by this function are the first two outputs from skimage.measure.marching_cubes. For unit correct output, ensure correct spacing was passed to skimage.measure.marching_cubes.", "This algorithm works properly only if the faces provided are all triangles.", "Calculate all raw image moments up to a certain order.", "Note that raw moments are neither translation, scale nor rotation invariant.", "Rasterized shape as image.", "Maximum order of moments. Default is 3.", "Raw image moments.", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment", "Calculate all central image moments up to a certain order.", "The center coordinates (cr, cc) can be calculated from the raw moments as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.", "Note that central moments are translation invariant but not scale and rotation invariant.", "Rasterized shape as image.", "Coordinates of the image centroid. This will be computed if it is not provided.", "The maximum order of moments computed.", "Central image moments.", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment", "Calculate all raw image moments up to a certain order.", "Note that raw moments are neither translation, scale nor rotation invariant.", "Array of N points that describe an image of D dimensionality in Cartesian space.", "Maximum order of moments. Default is 3.", "Raw image moments. (D dimensions)", "Johannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.", "Calculate all central image moments up to a certain order.", "Note that raw moments are neither translation, scale nor rotation invariant.", "Array of N points that describe an image of D dimensionality in Cartesian space. A tuple of coordinates as returned by np.nonzero is also accepted as input.", "Coordinates of the image centroid. This will be computed if it is not provided.", "Maximum order of moments. Default is 3.", "Central image moments. (D dimensions)", "Johannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.", "As seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows 1 and 3) are zero when centered on the centroid, or center of mass, of the object (the default). If we break the symmetry by adding a new point, this no longer holds:", "Image moments and central image moments are equivalent (by definition) when the center is (0, 0):", "Calculate Hu\u2019s set of image moments (2D-only).", "Note that this set of moments is proofed to be translation, scale and rotation invariant.", "Normalized central image moments, where M must be >= 4.", "Hu\u2019s set of image moments.", "M. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info. Theory, vol. IT-8, pp. 179-187, 1962", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment", "Calculate all normalized central image moments up to a certain order.", "Note that normalized central moments are translation and scale invariant but not rotation invariant.", "Central image moments, where M must be greater than or equal to order.", "Maximum order of moments. Default is 3.", "Normalized central image moments.", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment", "Calculate total perimeter of all objects in binary image.", "2D binary image.", "Neighborhood connectivity for border pixel determination. It is used to compute the contour. A higher neighbourhood widens the border on which the perimeter is computed.", "Total perimeter of all objects in binary image.", "K. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter Estimator. The Queen\u2019s University of Belfast. http://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc", "Different perimeters", "Calculate total Crofton perimeter of all objects in binary image.", "2D image. If image is not binary, all values strictly greater than zero are considered as the object.", "Number of directions used to approximate the Crofton perimeter. By default, 4 is used: it should be more accurate than 2. Computation time is the same in both cases.", "Total perimeter of all objects in binary image.", "This measure is based on Crofton formula [1], which is a measure from integral geometry. It is defined for general curve length evaluation via a double integral along all directions. In a discrete space, 2 or 4 directions give a quite good approximation, 4 being more accurate than 2 for more complex shapes.", "Similar to perimeter(), this function returns an approximation of the perimeter in continuous space.", "https://en.wikipedia.org/wiki/Crofton_formula", "S. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838", "Different perimeters", "Test whether points lie inside a polygon.", "Input points, (x, y).", "Vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.", "True if corresponding point is inside the polygon.", "See also", "Return the intensity profile of an image measured along a scan line.", "The image, either grayscale (2D array) or multichannel (3D array, where the final axis contains the channel information).", "The coordinates of the start point of the scan line.", "The coordinates of the end point of the scan line. The destination point is included in the profile, in contrast to standard numpy indexing.", "Width of the scan, perpendicular to the line", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "How to compute any values falling outside of the image.", "If mode is \u2018constant\u2019, what constant value to use outside the image.", "Function used to calculate the aggregation of pixel values perpendicular to the profile_line direction when linewidth > 1. If set to None the unreduced array will be returned.", "The intensity profile along the scan line. The length of the profile is the ceil of the computed length of the scan line.", "The destination point is included in the profile, in contrast to standard numpy indexing. For example:", "For different reduce_func inputs:", "The unreduced array will be returned when reduce_func is None or when reduce_func acts on each pixel value individually.", "Fit a model to data with the RANSAC (random sample consensus) algorithm.", "RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:", "These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model.", "Data set to which the model is fitted, where N is the number of data points and the remaining dimension are depending on model requirements. If the model class requires multiple input data arrays (e.g. source and destination coordinates of skimage.transform.AffineTransform), they can be optionally passed as tuple or list. Note, that in this case the functions estimate(*data), residuals(*data), is_model_valid(model, *random_data) and is_data_valid(*random_data) must all take each data array as separate arguments.", "Object with the following object methods:", "where success indicates whether the model estimation succeeded (True or None for success, False for failure).", "The minimum number of data points to fit a model to.", "Maximum distance for a data point to be classified as an inlier.", "This function is called with the randomly selected data before the model is fitted to it: is_data_valid(*random_data).", "This function is called with the estimated model and the randomly selected data: is_model_valid(model, *random_data), .", "Maximum number of iterations for random sample selection.", "Stop iteration if at least this number of inliers are found.", "Stop iteration if sum of residuals is less than or equal to this threshold.", "RANSAC iteration stops if at least one outlier-free set of the training data is sampled with probability >= stop_probability, depending on the current best model\u2019s inlier ratio and the number of trials. This requires to generate at least N samples (trials):", "N >= log(1 - probability) / log(1 - e**m)", "where the probability (confidence) is typically set to a high value such as 0.99, e is the current fraction of inliers w.r.t. the total number of samples, and m is the min_samples value.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.", "Initial samples selection for model estimation", "Best model with largest consensus set.", "Boolean mask of inliers classified as True.", "\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC", "Generate ellipse data without tilt and add noise:", "Add some faulty data:", "Estimate ellipse model using all available data:", "Estimate ellipse model using RANSAC:", "RANSAC can be used to robustly estimate a geometric transformation. In this section, we also show how to use a proportion of the total samples, rather than an absolute number.", "Measure properties of labeled image regions.", "Labeled input image. Labels with value 0 are ignored.", "Changed in version 0.14.1: Previously, label_image was processed by numpy.squeeze and so any number of singleton dimensions was allowed. This resulted in inconsistent handling of images with singleton dimensions. To recover the old behaviour, use regionprops(np.squeeze(label_image), ...).", "Intensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.", "Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.", "Determine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.", "This argument is deprecated and will be removed in a future version of scikit-image.", "See Coordinate conventions for more details.", "Deprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be sufficient to call numpy.transpose on your label image to get the same values as 0.15 and earlier. However, for some properties, the transformation will be less trivial. For example, the new orientation is \\(\\frac{\\pi}{2}\\) plus the old orientation.", "Add extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.", "Each item describes one labeled region, and can be accessed using the attributes listed below.", "See also", "The following properties can be accessed as attributes or keys:", "Number of pixels of the region.", "Bounding box (min_row, min_col, max_row, max_col). Pixels belonging to the bounding box are in the half-open interval [min_row; max_row) and [min_col; max_col).", "Number of pixels of bounding box.", "Centroid coordinate tuple (row, col).", "Number of pixels of convex hull image, which is the smallest convex polygon that encloses the region.", "Binary convex hull image which has the same size as bounding box.", "Coordinate list (row, col) of the region.", "Eccentricity of the ellipse that has the same second-moments as the region. The eccentricity is the ratio of the focal distance (distance between focal points) over the major axis length. The value is in the interval [0, 1). When it is 0, the ellipse becomes a circle.", "The diameter of a circle with the same area as the region.", "Euler characteristic of the set of non-zero pixels. Computed as number of connected components subtracted by number of holes (input.ndim connectivity). In 3D, number of connected components plus number of holes subtracted by number of tunnels.", "Ratio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)", "Maximum Feret\u2019s diameter computed as the longest distance between points around a region\u2019s convex hull contour as determined by find_contours. [5]", "Number of pixels of the region will all the holes filled in. Describes the area of the filled_image.", "Binary region image with filled holes which has the same size as bounding box.", "Sliced binary region image which has the same size as bounding box.", "Inertia tensor of the region for the rotation around its mass.", "The eigenvalues of the inertia tensor in decreasing order.", "Image inside region bounding box.", "The label in the labeled input image.", "Centroid coordinate tuple (row, col), relative to region bounding box.", "The length of the major axis of the ellipse that has the same normalized second central moments as the region.", "Value with the greatest intensity in the region.", "Value with the mean intensity in the region.", "Value with the least intensity in the region.", "The length of the minor axis of the ellipse that has the same normalized second central moments as the region.", "Spatial moments up to 3rd order:", "where the sum is over the row, col coordinates of the region.", "Central moments (translation invariant) up to 3rd order:", "where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.", "Hu moments (translation, scale and rotation invariant).", "Normalized moments (translation and scale invariant) up to 3rd order:", "where m_00 is the zeroth spatial moment.", "Angle between the 0th axis (rows) and the major axis of the ellipse that has the same second moments as the region, ranging from -pi/2 to pi/2 counter-clockwise.", "Perimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.", "Perimeter of object approximated by the Crofton formula in 4 directions.", "A slice to extract the object from the source image.", "Ratio of pixels in the region to pixels of the convex hull image.", "Centroid coordinate tuple (row, col) weighted with intensity image.", "Centroid coordinate tuple (row, col), relative to region bounding box, weighted with intensity image.", "Spatial moments of intensity image up to 3rd order:", "where the sum is over the row, col coordinates of the region.", "Central moments (translation invariant) of intensity image up to 3rd order:", "where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s weighted centroid.", "Hu moments (translation, scale and rotation invariant) of intensity image.", "Normalized moments (translation and scale invariant) of intensity image up to 3rd order:", "where wm_00 is the zeroth spatial moment (intensity-weighted area).", "Each region also supports iteration, so that you can do:", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment", "W. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems, pp. 27-28. ICT Prague, 2007. https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf", "Add custom measurements by passing functions as extra_properties", "Measure region properties", "Compute image properties and return them as a pandas-compatible table.", "The table is a dictionary mapping column names to value arrays. See Notes section below for details.", "New in version 0.16.", "Labeled input image. Labels with value 0 are ignored.", "Intensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.", "Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.", "Properties that will be included in the resulting dictionary For a list of available properties, please see regionprops(). Users should remember to add \u201clabel\u201d to keep track of region identities.", "Determine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.", "For non-scalar properties not listed in OBJECT_COLUMNS, each element will appear in its own column, with the index of that element separated from the property name by this separator. For example, the inertia tensor of a 2D region will appear in four columns: inertia_tensor-0-0, inertia_tensor-0-1, inertia_tensor-1-0, and inertia_tensor-1-1 (where the separator is -).", "Object columns are those that cannot be split in this way because the number of columns would change depending on the object. For example, image and coords.", "Add extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.", "Dictionary mapping property names to an array of values of that property, one value per region. This dictionary can be used as input to pandas DataFrame to map property names to columns in the frame and regions to rows. If the image has no regions, the arrays will have length 0, but the correct type.", "Each column contains either a scalar property, an object property, or an element in a multidimensional array.", "Properties with scalar values for each region, such as \u201ceccentricity\u201d, will appear as a float or int array with that property name as key.", "Multidimensional properties of fixed size for a given image dimension, such as \u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter the region size), will be split into that many columns, with the name {property_name}{separator}{element_num} (for 1D properties), {property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D properties), and so on.", "For multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d (the image of a region varies in size depending on the region size), an object array will be used, with the corresponding property name as the key.", "The resulting dictionary can be directly passed to pandas, if installed, to obtain a clean DataFrame:", "[5 rows x 7 columns]", "If we want to measure a feature that does not come as a built-in property, we can define custom functions and pass them as extra_properties. For example, we can create a custom function that measures the intensity quartiles in a region:", "Measure region properties", "Calculate the Shannon entropy of an image.", "The Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are frequency/probability of pixels of value k.", "Grayscale input image.", "The logarithmic base to use.", "The returned value is measured in bits or shannon (Sh) for base=2, natural unit (nat) for base=np.e and hartley (Hart) for base=10.", "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "https://en.wiktionary.org/wiki/Shannon_entropy", "Subdivision of polygonal curves using B-Splines.", "Note that the resulting curve is always within the convex hull of the original polygon. Circular polygons stay closed after subdivision.", "Coordinate array.", "Degree of B-Spline. Default is 2.", "Preserve first and last coordinate of non-circular polygon. Default is False.", "Subdivided coordinate array.", "http://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf", "Bases: skimage.measure.fit.BaseModel", "Total least squares estimator for 2D circles.", "The functional model of the circle is:", "This estimator minimizes the squared distances from all points to the circle:", "A minimum number of 3 points is required to solve for the parameters.", "Circle model parameters in the following order xc, yc, r.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate circle model from data using total least squares.", "N points with (x, y) coordinates, respectively.", "True, if model estimation succeeds.", "Predict x- and y-coordinates using the estimated model.", "Angles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.", "Optional custom parameter set.", "Predicted x- and y-coordinates.", "Determine residuals of data to model.", "For each point the shortest distance to the circle is returned.", "N points with (x, y) coordinates, respectively.", "Residual for each data point.", "Bases: skimage.measure.fit.BaseModel", "Total least squares estimator for 2D ellipses.", "The functional model of the ellipse is:", "where (xt, yt) is the closest point on the ellipse to (x, y). Thus d is the shortest distance from the point to the ellipse.", "The estimator is based on a least squares minimization. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method.", "The params attribute contains the parameters in the following order:", "Ellipse model parameters in the following order xc, yc, a, b, theta.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate circle model from data using total least squares.", "N points with (x, y) coordinates, respectively.", "True, if model estimation succeeds.", "Halir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).", "Predict x- and y-coordinates using the estimated model.", "Angles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.", "Optional custom parameter set.", "Predicted x- and y-coordinates.", "Determine residuals of data to model.", "For each point the shortest distance to the ellipse is returned.", "N points with (x, y) coordinates, respectively.", "Residual for each data point.", "Bases: skimage.measure.fit.BaseModel", "Total least squares estimator for N-dimensional lines.", "In contrast to ordinary least squares line estimation, this estimator minimizes the orthogonal distances of points to the estimated line.", "Lines are defined by a point (origin) and a unit vector (direction) according to the following vector equation:", "Line model parameters in the following order origin, direction.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate line model from data.", "This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.", "N points in a space of dimensionality dim >= 2.", "True, if model estimation succeeds.", "Predict intersection of the estimated line model with a hyperplane orthogonal to a given axis.", "Coordinates along an axis.", "Axis orthogonal to the hyperplane intersecting the line.", "Optional custom parameter set in the form (origin, direction).", "Predicted coordinates.", "If the line is parallel to the given axis.", "Predict x-coordinates for 2D lines using the estimated model.", "Alias for:", "y-coordinates.", "Optional custom parameter set in the form (origin, direction).", "Predicted x-coordinates.", "Predict y-coordinates for 2D lines using the estimated model.", "Alias for:", "x-coordinates.", "Optional custom parameter set in the form (origin, direction).", "Predicted y-coordinates.", "Determine residuals of data to model.", "For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.", "N points in a space of dimension dim.", "Optional custom parameter set in the form (origin, direction).", "Residual for each data point."]}, {"name": "measure.approximate_polygon()", "path": "api/skimage.measure#skimage.measure.approximate_polygon", "type": "measure", "text": ["Approximate a polygonal chain with the specified tolerance.", "It is based on the Douglas-Peucker algorithm.", "Note that the approximated polygon is always within the convex hull of the original polygon.", "Coordinate array.", "Maximum distance from original points of polygon to approximated polygonal chain. If tolerance is 0, the original coordinate array is returned.", "Approximated polygonal chain where M <= N.", "https://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm"]}, {"name": "measure.block_reduce()", "path": "api/skimage.measure#skimage.measure.block_reduce", "type": "measure", "text": ["Downsample image by applying function func to local blocks.", "This function is useful for max and mean pooling, for example.", "N-dimensional input image.", "Array containing down-sampling integer factor along each axis.", "Function object which is used to calculate the return value for each local block. This function must implement an axis parameter. Primary functions are numpy.sum, numpy.min, numpy.max, numpy.mean and numpy.median. See also func_kwargs.", "Constant padding value if image is not perfectly divisible by the block size.", "Keyword arguments passed to func. Notably useful for passing dtype argument to np.mean. Takes dictionary of inputs, e.g.: func_kwargs={'dtype': np.float16}).", "Down-sampled image with same number of dimensions as input image."]}, {"name": "measure.CircleModel", "path": "api/skimage.measure#skimage.measure.CircleModel", "type": "measure", "text": ["Bases: skimage.measure.fit.BaseModel", "Total least squares estimator for 2D circles.", "The functional model of the circle is:", "This estimator minimizes the squared distances from all points to the circle:", "A minimum number of 3 points is required to solve for the parameters.", "Circle model parameters in the following order xc, yc, r.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate circle model from data using total least squares.", "N points with (x, y) coordinates, respectively.", "True, if model estimation succeeds.", "Predict x- and y-coordinates using the estimated model.", "Angles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.", "Optional custom parameter set.", "Predicted x- and y-coordinates.", "Determine residuals of data to model.", "For each point the shortest distance to the circle is returned.", "N points with (x, y) coordinates, respectively.", "Residual for each data point."]}, {"name": "measure.CircleModel.estimate()", "path": "api/skimage.measure#skimage.measure.CircleModel.estimate", "type": "measure", "text": ["Estimate circle model from data using total least squares.", "N points with (x, y) coordinates, respectively.", "True, if model estimation succeeds."]}, {"name": "measure.CircleModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.CircleModel.predict_xy", "type": "measure", "text": ["Predict x- and y-coordinates using the estimated model.", "Angles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.", "Optional custom parameter set.", "Predicted x- and y-coordinates."]}, {"name": "measure.CircleModel.residuals()", "path": "api/skimage.measure#skimage.measure.CircleModel.residuals", "type": "measure", "text": ["Determine residuals of data to model.", "For each point the shortest distance to the circle is returned.", "N points with (x, y) coordinates, respectively.", "Residual for each data point."]}, {"name": "measure.CircleModel.__init__()", "path": "api/skimage.measure#skimage.measure.CircleModel.__init__", "type": "measure", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "measure.EllipseModel", "path": "api/skimage.measure#skimage.measure.EllipseModel", "type": "measure", "text": ["Bases: skimage.measure.fit.BaseModel", "Total least squares estimator for 2D ellipses.", "The functional model of the ellipse is:", "where (xt, yt) is the closest point on the ellipse to (x, y). Thus d is the shortest distance from the point to the ellipse.", "The estimator is based on a least squares minimization. The optimal solution is computed directly, no iterations are required. This leads to a simple, stable and robust fitting method.", "The params attribute contains the parameters in the following order:", "Ellipse model parameters in the following order xc, yc, a, b, theta.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate circle model from data using total least squares.", "N points with (x, y) coordinates, respectively.", "True, if model estimation succeeds.", "Halir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132).", "Predict x- and y-coordinates using the estimated model.", "Angles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.", "Optional custom parameter set.", "Predicted x- and y-coordinates.", "Determine residuals of data to model.", "For each point the shortest distance to the ellipse is returned.", "N points with (x, y) coordinates, respectively.", "Residual for each data point."]}, {"name": "measure.EllipseModel.estimate()", "path": "api/skimage.measure#skimage.measure.EllipseModel.estimate", "type": "measure", "text": ["Estimate circle model from data using total least squares.", "N points with (x, y) coordinates, respectively.", "True, if model estimation succeeds.", "Halir, R.; Flusser, J. \u201cNumerically stable direct least squares fitting of ellipses\u201d. In Proc. 6th International Conference in Central Europe on Computer Graphics and Visualization. WSCG (Vol. 98, pp. 125-132)."]}, {"name": "measure.EllipseModel.predict_xy()", "path": "api/skimage.measure#skimage.measure.EllipseModel.predict_xy", "type": "measure", "text": ["Predict x- and y-coordinates using the estimated model.", "Angles in circle in radians. Angles start to count from positive x-axis to positive y-axis in a right-handed system.", "Optional custom parameter set.", "Predicted x- and y-coordinates."]}, {"name": "measure.EllipseModel.residuals()", "path": "api/skimage.measure#skimage.measure.EllipseModel.residuals", "type": "measure", "text": ["Determine residuals of data to model.", "For each point the shortest distance to the ellipse is returned.", "N points with (x, y) coordinates, respectively.", "Residual for each data point."]}, {"name": "measure.EllipseModel.__init__()", "path": "api/skimage.measure#skimage.measure.EllipseModel.__init__", "type": "measure", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "measure.euler_number()", "path": "api/skimage.measure#skimage.measure.euler_number", "type": "measure", "text": ["Calculate the Euler characteristic in binary image.", "For 2D objects, the Euler number is the number of objects minus the number of holes. For 3D objects, the Euler number is obtained as the number of objects plus the number of holes, minus the number of tunnels, or loops.", "2D or 3D images. If image is not binary, all values strictly greater than zero are considered as the object.", "Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used. 4 or 8 neighborhoods are defined for 2D images (connectivity 1 and 2, respectively). 6 or 26 neighborhoods are defined for 3D images, (connectivity 1 and 3, respectively). Connectivity 2 is not defined.", "Euler characteristic of the set of all objects in the image.", "The Euler characteristic is an integer number that describes the topology of the set of all objects in the input image. If object is 4-connected, then background is 8-connected, and conversely.", "The computation of the Euler characteristic is based on an integral geometry formula in discretized space. In practice, a neighbourhood configuration is constructed, and a LUT is applied for each configuration. The coefficients used are the ones of Ohser et al.", "It can be useful to compute the Euler characteristic for several connectivities. A large relative difference between results for different connectivities suggests that the image resolution (with respect to the size of objects and holes) is too low.", "S. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838", "Ohser J., Nagel W., Schladitz K. (2002) The Euler Number of Discretized Sets - On the Choice of Adjacency in Homogeneous Lattices. In: Mecke K., Stoyan D. (eds) Morphology of Condensed Matter. Lecture Notes in Physics, vol 600. Springer, Berlin, Heidelberg."]}, {"name": "measure.find_contours()", "path": "api/skimage.measure#skimage.measure.find_contours", "type": "measure", "text": ["Find iso-valued contours in a 2D array for a given level value.", "Uses the \u201cmarching squares\u201d method to compute a the iso-valued contours of the input 2D array for a particular level value. Array values are linearly interpolated to provide better precision for the output contours.", "Input image in which to find contours.", "Value along which to find contours in the array. By default, the level is set to (max(image) + min(image)) / 2", "Changed in version 0.18: This parameter is now optional.", "Indicates whether array elements below the given level value are to be considered fully-connected (and hence elements above the value will only be face connected), or vice-versa. (See notes below for details.)", "Indicates whether the output contours will produce positively-oriented polygons around islands of low- or high-valued elements. If \u2018low\u2019 then contours will wind counter- clockwise around elements below the iso-value. Alternately, this means that low-valued elements are always on the left of the contour. (See below for details.)", "A boolean mask, True where we want to draw contours. Note that NaN values are always excluded from the considered region (mask is set to False wherever array is NaN).", "Each contour is an ndarray of shape (n, 2), consisting of n (row, column) coordinates along the contour.", "See also", "The marching squares algorithm is a special case of the marching cubes algorithm [1]. A simple explanation is available here:", "http://users.polytech.unice.fr/~lingrand/MarchingCubes/algo.html", "There is a single ambiguous case in the marching squares algorithm: when a given 2 x 2-element square has two high-valued and two low-valued elements, each pair diagonally adjacent. (Where high- and low-valued is with respect to the contour value sought.) In this case, either the high-valued elements can be \u2018connected together\u2019 via a thin isthmus that separates the low-valued elements, or vice-versa. When elements are connected together across a diagonal, they are considered \u2018fully connected\u2019 (also known as \u2018face+vertex-connected\u2019 or \u20188-connected\u2019). Only high-valued or low-valued elements can be fully-connected, the other set will be considered as \u2018face-connected\u2019 or \u20184-connected\u2019. By default, low-valued elements are considered fully-connected; this can be altered with the \u2018fully_connected\u2019 parameter.", "Output contours are not guaranteed to be closed: contours which intersect the array edge or a masked-off region (either where mask is False or where array is NaN) will be left open. All other contours will be closed. (The closed-ness of a contours can be tested by checking whether the beginning point is the same as the end point.)", "Contours are oriented. By default, array values lower than the contour value are to the left of the contour and values greater than the contour value are to the right. This means that contours will wind counter-clockwise (i.e. in \u2018positive orientation\u2019) around islands of low-valued pixels. This behavior can be altered with the \u2018positive_orientation\u2019 parameter.", "The order of the contours in the output list is determined by the position of the smallest x,y (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.", "Warning", "Array coordinates/values are assumed to refer to the center of the array element. Take a simple example input: [0, 1]. The interpolated position of 0.5 in this array is midway between the 0-element (at x=0) and the 1-element (at x=1), and thus would fall at x=0.5.", "This means that to find reasonable contours, it is best to find contours midway between the expected \u201clight\u201d and \u201cdark\u201d values. In particular, given a binarized array, do not choose to find contours at the low or high value of the array. This will often yield degenerate contours, especially around structures that are a single array element wide. Instead choose a middle value, as above.", "Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422"]}, {"name": "measure.grid_points_in_poly()", "path": "api/skimage.measure#skimage.measure.grid_points_in_poly", "type": "measure", "text": ["Test whether points on a specified grid are inside a polygon.", "For each (r, c) coordinate on a grid, i.e. (0, 0), (0, 1) etc., test whether that point lies inside a polygon.", "Shape of the grid.", "Specify the V vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.", "True where the grid falls inside the polygon.", "See also"]}, {"name": "measure.inertia_tensor()", "path": "api/skimage.measure#skimage.measure.inertia_tensor", "type": "measure", "text": ["Compute the inertia tensor of the input image.", "The input image.", "The pre-computed central moments of image. The inertia tensor computation requires the central moments of the image. If an application requires both the central moments and the inertia tensor (for example, skimage.measure.regionprops), then it is more efficient to pre-compute them and pass them to the inertia tensor call.", "The inertia tensor of the input image. \\(T_{i, j}\\) contains the covariance of image intensity along axes \\(i\\) and \\(j\\).", "https://en.wikipedia.org/wiki/Moment_of_inertia#Inertia_tensor", "Bernd J\u00e4hne. Spatio-Temporal Image Processing: Theory and Scientific Applications. (Chapter 8: Tensor Methods) Springer, 1993."]}, {"name": "measure.inertia_tensor_eigvals()", "path": "api/skimage.measure#skimage.measure.inertia_tensor_eigvals", "type": "measure", "text": ["Compute the eigenvalues of the inertia tensor of the image.", "The inertia tensor measures covariance of the image intensity along the image axes. (See inertia_tensor.) The relative magnitude of the eigenvalues of the tensor is thus a measure of the elongation of a (bright) object in the image.", "The input image.", "The pre-computed central moments of image.", "The pre-computed inertia tensor. If T is given, mu and image are ignored.", "The eigenvalues of the inertia tensor of image, in descending order.", "Computing the eigenvalues requires the inertia tensor of the input image. This is much faster if the central moments (mu) are provided, or, alternatively, one can provide the inertia tensor (T) directly."]}, {"name": "measure.label()", "path": "api/skimage.measure#skimage.measure.label", "type": "measure", "text": ["Label connected regions of an integer array.", "Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor:", "Image to label.", "Consider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.", "Whether to return the number of assigned labels.", "Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.", "Labeled array, where all connected regions are assigned the same integer value.", "Number of labels, which equals the maximum label index and is only returned if return_num is True.", "See also", "Christophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.", "Kensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864"]}, {"name": "measure.LineModelND", "path": "api/skimage.measure#skimage.measure.LineModelND", "type": "measure", "text": ["Bases: skimage.measure.fit.BaseModel", "Total least squares estimator for N-dimensional lines.", "In contrast to ordinary least squares line estimation, this estimator minimizes the orthogonal distances of points to the estimated line.", "Lines are defined by a point (origin) and a unit vector (direction) according to the following vector equation:", "Line model parameters in the following order origin, direction.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate line model from data.", "This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.", "N points in a space of dimensionality dim >= 2.", "True, if model estimation succeeds.", "Predict intersection of the estimated line model with a hyperplane orthogonal to a given axis.", "Coordinates along an axis.", "Axis orthogonal to the hyperplane intersecting the line.", "Optional custom parameter set in the form (origin, direction).", "Predicted coordinates.", "If the line is parallel to the given axis.", "Predict x-coordinates for 2D lines using the estimated model.", "Alias for:", "y-coordinates.", "Optional custom parameter set in the form (origin, direction).", "Predicted x-coordinates.", "Predict y-coordinates for 2D lines using the estimated model.", "Alias for:", "x-coordinates.", "Optional custom parameter set in the form (origin, direction).", "Predicted y-coordinates.", "Determine residuals of data to model.", "For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.", "N points in a space of dimension dim.", "Optional custom parameter set in the form (origin, direction).", "Residual for each data point."]}, {"name": "measure.LineModelND.estimate()", "path": "api/skimage.measure#skimage.measure.LineModelND.estimate", "type": "measure", "text": ["Estimate line model from data.", "This minimizes the sum of shortest (orthogonal) distances from the given data points to the estimated line.", "N points in a space of dimensionality dim >= 2.", "True, if model estimation succeeds."]}, {"name": "measure.LineModelND.predict()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict", "type": "measure", "text": ["Predict intersection of the estimated line model with a hyperplane orthogonal to a given axis.", "Coordinates along an axis.", "Axis orthogonal to the hyperplane intersecting the line.", "Optional custom parameter set in the form (origin, direction).", "Predicted coordinates.", "If the line is parallel to the given axis."]}, {"name": "measure.LineModelND.predict_x()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_x", "type": "measure", "text": ["Predict x-coordinates for 2D lines using the estimated model.", "Alias for:", "y-coordinates.", "Optional custom parameter set in the form (origin, direction).", "Predicted x-coordinates."]}, {"name": "measure.LineModelND.predict_y()", "path": "api/skimage.measure#skimage.measure.LineModelND.predict_y", "type": "measure", "text": ["Predict y-coordinates for 2D lines using the estimated model.", "Alias for:", "x-coordinates.", "Optional custom parameter set in the form (origin, direction).", "Predicted y-coordinates."]}, {"name": "measure.LineModelND.residuals()", "path": "api/skimage.measure#skimage.measure.LineModelND.residuals", "type": "measure", "text": ["Determine residuals of data to model.", "For each point, the shortest (orthogonal) distance to the line is returned. It is obtained by projecting the data onto the line.", "N points in a space of dimension dim.", "Optional custom parameter set in the form (origin, direction).", "Residual for each data point."]}, {"name": "measure.LineModelND.__init__()", "path": "api/skimage.measure#skimage.measure.LineModelND.__init__", "type": "measure", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "measure.marching_cubes()", "path": "api/skimage.measure#skimage.measure.marching_cubes", "type": "measure", "text": ["Marching cubes algorithm to find surfaces in 3d volumetric data.", "In contrast with Lorensen et al. approach [2], Lewiner et al. algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice.", "Input data volume to find isosurfaces. Will internally be converted to float32 if necessary.", "Contour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.", "Voxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.", "Controls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object", "Step size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.", "Whether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.", "One of \u2018lewiner\u2019, \u2018lorensen\u2019 or \u2018_lorensen\u2019. Specify witch of Lewiner et al. or Lorensen et al. method will be used. The \u2018_lorensen\u2019 flag correspond to an old implementation that will be deprecated in version 0.19.", "Boolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.", "Spatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.", "Define triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.", "The normal direction at each vertex, as calculated from the data.", "Gives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.", "See also", "The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation.", "To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area.", "Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package:", "Similarly using the visvis package:", "To reduce the number of triangles in the mesh for better performance, see this example using the mayavi package.", "Thomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582", "Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422"]}, {"name": "measure.marching_cubes_classic()", "path": "api/skimage.measure#skimage.measure.marching_cubes_classic", "type": "measure", "text": ["Classic marching cubes algorithm to find surfaces in 3d volumetric data.", "Note that the marching_cubes() algorithm is recommended over this algorithm, because it\u2019s faster and produces better results.", "Input data volume to find isosurfaces. Will be cast to np.float64.", "Contour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.", "Voxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.", "Controls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object", "Spatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.", "Define triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.", "See also", "The marching cubes algorithm is implemented as described in [1]. A simple explanation is available here:", "There are several known ambiguous cases in the marching cubes algorithm. Using point labeling as in [1], Figure 4, as shown:", "Most notably, if v4, v8, v2, and v6 are all >= level (or any generalization of this case) two parallel planes are generated by this algorithm, separating v4 and v8 from v2 and v6. An equally valid interpretation would be a single connected thin surface enclosing all four points. This is the best known ambiguity, though there are others.", "This algorithm does not attempt to resolve such ambiguities; it is a naive implementation of marching cubes as in [1], but may be a good beginning for work with more recent techniques (Dual Marching Cubes, Extended Marching Cubes, Cubic Marching Squares, etc.).", "Because of interactions between neighboring cubes, the isosurface(s) generated by this algorithm are NOT guaranteed to be closed, particularly for complicated contours. Furthermore, this algorithm does not guarantee a single contour will be returned. Indeed, ALL isosurfaces which cross level will be found, regardless of connectivity.", "The output is a triangular mesh consisting of a set of unique vertices and connecting triangles. The order of these vertices and triangles in the output list is determined by the position of the smallest x,y,z (in lexicographical order) coordinate in the contour. This is a side-effect of how the input array is traversed, but can be relied upon.", "The generated mesh guarantees coherent orientation as of version 0.12.", "To quantify the area of an isosurface generated by this algorithm, pass outputs directly into skimage.measure.mesh_surface_area.", "Lorensen, William and Harvey E. Cline. Marching Cubes: A High Resolution 3D Surface Construction Algorithm. Computer Graphics (SIGGRAPH 87 Proceedings) 21(4) July 1987, p. 163-170). DOI:10.1145/37401.37422"]}, {"name": "measure.marching_cubes_lewiner()", "path": "api/skimage.measure#skimage.measure.marching_cubes_lewiner", "type": "measure", "text": ["Lewiner marching cubes algorithm to find surfaces in 3d volumetric data.", "In contrast to marching_cubes_classic(), this algorithm is faster, resolves ambiguities, and guarantees topologically correct results. Therefore, this algorithm generally a better choice, unless there is a specific need for the classic algorithm.", "Input data volume to find isosurfaces. Will internally be converted to float32 if necessary.", "Contour value to search for isosurfaces in volume. If not given or None, the average of the min and max of vol is used.", "Voxel spacing in spatial dimensions corresponding to numpy array indexing dimensions (M, N, P) as in volume.", "Controls if the mesh was generated from an isosurface with gradient descent toward objects of interest (the default), or the opposite, considering the left-hand rule. The two options are: * descent : Object was greater than exterior * ascent : Exterior was greater than object", "Step size in voxels. Default 1. Larger steps yield faster but coarser results. The result will always be topologically correct though.", "Whether to allow degenerate (i.e. zero-area) triangles in the end-result. Default True. If False, degenerate triangles are removed, at the cost of making the algorithm slower.", "If given and True, the classic marching cubes by Lorensen (1987) is used. This option is included for reference purposes. Note that this algorithm has ambiguities and is not guaranteed to produce a topologically correct result. The results with using this option are not generally the same as the marching_cubes_classic() function.", "Boolean array. The marching cube algorithm will be computed only on True elements. This will save computational time when interfaces are located within certain region of the volume M, N, P-e.g. the top half of the cube-and also allow to compute finite surfaces-i.e. open surfaces that do not end at the border of the cube.", "Spatial coordinates for V unique mesh vertices. Coordinate order matches input volume (M, N, P). If allow_degenerate is set to True, then the presence of degenerate triangles in the mesh can make this array have duplicate vertices.", "Define triangular faces via referencing vertex indices from verts. This algorithm specifically outputs triangles, so each face has exactly three indices.", "The normal direction at each vertex, as calculated from the data.", "Gives a measure for the maximum value of the data in the local region near each vertex. This can be used by visualization tools to apply a colormap to the mesh.", "See also", "The algorithm [1] is an improved version of Chernyaev\u2019s Marching Cubes 33 algorithm. It is an efficient algorithm that relies on heavy use of lookup tables to handle the many different cases, keeping the algorithm relatively easy. This implementation is written in Cython, ported from Lewiner\u2019s C++ implementation.", "To quantify the area of an isosurface generated by this algorithm, pass verts and faces to skimage.measure.mesh_surface_area.", "Regarding visualization of algorithm output, to contour a volume named myvolume about the level 0.0, using the mayavi package:", "Similarly using the visvis package:", "Thomas Lewiner, Helio Lopes, Antonio Wilson Vieira and Geovan Tavares. Efficient implementation of Marching Cubes\u2019 cases with topological guarantees. Journal of Graphics Tools 8(2) pp. 1-15 (december 2003). DOI:10.1080/10867651.2003.10487582"]}, {"name": "measure.mesh_surface_area()", "path": "api/skimage.measure#skimage.measure.mesh_surface_area", "type": "measure", "text": ["Compute surface area, given vertices & triangular faces", "Array containing (x, y, z) coordinates for V unique mesh vertices.", "List of length-3 lists of integers, referencing vertex coordinates as provided in verts", "Surface area of mesh. Units now [coordinate units] ** 2.", "See also", "The arguments expected by this function are the first two outputs from skimage.measure.marching_cubes. For unit correct output, ensure correct spacing was passed to skimage.measure.marching_cubes.", "This algorithm works properly only if the faces provided are all triangles."]}, {"name": "measure.moments()", "path": "api/skimage.measure#skimage.measure.moments", "type": "measure", "text": ["Calculate all raw image moments up to a certain order.", "Note that raw moments are neither translation, scale nor rotation invariant.", "Rasterized shape as image.", "Maximum order of moments. Default is 3.", "Raw image moments.", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment"]}, {"name": "measure.moments_central()", "path": "api/skimage.measure#skimage.measure.moments_central", "type": "measure", "text": ["Calculate all central image moments up to a certain order.", "The center coordinates (cr, cc) can be calculated from the raw moments as: {M[1, 0] / M[0, 0], M[0, 1] / M[0, 0]}.", "Note that central moments are translation invariant but not scale and rotation invariant.", "Rasterized shape as image.", "Coordinates of the image centroid. This will be computed if it is not provided.", "The maximum order of moments computed.", "Central image moments.", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment"]}, {"name": "measure.moments_coords()", "path": "api/skimage.measure#skimage.measure.moments_coords", "type": "measure", "text": ["Calculate all raw image moments up to a certain order.", "Note that raw moments are neither translation, scale nor rotation invariant.", "Array of N points that describe an image of D dimensionality in Cartesian space.", "Maximum order of moments. Default is 3.", "Raw image moments. (D dimensions)", "Johannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001."]}, {"name": "measure.moments_coords_central()", "path": "api/skimage.measure#skimage.measure.moments_coords_central", "type": "measure", "text": ["Calculate all central image moments up to a certain order.", "Note that raw moments are neither translation, scale nor rotation invariant.", "Array of N points that describe an image of D dimensionality in Cartesian space. A tuple of coordinates as returned by np.nonzero is also accepted as input.", "Coordinates of the image centroid. This will be computed if it is not provided.", "Maximum order of moments. Default is 3.", "Central image moments. (D dimensions)", "Johannes Kilian. Simple Image Analysis By Moments. Durham University, version 0.2, Durham, 2001.", "As seen above, for symmetric objects, odd-order moments (columns 1 and 3, rows 1 and 3) are zero when centered on the centroid, or center of mass, of the object (the default). If we break the symmetry by adding a new point, this no longer holds:", "Image moments and central image moments are equivalent (by definition) when the center is (0, 0):"]}, {"name": "measure.moments_hu()", "path": "api/skimage.measure#skimage.measure.moments_hu", "type": "measure", "text": ["Calculate Hu\u2019s set of image moments (2D-only).", "Note that this set of moments is proofed to be translation, scale and rotation invariant.", "Normalized central image moments, where M must be >= 4.", "Hu\u2019s set of image moments.", "M. K. Hu, \u201cVisual Pattern Recognition by Moment Invariants\u201d, IRE Trans. Info. Theory, vol. IT-8, pp. 179-187, 1962", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment"]}, {"name": "measure.moments_normalized()", "path": "api/skimage.measure#skimage.measure.moments_normalized", "type": "measure", "text": ["Calculate all normalized central image moments up to a certain order.", "Note that normalized central moments are translation and scale invariant but not rotation invariant.", "Central image moments, where M must be greater than or equal to order.", "Maximum order of moments. Default is 3.", "Normalized central image moments.", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment"]}, {"name": "measure.perimeter()", "path": "api/skimage.measure#skimage.measure.perimeter", "type": "measure", "text": ["Calculate total perimeter of all objects in binary image.", "2D binary image.", "Neighborhood connectivity for border pixel determination. It is used to compute the contour. A higher neighbourhood widens the border on which the perimeter is computed.", "Total perimeter of all objects in binary image.", "K. Benkrid, D. Crookes. Design and FPGA Implementation of a Perimeter Estimator. The Queen\u2019s University of Belfast. http://www.cs.qub.ac.uk/~d.crookes/webpubs/papers/perimeter.doc"]}, {"name": "measure.perimeter_crofton()", "path": "api/skimage.measure#skimage.measure.perimeter_crofton", "type": "measure", "text": ["Calculate total Crofton perimeter of all objects in binary image.", "2D image. If image is not binary, all values strictly greater than zero are considered as the object.", "Number of directions used to approximate the Crofton perimeter. By default, 4 is used: it should be more accurate than 2. Computation time is the same in both cases.", "Total perimeter of all objects in binary image.", "This measure is based on Crofton formula [1], which is a measure from integral geometry. It is defined for general curve length evaluation via a double integral along all directions. In a discrete space, 2 or 4 directions give a quite good approximation, 4 being more accurate than 2 for more complex shapes.", "Similar to perimeter(), this function returns an approximation of the perimeter in continuous space.", "https://en.wikipedia.org/wiki/Crofton_formula", "S. Rivollier. Analyse d\u2019image geometrique et morphometrique par diagrammes de forme et voisinages adaptatifs generaux. PhD thesis, 2010. Ecole Nationale Superieure des Mines de Saint-Etienne. https://tel.archives-ouvertes.fr/tel-00560838"]}, {"name": "measure.points_in_poly()", "path": "api/skimage.measure#skimage.measure.points_in_poly", "type": "measure", "text": ["Test whether points lie inside a polygon.", "Input points, (x, y).", "Vertices of the polygon, sorted either clockwise or anti-clockwise. The first point may (but does not need to be) duplicated.", "True if corresponding point is inside the polygon.", "See also"]}, {"name": "measure.profile_line()", "path": "api/skimage.measure#skimage.measure.profile_line", "type": "measure", "text": ["Return the intensity profile of an image measured along a scan line.", "The image, either grayscale (2D array) or multichannel (3D array, where the final axis contains the channel information).", "The coordinates of the start point of the scan line.", "The coordinates of the end point of the scan line. The destination point is included in the profile, in contrast to standard numpy indexing.", "Width of the scan, perpendicular to the line", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "How to compute any values falling outside of the image.", "If mode is \u2018constant\u2019, what constant value to use outside the image.", "Function used to calculate the aggregation of pixel values perpendicular to the profile_line direction when linewidth > 1. If set to None the unreduced array will be returned.", "The intensity profile along the scan line. The length of the profile is the ceil of the computed length of the scan line.", "The destination point is included in the profile, in contrast to standard numpy indexing. For example:", "For different reduce_func inputs:", "The unreduced array will be returned when reduce_func is None or when reduce_func acts on each pixel value individually."]}, {"name": "measure.ransac()", "path": "api/skimage.measure#skimage.measure.ransac", "type": "measure", "text": ["Fit a model to data with the RANSAC (random sample consensus) algorithm.", "RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:", "These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model.", "Data set to which the model is fitted, where N is the number of data points and the remaining dimension are depending on model requirements. If the model class requires multiple input data arrays (e.g. source and destination coordinates of skimage.transform.AffineTransform), they can be optionally passed as tuple or list. Note, that in this case the functions estimate(*data), residuals(*data), is_model_valid(model, *random_data) and is_data_valid(*random_data) must all take each data array as separate arguments.", "Object with the following object methods:", "where success indicates whether the model estimation succeeded (True or None for success, False for failure).", "The minimum number of data points to fit a model to.", "Maximum distance for a data point to be classified as an inlier.", "This function is called with the randomly selected data before the model is fitted to it: is_data_valid(*random_data).", "This function is called with the estimated model and the randomly selected data: is_model_valid(model, *random_data), .", "Maximum number of iterations for random sample selection.", "Stop iteration if at least this number of inliers are found.", "Stop iteration if sum of residuals is less than or equal to this threshold.", "RANSAC iteration stops if at least one outlier-free set of the training data is sampled with probability >= stop_probability, depending on the current best model\u2019s inlier ratio and the number of trials. This requires to generate at least N samples (trials):", "N >= log(1 - probability) / log(1 - e**m)", "where the probability (confidence) is typically set to a high value such as 0.99, e is the current fraction of inliers w.r.t. the total number of samples, and m is the min_samples value.", "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.", "Initial samples selection for model estimation", "Best model with largest consensus set.", "Boolean mask of inliers classified as True.", "\u201cRANSAC\u201d, Wikipedia, https://en.wikipedia.org/wiki/RANSAC", "Generate ellipse data without tilt and add noise:", "Add some faulty data:", "Estimate ellipse model using all available data:", "Estimate ellipse model using RANSAC:", "RANSAC can be used to robustly estimate a geometric transformation. In this section, we also show how to use a proportion of the total samples, rather than an absolute number."]}, {"name": "measure.regionprops()", "path": "api/skimage.measure#skimage.measure.regionprops", "type": "measure", "text": ["Measure properties of labeled image regions.", "Labeled input image. Labels with value 0 are ignored.", "Changed in version 0.14.1: Previously, label_image was processed by numpy.squeeze and so any number of singleton dimensions was allowed. This resulted in inconsistent handling of images with singleton dimensions. To recover the old behaviour, use regionprops(np.squeeze(label_image), ...).", "Intensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.", "Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.", "Determine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.", "This argument is deprecated and will be removed in a future version of scikit-image.", "See Coordinate conventions for more details.", "Deprecated since version 0.16.0: Use \u201crc\u201d coordinates everywhere. It may be sufficient to call numpy.transpose on your label image to get the same values as 0.15 and earlier. However, for some properties, the transformation will be less trivial. For example, the new orientation is \\(\\frac{\\pi}{2}\\) plus the old orientation.", "Add extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.", "Each item describes one labeled region, and can be accessed using the attributes listed below.", "See also", "The following properties can be accessed as attributes or keys:", "Number of pixels of the region.", "Bounding box (min_row, min_col, max_row, max_col). Pixels belonging to the bounding box are in the half-open interval [min_row; max_row) and [min_col; max_col).", "Number of pixels of bounding box.", "Centroid coordinate tuple (row, col).", "Number of pixels of convex hull image, which is the smallest convex polygon that encloses the region.", "Binary convex hull image which has the same size as bounding box.", "Coordinate list (row, col) of the region.", "Eccentricity of the ellipse that has the same second-moments as the region. The eccentricity is the ratio of the focal distance (distance between focal points) over the major axis length. The value is in the interval [0, 1). When it is 0, the ellipse becomes a circle.", "The diameter of a circle with the same area as the region.", "Euler characteristic of the set of non-zero pixels. Computed as number of connected components subtracted by number of holes (input.ndim connectivity). In 3D, number of connected components plus number of holes subtracted by number of tunnels.", "Ratio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)", "Maximum Feret\u2019s diameter computed as the longest distance between points around a region\u2019s convex hull contour as determined by find_contours. [5]", "Number of pixels of the region will all the holes filled in. Describes the area of the filled_image.", "Binary region image with filled holes which has the same size as bounding box.", "Sliced binary region image which has the same size as bounding box.", "Inertia tensor of the region for the rotation around its mass.", "The eigenvalues of the inertia tensor in decreasing order.", "Image inside region bounding box.", "The label in the labeled input image.", "Centroid coordinate tuple (row, col), relative to region bounding box.", "The length of the major axis of the ellipse that has the same normalized second central moments as the region.", "Value with the greatest intensity in the region.", "Value with the mean intensity in the region.", "Value with the least intensity in the region.", "The length of the minor axis of the ellipse that has the same normalized second central moments as the region.", "Spatial moments up to 3rd order:", "where the sum is over the row, col coordinates of the region.", "Central moments (translation invariant) up to 3rd order:", "where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.", "Hu moments (translation, scale and rotation invariant).", "Normalized moments (translation and scale invariant) up to 3rd order:", "where m_00 is the zeroth spatial moment.", "Angle between the 0th axis (rows) and the major axis of the ellipse that has the same second moments as the region, ranging from -pi/2 to pi/2 counter-clockwise.", "Perimeter of object which approximates the contour as a line through the centers of border pixels using a 4-connectivity.", "Perimeter of object approximated by the Crofton formula in 4 directions.", "A slice to extract the object from the source image.", "Ratio of pixels in the region to pixels of the convex hull image.", "Centroid coordinate tuple (row, col) weighted with intensity image.", "Centroid coordinate tuple (row, col), relative to region bounding box, weighted with intensity image.", "Spatial moments of intensity image up to 3rd order:", "where the sum is over the row, col coordinates of the region.", "Central moments (translation invariant) of intensity image up to 3rd order:", "where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s weighted centroid.", "Hu moments (translation, scale and rotation invariant) of intensity image.", "Normalized moments (translation and scale invariant) of intensity image up to 3rd order:", "where wm_00 is the zeroth spatial moment (intensity-weighted area).", "Each region also supports iteration, so that you can do:", "Wilhelm Burger, Mark Burge. Principles of Digital Image Processing: Core Algorithms. Springer-Verlag, London, 2009.", "B. J\u00e4hne. Digital Image Processing. Springer-Verlag, Berlin-Heidelberg, 6. edition, 2005.", "T. H. Reiss. Recognizing Planar Objects Using Invariant Image Features, from Lecture notes in computer science, p. 676. Springer, Berlin, 1993.", "https://en.wikipedia.org/wiki/Image_moment", "W. Pabst, E. Gregorov\u00e1. Characterization of particles and particle systems, pp. 27-28. ICT Prague, 2007. https://old.vscht.cz/sil/keramika/Characterization_of_particles/CPPS%20_English%20version_.pdf", "Add custom measurements by passing functions as extra_properties"]}, {"name": "measure.regionprops_table()", "path": "api/skimage.measure#skimage.measure.regionprops_table", "type": "measure", "text": ["Compute image properties and return them as a pandas-compatible table.", "The table is a dictionary mapping column names to value arrays. See Notes section below for details.", "New in version 0.16.", "Labeled input image. Labels with value 0 are ignored.", "Intensity (i.e., input) image with same size as labeled image, plus optionally an extra dimension for multichannel data. Default is None.", "Changed in version 0.18.0: The ability to provide an extra dimension for channels was added.", "Properties that will be included in the resulting dictionary For a list of available properties, please see regionprops(). Users should remember to add \u201clabel\u201d to keep track of region identities.", "Determine whether to cache calculated properties. The computation is much faster for cached properties, whereas the memory consumption increases.", "For non-scalar properties not listed in OBJECT_COLUMNS, each element will appear in its own column, with the index of that element separated from the property name by this separator. For example, the inertia tensor of a 2D region will appear in four columns: inertia_tensor-0-0, inertia_tensor-0-1, inertia_tensor-1-0, and inertia_tensor-1-1 (where the separator is -).", "Object columns are those that cannot be split in this way because the number of columns would change depending on the object. For example, image and coords.", "Add extra property computation functions that are not included with skimage. The name of the property is derived from the function name, the dtype is inferred by calling the function on a small sample. If the name of an extra property clashes with the name of an existing property the extra property wil not be visible and a UserWarning is issued. A property computation function must take a region mask as its first argument. If the property requires an intensity image, it must accept the intensity image as the second argument.", "Dictionary mapping property names to an array of values of that property, one value per region. This dictionary can be used as input to pandas DataFrame to map property names to columns in the frame and regions to rows. If the image has no regions, the arrays will have length 0, but the correct type.", "Each column contains either a scalar property, an object property, or an element in a multidimensional array.", "Properties with scalar values for each region, such as \u201ceccentricity\u201d, will appear as a float or int array with that property name as key.", "Multidimensional properties of fixed size for a given image dimension, such as \u201ccentroid\u201d (every centroid will have three elements in a 3D image, no matter the region size), will be split into that many columns, with the name {property_name}{separator}{element_num} (for 1D properties), {property_name}{separator}{elem_num0}{separator}{elem_num1} (for 2D properties), and so on.", "For multidimensional properties that don\u2019t have a fixed size, such as \u201cimage\u201d (the image of a region varies in size depending on the region size), an object array will be used, with the corresponding property name as the key.", "The resulting dictionary can be directly passed to pandas, if installed, to obtain a clean DataFrame:", "[5 rows x 7 columns]", "If we want to measure a feature that does not come as a built-in property, we can define custom functions and pass them as extra_properties. For example, we can create a custom function that measures the intensity quartiles in a region:"]}, {"name": "measure.shannon_entropy()", "path": "api/skimage.measure#skimage.measure.shannon_entropy", "type": "measure", "text": ["Calculate the Shannon entropy of an image.", "The Shannon entropy is defined as S = -sum(pk * log(pk)), where pk are frequency/probability of pixels of value k.", "Grayscale input image.", "The logarithmic base to use.", "The returned value is measured in bits or shannon (Sh) for base=2, natural unit (nat) for base=np.e and hartley (Hart) for base=10.", "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "https://en.wiktionary.org/wiki/Shannon_entropy"]}, {"name": "measure.subdivide_polygon()", "path": "api/skimage.measure#skimage.measure.subdivide_polygon", "type": "measure", "text": ["Subdivision of polygonal curves using B-Splines.", "Note that the resulting curve is always within the convex hull of the original polygon. Circular polygons stay closed after subdivision.", "Coordinate array.", "Degree of B-Spline. Default is 2.", "Preserve first and last coordinate of non-circular polygon. Default is False.", "Subdivided coordinate array.", "http://mrl.nyu.edu/publications/subdiv-course2000/coursenotes00.pdf"]}, {"name": "metrics", "path": "api/skimage.metrics", "type": "metrics", "text": ["skimage.metrics.adapted_rand_error([\u2026])", "Compute Adapted Rand error as defined by the SNEMI3D contest.", "skimage.metrics.contingency_table(im_true, \u2026)", "Return the contingency table for all regions in matched segmentations.", "skimage.metrics.hausdorff_distance(image0, \u2026)", "Calculate the Hausdorff distance between nonzero elements of given images.", "skimage.metrics.mean_squared_error(image0, \u2026)", "Compute the mean-squared error between two images.", "skimage.metrics.normalized_root_mse(\u2026[, \u2026])", "Compute the normalized root mean-squared error (NRMSE) between two images.", "skimage.metrics.peak_signal_noise_ratio(\u2026)", "Compute the peak signal to noise ratio (PSNR) for an image.", "skimage.metrics.structural_similarity(im1, \u2026)", "Compute the mean structural similarity index between two images.", "skimage.metrics.variation_of_information([\u2026])", "Return symmetric conditional entropies associated with the VI.", "Compute Adapted Rand error as defined by the SNEMI3D contest. [1]", "Ground-truth label image, same shape as im_test.", "Test image.", "A contingency table built with skimage.evaluate.contingency_table. If None, it will be computed on the fly.", "Labels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.", "The adapted Rand error; equal to \\(1 - \\frac{2pr}{p + r}\\), where p and r are the precision and recall described below.", "The adapted Rand precision: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the test image.", "The adapted Rand recall: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the true image.", "Pixels with label 0 in the true segmentation are ignored in the score.", "Arganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the creation of image segmentation algorithms for connectomics. Front. Neuroanat. 9:142. DOI:10.3389/fnana.2015.00142", "Return the contingency table for all regions in matched segmentations.", "Ground-truth label image, same shape as im_test.", "Test image.", "Labels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.", "Determines if the contingency table is normalized by pixel count.", "A contingency table. cont[i, j] will equal the number of voxels labeled i in im_true and j in im_test.", "Calculate the Hausdorff distance between nonzero elements of given images.", "The Hausdorff distance [1] is the maximum distance between any point on image0 and its nearest point on image1, and vice-versa.", "Arrays where True represents a point that is included in a set of points. Both arrays must have the same shape.", "The Hausdorff distance between coordinates of nonzero pixels in image0 and image1, using the Euclidian distance.", "http://en.wikipedia.org/wiki/Hausdorff_distance", "Hausdorff Distance", "Compute the mean-squared error between two images.", "Images. Any dimensionality, must have same shape.", "The mean-squared error (MSE) metric.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_mse to skimage.metrics.mean_squared_error.", "Compute the normalized root mean-squared error (NRMSE) between two images.", "Ground-truth image, same shape as im_test.", "Test image.", "Controls the normalization method to use in the denominator of the NRMSE. There is no standard method of normalization across the literature [1]. The methods available here are as follows:", "\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of im_true:", "where || . || denotes the Frobenius norm and N = im_true.size. This result is equivalent to:", "The NRMSE metric.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_nrmse to skimage.metrics.normalized_root_mse.", "https://en.wikipedia.org/wiki/Root-mean-square_deviation", "Compute the peak signal to noise ratio (PSNR) for an image.", "Ground-truth image, same shape as im_test.", "Test image.", "The data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.", "The PSNR metric.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_psnr to skimage.metrics.peak_signal_noise_ratio.", "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio", "Compute the mean structural similarity index between two images.", "Images. Any dimensionality with same shape.", "The side-length of the sliding window used in comparison. Must be an odd value. If gaussian_weights is True, this is ignored and the window size will depend on sigma.", "If True, also return the gradient with respect to im2.", "The data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.", "If True, treat the last dimension of the array as channels. Similarity calculations are done independently for each channel then averaged.", "If True, each patch has its mean and variance spatially weighted by a normalized Gaussian kernel of width sigma=1.5.", "If True, also return the full structural similarity image.", "The mean structural similarity index over the image.", "The gradient of the structural similarity between im1 and im2 [2]. This is only returned if gradient is set to True.", "The full SSIM image. This is only returned if full is set to True.", "If True, normalize covariances by N-1 rather than, N where N is the number of pixels within the sliding window.", "Algorithm parameter, K1 (small constant, see [1]).", "Algorithm parameter, K2 (small constant, see [1]).", "Standard deviation for the Gaussian when gaussian_weights is True.", "To match the implementation of Wang et. al. [1], set gaussian_weights to True, sigma to 1.5, and use_sample_covariance to False.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity.", "Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13, 600-612. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf, DOI:10.1109/TIP.2003.819861", "Avanaki, A. N. (2009). Exact global histogram specification optimized for structural similarity. Optical Review, 16, 613-621. arXiv:0901.0065 DOI:10.1007/s10043-009-0119-z", "Return symmetric conditional entropies associated with the VI. [1]", "The variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is the ground-truth segmentation, then H(X|Y) can be interpreted as the amount of under-segmentation and H(X|Y) as the amount of over-segmentation. In other words, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-segmentation will have H(Y|X)=0.", "Label images / segmentations, must have same shape.", "A contingency table built with skimage.evaluate.contingency_table. If None, it will be computed with skimage.evaluate.contingency_table. If given, the entropies will be computed from this table and any images will be ignored.", "Labels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.", "The conditional entropies of image1|image0 and image0|image1.", "Marina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance, Journal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN 0047-259X, DOI:10.1016/j.jmva.2006.11.013."]}, {"name": "metrics.adapted_rand_error()", "path": "api/skimage.metrics#skimage.metrics.adapted_rand_error", "type": "metrics", "text": ["Compute Adapted Rand error as defined by the SNEMI3D contest. [1]", "Ground-truth label image, same shape as im_test.", "Test image.", "A contingency table built with skimage.evaluate.contingency_table. If None, it will be computed on the fly.", "Labels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.", "The adapted Rand error; equal to \\(1 - \\frac{2pr}{p + r}\\), where p and r are the precision and recall described below.", "The adapted Rand precision: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the test image.", "The adapted Rand recall: this is the number of pairs of pixels that have the same label in the test label image and in the true image, divided by the number in the true image.", "Pixels with label 0 in the true segmentation are ignored in the score.", "Arganda-Carreras I, Turaga SC, Berger DR, et al. (2015) Crowdsourcing the creation of image segmentation algorithms for connectomics. Front. Neuroanat. 9:142. DOI:10.3389/fnana.2015.00142"]}, {"name": "metrics.contingency_table()", "path": "api/skimage.metrics#skimage.metrics.contingency_table", "type": "metrics", "text": ["Return the contingency table for all regions in matched segmentations.", "Ground-truth label image, same shape as im_test.", "Test image.", "Labels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.", "Determines if the contingency table is normalized by pixel count.", "A contingency table. cont[i, j] will equal the number of voxels labeled i in im_true and j in im_test."]}, {"name": "metrics.hausdorff_distance()", "path": "api/skimage.metrics#skimage.metrics.hausdorff_distance", "type": "metrics", "text": ["Calculate the Hausdorff distance between nonzero elements of given images.", "The Hausdorff distance [1] is the maximum distance between any point on image0 and its nearest point on image1, and vice-versa.", "Arrays where True represents a point that is included in a set of points. Both arrays must have the same shape.", "The Hausdorff distance between coordinates of nonzero pixels in image0 and image1, using the Euclidian distance.", "http://en.wikipedia.org/wiki/Hausdorff_distance"]}, {"name": "metrics.mean_squared_error()", "path": "api/skimage.metrics#skimage.metrics.mean_squared_error", "type": "metrics", "text": ["Compute the mean-squared error between two images.", "Images. Any dimensionality, must have same shape.", "The mean-squared error (MSE) metric.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_mse to skimage.metrics.mean_squared_error."]}, {"name": "metrics.normalized_root_mse()", "path": "api/skimage.metrics#skimage.metrics.normalized_root_mse", "type": "metrics", "text": ["Compute the normalized root mean-squared error (NRMSE) between two images.", "Ground-truth image, same shape as im_test.", "Test image.", "Controls the normalization method to use in the denominator of the NRMSE. There is no standard method of normalization across the literature [1]. The methods available here are as follows:", "\u2018euclidean\u2019 : normalize by the averaged Euclidean norm of im_true:", "where || . || denotes the Frobenius norm and N = im_true.size. This result is equivalent to:", "The NRMSE metric.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_nrmse to skimage.metrics.normalized_root_mse.", "https://en.wikipedia.org/wiki/Root-mean-square_deviation"]}, {"name": "metrics.peak_signal_noise_ratio()", "path": "api/skimage.metrics#skimage.metrics.peak_signal_noise_ratio", "type": "metrics", "text": ["Compute the peak signal to noise ratio (PSNR) for an image.", "Ground-truth image, same shape as im_test.", "Test image.", "The data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.", "The PSNR metric.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_psnr to skimage.metrics.peak_signal_noise_ratio.", "https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio"]}, {"name": "metrics.structural_similarity()", "path": "api/skimage.metrics#skimage.metrics.structural_similarity", "type": "metrics", "text": ["Compute the mean structural similarity index between two images.", "Images. Any dimensionality with same shape.", "The side-length of the sliding window used in comparison. Must be an odd value. If gaussian_weights is True, this is ignored and the window size will depend on sigma.", "If True, also return the gradient with respect to im2.", "The data range of the input image (distance between minimum and maximum possible values). By default, this is estimated from the image data-type.", "If True, treat the last dimension of the array as channels. Similarity calculations are done independently for each channel then averaged.", "If True, each patch has its mean and variance spatially weighted by a normalized Gaussian kernel of width sigma=1.5.", "If True, also return the full structural similarity image.", "The mean structural similarity index over the image.", "The gradient of the structural similarity between im1 and im2 [2]. This is only returned if gradient is set to True.", "The full SSIM image. This is only returned if full is set to True.", "If True, normalize covariances by N-1 rather than, N where N is the number of pixels within the sliding window.", "Algorithm parameter, K1 (small constant, see [1]).", "Algorithm parameter, K2 (small constant, see [1]).", "Standard deviation for the Gaussian when gaussian_weights is True.", "To match the implementation of Wang et. al. [1], set gaussian_weights to True, sigma to 1.5, and use_sample_covariance to False.", "Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity.", "Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, 13, 600-612. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf, DOI:10.1109/TIP.2003.819861", "Avanaki, A. N. (2009). Exact global histogram specification optimized for structural similarity. Optical Review, 16, 613-621. arXiv:0901.0065 DOI:10.1007/s10043-009-0119-z"]}, {"name": "metrics.variation_of_information()", "path": "api/skimage.metrics#skimage.metrics.variation_of_information", "type": "metrics", "text": ["Return symmetric conditional entropies associated with the VI. [1]", "The variation of information is defined as VI(X,Y) = H(X|Y) + H(Y|X). If X is the ground-truth segmentation, then H(X|Y) can be interpreted as the amount of under-segmentation and H(X|Y) as the amount of over-segmentation. In other words, a perfect over-segmentation will have H(X|Y)=0 and a perfect under-segmentation will have H(Y|X)=0.", "Label images / segmentations, must have same shape.", "A contingency table built with skimage.evaluate.contingency_table. If None, it will be computed with skimage.evaluate.contingency_table. If given, the entropies will be computed from this table and any images will be ignored.", "Labels to ignore. Any part of the true image labeled with any of these values will not be counted in the score.", "The conditional entropies of image1|image0 and image0|image1.", "Marina Meil\u0103 (2007), Comparing clusterings\u2014an information based distance, Journal of Multivariate Analysis, Volume 98, Issue 5, Pages 873-895, ISSN 0047-259X, DOI:10.1016/j.jmva.2006.11.013."]}, {"name": "morphology", "path": "api/skimage.morphology", "type": "morphology", "text": ["skimage.morphology.area_closing(image[, \u2026])", "Perform an area closing of the image.", "skimage.morphology.area_opening(image[, \u2026])", "Perform an area opening of the image.", "skimage.morphology.ball(radius[, dtype])", "Generates a ball-shaped structuring element.", "skimage.morphology.binary_closing(image[, \u2026])", "Return fast binary morphological closing of an image.", "skimage.morphology.binary_dilation(image[, \u2026])", "Return fast binary morphological dilation of an image.", "skimage.morphology.binary_erosion(image[, \u2026])", "Return fast binary morphological erosion of an image.", "skimage.morphology.binary_opening(image[, \u2026])", "Return fast binary morphological opening of an image.", "skimage.morphology.black_tophat(image[, \u2026])", "Return black top hat of an image.", "skimage.morphology.closing(image[, selem, out])", "Return greyscale morphological closing of an image.", "skimage.morphology.convex_hull_image(image)", "Compute the convex hull image of a binary image.", "skimage.morphology.convex_hull_object(image, *)", "Compute the convex hull image of individual objects in a binary image.", "skimage.morphology.cube(width[, dtype])", "Generates a cube-shaped structuring element.", "skimage.morphology.diameter_closing(image[, \u2026])", "Perform a diameter closing of the image.", "skimage.morphology.diameter_opening(image[, \u2026])", "Perform a diameter opening of the image.", "skimage.morphology.diamond(radius[, dtype])", "Generates a flat, diamond-shaped structuring element.", "skimage.morphology.dilation(image[, selem, \u2026])", "Return greyscale morphological dilation of an image.", "skimage.morphology.disk(radius[, dtype])", "Generates a flat, disk-shaped structuring element.", "skimage.morphology.erosion(image[, selem, \u2026])", "Return greyscale morphological erosion of an image.", "skimage.morphology.flood(image, seed_point, *)", "Mask corresponding to a flood fill.", "skimage.morphology.flood_fill(image, \u2026[, \u2026])", "Perform flood filling on an image.", "skimage.morphology.h_maxima(image, h[, selem])", "Determine all maxima of the image with height >= h.", "skimage.morphology.h_minima(image, h[, selem])", "Determine all minima of the image with depth >= h.", "skimage.morphology.label(input[, \u2026])", "Label connected regions of an integer array.", "skimage.morphology.local_maxima(image[, \u2026])", "Find local maxima of n-dimensional array.", "skimage.morphology.local_minima(image[, \u2026])", "Find local minima of n-dimensional array.", "skimage.morphology.max_tree(image[, \u2026])", "Build the max tree from an image.", "skimage.morphology.max_tree_local_maxima(image)", "Determine all local maxima of the image.", "skimage.morphology.medial_axis(image[, \u2026])", "Compute the medial axis transform of a binary image", "skimage.morphology.octagon(m, n[, dtype])", "Generates an octagon shaped structuring element.", "skimage.morphology.octahedron(radius[, dtype])", "Generates a octahedron-shaped structuring element.", "skimage.morphology.opening(image[, selem, out])", "Return greyscale morphological opening of an image.", "skimage.morphology.reconstruction(seed, mask)", "Perform a morphological reconstruction of an image.", "skimage.morphology.rectangle(nrows, ncols[, \u2026])", "Generates a flat, rectangular-shaped structuring element.", "skimage.morphology.remove_small_holes(ar[, \u2026])", "Remove contiguous holes smaller than the specified size.", "skimage.morphology.remove_small_objects(ar)", "Remove objects smaller than the specified size.", "skimage.morphology.skeletonize(image, *[, \u2026])", "Compute the skeleton of a binary image.", "skimage.morphology.skeletonize_3d(image)", "Compute the skeleton of a binary image.", "skimage.morphology.square(width[, dtype])", "Generates a flat, square-shaped structuring element.", "skimage.morphology.star(a[, dtype])", "Generates a star shaped structuring element.", "skimage.morphology.thin(image[, max_iter])", "Perform morphological thinning of a binary image.", "skimage.morphology.watershed(image[, \u2026])", "Deprecated function.", "skimage.morphology.white_tophat(image[, \u2026])", "Return white top hat of an image.", "Perform an area closing of the image.", "Area closing removes all dark structures of an image with a surface smaller than area_threshold. The output image is larger than or equal to the input image for every pixel and all local minima have at least a surface of area_threshold pixels.", "Area closings are similar to morphological closings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold.", "In the binary case, area closings are equivalent to remove_small_holes; this operator is thus extended to gray-level images.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the area_closing is to be calculated. This image can be of any type.", "The size parameter (number of pixels). The default value is arbitrarily chosen to be 64.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Parent image representing the max tree of the inverted image. The value of each pixel is the index of its parent in the ravelled array. See Note for further details.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Output image of the same shape and type as input image.", "See also", "If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S)", "Vincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a minimum in the center and 4 additional local minima.", "We can calculate the area closing:", "All small minima are removed, and the remaining minima have at least a size of 8.", "Perform an area opening of the image.", "Area opening removes all bright structures of an image with a surface smaller than area_threshold. The output image is thus the largest image smaller than the input for which all local maxima have at least a surface of area_threshold pixels.", "Area openings are similar to morphological openings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. Consequently, the area_opening with area_threshold=1 is the identity.", "In the binary case, area openings are equivalent to remove_small_objects; this operator is thus extended to gray-level images.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the area_opening is to be calculated. This image can be of any type.", "The size parameter (number of pixels). The default value is arbitrarily chosen to be 64.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Parent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Output image of the same shape and type as the input image.", "See also", "Vincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. :DOI:10.1007/978-3-662-05088-0", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. :DOI:10.1109/83.663500", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. :DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. :DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a maximum in the center and 4 additional local maxima.", "We can calculate the area opening:", "The peaks with a surface smaller than 8 are removed.", "Generates a ball-shaped structuring element.", "This is the 3D equivalent of a disk. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.", "The radius of the ball-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element.", "Local Histogram Equalization", "Rank filters", "Return fast binary morphological closing of an image.", "This function returns the same result as greyscale closing but performs faster for binary images.", "The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None, is passed, a new array will be allocated.", "The result of the morphological closing.", "Flood Fill", "Return fast binary morphological dilation of an image.", "This function returns the same result as greyscale dilation but performs faster for binary images.", "Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological dilation with values in [False, True].", "Return fast binary morphological erosion of an image.", "This function returns the same result as greyscale erosion but performs faster for binary images.", "Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological erosion taking values in [False, True].", "Return fast binary morphological opening of an image.", "This function returns the same result as greyscale opening but performs faster for binary images.", "The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological opening.", "Flood Fill", "Return black top hat of an image.", "The black top hat of an image is defined as its morphological closing minus the original image. This operation returns the dark spots of the image that are smaller than the structuring element. Note that dark spots in the original image are bright spots after the black top hat.", "Image array.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological black top hat.", "See also", "https://en.wikipedia.org/wiki/Top-hat_transform", "Return greyscale morphological closing of an image.", "The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None, is passed, a new array will be allocated.", "The result of the morphological closing.", "Compute the convex hull image of a binary image.", "The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.", "Binary input image. This array is cast to bool before processing.", "If True, a pixel at coordinate, e.g., (4, 7) will be represented by coordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some \u201cextent\u201d to a pixel when computing the hull.", "Tolerance when determining whether a point is inside the hull. Due to numerical floating point errors, a tolerance of 0 can result in some points erroneously being classified as being outside the hull.", "Binary image with pixels in convex hull set to True.", "https://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-algorithm-notes/", "Compute the convex hull image of individual objects in a binary image.", "The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.", "Binary input image.", "Determines the neighbors of each pixel. Adjacent elements within a squared distance of connectivity from pixel center are considered neighbors.:", "Binary image with pixels inside convex hull set to True.", "This function uses skimage.morphology.label to define unique objects, finds the convex hull of each using convex_hull_image, and combines these regions with logical OR. Be aware the convex hulls of unconnected objects may overlap in the result. If this is suspected, consider using convex_hull_image separately on each object or adjust connectivity.", "Generates a cube-shaped structuring element.", "This is the 3D equivalent of a square. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.", "The width, height and depth of the cube.", "A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.", "The data type of the structuring element.", "Perform a diameter closing of the image.", "Diameter closing removes all dark structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Closing. In practice, the result is similar to a morphological closing, but long and thin structures are not removed.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the diameter_closing is to be calculated. This image can be of any type.", "The maximal extension parameter (number of pixels). The default value is 8.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Precomputed parent image representing the max tree of the inverted image. This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.", "Precomputed traverser, where the pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent). This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.", "Output image of the same shape and type as input image.", "See also", "If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S)", "Walter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a minimum in the center and 4 additional local minima.", "We can calculate the diameter closing:", "All small minima with a maximal extension of 2 or less are removed. The remaining minima have all a maximal extension of at least 3.", "Perform a diameter opening of the image.", "Diameter opening removes all bright structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Opening. In practice, the result is similar to a morphological opening, but long and thin structures are not removed.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the area_opening is to be calculated. This image can be of any type.", "The maximal extension parameter (number of pixels). The default value is 8.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Parent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Output image of the same shape and type as the input image.", "See also", "Walter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a maximum in the center and 4 additional local maxima.", "We can calculate the diameter opening:", "The peaks with a maximal extension of 2 or less are removed. The remaining peaks have all a maximal extension of at least 3.", "Generates a flat, diamond-shaped structuring element.", "A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.", "The radius of the diamond-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element.", "Return greyscale morphological dilation of an image.", "Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.", "Image array.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None, is passed, a new array will be allocated.", "shift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).", "The result of the morphological dilation.", "For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.maximum function more efficient for larger images and structuring elements.", "Rank filters", "Generates a flat, disk-shaped structuring element.", "A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.", "The radius of the disk-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element.", "Local Histogram Equalization", "Entropy", "Markers for watershed transform", "Flood Fill", "Segment human cells (in mitosis)", "Rank filters", "Return greyscale morphological erosion of an image.", "Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "shift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).", "The result of the morphological erosion.", "For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.minimum function more efficient for larger images and structuring elements.", "Mask corresponding to a flood fill.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).", "A Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill.", "If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:", "Perform flood filling on an image.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "New value to set the entire fill. This must be chosen in agreement with the dtype of image.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).", "If True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).", "This parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).", "An array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:", "Determine all maxima of the image with height >= h.", "The local maxima are defined as connected sets of pixels with equal grey level strictly greater than the grey level of all pixels in direct neighborhood of the set.", "A local maximum M of height h is a local maximum for which there is at least one path joining M with an equal or higher local maximum on which the minimal value is f(M) - h (i.e. the values along the path are not decreasing by more than h with respect to the maximum\u2019s value) and no path to an equal or higher local maximum for which the minimal value is greater.", "The global maxima of the image are also found by this function.", "The input image for which the maxima are to be calculated.", "The minimal height of all extracted maxima.", "The neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)", "The local maxima of height >= h and the global maxima. The resulting image is a binary image, where pixels belonging to the determined maxima take value 1, the others take value 0.", "See also", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.", "We create an image (quadratic function with a maximum in the center and 4 additional constant maxima. The heights of the maxima are: 1, 21, 41, 61, 81", "We can calculate all maxima with a height of at least 40:", "The resulting image will contain 3 local maxima.", "Determine all minima of the image with depth >= h.", "The local minima are defined as connected sets of pixels with equal grey level strictly smaller than the grey levels of all pixels in direct neighborhood of the set.", "A local minimum M of depth h is a local minimum for which there is at least one path joining M with an equal or lower local minimum on which the maximal value is f(M) + h (i.e. the values along the path are not increasing by more than h with respect to the minimum\u2019s value) and no path to an equal or lower local minimum for which the maximal value is smaller.", "The global minima of the image are also found by this function.", "The input image for which the minima are to be calculated.", "The minimal depth of all extracted minima.", "The neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)", "The local minima of depth >= h and the global minima. The resulting image is a binary image, where pixels belonging to the determined minima take value 1, the others take value 0.", "See also", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.", "We create an image (quadratic function with a minimum in the center and 4 additional constant maxima. The depth of the minima are: 1, 21, 41, 61, 81", "We can calculate all minima with a depth of at least 40:", "The resulting image will contain 3 local minima.", "Label connected regions of an integer array.", "Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor:", "Image to label.", "Consider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.", "Whether to return the number of assigned labels.", "Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.", "Labeled array, where all connected regions are assigned the same integer value.", "Number of labels, which equals the maximum label index and is only returned if return_num is True.", "See also", "Christophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.", "Kensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864", "Find local maxima of n-dimensional array.", "The local maxima are defined as connected sets of pixels with equal gray level (plateaus) strictly greater than the gray levels of all pixels in the neighborhood.", "An n-dimensional array.", "A structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If True, the output will be a tuple of one-dimensional arrays representing the indices of local maxima in each dimension. If False, the output will be a boolean array with the same shape as image.", "If true, plateaus that touch the image border are valid maxima.", "If indices is false, a boolean array with the same shape as image is returned with True indicating the position of local maxima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found maxima.", "If allow_borders is false and any dimension of the given image is shorter than 3 samples, maxima can\u2019t exist and a warning is shown.", "See also", "This function operates on the following ideas:", "For each candidate:", "Find local maxima by comparing to all neighboring pixels (maximal connectivity):", "Find local maxima without comparing to diagonal pixels (connectivity 1):", "and exclude maxima that border the image edge:", "Find local minima of n-dimensional array.", "The local minima are defined as connected sets of pixels with equal gray level (plateaus) strictly smaller than the gray levels of all pixels in the neighborhood.", "An n-dimensional array.", "A structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If True, the output will be a tuple of one-dimensional arrays representing the indices of local minima in each dimension. If False, the output will be a boolean array with the same shape as image.", "If true, plateaus that touch the image border are valid minima.", "If indices is false, a boolean array with the same shape as image is returned with True indicating the position of local minima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found minima.", "See also", "This function operates on the following ideas:", "For each candidate:", "Find local minima by comparing to all neighboring pixels (maximal connectivity):", "Find local minima without comparing to diagonal pixels (connectivity 1):", "and exclude minima that border the image edge:", "Build the max tree from an image.", "Component trees represent the hierarchical structure of the connected components resulting from sequential thresholding operations applied to an image. A connected component at one level is parent of a component at a higher level if the latter is included in the first. A max-tree is an efficient representation of a component tree. A connected component at one level is represented by one reference pixel at this level, which is parent to all other pixels at that level and to the reference pixel at the level above. The max-tree is the basis for many morphological operators, namely connected operators.", "The input image for which the max-tree is to be calculated. This image can be of any type.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Array of same shape as image. The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500", "Berger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E. (2007). Effective Component Tree Computation with Application to Pattern Recognition in Astronomical Imaging. In International Conference on Image Processing (ICIP) (pp. 41-44). DOI:10.1109/ICIP.2007.4379949", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create a small sample image (Figure 1 from [4]) and build the max-tree.", "Determine all local maxima of the image.", "The local maxima are defined as connected sets of pixels with equal gray level strictly greater than the gray levels of all pixels in direct neighborhood of the set. The function labels the local maxima.", "Technically, the implementation is based on the max-tree representation of an image. The function is very efficient if the max-tree representation has already been computed. Otherwise, it is preferable to use the function local_maxima.", "The input image for which the maxima are to be calculated.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Labeled local maxima of the image.", "See also", "Vincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a maximum in the center and 4 additional constant maxima.", "We can calculate all local maxima:", "The resulting image contains the labeled local maxima.", "Compute the medial axis transform of a binary image", "The image of the shape to be skeletonized.", "If a mask is given, only those elements in image with a true value in mask are used for computing the medial axis.", "If true, the distance transform is returned as well as the skeleton.", "Medial axis transform of the image", "Distance transform of the image (only returned if return_distance is True)", "See also", "This algorithm computes the medial axis transform of an image as the ridges of its distance transform.", "Generates an octagon shaped structuring element.", "For a given size of (m) horizontal and vertical sides and a given (n) height or width of slanted sides octagon is generated. The slanted sides are 45 or 135 degrees to the horizontal axis and hence the widths and heights are equal.", "The size of the horizontal and vertical sides.", "The height or width of the slanted sides.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element.", "Generates a octahedron-shaped structuring element.", "This is the 3D equivalent of a diamond. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.", "The radius of the octahedron-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element.", "Return greyscale morphological opening of an image.", "The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological opening.", "Perform a morphological reconstruction of an image.", "Morphological reconstruction by dilation is similar to basic morphological dilation: high-intensity values will replace nearby low-intensity values. The basic dilation operator, however, uses a structuring element to determine how far a value in the input image can spread. In contrast, reconstruction uses two images: a \u201cseed\u201d image, which specifies the values that spread, and a \u201cmask\u201d image, which gives the maximum allowed value at each pixel. The mask image, like the structuring element, limits the spread of high-intensity values. Reconstruction by erosion is simply the inverse: low-intensity values spread from the seed image and are limited by the mask image, which represents the minimum allowed value.", "Alternatively, you can think of reconstruction as a way to isolate the connected regions of an image. For dilation, reconstruction connects regions marked by local maxima in the seed image: neighboring pixels less-than-or-equal-to those seeds are connected to the seeded region. Local maxima with values larger than the seed image will get truncated to the seed value.", "The seed image (a.k.a. marker image), which specifies the values that are dilated or eroded.", "The maximum (dilation) / minimum (erosion) allowed value at each pixel.", "Perform reconstruction by dilation or erosion. In dilation (or erosion), the seed image is dilated (or eroded) until limited by the mask image. For dilation, each seed value must be less than or equal to the corresponding mask value; for erosion, the reverse is true. Default is \u2018dilation\u2019.", "The neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the n-D square of radius equal to 1 (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)", "The coordinates of the center of the structuring element. Default is located on the geometrical center of the selem, in that case selem dimensions must be odd.", "The result of morphological reconstruction.", "The algorithm is taken from [1]. Applications for greyscale reconstruction are discussed in [2] and [3].", "Robinson, \u201cEfficient morphological reconstruction: a downhill filter\u201d, Pattern Recognition Letters 25 (2004) 1759-1767.", "Vincent, L., \u201cMorphological Grayscale Reconstruction in Image Analysis: Applications and Efficient Algorithms\u201d, IEEE Transactions on Image Processing (1993)", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d, Chapter 6, 2nd edition (2003), ISBN 3540429883.", "First, we create a sinusoidal mask image with peaks at middle and ends.", "Then, we create a seed image initialized to the minimum mask value (for reconstruction by dilation, min-intensity values don\u2019t spread) and add \u201cseeds\u201d to the left and right peak, but at a fraction of peak value (1).", "The reconstructed image (or curve, in this case) is exactly the same as the mask image, except that the peaks are truncated to 0.5 and 0. The middle peak disappears completely: Since there were no seed values in this peak region, its reconstructed value is truncated to the surrounding value (-1).", "As a more practical example, we try to extract the bright features of an image by subtracting a background image created by reconstruction.", "To create the background image, set the mask image to the original image, and the seed image to the original image with an intensity offset, h.", "The resulting reconstructed image looks exactly like the original image, but with the peaks of the bumps cut off. Subtracting this reconstructed image from the original image leaves just the peaks of the bumps", "This operation is known as the h-dome of the image and leaves features of height h in the subtracted image.", "Generates a flat, rectangular-shaped structuring element.", "Every pixel in the rectangle generated for a given width and given height belongs to the neighborhood.", "The number of rows of the rectangle.", "The number of columns of the rectangle.", "A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.", "The data type of the structuring element.", "Remove contiguous holes smaller than the specified size.", "The array containing the connected components of interest.", "The maximum area, in pixels, of a contiguous hole that will be filled. Replaces min_size.", "The connectivity defining the neighborhood of a pixel.", "If True, remove the connected components in the input array itself. Otherwise, make a copy.", "The input array with small holes within connected components removed.", "If the input array is of an invalid type, such as float or string.", "If the input array contains negative values.", "If the array type is int, it is assumed that it contains already-labeled objects. The labels are not kept in the output image (this function always outputs a bool image). It is suggested that labeling is completed after using this function.", "Measure region properties", "Remove objects smaller than the specified size.", "Expects ar to be an array with labeled objects, and removes objects smaller than min_size. If ar is bool, the image is first labeled. This leads to potentially different behavior for bool and 0-and-1 arrays.", "The array containing the objects of interest. If the array type is int, the ints must be non-negative.", "The smallest allowable object size.", "The connectivity defining the neighborhood of a pixel. Used during labelling if ar is bool.", "If True, remove the objects in the input array itself. Otherwise, make a copy.", "The input array with small connected components removed.", "If the input array is of an invalid type, such as float or string.", "If the input array contains negative values.", "Measure region properties", "Compute the skeleton of a binary image.", "Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.", "A binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.", "Which algorithm to use. Zhang\u2019s algorithm [Zha84] only works for 2D images, and is the default for 2D. Lee\u2019s algorithm [Lee94] works for 2D or 3D images and is the default for 3D.", "The thinned image.", "See also", "T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.", "A fast parallel algorithm for thinning digital patterns, T. Y. Zhang and C. Y. Suen, Communications of the ACM, March 1984, Volume 27, Number 3.", "Compute the skeleton of a binary image.", "Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.", "A binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.", "The thinned image.", "See also", "The method of [Lee94] uses an octree data structure to examine a 3x3x3 neighborhood of a pixel. The algorithm proceeds by iteratively sweeping over the image, and removing pixels at each iteration until the image stops changing. Each iteration consists of two steps: first, a list of candidates for removal is assembled; then pixels from this list are rechecked sequentially, to better preserve connectivity of the image.", "The algorithm this function implements is different from the algorithms used by either skeletonize or medial_axis, thus for 2D images the results produced by this function are generally different.", "T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.", "Generates a flat, square-shaped structuring element.", "Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.", "The width and height of the square.", "A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.", "The data type of the structuring element.", "Generates a star shaped structuring element.", "Start has 8 vertices and is an overlap of square of size 2*a + 1 with its 45 degree rotated version. The slanted sides are 45 or 135 degrees to the horizontal axis.", "Parameter deciding the size of the star structural element. The side of the square array returned is 2*a + 1 + 2*floor(a / 2).", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element.", "Perform morphological thinning of a binary image.", "The image to be thinned.", "Regardless of the value of this parameter, the thinned image is returned immediately if an iteration produces no change. If this parameter is specified it thus sets an upper bound on the number of iterations performed.", "Thinned image.", "See also", "This algorithm [1] works by making multiple passes over the image, removing pixels matching a set of criteria designed to thin connected regions while preserving eight-connected components and 2 x 2 squares [2]. In each of the two sub-iterations the algorithm correlates the intermediate skeleton image with a neighborhood mask, then looks up each neighborhood in a lookup table indicating whether the central pixel should be deleted in that sub-iteration.", "Z. Guo and R. W. Hall, \u201cParallel thinning with two-subiteration algorithms,\u201d Comm. ACM, vol. 32, no. 3, pp. 359-373, 1989. DOI:10.1145/62065.62074", "Lam, L., Seong-Whan Lee, and Ching Y. Suen, \u201cThinning Methodologies-A Comprehensive Survey,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 14, No. 9, p. 879, 1992. DOI:10.1109/34.161346", "Deprecated function. Use skimage.segmentation.watershed instead.", "Find watershed basins in image flooded from given markers.", "Data array where the lowest value points are labeled first.", "The desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.", "An array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.", "offset of the connectivity (one offset per dimension)", "Array of same shape as image. Only points at which mask == True will be labeled.", "Use compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.", "If watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.", "A labeled matrix of the same type and shape as markers", "See also", "random walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.", "This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker. Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides. This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm. Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example).", "https://en.wikipedia.org/wiki/Watershed_%28image_processing%29", "http://cmm.ensmp.fr/~beucher/wtshed.html", "Peer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf", "The watershed algorithm is useful to separate overlapping objects.", "We first generate an initial image with two overlapping circles:", "Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background:", "Finally, we run the watershed on the image and markers:", "The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres.", "Return white top hat of an image.", "The white top hat of an image is defined as the image minus its morphological opening. This operation returns the bright spots of the image that are smaller than the structuring element.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological white top hat.", "See also", "https://en.wikipedia.org/wiki/Top-hat_transform"]}, {"name": "morphology.area_closing()", "path": "api/skimage.morphology#skimage.morphology.area_closing", "type": "morphology", "text": ["Perform an area closing of the image.", "Area closing removes all dark structures of an image with a surface smaller than area_threshold. The output image is larger than or equal to the input image for every pixel and all local minima have at least a surface of area_threshold pixels.", "Area closings are similar to morphological closings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold.", "In the binary case, area closings are equivalent to remove_small_holes; this operator is thus extended to gray-level images.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the area_closing is to be calculated. This image can be of any type.", "The size parameter (number of pixels). The default value is arbitrarily chosen to be 64.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Parent image representing the max tree of the inverted image. The value of each pixel is the index of its parent in the ravelled array. See Note for further details.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Output image of the same shape and type as input image.", "See also", "If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S)", "Vincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a minimum in the center and 4 additional local minima.", "We can calculate the area closing:", "All small minima are removed, and the remaining minima have at least a size of 8."]}, {"name": "morphology.area_opening()", "path": "api/skimage.morphology#skimage.morphology.area_opening", "type": "morphology", "text": ["Perform an area opening of the image.", "Area opening removes all bright structures of an image with a surface smaller than area_threshold. The output image is thus the largest image smaller than the input for which all local maxima have at least a surface of area_threshold pixels.", "Area openings are similar to morphological openings, but they do not use a fixed structuring element, but rather a deformable one, with surface = area_threshold. Consequently, the area_opening with area_threshold=1 is the identity.", "In the binary case, area openings are equivalent to remove_small_objects; this operator is thus extended to gray-level images.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the area_opening is to be calculated. This image can be of any type.", "The size parameter (number of pixels). The default value is arbitrarily chosen to be 64.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Parent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Output image of the same shape and type as the input image.", "See also", "Vincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. :DOI:10.1007/978-3-662-05088-0", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. :DOI:10.1109/83.663500", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. :DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. :DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a maximum in the center and 4 additional local maxima.", "We can calculate the area opening:", "The peaks with a surface smaller than 8 are removed."]}, {"name": "morphology.ball()", "path": "api/skimage.morphology#skimage.morphology.ball", "type": "morphology", "text": ["Generates a ball-shaped structuring element.", "This is the 3D equivalent of a disk. A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.", "The radius of the ball-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element."]}, {"name": "morphology.binary_closing()", "path": "api/skimage.morphology#skimage.morphology.binary_closing", "type": "morphology", "text": ["Return fast binary morphological closing of an image.", "This function returns the same result as greyscale closing but performs faster for binary images.", "The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None, is passed, a new array will be allocated.", "The result of the morphological closing."]}, {"name": "morphology.binary_dilation()", "path": "api/skimage.morphology#skimage.morphology.binary_dilation", "type": "morphology", "text": ["Return fast binary morphological dilation of an image.", "This function returns the same result as greyscale dilation but performs faster for binary images.", "Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological dilation with values in [False, True]."]}, {"name": "morphology.binary_erosion()", "path": "api/skimage.morphology#skimage.morphology.binary_erosion", "type": "morphology", "text": ["Return fast binary morphological erosion of an image.", "This function returns the same result as greyscale erosion but performs faster for binary images.", "Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological erosion taking values in [False, True]."]}, {"name": "morphology.binary_opening()", "path": "api/skimage.morphology#skimage.morphology.binary_opening", "type": "morphology", "text": ["Return fast binary morphological opening of an image.", "This function returns the same result as greyscale opening but performs faster for binary images.", "The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.", "Binary input image.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use a cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological opening."]}, {"name": "morphology.black_tophat()", "path": "api/skimage.morphology#skimage.morphology.black_tophat", "type": "morphology", "text": ["Return black top hat of an image.", "The black top hat of an image is defined as its morphological closing minus the original image. This operation returns the dark spots of the image that are smaller than the structuring element. Note that dark spots in the original image are bright spots after the black top hat.", "Image array.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological black top hat.", "See also", "https://en.wikipedia.org/wiki/Top-hat_transform"]}, {"name": "morphology.closing()", "path": "api/skimage.morphology#skimage.morphology.closing", "type": "morphology", "text": ["Return greyscale morphological closing of an image.", "The morphological closing on an image is defined as a dilation followed by an erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None, is passed, a new array will be allocated.", "The result of the morphological closing."]}, {"name": "morphology.convex_hull_image()", "path": "api/skimage.morphology#skimage.morphology.convex_hull_image", "type": "morphology", "text": ["Compute the convex hull image of a binary image.", "The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.", "Binary input image. This array is cast to bool before processing.", "If True, a pixel at coordinate, e.g., (4, 7) will be represented by coordinates (3.5, 7), (4.5, 7), (4, 6.5), and (4, 7.5). This adds some \u201cextent\u201d to a pixel when computing the hull.", "Tolerance when determining whether a point is inside the hull. Due to numerical floating point errors, a tolerance of 0 can result in some points erroneously being classified as being outside the hull.", "Binary image with pixels in convex hull set to True.", "https://blogs.mathworks.com/steve/2011/10/04/binary-image-convex-hull-algorithm-notes/"]}, {"name": "morphology.convex_hull_object()", "path": "api/skimage.morphology#skimage.morphology.convex_hull_object", "type": "morphology", "text": ["Compute the convex hull image of individual objects in a binary image.", "The convex hull is the set of pixels included in the smallest convex polygon that surround all white pixels in the input image.", "Binary input image.", "Determines the neighbors of each pixel. Adjacent elements within a squared distance of connectivity from pixel center are considered neighbors.:", "Binary image with pixels inside convex hull set to True.", "This function uses skimage.morphology.label to define unique objects, finds the convex hull of each using convex_hull_image, and combines these regions with logical OR. Be aware the convex hulls of unconnected objects may overlap in the result. If this is suspected, consider using convex_hull_image separately on each object or adjust connectivity."]}, {"name": "morphology.cube()", "path": "api/skimage.morphology#skimage.morphology.cube", "type": "morphology", "text": ["Generates a cube-shaped structuring element.", "This is the 3D equivalent of a square. Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.", "The width, height and depth of the cube.", "A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.", "The data type of the structuring element."]}, {"name": "morphology.diameter_closing()", "path": "api/skimage.morphology#skimage.morphology.diameter_closing", "type": "morphology", "text": ["Perform a diameter closing of the image.", "Diameter closing removes all dark structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Closing. In practice, the result is similar to a morphological closing, but long and thin structures are not removed.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the diameter_closing is to be calculated. This image can be of any type.", "The maximal extension parameter (number of pixels). The default value is 8.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Precomputed parent image representing the max tree of the inverted image. This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.", "Precomputed traverser, where the pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent). This function is fast, if precomputed parent and tree_traverser are provided. See Note for further details.", "Output image of the same shape and type as input image.", "See also", "If a max-tree representation (parent and tree_traverser) are given to the function, they must be calculated from the inverted image for this function, i.e.: >>> P, S = max_tree(invert(f)) >>> closed = diameter_closing(f, 3, parent=P, tree_traverser=S)", "Walter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a minimum in the center and 4 additional local minima.", "We can calculate the diameter closing:", "All small minima with a maximal extension of 2 or less are removed. The remaining minima have all a maximal extension of at least 3."]}, {"name": "morphology.diameter_opening()", "path": "api/skimage.morphology#skimage.morphology.diameter_opening", "type": "morphology", "text": ["Perform a diameter opening of the image.", "Diameter opening removes all bright structures of an image with maximal extension smaller than diameter_threshold. The maximal extension is defined as the maximal extension of the bounding box. The operator is also called Bounding Box Opening. In practice, the result is similar to a morphological opening, but long and thin structures are not removed.", "Technically, this operator is based on the max-tree representation of the image.", "The input image for which the area_opening is to be calculated. This image can be of any type.", "The maximal extension parameter (number of pixels). The default value is 8.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Parent image representing the max tree of the image. The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Output image of the same shape and type as the input image.", "See also", "Walter, T., & Klein, J.-C. (2002). Automatic Detection of Microaneurysms in Color Fundus Images of the Human Retina by Means of the Bounding Box Closing. In A. Colosimo, P. Sirabella, A. Giuliani (Eds.), Medical Data Analysis. Lecture Notes in Computer Science, vol 2526, pp. 210-220. Springer Berlin Heidelberg. DOI:10.1007/3-540-36104-9_23", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a maximum in the center and 4 additional local maxima.", "We can calculate the diameter opening:", "The peaks with a maximal extension of 2 or less are removed. The remaining peaks have all a maximal extension of at least 3."]}, {"name": "morphology.diamond()", "path": "api/skimage.morphology#skimage.morphology.diamond", "type": "morphology", "text": ["Generates a flat, diamond-shaped structuring element.", "A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.", "The radius of the diamond-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element."]}, {"name": "morphology.dilation()", "path": "api/skimage.morphology#skimage.morphology.dilation", "type": "morphology", "text": ["Return greyscale morphological dilation of an image.", "Morphological dilation sets a pixel at (i,j) to the maximum over all pixels in the neighborhood centered at (i,j). Dilation enlarges bright regions and shrinks dark regions.", "Image array.", "The neighborhood expressed as a 2-D array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None, is passed, a new array will be allocated.", "shift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).", "The result of the morphological dilation.", "For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.maximum function more efficient for larger images and structuring elements."]}, {"name": "morphology.disk()", "path": "api/skimage.morphology#skimage.morphology.disk", "type": "morphology", "text": ["Generates a flat, disk-shaped structuring element.", "A pixel is within the neighborhood if the Euclidean distance between it and the origin is no greater than radius.", "The radius of the disk-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element."]}, {"name": "morphology.erosion()", "path": "api/skimage.morphology#skimage.morphology.erosion", "type": "morphology", "text": ["Return greyscale morphological erosion of an image.", "Morphological erosion sets a pixel at (i,j) to the minimum over all pixels in the neighborhood centered at (i,j). Erosion shrinks bright regions and enlarges dark regions.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "shift structuring element about center point. This only affects eccentric structuring elements (i.e. selem with even numbered sides).", "The result of the morphological erosion.", "For uint8 (and uint16 up to a certain bit-depth) data, the lower algorithm complexity makes the skimage.filters.rank.minimum function more efficient for larger images and structuring elements."]}, {"name": "morphology.flood()", "path": "api/skimage.morphology#skimage.morphology.flood", "type": "morphology", "text": ["Mask corresponding to a flood fill.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).", "A Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill.", "If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:"]}, {"name": "morphology.flood_fill()", "path": "api/skimage.morphology#skimage.morphology.flood_fill", "type": "morphology", "text": ["Perform flood filling on an image.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "New value to set the entire fill. This must be chosen in agreement with the dtype of image.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).", "If True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).", "This parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).", "An array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:"]}, {"name": "morphology.h_maxima()", "path": "api/skimage.morphology#skimage.morphology.h_maxima", "type": "morphology", "text": ["Determine all maxima of the image with height >= h.", "The local maxima are defined as connected sets of pixels with equal grey level strictly greater than the grey level of all pixels in direct neighborhood of the set.", "A local maximum M of height h is a local maximum for which there is at least one path joining M with an equal or higher local maximum on which the minimal value is f(M) - h (i.e. the values along the path are not decreasing by more than h with respect to the maximum\u2019s value) and no path to an equal or higher local maximum for which the minimal value is greater.", "The global maxima of the image are also found by this function.", "The input image for which the maxima are to be calculated.", "The minimal height of all extracted maxima.", "The neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)", "The local maxima of height >= h and the global maxima. The resulting image is a binary image, where pixels belonging to the determined maxima take value 1, the others take value 0.", "See also", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.", "We create an image (quadratic function with a maximum in the center and 4 additional constant maxima. The heights of the maxima are: 1, 21, 41, 61, 81", "We can calculate all maxima with a height of at least 40:", "The resulting image will contain 3 local maxima."]}, {"name": "morphology.h_minima()", "path": "api/skimage.morphology#skimage.morphology.h_minima", "type": "morphology", "text": ["Determine all minima of the image with depth >= h.", "The local minima are defined as connected sets of pixels with equal grey level strictly smaller than the grey levels of all pixels in direct neighborhood of the set.", "A local minimum M of depth h is a local minimum for which there is at least one path joining M with an equal or lower local minimum on which the maximal value is f(M) + h (i.e. the values along the path are not increasing by more than h with respect to the minimum\u2019s value) and no path to an equal or lower local minimum for which the maximal value is smaller.", "The global minima of the image are also found by this function.", "The input image for which the minima are to be calculated.", "The minimal depth of all extracted minima.", "The neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the ball of radius 1 according to the maximum norm (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)", "The local minima of depth >= h and the global minima. The resulting image is a binary image, where pixels belonging to the determined minima take value 1, the others take value 0.", "See also", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883.", "We create an image (quadratic function with a minimum in the center and 4 additional constant maxima. The depth of the minima are: 1, 21, 41, 61, 81", "We can calculate all minima with a depth of at least 40:", "The resulting image will contain 3 local minima."]}, {"name": "morphology.label()", "path": "api/skimage.morphology#skimage.morphology.label", "type": "morphology", "text": ["Label connected regions of an integer array.", "Two pixels are connected when they are neighbors and have the same value. In 2D, they can be neighbors either in a 1- or 2-connected sense. The value refers to the maximum number of orthogonal hops to consider a pixel/voxel a neighbor:", "Image to label.", "Consider all pixels with this value as background pixels, and label them as 0. By default, 0-valued pixels are considered as background pixels.", "Whether to return the number of assigned labels.", "Maximum number of orthogonal hops to consider a pixel/voxel as a neighbor. Accepted values are ranging from 1 to input.ndim. If None, a full connectivity of input.ndim is used.", "Labeled array, where all connected regions are assigned the same integer value.", "Number of labels, which equals the maximum label index and is only returned if return_num is True.", "See also", "Christophe Fiorio and Jens Gustedt, \u201cTwo linear time Union-Find strategies for image processing\u201d, Theoretical Computer Science 154 (1996), pp. 165-181.", "Kensheng Wu, Ekow Otoo and Arie Shoshani, \u201cOptimizing connected component labeling algorithms\u201d, Paper LBNL-56864, 2005, Lawrence Berkeley National Laboratory (University of California), http://repositories.cdlib.org/lbnl/LBNL-56864"]}, {"name": "morphology.local_maxima()", "path": "api/skimage.morphology#skimage.morphology.local_maxima", "type": "morphology", "text": ["Find local maxima of n-dimensional array.", "The local maxima are defined as connected sets of pixels with equal gray level (plateaus) strictly greater than the gray levels of all pixels in the neighborhood.", "An n-dimensional array.", "A structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If True, the output will be a tuple of one-dimensional arrays representing the indices of local maxima in each dimension. If False, the output will be a boolean array with the same shape as image.", "If true, plateaus that touch the image border are valid maxima.", "If indices is false, a boolean array with the same shape as image is returned with True indicating the position of local maxima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found maxima.", "If allow_borders is false and any dimension of the given image is shorter than 3 samples, maxima can\u2019t exist and a warning is shown.", "See also", "This function operates on the following ideas:", "For each candidate:", "Find local maxima by comparing to all neighboring pixels (maximal connectivity):", "Find local maxima without comparing to diagonal pixels (connectivity 1):", "and exclude maxima that border the image edge:"]}, {"name": "morphology.local_minima()", "path": "api/skimage.morphology#skimage.morphology.local_minima", "type": "morphology", "text": ["Find local minima of n-dimensional array.", "The local minima are defined as connected sets of pixels with equal gray level (plateaus) strictly smaller than the gray levels of all pixels in the neighborhood.", "An n-dimensional array.", "A structuring element used to determine the neighborhood of each evaluated pixel (True denotes a connected pixel). It must be a boolean array and have the same number of dimensions as image. If neither selem nor connectivity are given, all adjacent pixels are considered as part of the neighborhood.", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If True, the output will be a tuple of one-dimensional arrays representing the indices of local minima in each dimension. If False, the output will be a boolean array with the same shape as image.", "If true, plateaus that touch the image border are valid minima.", "If indices is false, a boolean array with the same shape as image is returned with True indicating the position of local minima (False otherwise). If indices is true, a tuple of one-dimensional arrays containing the coordinates (indices) of all found minima.", "See also", "This function operates on the following ideas:", "For each candidate:", "Find local minima by comparing to all neighboring pixels (maximal connectivity):", "Find local minima without comparing to diagonal pixels (connectivity 1):", "and exclude minima that border the image edge:"]}, {"name": "morphology.max_tree()", "path": "api/skimage.morphology#skimage.morphology.max_tree", "type": "morphology", "text": ["Build the max tree from an image.", "Component trees represent the hierarchical structure of the connected components resulting from sequential thresholding operations applied to an image. A connected component at one level is parent of a component at a higher level if the latter is included in the first. A max-tree is an efficient representation of a component tree. A connected component at one level is represented by one reference pixel at this level, which is parent to all other pixels at that level and to the reference pixel at the level above. The max-tree is the basis for many morphological operators, namely connected operators.", "The input image for which the max-tree is to be calculated. This image can be of any type.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "Array of same shape as image. The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500", "Berger, C., Geraud, T., Levillain, R., Widynski, N., Baillard, A., Bertin, E. (2007). Effective Component Tree Computation with Application to Pattern Recognition in Astronomical Imaging. In International Conference on Image Processing (ICIP) (pp. 41-44). DOI:10.1109/ICIP.2007.4379949", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create a small sample image (Figure 1 from [4]) and build the max-tree."]}, {"name": "morphology.max_tree_local_maxima()", "path": "api/skimage.morphology#skimage.morphology.max_tree_local_maxima", "type": "morphology", "text": ["Determine all local maxima of the image.", "The local maxima are defined as connected sets of pixels with equal gray level strictly greater than the gray levels of all pixels in direct neighborhood of the set. The function labels the local maxima.", "Technically, the implementation is based on the max-tree representation of an image. The function is very efficient if the max-tree representation has already been computed. Otherwise, it is preferable to use the function local_maxima.", "The input image for which the maxima are to be calculated.", "The neighborhood connectivity. The integer represents the maximum number of orthogonal steps to reach a neighbor. In 2D, it is 1 for a 4-neighborhood and 2 for a 8-neighborhood. Default value is 1.", "The value of each pixel is the index of its parent in the ravelled array.", "The ordered pixel indices (referring to the ravelled array). The pixels are ordered such that every pixel is preceded by its parent (except for the root which has no parent).", "Labeled local maxima of the image.", "See also", "Vincent L., Proc. \u201cGrayscale area openings and closings, their efficient implementation and applications\u201d, EURASIP Workshop on Mathematical Morphology and its Applications to Signal Processing, Barcelona, Spain, pp.22-27, May 1993.", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d (Chapter 6), 2nd edition (2003), ISBN 3540429883. DOI:10.1007/978-3-662-05088-0", "Salembier, P., Oliveras, A., & Garrido, L. (1998). Antiextensive Connected Operators for Image and Sequence Processing. IEEE Transactions on Image Processing, 7(4), 555-570. DOI:10.1109/83.663500", "Najman, L., & Couprie, M. (2006). Building the component tree in quasi-linear time. IEEE Transactions on Image Processing, 15(11), 3531-3539. DOI:10.1109/TIP.2006.877518", "Carlinet, E., & Geraud, T. (2014). A Comparative Review of Component Tree Computation Algorithms. IEEE Transactions on Image Processing, 23(9), 3885-3895. DOI:10.1109/TIP.2014.2336551", "We create an image (quadratic function with a maximum in the center and 4 additional constant maxima.", "We can calculate all local maxima:", "The resulting image contains the labeled local maxima."]}, {"name": "morphology.medial_axis()", "path": "api/skimage.morphology#skimage.morphology.medial_axis", "type": "morphology", "text": ["Compute the medial axis transform of a binary image", "The image of the shape to be skeletonized.", "If a mask is given, only those elements in image with a true value in mask are used for computing the medial axis.", "If true, the distance transform is returned as well as the skeleton.", "Medial axis transform of the image", "Distance transform of the image (only returned if return_distance is True)", "See also", "This algorithm computes the medial axis transform of an image as the ridges of its distance transform."]}, {"name": "morphology.octagon()", "path": "api/skimage.morphology#skimage.morphology.octagon", "type": "morphology", "text": ["Generates an octagon shaped structuring element.", "For a given size of (m) horizontal and vertical sides and a given (n) height or width of slanted sides octagon is generated. The slanted sides are 45 or 135 degrees to the horizontal axis and hence the widths and heights are equal.", "The size of the horizontal and vertical sides.", "The height or width of the slanted sides.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element."]}, {"name": "morphology.octahedron()", "path": "api/skimage.morphology#skimage.morphology.octahedron", "type": "morphology", "text": ["Generates a octahedron-shaped structuring element.", "This is the 3D equivalent of a diamond. A pixel is part of the neighborhood (i.e. labeled 1) if the city block/Manhattan distance between it and the center of the neighborhood is no greater than radius.", "The radius of the octahedron-shaped structuring element.", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element."]}, {"name": "morphology.opening()", "path": "api/skimage.morphology#skimage.morphology.opening", "type": "morphology", "text": ["Return greyscale morphological opening of an image.", "The morphological opening on an image is defined as an erosion followed by a dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological opening."]}, {"name": "morphology.reconstruction()", "path": "api/skimage.morphology#skimage.morphology.reconstruction", "type": "morphology", "text": ["Perform a morphological reconstruction of an image.", "Morphological reconstruction by dilation is similar to basic morphological dilation: high-intensity values will replace nearby low-intensity values. The basic dilation operator, however, uses a structuring element to determine how far a value in the input image can spread. In contrast, reconstruction uses two images: a \u201cseed\u201d image, which specifies the values that spread, and a \u201cmask\u201d image, which gives the maximum allowed value at each pixel. The mask image, like the structuring element, limits the spread of high-intensity values. Reconstruction by erosion is simply the inverse: low-intensity values spread from the seed image and are limited by the mask image, which represents the minimum allowed value.", "Alternatively, you can think of reconstruction as a way to isolate the connected regions of an image. For dilation, reconstruction connects regions marked by local maxima in the seed image: neighboring pixels less-than-or-equal-to those seeds are connected to the seeded region. Local maxima with values larger than the seed image will get truncated to the seed value.", "The seed image (a.k.a. marker image), which specifies the values that are dilated or eroded.", "The maximum (dilation) / minimum (erosion) allowed value at each pixel.", "Perform reconstruction by dilation or erosion. In dilation (or erosion), the seed image is dilated (or eroded) until limited by the mask image. For dilation, each seed value must be less than or equal to the corresponding mask value; for erosion, the reverse is true. Default is \u2018dilation\u2019.", "The neighborhood expressed as an n-D array of 1\u2019s and 0\u2019s. Default is the n-D square of radius equal to 1 (i.e. a 3x3 square for 2D images, a 3x3x3 cube for 3D images, etc.)", "The coordinates of the center of the structuring element. Default is located on the geometrical center of the selem, in that case selem dimensions must be odd.", "The result of morphological reconstruction.", "The algorithm is taken from [1]. Applications for greyscale reconstruction are discussed in [2] and [3].", "Robinson, \u201cEfficient morphological reconstruction: a downhill filter\u201d, Pattern Recognition Letters 25 (2004) 1759-1767.", "Vincent, L., \u201cMorphological Grayscale Reconstruction in Image Analysis: Applications and Efficient Algorithms\u201d, IEEE Transactions on Image Processing (1993)", "Soille, P., \u201cMorphological Image Analysis: Principles and Applications\u201d, Chapter 6, 2nd edition (2003), ISBN 3540429883.", "First, we create a sinusoidal mask image with peaks at middle and ends.", "Then, we create a seed image initialized to the minimum mask value (for reconstruction by dilation, min-intensity values don\u2019t spread) and add \u201cseeds\u201d to the left and right peak, but at a fraction of peak value (1).", "The reconstructed image (or curve, in this case) is exactly the same as the mask image, except that the peaks are truncated to 0.5 and 0. The middle peak disappears completely: Since there were no seed values in this peak region, its reconstructed value is truncated to the surrounding value (-1).", "As a more practical example, we try to extract the bright features of an image by subtracting a background image created by reconstruction.", "To create the background image, set the mask image to the original image, and the seed image to the original image with an intensity offset, h.", "The resulting reconstructed image looks exactly like the original image, but with the peaks of the bumps cut off. Subtracting this reconstructed image from the original image leaves just the peaks of the bumps", "This operation is known as the h-dome of the image and leaves features of height h in the subtracted image."]}, {"name": "morphology.rectangle()", "path": "api/skimage.morphology#skimage.morphology.rectangle", "type": "morphology", "text": ["Generates a flat, rectangular-shaped structuring element.", "Every pixel in the rectangle generated for a given width and given height belongs to the neighborhood.", "The number of rows of the rectangle.", "The number of columns of the rectangle.", "A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.", "The data type of the structuring element."]}, {"name": "morphology.remove_small_holes()", "path": "api/skimage.morphology#skimage.morphology.remove_small_holes", "type": "morphology", "text": ["Remove contiguous holes smaller than the specified size.", "The array containing the connected components of interest.", "The maximum area, in pixels, of a contiguous hole that will be filled. Replaces min_size.", "The connectivity defining the neighborhood of a pixel.", "If True, remove the connected components in the input array itself. Otherwise, make a copy.", "The input array with small holes within connected components removed.", "If the input array is of an invalid type, such as float or string.", "If the input array contains negative values.", "If the array type is int, it is assumed that it contains already-labeled objects. The labels are not kept in the output image (this function always outputs a bool image). It is suggested that labeling is completed after using this function."]}, {"name": "morphology.remove_small_objects()", "path": "api/skimage.morphology#skimage.morphology.remove_small_objects", "type": "morphology", "text": ["Remove objects smaller than the specified size.", "Expects ar to be an array with labeled objects, and removes objects smaller than min_size. If ar is bool, the image is first labeled. This leads to potentially different behavior for bool and 0-and-1 arrays.", "The array containing the objects of interest. If the array type is int, the ints must be non-negative.", "The smallest allowable object size.", "The connectivity defining the neighborhood of a pixel. Used during labelling if ar is bool.", "If True, remove the objects in the input array itself. Otherwise, make a copy.", "The input array with small connected components removed.", "If the input array is of an invalid type, such as float or string.", "If the input array contains negative values."]}, {"name": "morphology.skeletonize()", "path": "api/skimage.morphology#skimage.morphology.skeletonize", "type": "morphology", "text": ["Compute the skeleton of a binary image.", "Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.", "A binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.", "Which algorithm to use. Zhang\u2019s algorithm [Zha84] only works for 2D images, and is the default for 2D. Lee\u2019s algorithm [Lee94] works for 2D or 3D images and is the default for 3D.", "The thinned image.", "See also", "T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994.", "A fast parallel algorithm for thinning digital patterns, T. Y. Zhang and C. Y. Suen, Communications of the ACM, March 1984, Volume 27, Number 3."]}, {"name": "morphology.skeletonize_3d()", "path": "api/skimage.morphology#skimage.morphology.skeletonize_3d", "type": "morphology", "text": ["Compute the skeleton of a binary image.", "Thinning is used to reduce each connected component in a binary image to a single-pixel wide skeleton.", "A binary image containing the objects to be skeletonized. Zeros represent background, nonzero values are foreground.", "The thinned image.", "See also", "The method of [Lee94] uses an octree data structure to examine a 3x3x3 neighborhood of a pixel. The algorithm proceeds by iteratively sweeping over the image, and removing pixels at each iteration until the image stops changing. Each iteration consists of two steps: first, a list of candidates for removal is assembled; then pixels from this list are rechecked sequentially, to better preserve connectivity of the image.", "The algorithm this function implements is different from the algorithms used by either skeletonize or medial_axis, thus for 2D images the results produced by this function are generally different.", "T.-C. Lee, R.L. Kashyap and C.-N. Chu, Building skeleton models via 3-D medial surface/axis thinning algorithms. Computer Vision, Graphics, and Image Processing, 56(6):462-478, 1994."]}, {"name": "morphology.square()", "path": "api/skimage.morphology#skimage.morphology.square", "type": "morphology", "text": ["Generates a flat, square-shaped structuring element.", "Every pixel along the perimeter has a chessboard distance no greater than radius (radius=floor(width/2)) pixels.", "The width and height of the square.", "A structuring element consisting only of ones, i.e. every pixel belongs to the neighborhood.", "The data type of the structuring element."]}, {"name": "morphology.star()", "path": "api/skimage.morphology#skimage.morphology.star", "type": "morphology", "text": ["Generates a star shaped structuring element.", "Start has 8 vertices and is an overlap of square of size 2*a + 1 with its 45 degree rotated version. The slanted sides are 45 or 135 degrees to the horizontal axis.", "Parameter deciding the size of the star structural element. The side of the square array returned is 2*a + 1 + 2*floor(a / 2).", "The structuring element where elements of the neighborhood are 1 and 0 otherwise.", "The data type of the structuring element."]}, {"name": "morphology.thin()", "path": "api/skimage.morphology#skimage.morphology.thin", "type": "morphology", "text": ["Perform morphological thinning of a binary image.", "The image to be thinned.", "Regardless of the value of this parameter, the thinned image is returned immediately if an iteration produces no change. If this parameter is specified it thus sets an upper bound on the number of iterations performed.", "Thinned image.", "See also", "This algorithm [1] works by making multiple passes over the image, removing pixels matching a set of criteria designed to thin connected regions while preserving eight-connected components and 2 x 2 squares [2]. In each of the two sub-iterations the algorithm correlates the intermediate skeleton image with a neighborhood mask, then looks up each neighborhood in a lookup table indicating whether the central pixel should be deleted in that sub-iteration.", "Z. Guo and R. W. Hall, \u201cParallel thinning with two-subiteration algorithms,\u201d Comm. ACM, vol. 32, no. 3, pp. 359-373, 1989. DOI:10.1145/62065.62074", "Lam, L., Seong-Whan Lee, and Ching Y. Suen, \u201cThinning Methodologies-A Comprehensive Survey,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 14, No. 9, p. 879, 1992. DOI:10.1109/34.161346"]}, {"name": "morphology.watershed()", "path": "api/skimage.morphology#skimage.morphology.watershed", "type": "morphology", "text": ["Deprecated function. Use skimage.segmentation.watershed instead.", "Find watershed basins in image flooded from given markers.", "Data array where the lowest value points are labeled first.", "The desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.", "An array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.", "offset of the connectivity (one offset per dimension)", "Array of same shape as image. Only points at which mask == True will be labeled.", "Use compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.", "If watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.", "A labeled matrix of the same type and shape as markers", "See also", "random walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.", "This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker. Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182 The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides. This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm. Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example).", "https://en.wikipedia.org/wiki/Watershed_%28image_processing%29", "http://cmm.ensmp.fr/~beucher/wtshed.html", "Peer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf", "The watershed algorithm is useful to separate overlapping objects.", "We first generate an initial image with two overlapping circles:", "Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background:", "Finally, we run the watershed on the image and markers:", "The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres."]}, {"name": "morphology.white_tophat()", "path": "api/skimage.morphology#skimage.morphology.white_tophat", "type": "morphology", "text": ["Return white top hat of an image.", "The white top hat of an image is defined as the image minus its morphological opening. This operation returns the bright spots of the image that are smaller than the structuring element.", "Image array.", "The neighborhood expressed as an array of 1\u2019s and 0\u2019s. If None, use cross-shaped structuring element (connectivity=1).", "The array to store the result of the morphology. If None is passed, a new array will be allocated.", "The result of the morphological white top hat.", "See also", "https://en.wikipedia.org/wiki/Top-hat_transform"]}, {"name": "registration", "path": "api/skimage.registration", "type": "registration", "text": ["skimage.registration.optical_flow_ilk(\u2026[, \u2026])", "Coarse to fine optical flow estimator.", "skimage.registration.optical_flow_tvl1(\u2026)", "Coarse to fine optical flow estimator.", "skimage.registration.phase_cross_correlation(\u2026)", "Efficient subpixel image translation registration by cross-correlation.", "Coarse to fine optical flow estimator.", "The iterative Lucas-Kanade (iLK) solver is applied at each level of the image pyramid. iLK [1] is a fast and robust alternative to TVL1 algorithm although less accurate for rendering flat surfaces and object boundaries (see [2]).", "The first gray scale image of the sequence.", "The second gray scale image of the sequence.", "Radius of the window considered around each pixel.", "Number of times moving_image is warped.", "If True, a Gaussian kernel is used for the local integration. Otherwise, a uniform kernel is used.", "Whether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.", "Output data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.", "The estimated optical flow components for each axis.", "Le Besnerais, G., & Champagnat, F. (2005, September). Dense optical flow by iterative local window registration. In IEEE International Conference on Image Processing 2005 (Vol. 1, pp. I-137). IEEE. DOI:10.1109/ICIP.2005.1529706", "Plyer, A., Le Besnerais, G., & Champagnat, F. (2016). Massively parallel Lucas Kanade optical flow for real-time video processing applications. Journal of Real-Time Image Processing, 11(4), 713-730. DOI:10.1007/s11554-014-0423-0", "Registration using optical flow", "Coarse to fine optical flow estimator.", "The TV-L1 solver is applied at each level of the image pyramid. TV-L1 is a popular algorithm for optical flow estimation introduced by Zack et al. [1], improved in [2] and detailed in [3].", "The first gray scale image of the sequence.", "The second gray scale image of the sequence.", "Attachment parameter (\\(\\lambda\\) in [1]). The smaller this parameter is, the smoother the returned result will be.", "Tightness parameter (\\(\\tau\\) in [1]). It should have a small value in order to maintain attachement and regularization parts in correspondence.", "Number of times image1 is warped.", "Number of fixed point iteration.", "Tolerance used as stopping criterion based on the L\u00b2 distance between two consecutive values of (u, v).", "Whether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.", "Output data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.", "The estimated optical flow components for each axis.", "Color images are not supported.", "Zach, C., Pock, T., & Bischof, H. (2007, September). A duality based approach for realtime TV-L 1 optical flow. In Joint pattern recognition symposium (pp. 214-223). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-540-74936-3_22", "Wedel, A., Pock, T., Zach, C., Bischof, H., & Cremers, D. (2009). An improved algorithm for TV-L 1 optical flow. In Statistical and geometrical approaches to visual motion analysis (pp. 23-45). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-642-03061-1_2", "P\u00e9rez, J. S., Meinhardt-Llopis, E., & Facciolo, G. (2013). TV-L1 optical flow estimation. Image Processing On Line, 2013, 137-150. DOI:10.5201/ipol.2013.26", "Registration using optical flow", "Efficient subpixel image translation registration by cross-correlation.", "This code gives the same precision as the FFT upsampled cross-correlation in a fraction of the computation time and with reduced memory requirements. It obtains an initial estimate of the cross-correlation peak by an FFT and then refines the shift estimation by upsampling the DFT only in a small neighborhood of that estimate by means of a matrix-multiply DFT.", "Reference image.", "Image to register. Must be same dimensionality as reference_image.", "Upsampling factor. Images will be registered to within 1 / upsample_factor of a pixel. For example upsample_factor == 20 means the images will be registered within 1/20th of a pixel. Default is 1 (no upsampling). Not used if any of reference_mask or moving_mask is not None.", "Defines how the algorithm interprets input data. \u201creal\u201d means data will be FFT\u2019d to compute the correlation, while \u201cfourier\u201d data will bypass FFT of input data. Case insensitive. Not used if any of reference_mask or moving_mask is not None.", "Returns error and phase difference if on, otherwise only shifts are returned. Has noeffect if any of reference_mask or moving_mask is not None. In this case only shifts is returned.", "Boolean mask for reference_image. The mask should evaluate to True (or 1) on valid pixels. reference_mask should have the same shape as reference_image.", "Boolean mask for moving_image. The mask should evaluate to True (or 1) on valid pixels. moving_mask should have the same shape as moving_image. If None, reference_mask will be used.", "Minimum allowed overlap ratio between images. The correlation for translations corresponding with an overlap ratio lower than this threshold will be ignored. A lower overlap_ratio leads to smaller maximum translation, while a higher overlap_ratio leads to greater robustness against spurious matches due to small overlap between masked images. Used only if one of reference_mask or moving_mask is None.", "Shift vector (in pixels) required to register moving_image with reference_image. Axis ordering is consistent with numpy (e.g. Z, Y, X)", "Translation invariant normalized RMS error between reference_image and moving_image.", "Global phase difference between the two images (should be zero if images are non-negative).", "Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, \u201cEfficient subpixel image registration algorithms,\u201d Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156", "James R. Fienup, \u201cInvariant error metrics for image reconstruction\u201d Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352", "Dirk Padfield. Masked Object Registration in the Fourier Domain. IEEE Transactions on Image Processing, vol. 21(5), pp. 2706-2718 (2012). DOI:10.1109/TIP.2011.2181402", "D. Padfield. \u201cMasked FFT registration\u201d. In Proc. Computer Vision and Pattern Recognition, pp. 2918-2925 (2010). DOI:10.1109/CVPR.2010.5540032", "Masked Normalized Cross-Correlation"]}, {"name": "registration.optical_flow_ilk()", "path": "api/skimage.registration#skimage.registration.optical_flow_ilk", "type": "registration", "text": ["Coarse to fine optical flow estimator.", "The iterative Lucas-Kanade (iLK) solver is applied at each level of the image pyramid. iLK [1] is a fast and robust alternative to TVL1 algorithm although less accurate for rendering flat surfaces and object boundaries (see [2]).", "The first gray scale image of the sequence.", "The second gray scale image of the sequence.", "Radius of the window considered around each pixel.", "Number of times moving_image is warped.", "If True, a Gaussian kernel is used for the local integration. Otherwise, a uniform kernel is used.", "Whether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.", "Output data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.", "The estimated optical flow components for each axis.", "Le Besnerais, G., & Champagnat, F. (2005, September). Dense optical flow by iterative local window registration. In IEEE International Conference on Image Processing 2005 (Vol. 1, pp. I-137). IEEE. DOI:10.1109/ICIP.2005.1529706", "Plyer, A., Le Besnerais, G., & Champagnat, F. (2016). Massively parallel Lucas Kanade optical flow for real-time video processing applications. Journal of Real-Time Image Processing, 11(4), 713-730. DOI:10.1007/s11554-014-0423-0"]}, {"name": "registration.optical_flow_tvl1()", "path": "api/skimage.registration#skimage.registration.optical_flow_tvl1", "type": "registration", "text": ["Coarse to fine optical flow estimator.", "The TV-L1 solver is applied at each level of the image pyramid. TV-L1 is a popular algorithm for optical flow estimation introduced by Zack et al. [1], improved in [2] and detailed in [3].", "The first gray scale image of the sequence.", "The second gray scale image of the sequence.", "Attachment parameter (\\(\\lambda\\) in [1]). The smaller this parameter is, the smoother the returned result will be.", "Tightness parameter (\\(\\tau\\) in [1]). It should have a small value in order to maintain attachement and regularization parts in correspondence.", "Number of times image1 is warped.", "Number of fixed point iteration.", "Tolerance used as stopping criterion based on the L\u00b2 distance between two consecutive values of (u, v).", "Whether to prefilter the estimated optical flow before each image warp. When True, a median filter with window size 3 along each axis is applied. This helps to remove potential outliers.", "Output data type: must be floating point. Single precision provides good results and saves memory usage and computation time compared to double precision.", "The estimated optical flow components for each axis.", "Color images are not supported.", "Zach, C., Pock, T., & Bischof, H. (2007, September). A duality based approach for realtime TV-L 1 optical flow. In Joint pattern recognition symposium (pp. 214-223). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-540-74936-3_22", "Wedel, A., Pock, T., Zach, C., Bischof, H., & Cremers, D. (2009). An improved algorithm for TV-L 1 optical flow. In Statistical and geometrical approaches to visual motion analysis (pp. 23-45). Springer, Berlin, Heidelberg. DOI:10.1007/978-3-642-03061-1_2", "P\u00e9rez, J. S., Meinhardt-Llopis, E., & Facciolo, G. (2013). TV-L1 optical flow estimation. Image Processing On Line, 2013, 137-150. DOI:10.5201/ipol.2013.26"]}, {"name": "registration.phase_cross_correlation()", "path": "api/skimage.registration#skimage.registration.phase_cross_correlation", "type": "registration", "text": ["Efficient subpixel image translation registration by cross-correlation.", "This code gives the same precision as the FFT upsampled cross-correlation in a fraction of the computation time and with reduced memory requirements. It obtains an initial estimate of the cross-correlation peak by an FFT and then refines the shift estimation by upsampling the DFT only in a small neighborhood of that estimate by means of a matrix-multiply DFT.", "Reference image.", "Image to register. Must be same dimensionality as reference_image.", "Upsampling factor. Images will be registered to within 1 / upsample_factor of a pixel. For example upsample_factor == 20 means the images will be registered within 1/20th of a pixel. Default is 1 (no upsampling). Not used if any of reference_mask or moving_mask is not None.", "Defines how the algorithm interprets input data. \u201creal\u201d means data will be FFT\u2019d to compute the correlation, while \u201cfourier\u201d data will bypass FFT of input data. Case insensitive. Not used if any of reference_mask or moving_mask is not None.", "Returns error and phase difference if on, otherwise only shifts are returned. Has noeffect if any of reference_mask or moving_mask is not None. In this case only shifts is returned.", "Boolean mask for reference_image. The mask should evaluate to True (or 1) on valid pixels. reference_mask should have the same shape as reference_image.", "Boolean mask for moving_image. The mask should evaluate to True (or 1) on valid pixels. moving_mask should have the same shape as moving_image. If None, reference_mask will be used.", "Minimum allowed overlap ratio between images. The correlation for translations corresponding with an overlap ratio lower than this threshold will be ignored. A lower overlap_ratio leads to smaller maximum translation, while a higher overlap_ratio leads to greater robustness against spurious matches due to small overlap between masked images. Used only if one of reference_mask or moving_mask is None.", "Shift vector (in pixels) required to register moving_image with reference_image. Axis ordering is consistent with numpy (e.g. Z, Y, X)", "Translation invariant normalized RMS error between reference_image and moving_image.", "Global phase difference between the two images (should be zero if images are non-negative).", "Manuel Guizar-Sicairos, Samuel T. Thurman, and James R. Fienup, \u201cEfficient subpixel image registration algorithms,\u201d Optics Letters 33, 156-158 (2008). DOI:10.1364/OL.33.000156", "James R. Fienup, \u201cInvariant error metrics for image reconstruction\u201d Optics Letters 36, 8352-8357 (1997). DOI:10.1364/AO.36.008352", "Dirk Padfield. Masked Object Registration in the Fourier Domain. IEEE Transactions on Image Processing, vol. 21(5), pp. 2706-2718 (2012). DOI:10.1109/TIP.2011.2181402", "D. Padfield. \u201cMasked FFT registration\u201d. In Proc. Computer Vision and Pattern Recognition, pp. 2918-2925 (2010). DOI:10.1109/CVPR.2010.5540032"]}, {"name": "restoration", "path": "api/skimage.restoration", "type": "restoration", "text": ["Image restoration module.", "skimage.restoration.ball_kernel(radius, ndim)", "Create a ball kernel for restoration.rolling_ball.", "skimage.restoration.calibrate_denoiser(\u2026)", "Calibrate a denoising function and return optimal J-invariant version.", "skimage.restoration.cycle_spin(x, func, \u2026)", "Cycle spinning (repeatedly apply func to shifted versions of x).", "skimage.restoration.denoise_bilateral(image)", "Denoise image using bilateral filter.", "skimage.restoration.denoise_nl_means(image)", "Perform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.", "skimage.restoration.denoise_tv_bregman(\u2026)", "Perform total-variation denoising using split-Bregman optimization.", "skimage.restoration.denoise_tv_chambolle(image)", "Perform total-variation denoising on n-dimensional images.", "skimage.restoration.denoise_wavelet(image[, \u2026])", "Perform wavelet denoising on an image.", "skimage.restoration.ellipsoid_kernel(shape, \u2026)", "Create an ellipoid kernel for restoration.rolling_ball.", "skimage.restoration.estimate_sigma(image[, \u2026])", "Robust wavelet-based estimator of the (Gaussian) noise standard deviation.", "skimage.restoration.inpaint_biharmonic(\u2026)", "Inpaint masked points in image with biharmonic equations.", "skimage.restoration.richardson_lucy(image, psf)", "Richardson-Lucy deconvolution.", "skimage.restoration.rolling_ball(image, *[, \u2026])", "Estimate background intensity by rolling/translating a kernel.", "skimage.restoration.unsupervised_wiener(\u2026)", "Unsupervised Wiener-Hunt deconvolution.", "skimage.restoration.unwrap_phase(image[, \u2026])", "Recover the original from a wrapped phase image.", "skimage.restoration.wiener(image, psf, balance)", "Wiener-Hunt deconvolution", "Create a ball kernel for restoration.rolling_ball.", "Radius of the ball.", "Number of dimensions of the ball. ndim should match the dimensionality of the image the kernel will be applied to.", "The kernel containing the surface intensity of the top half of the ellipsoid.", "See also", "Calibrate a denoising function and return optimal J-invariant version.", "The returned function is partially evaluated with optimal parameter values set for denoising the input image.", "Input data to be denoised (converted using img_as_float).", "Denoising function to be calibrated.", "Ranges of parameters for denoise_function to be calibrated over.", "Stride used in masking procedure that converts denoise_function to J-invariance.", "Whether to approximate the self-supervised loss used to evaluate the denoiser by only computing it on one masked version of the image. If False, the runtime will be a factor of stride**image.ndim longer.", "If True, return parameters and losses in addition to the calibrated denoising function", "The optimal J-invariant version of denoise_function.", "List of parameters tested for denoise_function, as a dictionary of kwargs Self-supervised loss for each set of parameters in parameters_tested.", "The calibration procedure uses a self-supervised mean-square-error loss to evaluate the performance of J-invariant versions of denoise_function. The minimizer of the self-supervised loss is also the minimizer of the ground-truth loss (i.e., the true MSE error) [1]. The returned function can be used on the original noisy image, or other images with similar characteristics.", "at the expense of increasing its runtime. It has no effect on the runtime of the calibration.", "J. Batson & L. Royer. Noise2Self: Blind Denoising by Self-Supervision, International Conference on Machine Learning, p. 524-533 (2019).", "Cycle spinning (repeatedly apply func to shifted versions of x).", "Data for input to func.", "A function to apply to circularly shifted versions of x. Should take x as its first argument. Any additional arguments can be supplied via func_kw.", "If an integer, shifts in range(0, max_shifts+1) will be used along each axis of x. If a tuple, range(0, max_shifts[i]+1) will be along axis i.", "The step size for the shifts applied along axis, i, are:: range((0, max_shifts[i]+1, shift_steps[i])). If an integer is provided, the same step size is used for all axes.", "The number of parallel threads to use during cycle spinning. If set to None, the full set of available cores are used.", "Whether to treat the final axis as channels (no cycle shifts are performed over the channels axis).", "Additional keyword arguments to supply to func.", "The output of func(x, **func_kw) averaged over all combinations of the specified axis shifts.", "Cycle spinning was proposed as a way to approach shift-invariance via performing several circular shifts of a shift-variant transform [1].", "For a n-level discrete wavelet transforms, one may wish to perform all shifts up to max_shifts = 2**n - 1. In practice, much of the benefit can often be realized with only a small number of shifts per axis.", "For transforms such as the blockwise discrete cosine transform, one may wish to evaluate shifts up to the block size used by the transform.", "R.R. Coifman and D.L. Donoho. \u201cTranslation-Invariant De-Noising\u201d. Wavelets and Statistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995, pp.125-150. DOI:10.1007/978-1-4612-2544-7_9", "Denoise image using bilateral filter.", "Input image, 2D grayscale or RGB.", "Window size for filtering. If win_size is not specified, it is calculated as max(5, 2 * ceil(3 * sigma_spatial) + 1).", "Standard deviation for grayvalue/color distance (radiometric similarity). A larger value results in averaging of pixels with larger radiometric differences. Note, that the image will be converted using the img_as_float function and thus the standard deviation is in respect to the range [0, 1]. If the value is None the standard deviation of the image will be used.", "Standard deviation for range distance. A larger value results in averaging of pixels with larger spatial differences.", "Number of discrete values for Gaussian weights of color filtering. A larger value results in improved accuracy.", "How to handle values outside the image borders. See numpy.pad for detail.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Denoised image.", "This is an edge-preserving, denoising filter. It averages pixels based on their spatial closeness and radiometric similarity [1].", "Spatial closeness is measured by the Gaussian function of the Euclidean distance between two pixels and a certain standard deviation (sigma_spatial).", "Radiometric similarity is measured by the Gaussian function of the Euclidean distance between two color values and a certain standard deviation (sigma_color).", "C. Tomasi and R. Manduchi. \u201cBilateral Filtering for Gray and Color Images.\u201d IEEE International Conference on Computer Vision (1998) 839-846. DOI:10.1109/ICCV.1998.710815", "Rank filters", "Perform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.", "Input image to be denoised, which can be 2D or 3D, and grayscale or RGB (for 2D images only, see multichannel parameter).", "Size of patches used for denoising.", "Maximal distance in pixels where to search patches used for denoising.", "Cut-off distance (in gray levels). The higher h, the more permissive one is in accepting patches. A higher h results in a smoother image, at the expense of blurring features. For a Gaussian noise of standard deviation sigma, a rule of thumb is to choose the value of h to be sigma of slightly less.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "If True (default value), a fast version of the non-local means algorithm is used. If False, the original version of non-local means is used. See the Notes section for more details about the algorithms.", "The standard deviation of the (Gaussian) noise. If provided, a more robust computation of patch weights is computed that takes the expected noise variance into account (see Notes below).", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Denoised image, of same shape as image.", "The non-local means algorithm is well suited for denoising images with specific textures. The principle of the algorithm is to average the value of a given pixel with values of other pixels in a limited neighbourhood, provided that the patches centered on the other pixels are similar enough to the patch centered on the pixel of interest.", "In the original version of the algorithm [1], corresponding to fast=False, the computational complexity is:", "Hence, changing the size of patches or their maximal distance has a strong effect on computing times, especially for 3-D images.", "However, the default behavior corresponds to fast_mode=True, for which another version of non-local means [2] is used, corresponding to a complexity of:", "The computing time depends only weakly on the patch size, thanks to the computation of the integral of patches distances for a given shift, that reduces the number of operations [1]. Therefore, this algorithm executes faster than the classic algorithm (fast_mode=False), at the expense of using twice as much memory. This implementation has been proven to be more efficient compared to other alternatives, see e.g. [3].", "Compared to the classic algorithm, all pixels of a patch contribute to the distance to another patch with the same weight, no matter their distance to the center of the patch. This coarser computation of the distance can result in a slightly poorer denoising performance. Moreover, for small images (images with a linear size that is only a few times the patch size), the classic algorithm can be faster due to boundary effects.", "The image is padded using the reflect mode of skimage.util.pad before denoising.", "If the noise standard deviation, sigma, is provided a more robust computation of patch weights is used. Subtracting the known noise variance from the computed patch distances improves the estimates of patch similarity, giving a moderate improvement to denoising performance [4]. It was also mentioned as an option for the fast variant of the algorithm in [3].", "When sigma is provided, a smaller h should typically be used to avoid oversmoothing. The optimal value for h depends on the image content and noise level, but a reasonable starting point is h = 0.8 * sigma when fast_mode is True, or h = 0.6 * sigma when fast_mode is False.", "A. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising. In CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38", "J. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal filtering applied to electron cryomicroscopy, in 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334. DOI:10.1109/ISBI.2008.4541250", "Jacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising. Image Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120", "A. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing On Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm", "Perform total-variation denoising using split-Bregman optimization.", "Total-variation denoising (also know as total-variation regularization) tries to find an image with less total-variation under the constraint of being similar to the input image, which is controlled by the regularization parameter ([1], [2], [3], [4]).", "Input data to be denoised (converted using img_as_float`).", "Denoising weight. The smaller the weight, the more denoising (at the expense of less similarity to the input). The regularization parameter lambda is chosen as 2 * weight.", "Relative difference of the value of the cost function that determines the stop criterion. The algorithm stops when:", "Maximal number of iterations used for the optimization.", "Switch between isotropic and anisotropic TV denoising.", "Apply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.", "Denoised image.", "https://en.wikipedia.org/wiki/Total_variation_denoising", "Tom Goldstein and Stanley Osher, \u201cThe Split Bregman Method For L1 Regularized Problems\u201d, ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf", "Pascal Getreuer, \u201cRudin\u2013Osher\u2013Fatemi Total Variation Denoising using Split Bregman\u201d in Image Processing On Line on 2012\u201305\u201319, https://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf", "https://web.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf", "Perform total-variation denoising on n-dimensional images.", "Input data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.", "Denoising weight. The greater weight, the more denoising (at the expense of fidelity to input).", "Relative difference of the value of the cost function that determines the stop criterion. The algorithm stops when:", "(E_(n-1) - E_n) < eps * E_0", "Maximal number of iterations used for the optimization.", "Apply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.", "Denoised image.", "Make sure to set the multichannel parameter appropriately for color images.", "The principle of total variation denoising is explained in https://en.wikipedia.org/wiki/Total_variation_denoising", "The principle of total variation denoising is to minimize the total variation of the image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce \u201ccartoon-like\u201d images, that is, piecewise-constant images.", "This code is an implementation of the algorithm of Rudin, Fatemi and Osher that was proposed by Chambolle in [1].", "A. Chambolle, An algorithm for total variation minimization and applications, Journal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.", "2D example on astronaut image:", "3D example on synthetic data:", "Perform wavelet denoising on an image.", "Input data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.", "The noise standard deviation used when computing the wavelet detail coefficient threshold(s). When None (default), the noise standard deviation is estimated via the method in [2].", "The type of wavelet to perform and can be any of the options pywt.wavelist outputs. The default is \u2018db1\u2019. For example, wavelet can be any of {'db2', 'haar', 'sym9'} and many more.", "An optional argument to choose the type of denoising performed. It noted that choosing soft thresholding given additive noise finds the best approximation of the original image.", "The number of wavelet decomposition levels to use. The default is three less than the maximum number of possible decomposition levels.", "Apply wavelet denoising separately for each channel (where channels correspond to the final axis of the array).", "If True and multichannel True, do the wavelet denoising in the YCbCr colorspace instead of the RGB color space. This typically results in better performance for RGB images.", "Thresholding method to be used. The currently supported methods are \u201cBayesShrink\u201d [1] and \u201cVisuShrink\u201d [2]. Defaults to \u201cBayesShrink\u201d.", "If False, no rescaling of the user-provided sigma will be performed. The default of True rescales sigma appropriately if the image is rescaled internally.", "New in version 0.16: rescale_sigma was introduced in 0.16", "Denoised image.", "The wavelet domain is a sparse representation of the image, and can be thought of similarly to the frequency domain of the Fourier transform. Sparse representations have most values zero or near-zero and truly random noise is (usually) represented by many small values in the wavelet domain. Setting all values below some threshold to 0 reduces the noise in the image, but larger thresholds also decrease the detail present in the image.", "If the input is 3D, this function performs wavelet denoising on each color plane separately.", "Changed in version 0.16: For floating point inputs, the original input range is maintained and there is no clipping applied to the output. Other input types will be converted to a floating point value in the range [-1, 1] or [0, 1] depending on the input image range. Unless rescale_sigma = False, any internal rescaling applied to the image will also be applied to sigma to maintain the same relative amplitude.", "Many wavelet coefficient thresholding approaches have been proposed. By default, denoise_wavelet applies BayesShrink, which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in [1].", "If method == \"VisuShrink\", a single \u201cuniversal threshold\u201d is applied to all wavelet detail coefficients as described in [2]. This threshold is designed to remove all Gaussian noise at a given sigma with high probability, but tends to produce images that appear overly smooth.", "Although any of the wavelets from PyWavelets can be selected, the thresholding methods assume an orthogonal wavelet transform and may not choose the threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are desirable because white noise in the input remains white noise in the subbands. Biorthogonal wavelets lead to colored noise in the subbands. Additionally, the orthogonal wavelets in PyWavelets are orthonormal so that noise variance in the subbands remains identical to the noise variance of the input. Example orthogonal wavelets are the Daubechies (e.g. \u2018db2\u2019) or symmlet (e.g. \u2018sym2\u2019) families.", "Chang, S. Grace, Bin Yu, and Martin Vetterli. \u201cAdaptive wavelet thresholding for image denoising and compression.\u201d Image Processing, IEEE Transactions on 9.9 (2000): 1532-1546. DOI:10.1109/83.862633", "D. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425", "Create an ellipoid kernel for restoration.rolling_ball.", "Length of the principal axis of the ellipsoid (excluding the intensity axis). The kernel needs to have the same dimensionality as the image it will be applied to.", "Length of the intensity axis of the ellipsoid.", "The kernel containing the surface intensity of the top half of the ellipsoid.", "See also", "Use rolling-ball algorithm for estimating background intensity", "Robust wavelet-based estimator of the (Gaussian) noise standard deviation.", "Image for which to estimate the noise standard deviation.", "If true, average the channel estimates of sigma. Otherwise return a list of sigmas corresponding to each channel.", "Estimate sigma separately for each channel.", "Estimated noise standard deviation(s). If multichannel is True and average_sigmas is False, a separate noise estimate for each channel is returned. Otherwise, the average of the individual channel estimates is returned.", "This function assumes the noise follows a Gaussian distribution. The estimation algorithm is based on the median absolute deviation of the wavelet detail coefficients as described in section 4.2 of [1].", "D. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425", "Inpaint masked points in image with biharmonic equations.", "Input image.", "Array of pixels to be inpainted. Have to be the same shape as one of the \u2018image\u2019 channels. Unknown pixels have to be represented with 1, known pixels - with 0.", "If True, the last image dimension is considered as a color channel, otherwise as spatial.", "Input image with masked pixels inpainted.", "N.S.Hoang, S.B.Damelin, \u201cOn surface completion and image inpainting by biharmonic functions: numerical aspects\u201d, arXiv:1707.06567", "C. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth Functions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113, DOI:10.1016/j.acha.2009.04.004", "Richardson-Lucy deconvolution.", "Input degraded image (can be N dimensional).", "The point spread function.", "Number of iterations. This parameter plays the role of regularisation.", "True by default. If true, pixel value of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.", "Value below which intermediate results become 0 to avoid division by small numbers.", "The deconvolved image.", "https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution", "Estimate background intensity by rolling/translating a kernel.", "This rolling ball algorithm estimates background intensity for a ndimage in case of uneven exposure. It is a generalization of the frequently used rolling ball algorithm [1].", "The image to be filtered.", "Radius of a ball shaped kernel to be rolled/translated in the image. Used if kernel = None.", "The kernel to be rolled/translated in the image. It must have the same number of dimensions as image. Kernel is filled with the intensity of the kernel at that position.", "If False (default) assumes that none of the values in image are np.nan, and uses a faster implementation.", "The maximum number of threads to use. If None use the OpenMP default value; typically equal to the maximum number of virtual cores. Note: This is an upper limit to the number of threads. The exact number is determined by the system\u2019s OpenMP library.", "The estimated background of the image.", "For the pixel that has its background intensity estimated (without loss of generality at center) the rolling ball method centers kernel under it and raises the kernel until the surface touches the image umbra at some pos=(y,x). The background intensity is then estimated using the image intensity at that position (image[pos]) plus the difference of kernel[center] - kernel[pos].", "This algorithm assumes that dark pixels correspond to the background. If you have a bright background, invert the image before passing it to the function, e.g., using utils.invert. See the gallery example for details.", "This algorithm is sensitive to noise (in particular salt-and-pepper noise). If this is a problem in your image, you can apply mild gaussian smoothing before passing the image to this function.", "Sternberg, Stanley R. \u201cBiomedical image processing.\u201d Computer 1 (1983): 22-34. DOI:10.1109/MC.1983.1654163", "Use rolling-ball algorithm for estimating background intensity", "Unsupervised Wiener-Hunt deconvolution.", "Return the deconvolution with a Wiener-Hunt approach, where the hyperparameters are automatically estimated. The algorithm is a stochastic iterative process (Gibbs sampler) described in the reference below. See also wiener function.", "The input degraded image.", "The impulse response (input image\u2019s space) or the transfer function (Fourier space). Both are accepted. The transfer function is automatically recognized as being complex (np.iscomplexobj(psf)).", "The regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf.", "Dictionary of parameters for the Gibbs sampler. See below.", "True by default. If true, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.", "The deconvolved image (the posterior mean).", "The keys noise and prior contain the chain list of noise and prior precision respectively.", "The stopping criterion: the norm of the difference between to successive approximated solution (empirical mean of object samples, see Notes section). 1e-4 by default.", "The number of sample to ignore to start computation of the mean. 15 by default.", "The minimum number of iterations. 30 by default.", "The maximum number of iterations if threshold is not satisfied. 200 by default.", "A user provided callable to which is passed, if the function exists, the current image sample for whatever purpose. The user can store the sample, or compute other moments than the mean. It has no influence on the algorithm execution and is only for inspection.", "The estimated image is design as the posterior mean of a probability law (from a Bayesian analysis). The mean is defined as a sum over all the possible images weighted by their respective probability. Given the size of the problem, the exact sum is not tractable. This algorithm use of MCMC to draw image under the posterior law. The practical idea is to only draw highly probable images since they have the biggest contribution to the mean. At the opposite, the less probable images are drawn less often since their contribution is low. Finally the empirical mean of these samples give us an estimation of the mean, and an exact computation with an infinite sample set.", "Fran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)", "https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593", "http://research.orieux.fr/files/papers/OGR-JOSA10.pdf", "Recover the original from a wrapped phase image.", "From an image wrapped to lie in the interval [-pi, pi), recover the original, unwrapped image.", "The values should be in the range [-pi, pi). If a masked array is provided, the masked entries will not be changed, and their values will not be used to guide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not allowed, and will raise a ValueError.", "When an element of the sequence is True, the unwrapping process will regard the edges along the corresponding axis of the image to be connected and use this connectivity to guide the phase unwrapping process. If only a single boolean is given, it will apply to all axes. Wrap around is not supported for 1D arrays.", "Unwrapping 2D or 3D images uses random initialization. This sets the seed of the PRNG to achieve deterministic behavior.", "Unwrapped image of the same shape as the input. If the input image was a masked array, the mask will be preserved.", "If called with a masked 1D array or called with a 1D array and wrap_around=True.", "Miguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A. Gdeisat, \u201cFast two-dimensional phase-unwrapping algorithm based on sorting by reliability following a noncontinuous path\u201d, Journal Applied Optics, Vol. 41, No. 35 (2002) 7437,", "Abdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., \u201cFast three-dimensional phase-unwrapping algorithm based on sorting by reliability following a non-continuous path. In W. Osten, C. Gorecki, & E. L. Novak (Eds.), Optical Metrology (2005) 32\u201340, International Society for Optics and Photonics.", "Phase Unwrapping", "Wiener-Hunt deconvolution", "Return the deconvolution with a Wiener-Hunt approach (i.e. with Fourier diagonalisation).", "Input degraded image", "Point Spread Function. This is assumed to be the impulse response (input image space) if the data-type is real, or the transfer function (Fourier space) if the data-type is complex. There is no constraints on the shape of the impulse response. The transfer function must be of shape (M, N) if is_real is True, (M, N // 2 + 1) otherwise (see np.fft.rfftn).", "The regularisation parameter value that tunes the balance between the data adequacy that improve frequency restoration and the prior adequacy that reduce frequency restoration (to avoid noise artifacts).", "The regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf. Shape constraint is the same as for the psf parameter.", "True by default. Specify if psf and reg are provided with hermitian hypothesis, that is only half of the frequency plane is provided (due to the redundancy of Fourier transform of real signal). It\u2019s apply only if psf and/or reg are provided as transfer function. For the hermitian property see uft module or np.fft.rfftn.", "True by default. If True, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.", "The deconvolved image.", "This function applies the Wiener filter to a noisy and degraded image by an impulse response (or PSF). If the data model is", "where \\(n\\) is noise, \\(H\\) the PSF and \\(x\\) the unknown original image, the Wiener filter is", "where \\(F\\) and \\(F^\\dagger\\) are the Fourier and inverse Fourier transforms respectively, \\(\\Lambda_H\\) the transfer function (or the Fourier transform of the PSF, see [Hunt] below) and \\(\\Lambda_D\\) the filter to penalize the restored image frequencies (Laplacian by default, that is penalization of high frequency). The parameter \\(\\lambda\\) tunes the balance between the data (that tends to increase high frequency, even those coming from noise), and the regularization.", "These methods are then specific to a prior model. Consequently, the application or the true image nature must corresponds to the prior model. By default, the prior model (Laplacian) introduce image smoothness or pixel correlation. It can also be interpreted as high-frequency penalization to compensate the instability of the solution with respect to the data (sometimes called noise amplification or \u201cexplosive\u201d solution).", "Finally, the use of Fourier space implies a circulant property of \\(H\\), see [Hunt].", "Fran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)", "https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593", "http://research.orieux.fr/files/papers/OGR-JOSA10.pdf", "B. R. Hunt \u201cA matrix theory proof of the discrete convolution theorem\u201d, IEEE Trans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec. 1971"]}, {"name": "restoration.ball_kernel()", "path": "api/skimage.restoration#skimage.restoration.ball_kernel", "type": "restoration", "text": ["Create a ball kernel for restoration.rolling_ball.", "Radius of the ball.", "Number of dimensions of the ball. ndim should match the dimensionality of the image the kernel will be applied to.", "The kernel containing the surface intensity of the top half of the ellipsoid.", "See also"]}, {"name": "restoration.calibrate_denoiser()", "path": "api/skimage.restoration#skimage.restoration.calibrate_denoiser", "type": "restoration", "text": ["Calibrate a denoising function and return optimal J-invariant version.", "The returned function is partially evaluated with optimal parameter values set for denoising the input image.", "Input data to be denoised (converted using img_as_float).", "Denoising function to be calibrated.", "Ranges of parameters for denoise_function to be calibrated over.", "Stride used in masking procedure that converts denoise_function to J-invariance.", "Whether to approximate the self-supervised loss used to evaluate the denoiser by only computing it on one masked version of the image. If False, the runtime will be a factor of stride**image.ndim longer.", "If True, return parameters and losses in addition to the calibrated denoising function", "The optimal J-invariant version of denoise_function.", "List of parameters tested for denoise_function, as a dictionary of kwargs Self-supervised loss for each set of parameters in parameters_tested.", "The calibration procedure uses a self-supervised mean-square-error loss to evaluate the performance of J-invariant versions of denoise_function. The minimizer of the self-supervised loss is also the minimizer of the ground-truth loss (i.e., the true MSE error) [1]. The returned function can be used on the original noisy image, or other images with similar characteristics.", "at the expense of increasing its runtime. It has no effect on the runtime of the calibration.", "J. Batson & L. Royer. Noise2Self: Blind Denoising by Self-Supervision, International Conference on Machine Learning, p. 524-533 (2019)."]}, {"name": "restoration.cycle_spin()", "path": "api/skimage.restoration#skimage.restoration.cycle_spin", "type": "restoration", "text": ["Cycle spinning (repeatedly apply func to shifted versions of x).", "Data for input to func.", "A function to apply to circularly shifted versions of x. Should take x as its first argument. Any additional arguments can be supplied via func_kw.", "If an integer, shifts in range(0, max_shifts+1) will be used along each axis of x. If a tuple, range(0, max_shifts[i]+1) will be along axis i.", "The step size for the shifts applied along axis, i, are:: range((0, max_shifts[i]+1, shift_steps[i])). If an integer is provided, the same step size is used for all axes.", "The number of parallel threads to use during cycle spinning. If set to None, the full set of available cores are used.", "Whether to treat the final axis as channels (no cycle shifts are performed over the channels axis).", "Additional keyword arguments to supply to func.", "The output of func(x, **func_kw) averaged over all combinations of the specified axis shifts.", "Cycle spinning was proposed as a way to approach shift-invariance via performing several circular shifts of a shift-variant transform [1].", "For a n-level discrete wavelet transforms, one may wish to perform all shifts up to max_shifts = 2**n - 1. In practice, much of the benefit can often be realized with only a small number of shifts per axis.", "For transforms such as the blockwise discrete cosine transform, one may wish to evaluate shifts up to the block size used by the transform.", "R.R. Coifman and D.L. Donoho. \u201cTranslation-Invariant De-Noising\u201d. Wavelets and Statistics, Lecture Notes in Statistics, vol.103. Springer, New York, 1995, pp.125-150. DOI:10.1007/978-1-4612-2544-7_9"]}, {"name": "restoration.denoise_bilateral()", "path": "api/skimage.restoration#skimage.restoration.denoise_bilateral", "type": "restoration", "text": ["Denoise image using bilateral filter.", "Input image, 2D grayscale or RGB.", "Window size for filtering. If win_size is not specified, it is calculated as max(5, 2 * ceil(3 * sigma_spatial) + 1).", "Standard deviation for grayvalue/color distance (radiometric similarity). A larger value results in averaging of pixels with larger radiometric differences. Note, that the image will be converted using the img_as_float function and thus the standard deviation is in respect to the range [0, 1]. If the value is None the standard deviation of the image will be used.", "Standard deviation for range distance. A larger value results in averaging of pixels with larger spatial differences.", "Number of discrete values for Gaussian weights of color filtering. A larger value results in improved accuracy.", "How to handle values outside the image borders. See numpy.pad for detail.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Denoised image.", "This is an edge-preserving, denoising filter. It averages pixels based on their spatial closeness and radiometric similarity [1].", "Spatial closeness is measured by the Gaussian function of the Euclidean distance between two pixels and a certain standard deviation (sigma_spatial).", "Radiometric similarity is measured by the Gaussian function of the Euclidean distance between two color values and a certain standard deviation (sigma_color).", "C. Tomasi and R. Manduchi. \u201cBilateral Filtering for Gray and Color Images.\u201d IEEE International Conference on Computer Vision (1998) 839-846. DOI:10.1109/ICCV.1998.710815"]}, {"name": "restoration.denoise_nl_means()", "path": "api/skimage.restoration#skimage.restoration.denoise_nl_means", "type": "restoration", "text": ["Perform non-local means denoising on 2-D or 3-D grayscale images, and 2-D RGB images.", "Input image to be denoised, which can be 2D or 3D, and grayscale or RGB (for 2D images only, see multichannel parameter).", "Size of patches used for denoising.", "Maximal distance in pixels where to search patches used for denoising.", "Cut-off distance (in gray levels). The higher h, the more permissive one is in accepting patches. A higher h results in a smoother image, at the expense of blurring features. For a Gaussian noise of standard deviation sigma, a rule of thumb is to choose the value of h to be sigma of slightly less.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "If True (default value), a fast version of the non-local means algorithm is used. If False, the original version of non-local means is used. See the Notes section for more details about the algorithms.", "The standard deviation of the (Gaussian) noise. If provided, a more robust computation of patch weights is computed that takes the expected noise variance into account (see Notes below).", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Denoised image, of same shape as image.", "The non-local means algorithm is well suited for denoising images with specific textures. The principle of the algorithm is to average the value of a given pixel with values of other pixels in a limited neighbourhood, provided that the patches centered on the other pixels are similar enough to the patch centered on the pixel of interest.", "In the original version of the algorithm [1], corresponding to fast=False, the computational complexity is:", "Hence, changing the size of patches or their maximal distance has a strong effect on computing times, especially for 3-D images.", "However, the default behavior corresponds to fast_mode=True, for which another version of non-local means [2] is used, corresponding to a complexity of:", "The computing time depends only weakly on the patch size, thanks to the computation of the integral of patches distances for a given shift, that reduces the number of operations [1]. Therefore, this algorithm executes faster than the classic algorithm (fast_mode=False), at the expense of using twice as much memory. This implementation has been proven to be more efficient compared to other alternatives, see e.g. [3].", "Compared to the classic algorithm, all pixels of a patch contribute to the distance to another patch with the same weight, no matter their distance to the center of the patch. This coarser computation of the distance can result in a slightly poorer denoising performance. Moreover, for small images (images with a linear size that is only a few times the patch size), the classic algorithm can be faster due to boundary effects.", "The image is padded using the reflect mode of skimage.util.pad before denoising.", "If the noise standard deviation, sigma, is provided a more robust computation of patch weights is used. Subtracting the known noise variance from the computed patch distances improves the estimates of patch similarity, giving a moderate improvement to denoising performance [4]. It was also mentioned as an option for the fast variant of the algorithm in [3].", "When sigma is provided, a smaller h should typically be used to avoid oversmoothing. The optimal value for h depends on the image content and noise level, but a reasonable starting point is h = 0.8 * sigma when fast_mode is True, or h = 0.6 * sigma when fast_mode is False.", "A. Buades, B. Coll, & J-M. Morel. A non-local algorithm for image denoising. In CVPR 2005, Vol. 2, pp. 60-65, IEEE. DOI:10.1109/CVPR.2005.38", "J. Darbon, A. Cunha, T.F. Chan, S. Osher, and G.J. Jensen, Fast nonlocal filtering applied to electron cryomicroscopy, in 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro, 2008, pp. 1331-1334. DOI:10.1109/ISBI.2008.4541250", "Jacques Froment. Parameter-Free Fast Pixelwise Non-Local Means Denoising. Image Processing On Line, 2014, vol. 4, pp. 300-326. DOI:10.5201/ipol.2014.120", "A. Buades, B. Coll, & J-M. Morel. Non-Local Means Denoising. Image Processing On Line, 2011, vol. 1, pp. 208-212. DOI:10.5201/ipol.2011.bcm_nlm"]}, {"name": "restoration.denoise_tv_bregman()", "path": "api/skimage.restoration#skimage.restoration.denoise_tv_bregman", "type": "restoration", "text": ["Perform total-variation denoising using split-Bregman optimization.", "Total-variation denoising (also know as total-variation regularization) tries to find an image with less total-variation under the constraint of being similar to the input image, which is controlled by the regularization parameter ([1], [2], [3], [4]).", "Input data to be denoised (converted using img_as_float`).", "Denoising weight. The smaller the weight, the more denoising (at the expense of less similarity to the input). The regularization parameter lambda is chosen as 2 * weight.", "Relative difference of the value of the cost function that determines the stop criterion. The algorithm stops when:", "Maximal number of iterations used for the optimization.", "Switch between isotropic and anisotropic TV denoising.", "Apply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.", "Denoised image.", "https://en.wikipedia.org/wiki/Total_variation_denoising", "Tom Goldstein and Stanley Osher, \u201cThe Split Bregman Method For L1 Regularized Problems\u201d, ftp://ftp.math.ucla.edu/pub/camreport/cam08-29.pdf", "Pascal Getreuer, \u201cRudin\u2013Osher\u2013Fatemi Total Variation Denoising using Split Bregman\u201d in Image Processing On Line on 2012\u201305\u201319, https://www.ipol.im/pub/art/2012/g-tvd/article_lr.pdf", "https://web.math.ucsb.edu/~cgarcia/UGProjects/BregmanAlgorithms_JacquelineBush.pdf"]}, {"name": "restoration.denoise_tv_chambolle()", "path": "api/skimage.restoration#skimage.restoration.denoise_tv_chambolle", "type": "restoration", "text": ["Perform total-variation denoising on n-dimensional images.", "Input data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.", "Denoising weight. The greater weight, the more denoising (at the expense of fidelity to input).", "Relative difference of the value of the cost function that determines the stop criterion. The algorithm stops when:", "(E_(n-1) - E_n) < eps * E_0", "Maximal number of iterations used for the optimization.", "Apply total-variation denoising separately for each channel. This option should be true for color images, otherwise the denoising is also applied in the channels dimension.", "Denoised image.", "Make sure to set the multichannel parameter appropriately for color images.", "The principle of total variation denoising is explained in https://en.wikipedia.org/wiki/Total_variation_denoising", "The principle of total variation denoising is to minimize the total variation of the image, which can be roughly described as the integral of the norm of the image gradient. Total variation denoising tends to produce \u201ccartoon-like\u201d images, that is, piecewise-constant images.", "This code is an implementation of the algorithm of Rudin, Fatemi and Osher that was proposed by Chambolle in [1].", "A. Chambolle, An algorithm for total variation minimization and applications, Journal of Mathematical Imaging and Vision, Springer, 2004, 20, 89-97.", "2D example on astronaut image:", "3D example on synthetic data:"]}, {"name": "restoration.denoise_wavelet()", "path": "api/skimage.restoration#skimage.restoration.denoise_wavelet", "type": "restoration", "text": ["Perform wavelet denoising on an image.", "Input data to be denoised. image can be of any numeric type, but it is cast into an ndarray of floats for the computation of the denoised image.", "The noise standard deviation used when computing the wavelet detail coefficient threshold(s). When None (default), the noise standard deviation is estimated via the method in [2].", "The type of wavelet to perform and can be any of the options pywt.wavelist outputs. The default is \u2018db1\u2019. For example, wavelet can be any of {'db2', 'haar', 'sym9'} and many more.", "An optional argument to choose the type of denoising performed. It noted that choosing soft thresholding given additive noise finds the best approximation of the original image.", "The number of wavelet decomposition levels to use. The default is three less than the maximum number of possible decomposition levels.", "Apply wavelet denoising separately for each channel (where channels correspond to the final axis of the array).", "If True and multichannel True, do the wavelet denoising in the YCbCr colorspace instead of the RGB color space. This typically results in better performance for RGB images.", "Thresholding method to be used. The currently supported methods are \u201cBayesShrink\u201d [1] and \u201cVisuShrink\u201d [2]. Defaults to \u201cBayesShrink\u201d.", "If False, no rescaling of the user-provided sigma will be performed. The default of True rescales sigma appropriately if the image is rescaled internally.", "New in version 0.16: rescale_sigma was introduced in 0.16", "Denoised image.", "The wavelet domain is a sparse representation of the image, and can be thought of similarly to the frequency domain of the Fourier transform. Sparse representations have most values zero or near-zero and truly random noise is (usually) represented by many small values in the wavelet domain. Setting all values below some threshold to 0 reduces the noise in the image, but larger thresholds also decrease the detail present in the image.", "If the input is 3D, this function performs wavelet denoising on each color plane separately.", "Changed in version 0.16: For floating point inputs, the original input range is maintained and there is no clipping applied to the output. Other input types will be converted to a floating point value in the range [-1, 1] or [0, 1] depending on the input image range. Unless rescale_sigma = False, any internal rescaling applied to the image will also be applied to sigma to maintain the same relative amplitude.", "Many wavelet coefficient thresholding approaches have been proposed. By default, denoise_wavelet applies BayesShrink, which is an adaptive thresholding method that computes separate thresholds for each wavelet sub-band as described in [1].", "If method == \"VisuShrink\", a single \u201cuniversal threshold\u201d is applied to all wavelet detail coefficients as described in [2]. This threshold is designed to remove all Gaussian noise at a given sigma with high probability, but tends to produce images that appear overly smooth.", "Although any of the wavelets from PyWavelets can be selected, the thresholding methods assume an orthogonal wavelet transform and may not choose the threshold appropriately for biorthogonal wavelets. Orthogonal wavelets are desirable because white noise in the input remains white noise in the subbands. Biorthogonal wavelets lead to colored noise in the subbands. Additionally, the orthogonal wavelets in PyWavelets are orthonormal so that noise variance in the subbands remains identical to the noise variance of the input. Example orthogonal wavelets are the Daubechies (e.g. \u2018db2\u2019) or symmlet (e.g. \u2018sym2\u2019) families.", "Chang, S. Grace, Bin Yu, and Martin Vetterli. \u201cAdaptive wavelet thresholding for image denoising and compression.\u201d Image Processing, IEEE Transactions on 9.9 (2000): 1532-1546. DOI:10.1109/83.862633", "D. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425"]}, {"name": "restoration.ellipsoid_kernel()", "path": "api/skimage.restoration#skimage.restoration.ellipsoid_kernel", "type": "restoration", "text": ["Create an ellipoid kernel for restoration.rolling_ball.", "Length of the principal axis of the ellipsoid (excluding the intensity axis). The kernel needs to have the same dimensionality as the image it will be applied to.", "Length of the intensity axis of the ellipsoid.", "The kernel containing the surface intensity of the top half of the ellipsoid.", "See also"]}, {"name": "restoration.estimate_sigma()", "path": "api/skimage.restoration#skimage.restoration.estimate_sigma", "type": "restoration", "text": ["Robust wavelet-based estimator of the (Gaussian) noise standard deviation.", "Image for which to estimate the noise standard deviation.", "If true, average the channel estimates of sigma. Otherwise return a list of sigmas corresponding to each channel.", "Estimate sigma separately for each channel.", "Estimated noise standard deviation(s). If multichannel is True and average_sigmas is False, a separate noise estimate for each channel is returned. Otherwise, the average of the individual channel estimates is returned.", "This function assumes the noise follows a Gaussian distribution. The estimation algorithm is based on the median absolute deviation of the wavelet detail coefficients as described in section 4.2 of [1].", "D. L. Donoho and I. M. Johnstone. \u201cIdeal spatial adaptation by wavelet shrinkage.\u201d Biometrika 81.3 (1994): 425-455. DOI:10.1093/biomet/81.3.425"]}, {"name": "restoration.inpaint_biharmonic()", "path": "api/skimage.restoration#skimage.restoration.inpaint_biharmonic", "type": "restoration", "text": ["Inpaint masked points in image with biharmonic equations.", "Input image.", "Array of pixels to be inpainted. Have to be the same shape as one of the \u2018image\u2019 channels. Unknown pixels have to be represented with 1, known pixels - with 0.", "If True, the last image dimension is considered as a color channel, otherwise as spatial.", "Input image with masked pixels inpainted.", "N.S.Hoang, S.B.Damelin, \u201cOn surface completion and image inpainting by biharmonic functions: numerical aspects\u201d, arXiv:1707.06567", "C. K. Chui and H. N. Mhaskar, MRA Contextual-Recovery Extension of Smooth Functions on Manifolds, Appl. and Comp. Harmonic Anal., 28 (2010), 104-113, DOI:10.1016/j.acha.2009.04.004"]}, {"name": "restoration.richardson_lucy()", "path": "api/skimage.restoration#skimage.restoration.richardson_lucy", "type": "restoration", "text": ["Richardson-Lucy deconvolution.", "Input degraded image (can be N dimensional).", "The point spread function.", "Number of iterations. This parameter plays the role of regularisation.", "True by default. If true, pixel value of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.", "Value below which intermediate results become 0 to avoid division by small numbers.", "The deconvolved image.", "https://en.wikipedia.org/wiki/Richardson%E2%80%93Lucy_deconvolution"]}, {"name": "restoration.rolling_ball()", "path": "api/skimage.restoration#skimage.restoration.rolling_ball", "type": "restoration", "text": ["Estimate background intensity by rolling/translating a kernel.", "This rolling ball algorithm estimates background intensity for a ndimage in case of uneven exposure. It is a generalization of the frequently used rolling ball algorithm [1].", "The image to be filtered.", "Radius of a ball shaped kernel to be rolled/translated in the image. Used if kernel = None.", "The kernel to be rolled/translated in the image. It must have the same number of dimensions as image. Kernel is filled with the intensity of the kernel at that position.", "If False (default) assumes that none of the values in image are np.nan, and uses a faster implementation.", "The maximum number of threads to use. If None use the OpenMP default value; typically equal to the maximum number of virtual cores. Note: This is an upper limit to the number of threads. The exact number is determined by the system\u2019s OpenMP library.", "The estimated background of the image.", "For the pixel that has its background intensity estimated (without loss of generality at center) the rolling ball method centers kernel under it and raises the kernel until the surface touches the image umbra at some pos=(y,x). The background intensity is then estimated using the image intensity at that position (image[pos]) plus the difference of kernel[center] - kernel[pos].", "This algorithm assumes that dark pixels correspond to the background. If you have a bright background, invert the image before passing it to the function, e.g., using utils.invert. See the gallery example for details.", "This algorithm is sensitive to noise (in particular salt-and-pepper noise). If this is a problem in your image, you can apply mild gaussian smoothing before passing the image to this function.", "Sternberg, Stanley R. \u201cBiomedical image processing.\u201d Computer 1 (1983): 22-34. DOI:10.1109/MC.1983.1654163"]}, {"name": "restoration.unsupervised_wiener()", "path": "api/skimage.restoration#skimage.restoration.unsupervised_wiener", "type": "restoration", "text": ["Unsupervised Wiener-Hunt deconvolution.", "Return the deconvolution with a Wiener-Hunt approach, where the hyperparameters are automatically estimated. The algorithm is a stochastic iterative process (Gibbs sampler) described in the reference below. See also wiener function.", "The input degraded image.", "The impulse response (input image\u2019s space) or the transfer function (Fourier space). Both are accepted. The transfer function is automatically recognized as being complex (np.iscomplexobj(psf)).", "The regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf.", "Dictionary of parameters for the Gibbs sampler. See below.", "True by default. If true, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.", "The deconvolved image (the posterior mean).", "The keys noise and prior contain the chain list of noise and prior precision respectively.", "The stopping criterion: the norm of the difference between to successive approximated solution (empirical mean of object samples, see Notes section). 1e-4 by default.", "The number of sample to ignore to start computation of the mean. 15 by default.", "The minimum number of iterations. 30 by default.", "The maximum number of iterations if threshold is not satisfied. 200 by default.", "A user provided callable to which is passed, if the function exists, the current image sample for whatever purpose. The user can store the sample, or compute other moments than the mean. It has no influence on the algorithm execution and is only for inspection.", "The estimated image is design as the posterior mean of a probability law (from a Bayesian analysis). The mean is defined as a sum over all the possible images weighted by their respective probability. Given the size of the problem, the exact sum is not tractable. This algorithm use of MCMC to draw image under the posterior law. The practical idea is to only draw highly probable images since they have the biggest contribution to the mean. At the opposite, the less probable images are drawn less often since their contribution is low. Finally the empirical mean of these samples give us an estimation of the mean, and an exact computation with an infinite sample set.", "Fran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)", "https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593", "http://research.orieux.fr/files/papers/OGR-JOSA10.pdf"]}, {"name": "restoration.unwrap_phase()", "path": "api/skimage.restoration#skimage.restoration.unwrap_phase", "type": "restoration", "text": ["Recover the original from a wrapped phase image.", "From an image wrapped to lie in the interval [-pi, pi), recover the original, unwrapped image.", "The values should be in the range [-pi, pi). If a masked array is provided, the masked entries will not be changed, and their values will not be used to guide the unwrapping of neighboring, unmasked values. Masked 1D arrays are not allowed, and will raise a ValueError.", "When an element of the sequence is True, the unwrapping process will regard the edges along the corresponding axis of the image to be connected and use this connectivity to guide the phase unwrapping process. If only a single boolean is given, it will apply to all axes. Wrap around is not supported for 1D arrays.", "Unwrapping 2D or 3D images uses random initialization. This sets the seed of the PRNG to achieve deterministic behavior.", "Unwrapped image of the same shape as the input. If the input image was a masked array, the mask will be preserved.", "If called with a masked 1D array or called with a 1D array and wrap_around=True.", "Miguel Arevallilo Herraez, David R. Burton, Michael J. Lalor, and Munther A. Gdeisat, \u201cFast two-dimensional phase-unwrapping algorithm based on sorting by reliability following a noncontinuous path\u201d, Journal Applied Optics, Vol. 41, No. 35 (2002) 7437,", "Abdul-Rahman, H., Gdeisat, M., Burton, D., & Lalor, M., \u201cFast three-dimensional phase-unwrapping algorithm based on sorting by reliability following a non-continuous path. In W. Osten, C. Gorecki, & E. L. Novak (Eds.), Optical Metrology (2005) 32\u201340, International Society for Optics and Photonics."]}, {"name": "restoration.wiener()", "path": "api/skimage.restoration#skimage.restoration.wiener", "type": "restoration", "text": ["Wiener-Hunt deconvolution", "Return the deconvolution with a Wiener-Hunt approach (i.e. with Fourier diagonalisation).", "Input degraded image", "Point Spread Function. This is assumed to be the impulse response (input image space) if the data-type is real, or the transfer function (Fourier space) if the data-type is complex. There is no constraints on the shape of the impulse response. The transfer function must be of shape (M, N) if is_real is True, (M, N // 2 + 1) otherwise (see np.fft.rfftn).", "The regularisation parameter value that tunes the balance between the data adequacy that improve frequency restoration and the prior adequacy that reduce frequency restoration (to avoid noise artifacts).", "The regularisation operator. The Laplacian by default. It can be an impulse response or a transfer function, as for the psf. Shape constraint is the same as for the psf parameter.", "True by default. Specify if psf and reg are provided with hermitian hypothesis, that is only half of the frequency plane is provided (due to the redundancy of Fourier transform of real signal). It\u2019s apply only if psf and/or reg are provided as transfer function. For the hermitian property see uft module or np.fft.rfftn.", "True by default. If True, pixel values of the result above 1 or under -1 are thresholded for skimage pipeline compatibility.", "The deconvolved image.", "This function applies the Wiener filter to a noisy and degraded image by an impulse response (or PSF). If the data model is", "where \\(n\\) is noise, \\(H\\) the PSF and \\(x\\) the unknown original image, the Wiener filter is", "where \\(F\\) and \\(F^\\dagger\\) are the Fourier and inverse Fourier transforms respectively, \\(\\Lambda_H\\) the transfer function (or the Fourier transform of the PSF, see [Hunt] below) and \\(\\Lambda_D\\) the filter to penalize the restored image frequencies (Laplacian by default, that is penalization of high frequency). The parameter \\(\\lambda\\) tunes the balance between the data (that tends to increase high frequency, even those coming from noise), and the regularization.", "These methods are then specific to a prior model. Consequently, the application or the true image nature must corresponds to the prior model. By default, the prior model (Laplacian) introduce image smoothness or pixel correlation. It can also be interpreted as high-frequency penalization to compensate the instability of the solution with respect to the data (sometimes called noise amplification or \u201cexplosive\u201d solution).", "Finally, the use of Fourier space implies a circulant property of \\(H\\), see [Hunt].", "Fran\u00e7ois Orieux, Jean-Fran\u00e7ois Giovannelli, and Thomas Rodet, \u201cBayesian estimation of regularization and point spread function parameters for Wiener-Hunt deconvolution\u201d, J. Opt. Soc. Am. A 27, 1593-1607 (2010)", "https://www.osapublishing.org/josaa/abstract.cfm?URI=josaa-27-7-1593", "http://research.orieux.fr/files/papers/OGR-JOSA10.pdf", "B. R. Hunt \u201cA matrix theory proof of the discrete convolution theorem\u201d, IEEE Trans. on Audio and Electroacoustics, vol. au-19, no. 4, pp. 285-288, dec. 1971"]}, {"name": "segmentation", "path": "api/skimage.segmentation", "type": "segmentation", "text": ["skimage.segmentation.active_contour(image, snake)", "Active contour model.", "skimage.segmentation.chan_vese(image[, mu, \u2026])", "Chan-Vese segmentation algorithm.", "skimage.segmentation.checkerboard_level_set(\u2026)", "Create a checkerboard level set with binary values.", "skimage.segmentation.circle_level_set(\u2026[, \u2026])", "Create a circle level set with binary values.", "skimage.segmentation.clear_border(labels[, \u2026])", "Clear objects connected to the label image border.", "skimage.segmentation.disk_level_set(\u2026[, \u2026])", "Create a disk level set with binary values.", "skimage.segmentation.expand_labels(label_image)", "Expand labels in label image by distance pixels without overlapping.", "skimage.segmentation.felzenszwalb(image[, \u2026])", "Computes Felsenszwalb\u2019s efficient graph based image segmentation.", "skimage.segmentation.find_boundaries(label_img)", "Return bool array where boundaries between labeled regions are True.", "skimage.segmentation.flood(image, seed_point, *)", "Mask corresponding to a flood fill.", "skimage.segmentation.flood_fill(image, \u2026)", "Perform flood filling on an image.", "skimage.segmentation.inverse_gaussian_gradient(image)", "Inverse of gradient magnitude.", "skimage.segmentation.join_segmentations(s1, s2)", "Return the join of the two input segmentations.", "skimage.segmentation.mark_boundaries(image, \u2026)", "Return image with boundaries between labeled regions highlighted.", "skimage.segmentation.morphological_chan_vese(\u2026)", "Morphological Active Contours without Edges (MorphACWE)", "skimage.segmentation.morphological_geodesic_active_contour(\u2026)", "Morphological Geodesic Active Contours (MorphGAC).", "skimage.segmentation.quickshift(image[, \u2026])", "Segments image using quickshift clustering in Color-(x,y) space.", "skimage.segmentation.random_walker(data, labels)", "Random walker algorithm for segmentation from markers.", "skimage.segmentation.relabel_sequential(\u2026)", "Relabel arbitrary labels to {offset, \u2026", "skimage.segmentation.slic(image[, \u2026])", "Segments image using k-means clustering in Color-(x,y,z) space.", "skimage.segmentation.watershed(image[, \u2026])", "Find watershed basins in image flooded from given markers.", "Active contour model.", "Active contours by fitting snakes to features of images. Supports single and multichannel 2D images. Snakes can be periodic (for segmentation) or have fixed and/or free ends. The output snake has the same length as the input boundary. As the number of points is constant, make sure that the initial snake has enough points to capture the details of the final contour.", "Input image.", "Initial snake coordinates. For periodic boundary conditions, endpoints must not be duplicated.", "Snake length shape parameter. Higher values makes snake contract faster.", "Snake smoothness shape parameter. Higher values makes snake smoother.", "Controls attraction to brightness. Use negative values to attract toward dark regions.", "Controls attraction to edges. Use negative values to repel snake from edges.", "Explicit time stepping parameter.", "Maximum pixel distance to move per iteration.", "Maximum iterations to optimize snake shape.", "Convergence criteria.", "Boundary conditions for the contour. Can be one of \u2018periodic\u2019, \u2018free\u2019, \u2018fixed\u2019, \u2018free-fixed\u2019, or \u2018fixed-free\u2019. \u2018periodic\u2019 attaches the two ends of the snake, \u2018fixed\u2019 holds the end-points in place, and \u2018free\u2019 allows free movement of the ends. \u2018fixed\u2019 and \u2018free\u2019 can be combined by parsing \u2018fixed-free\u2019, \u2018free-fixed\u2019. Parsing \u2018fixed-fixed\u2019 or \u2018free-free\u2019 yields same behaviour as \u2018fixed\u2019 and \u2018free\u2019, respectively.", "This option remains for compatibility purpose only and has no effect. It was introduced in 0.16 with the 'xy' option, but since 0.18, only the 'rc' option is valid. Coordinates must be set in a row-column format.", "Optimised snake, same shape as input parameter.", "Kass, M.; Witkin, A.; Terzopoulos, D. \u201cSnakes: Active contour models\u201d. International Journal of Computer Vision 1 (4): 321 (1988). DOI:10.1007/BF00133570", "Create and smooth image:", "Initialize spline:", "Fit spline to image:", "Chan-Vese segmentation algorithm.", "Active contour model by evolving a level set. Can be used to segment objects without clearly defined boundaries.", "Grayscale image to be segmented.", "\u2018edge length\u2019 weight parameter. Higher mu values will produce a \u2018round\u2019 edge, while values closer to zero will detect smaller objects.", "\u2018difference from average\u2019 weight parameter for the output region with value \u2018True\u2019. If it is lower than lambda2, this region will have a larger range of values than the other.", "\u2018difference from average\u2019 weight parameter for the output region with value \u2018False\u2019. If it is lower than lambda1, this region will have a larger range of values than the other.", "Level set variation tolerance between iterations. If the L2 norm difference between the level sets of successive iterations normalized by the area of the image is below this value, the algorithm will assume that the solution was reached.", "Maximum number of iterations allowed before the algorithm interrupts itself.", "A multiplication factor applied at calculations for each step, serves to accelerate the algorithm. While higher values may speed up the algorithm, they may also lead to convergence problems.", "Defines the starting level set used by the algorithm. If a string is inputted, a level set that matches the image size will automatically be generated. Alternatively, it is possible to define a custom level set, which should be an array of float values, with the same shape as \u2018image\u2019. Accepted string values are as follows.", "the starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y are pixel coordinates. This level set has fast convergence, but may fail to detect implicit edges.", "the starting level set is defined as the opposite of the distance from the center of the image minus half of the minimum value between image width and image height. This is somewhat slower, but is more likely to properly detect implicit edges.", "the starting level set is defined as the opposite of the distance from the center of the image minus a quarter of the minimum value between image width and image height.", "If set to True, the return value will be a tuple containing the three return values (see below). If set to False which is the default value, only the \u2018segmentation\u2019 array will be returned.", "Segmentation produced by the algorithm.", "Final level set computed by the algorithm.", "Shows the evolution of the \u2018energy\u2019 for each step of the algorithm. This should allow to check whether the algorithm converged.", "The Chan-Vese Algorithm is designed to segment objects without clearly defined boundaries. This algorithm is based on level sets that are evolved iteratively to minimize an energy, which is defined by weighted values corresponding to the sum of differences intensity from the average value outside the segmented region, the sum of differences from the average value inside the segmented region, and a term which is dependent on the length of the boundary of the segmented region.", "This algorithm was first proposed by Tony Chan and Luminita Vese, in a publication entitled \u201cAn Active Contour Model Without Edges\u201d [1].", "This implementation of the algorithm is somewhat simplified in the sense that the area factor \u2018nu\u2019 described in the original paper is not implemented, and is only suitable for grayscale images.", "Typical values for lambda1 and lambda2 are 1. If the \u2018background\u2019 is very different from the segmented object in terms of distribution (for example, a uniform black image with figures of varying intensity), then these values should be different from each other.", "Typical values for mu are between 0 and 1, though higher values can be used when dealing with shapes with very ill-defined contours.", "The \u2018energy\u2019 which this algorithm tries to minimize is defined as the sum of the differences from the average within the region squared and weighed by the \u2018lambda\u2019 factors to which is added the length of the contour multiplied by the \u2018mu\u2019 factor.", "Supports 2D grayscale images only, and does not implement the area term described in the original article.", "An Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-Space Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13", "Chan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012), pp. 214-224, DOI:10.5201/ipol.2012.g-cv", "The Chan-Vese Algorithm - Project Report, Rami Cohen, 2011 arXiv:1107.2782", "Create a checkerboard level set with binary values.", "Shape of the image.", "Size of the squares of the checkerboard. It defaults to 5.", "Binary level set of the checkerboard.", "See also", "Create a circle level set with binary values.", "Shape of the image", "Coordinates of the center of the circle given in (row, column). If not given, it defaults to the center of the image.", "Radius of the circle. If not given, it is set to the 75% of the smallest image dimension.", "Binary level set of the circle with the given radius and center.", "New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk_level_set instead.", "See also", "Clear objects connected to the label image border.", "Imaging data labels.", "The width of the border examined. By default, only objects that touch the outside of the image are removed.", "Cleared objects are set to this value.", "Whether or not to manipulate the labels array in-place.", "Image data mask. Objects in labels image overlapping with False pixels of mask will be removed. If defined, the argument buffer_size will be ignored.", "Imaging data labels with cleared borders", "Create a disk level set with binary values.", "Shape of the image", "Coordinates of the center of the disk given in (row, column). If not given, it defaults to the center of the image.", "Radius of the disk. If not given, it is set to the 75% of the smallest image dimension.", "Binary level set of the disk with the given radius and center.", "See also", "Expand labels in label image by distance pixels without overlapping.", "Given a label image, expand_labels grows label regions (connected components) outwards by up to distance pixels without overflowing into neighboring regions. More specifically, each background pixel that is within Euclidean distance of <= distance pixels of a connected component is assigned the label of that connected component. Where multiple connected components are within distance pixels of a background pixel, the label value of the closest connected component will be assigned (see Notes for the case of multiple labels at equal distance).", "label image", "Euclidean distance in pixels by which to grow the labels. Default is one.", "Labeled array, where all connected regions have been enlarged", "See also", "Where labels are spaced more than distance pixels are apart, this is equivalent to a morphological dilation with a disc or hyperball of radius distance. However, in contrast to a morphological dilation, expand_labels will not expand a label region into a neighboring region.", "This implementation of expand_labels is derived from CellProfiler [1], where it is known as module \u201cIdentifySecondaryObjects (Distance-N)\u201d [2].", "There is an important edge case when a pixel has the same distance to multiple regions, as it is not defined which region expands into that space. Here, the exact behavior depends on the upstream implementation of scipy.ndimage.distance_transform_edt.", "https://cellprofiler.org", "https://github.com/CellProfiler/CellProfiler/blob/082930ea95add7b72243a4fa3d39ae5145995e9c/cellprofiler/modules/identifysecondaryobjects.py#L559", "Labels will not overwrite each other:", "In case of ties, behavior is undefined, but currently resolves to the label closest to (0,) * ndim in lexicographical order.", "Computes Felsenszwalb\u2019s efficient graph based image segmentation.", "Produces an oversegmentation of a multichannel (i.e. RGB) image using a fast, minimum spanning tree based clustering on the image grid. The parameter scale sets an observation level. Higher scale means less and larger segments. sigma is the diameter of a Gaussian kernel, used for smoothing the image prior to segmentation.", "The number of produced segments as well as their size can only be controlled indirectly through scale. Segment size within an image can vary greatly depending on local contrast.", "For RGB images, the algorithm uses the euclidean distance between pixels in color space.", "Input image.", "Free parameter. Higher means larger clusters.", "Width (standard deviation) of Gaussian kernel used in preprocessing.", "Minimum component size. Enforced using postprocessing.", "Whether the last axis of the image is to be interpreted as multiple channels. A value of False, for a 3D image, is not currently supported.", "Integer mask indicating segment labels.", "The k parameter used in the original paper renamed to scale here.", "Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004", "Return bool array where boundaries between labeled regions are True.", "An array in which different regions are labeled with either different integers or boolean values.", "A pixel is considered a boundary pixel if any of its neighbors has a different label. connectivity controls which pixels are considered neighbors. A connectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in 3D) will be considered neighbors. A connectivity of label_img.ndim means pixels sharing a corner will be considered neighbors.", "How to mark the boundaries:", "For modes \u2018inner\u2019 and \u2018outer\u2019, a definition of a background label is required. See mode for descriptions of these two.", "A bool image where True represents a boundary pixel. For mode equal to \u2018subpixel\u2019, boundaries.shape[i] is equal to 2 * label_img.shape[i] - 1 for all i (a pixel is inserted in between all other pairs of pixels).", "Mask corresponding to a flood fill.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).", "A Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill.", "If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:", "Flood Fill", "Perform flood filling on an image.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "New value to set the entire fill. This must be chosen in agreement with the dtype of image.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).", "If True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).", "This parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).", "An array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:", "Flood Fill", "Inverse of gradient magnitude.", "Compute the magnitude of the gradients in the image and then inverts the result in the range [0, 1]. Flat areas are assigned values close to 1, while areas close to borders are assigned values close to 0.", "This function or a similar one defined by the user should be applied over the image as a preprocessing step before calling morphological_geodesic_active_contour.", "Grayscale image or volume.", "Controls the steepness of the inversion. A larger value will make the transition between the flat areas and border areas steeper in the resulting array.", "Standard deviation of the Gaussian filter applied over the image.", "Preprocessed image (or volume) suitable for morphological_geodesic_active_contour.", "Return the join of the two input segmentations.", "The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.", "s1 and s2 are label fields of the same shape.", "The join segmentation of s1 and s2.", "Return image with boundaries between labeled regions highlighted.", "Grayscale or RGB image.", "Label array where regions are marked by different integer values.", "RGB color of boundaries in the output image.", "RGB color surrounding boundaries in the output image. If None, no outline is drawn.", "The mode for finding boundaries.", "Which label to consider background (this is only useful for modes inner and outer).", "An image in which the boundaries between labels are superimposed on the original image.", "See also", "Trainable segmentation using local features and random forests", "Morphological Active Contours without Edges (MorphACWE)", "Active contours without edges implemented with morphological operators. It can be used to segment objects in images and volumes without well defined borders. It is required that the inside of the object looks different on average than the outside (i.e., the inner area of the object should be darker or lighter than the outer area on average).", "Grayscale image or volume to be segmented.", "Number of iterations to run", "Initial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.", "Number of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.", "Weight parameter for the outer region. If lambda1 is larger than lambda2, the outer region will contain a larger range of values than the inner region.", "Weight parameter for the inner region. If lambda2 is larger than lambda1, the inner region will contain a larger range of values than the outer region.", "If given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.", "Final segmentation (i.e., the final level set)", "See also", "This is a version of the Chan-Vese algorithm that uses morphological operators instead of solving a partial differential equation (PDE) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the Chan-Vese PDE (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (it is not necessary to find the right time step for the evolution), and are computationally faster.", "The algorithm and its theoretical derivation are described in [1].", "A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106", "Morphological Geodesic Active Contours (MorphGAC).", "Geodesic active contours implemented with morphological operators. It can be used to segment objects with visible but noisy, cluttered, broken borders.", "Preprocessed image or volume to be segmented. This is very rarely the original image. Instead, this is usually a preprocessed version of the original image that enhances and highlights the borders (or other structures) of the object to segment. morphological_geodesic_active_contour will try to stop the contour evolution in areas where gimage is small. See morphsnakes.inverse_gaussian_gradient as an example function to perform this preprocessing. Note that the quality of morphological_geodesic_active_contour might greatly depend on this preprocessing.", "Number of iterations to run.", "Initial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.", "Number of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.", "Areas of the image with a value smaller than this threshold will be considered borders. The evolution of the contour will stop in this areas.", "Balloon force to guide the contour in non-informative areas of the image, i.e., areas where the gradient of the image is too small to push the contour towards a border. A negative value will shrink the contour, while a positive value will expand the contour in these areas. Setting this to zero will disable the balloon force.", "If given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.", "Final segmentation (i.e., the final level set)", "See also", "This is a version of the Geodesic Active Contours (GAC) algorithm that uses morphological operators instead of solving partial differential equations (PDEs) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the GAC PDEs (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (e.g., it is not necessary to find the right time step for the evolution), and are computationally faster.", "The algorithm and its theoretical derivation are described in [1].", "A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106", "Segments image using quickshift clustering in Color-(x,y) space.", "Produces an oversegmentation of the image using the quickshift mode-seeking algorithm.", "Input image.", "Balances color-space proximity and image-space proximity. Higher values give more weight to color-space.", "Width of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.", "Cut-off point for data distances. Higher means fewer clusters.", "Whether to return the full segmentation hierarchy tree and distances.", "Width for Gaussian smoothing as preprocessing. Zero means no smoothing.", "Whether the input should be converted to Lab colorspace prior to segmentation. For this purpose, the input is assumed to be RGB.", "Random seed used for breaking ties.", "Integer mask indicating segment labels.", "The authors advocate to convert the image to Lab color space prior to segmentation, though this is not strictly necessary. For this to work, the image must be given in RGB format.", "Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008", "Random walker algorithm for segmentation from markers.", "Random walker algorithm is implemented for gray-level or multichannel images.", "Image to be segmented in phases. Gray-level data can be two- or three-dimensional; multichannel data can be three- or four- dimensional (multichannel=True) with the highest dimension denoting channels. Data spacing is assumed isotropic unless the spacing keyword argument is used.", "Array of seed markers labeled with different positive integers for different phases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond to inactive pixels that are not taken into account (they are removed from the graph). If labels are not consecutive integers, the labels array will be transformed so that labels are consecutive. In the multichannel case, labels should have the same shape as a single channel of data, i.e. without the final dimension denoting channels.", "Penalization coefficient for the random walker motion (the greater beta, the more difficult the diffusion).", "Mode for solving the linear system in the random walker algorithm.", "Tolerance to achieve when solving the linear system using the conjugate gradient based modes (\u2018cg\u2019, \u2018cg_j\u2019 and \u2018cg_mg\u2019).", "If copy is False, the labels array will be overwritten with the result of the segmentation. Use copy=False if you want to save on memory.", "If True, input data is parsed as multichannel data (see \u2018data\u2019 above for proper input format in this case).", "If True, the probability that a pixel belongs to each of the labels will be returned, instead of only the most likely label.", "Spacing between voxels in each spatial dimension. If None, then the spacing between pixels/voxels in each dimension is assumed 1.", "Tolerance on the resulting probability to be in the interval [0, 1]. If the tolerance is not satisfied, a warning is displayed.", "See also", "watershed segmentation A segmentation algorithm based on mathematical morphology and \u201cflooding\u201d of regions from markers.", "Multichannel inputs are scaled with all channel data combined. Ensure all channels are separately normalized prior to running this algorithm.", "The spacing argument is specifically for anisotropic datasets, where data points are spaced differently in one or more spatial dimensions. Anisotropic data is commonly encountered in medical imaging.", "The algorithm was first proposed in [1].", "The algorithm solves the diffusion equation at infinite times for sources placed on markers of each phase in turn. A pixel is labeled with the phase that has the greatest probability to diffuse first to the pixel.", "The diffusion equation is solved by minimizing x.T L x for each phase, where L is the Laplacian of the weighted graph of the image, and x is the probability that a marker of the given phase arrives first at a pixel by diffusion (x=1 on markers of the phase, x=0 on the other markers, and the other coefficients are looked for). Each pixel is attributed the label for which it has a maximal value of x. The Laplacian L of the image is defined as:", "The weight w_ij is a decreasing function of the norm of the local gradient. This ensures that diffusion is easier between pixels of similar values.", "When the Laplacian is decomposed into blocks of marked and unmarked pixels:", "with first indices corresponding to marked pixels, and then to unmarked pixels, minimizing x.T L x for one phase amount to solving:", "where x_m = 1 on markers of the given phase, and 0 on other markers. This linear system is solved in the algorithm using a direct method for small images, and an iterative method for larger images.", "Leo Grady, Random walks for image segmentation, IEEE Trans Pattern Anal Mach Intell. 2006 Nov;28(11):1768-83. DOI:10.1109/TPAMI.2006.233.", "Relabel arbitrary labels to {offset, \u2026 offset + number_of_labels}.", "This function also returns the forward map (mapping the original labels to the reduced labels) and the inverse map (mapping the reduced labels back to the original ones).", "An array of labels, which must be non-negative integers.", "The return labels will start at offset, which should be strictly positive.", "The input label field with labels mapped to {offset, \u2026, number_of_labels + offset - 1}. The data type will be the same as label_field, except when offset + number_of_labels causes overflow of the current data type.", "The map from the original label space to the returned label space. Can be used to re-apply the same mapping. See examples for usage. The output data type will be the same as relabeled.", "The map from the new label space to the original space. This can be used to reconstruct the original label field from the relabeled one. The output data type will be the same as label_field.", "The label 0 is assumed to denote the background and is never remapped.", "The forward map can be extremely big for some inputs, since its length is given by the maximum of the label field. However, in most situations, label_field.max() is much smaller than label_field.size, and in these cases the forward map is guaranteed to be smaller than either the input or output images.", "Segments image using k-means clustering in Color-(x,y,z) space.", "Input image, which can be 2D or 3D, and grayscale or multichannel (see multichannel parameter). Input image must either be NaN-free or the NaN\u2019s must be masked out", "The (approximate) number of labels in the segmented output image.", "Balances color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic. In SLICO mode, this is the initial compactness. This parameter depends strongly on image contrast and on the shapes of objects in the image. We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining around a chosen value.", "Maximum number of iterations of k-means.", "Width of Gaussian smoothing kernel for pre-processing for each dimension of the image. The same sigma is applied to each dimension in case of a scalar value. Zero means no smoothing. Note, that sigma is automatically scaled if it is scalar and a manual voxel spacing is provided (see Notes section).", "The voxel spacing along each image dimension. By default, slic assumes uniform spacing (same voxel resolution along z, y and x). This parameter controls the weights of the distances along z, y, and x during k-means clustering.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether the input should be converted to Lab colorspace prior to segmentation. The input image must be RGB. Highly recommended. This option defaults to True when multichannel=True and image.shape[-1] == 3.", "Whether the generated segments are connected or not", "Proportion of the minimum segment size to be removed with respect to the supposed segment size `depth*width*height/n_segments`", "Proportion of the maximum connected segment size. A value of 3 works in most of the cases.", "Run SLIC-zero, the zero-parameter mode of SLIC. [2]", "The labels\u2019 index start. Should be 0 or 1.", "New in version 0.17: start_label was introduced in 0.17", "If provided, superpixels are computed only where mask is True, and seed points are homogeneously distributed over the mask using a K-means clustering strategy.", "New in version 0.17: mask was introduced in 0.17", "Integer mask indicating segment labels.", "If convert2lab is set to True but the last array dimension is not of length 3.", "If start_label is not 0 or 1.", "Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S\u00fcsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012. DOI:10.1109/TPAMI.2012.120", "https://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO", "Irving, Benjamin. \u201cmaskSLIC: regional superpixel generation with application to local pathology characterisation in medical images.\u201d, 2016, arXiv:1606.09518", "https://github.com/scikit-image/scikit-image/issues/3722", "Increasing the compactness parameter yields more square regions:", "Find watershed basins in image flooded from given markers.", "Data array where the lowest value points are labeled first.", "The desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.", "An array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.", "offset of the connectivity (one offset per dimension)", "Array of same shape as image. Only points at which mask == True will be labeled.", "Use compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.", "If watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.", "A labeled matrix of the same type and shape as markers", "See also", "random walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.", "This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker.", "Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182", "The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides.", "This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm.", "Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example).", "https://en.wikipedia.org/wiki/Watershed_%28image_processing%29", "http://cmm.ensmp.fr/~beucher/wtshed.html", "Peer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf", "The watershed algorithm is useful to separate overlapping objects.", "We first generate an initial image with two overlapping circles:", "Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background:", "Finally, we run the watershed on the image and markers:", "The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres.", "Watershed segmentation", "Markers for watershed transform", "Segment human cells (in mitosis)"]}, {"name": "segmentation.active_contour()", "path": "api/skimage.segmentation#skimage.segmentation.active_contour", "type": "segmentation", "text": ["Active contour model.", "Active contours by fitting snakes to features of images. Supports single and multichannel 2D images. Snakes can be periodic (for segmentation) or have fixed and/or free ends. The output snake has the same length as the input boundary. As the number of points is constant, make sure that the initial snake has enough points to capture the details of the final contour.", "Input image.", "Initial snake coordinates. For periodic boundary conditions, endpoints must not be duplicated.", "Snake length shape parameter. Higher values makes snake contract faster.", "Snake smoothness shape parameter. Higher values makes snake smoother.", "Controls attraction to brightness. Use negative values to attract toward dark regions.", "Controls attraction to edges. Use negative values to repel snake from edges.", "Explicit time stepping parameter.", "Maximum pixel distance to move per iteration.", "Maximum iterations to optimize snake shape.", "Convergence criteria.", "Boundary conditions for the contour. Can be one of \u2018periodic\u2019, \u2018free\u2019, \u2018fixed\u2019, \u2018free-fixed\u2019, or \u2018fixed-free\u2019. \u2018periodic\u2019 attaches the two ends of the snake, \u2018fixed\u2019 holds the end-points in place, and \u2018free\u2019 allows free movement of the ends. \u2018fixed\u2019 and \u2018free\u2019 can be combined by parsing \u2018fixed-free\u2019, \u2018free-fixed\u2019. Parsing \u2018fixed-fixed\u2019 or \u2018free-free\u2019 yields same behaviour as \u2018fixed\u2019 and \u2018free\u2019, respectively.", "This option remains for compatibility purpose only and has no effect. It was introduced in 0.16 with the 'xy' option, but since 0.18, only the 'rc' option is valid. Coordinates must be set in a row-column format.", "Optimised snake, same shape as input parameter.", "Kass, M.; Witkin, A.; Terzopoulos, D. \u201cSnakes: Active contour models\u201d. International Journal of Computer Vision 1 (4): 321 (1988). DOI:10.1007/BF00133570", "Create and smooth image:", "Initialize spline:", "Fit spline to image:"]}, {"name": "segmentation.chan_vese()", "path": "api/skimage.segmentation#skimage.segmentation.chan_vese", "type": "segmentation", "text": ["Chan-Vese segmentation algorithm.", "Active contour model by evolving a level set. Can be used to segment objects without clearly defined boundaries.", "Grayscale image to be segmented.", "\u2018edge length\u2019 weight parameter. Higher mu values will produce a \u2018round\u2019 edge, while values closer to zero will detect smaller objects.", "\u2018difference from average\u2019 weight parameter for the output region with value \u2018True\u2019. If it is lower than lambda2, this region will have a larger range of values than the other.", "\u2018difference from average\u2019 weight parameter for the output region with value \u2018False\u2019. If it is lower than lambda1, this region will have a larger range of values than the other.", "Level set variation tolerance between iterations. If the L2 norm difference between the level sets of successive iterations normalized by the area of the image is below this value, the algorithm will assume that the solution was reached.", "Maximum number of iterations allowed before the algorithm interrupts itself.", "A multiplication factor applied at calculations for each step, serves to accelerate the algorithm. While higher values may speed up the algorithm, they may also lead to convergence problems.", "Defines the starting level set used by the algorithm. If a string is inputted, a level set that matches the image size will automatically be generated. Alternatively, it is possible to define a custom level set, which should be an array of float values, with the same shape as \u2018image\u2019. Accepted string values are as follows.", "the starting level set is defined as sin(x/5*pi)*sin(y/5*pi), where x and y are pixel coordinates. This level set has fast convergence, but may fail to detect implicit edges.", "the starting level set is defined as the opposite of the distance from the center of the image minus half of the minimum value between image width and image height. This is somewhat slower, but is more likely to properly detect implicit edges.", "the starting level set is defined as the opposite of the distance from the center of the image minus a quarter of the minimum value between image width and image height.", "If set to True, the return value will be a tuple containing the three return values (see below). If set to False which is the default value, only the \u2018segmentation\u2019 array will be returned.", "Segmentation produced by the algorithm.", "Final level set computed by the algorithm.", "Shows the evolution of the \u2018energy\u2019 for each step of the algorithm. This should allow to check whether the algorithm converged.", "The Chan-Vese Algorithm is designed to segment objects without clearly defined boundaries. This algorithm is based on level sets that are evolved iteratively to minimize an energy, which is defined by weighted values corresponding to the sum of differences intensity from the average value outside the segmented region, the sum of differences from the average value inside the segmented region, and a term which is dependent on the length of the boundary of the segmented region.", "This algorithm was first proposed by Tony Chan and Luminita Vese, in a publication entitled \u201cAn Active Contour Model Without Edges\u201d [1].", "This implementation of the algorithm is somewhat simplified in the sense that the area factor \u2018nu\u2019 described in the original paper is not implemented, and is only suitable for grayscale images.", "Typical values for lambda1 and lambda2 are 1. If the \u2018background\u2019 is very different from the segmented object in terms of distribution (for example, a uniform black image with figures of varying intensity), then these values should be different from each other.", "Typical values for mu are between 0 and 1, though higher values can be used when dealing with shapes with very ill-defined contours.", "The \u2018energy\u2019 which this algorithm tries to minimize is defined as the sum of the differences from the average within the region squared and weighed by the \u2018lambda\u2019 factors to which is added the length of the contour multiplied by the \u2018mu\u2019 factor.", "Supports 2D grayscale images only, and does not implement the area term described in the original article.", "An Active Contour Model without Edges, Tony Chan and Luminita Vese, Scale-Space Theories in Computer Vision, 1999, DOI:10.1007/3-540-48236-9_13", "Chan-Vese Segmentation, Pascal Getreuer Image Processing On Line, 2 (2012), pp. 214-224, DOI:10.5201/ipol.2012.g-cv", "The Chan-Vese Algorithm - Project Report, Rami Cohen, 2011 arXiv:1107.2782"]}, {"name": "segmentation.checkerboard_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.checkerboard_level_set", "type": "segmentation", "text": ["Create a checkerboard level set with binary values.", "Shape of the image.", "Size of the squares of the checkerboard. It defaults to 5.", "Binary level set of the checkerboard.", "See also"]}, {"name": "segmentation.circle_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.circle_level_set", "type": "segmentation", "text": ["Create a circle level set with binary values.", "Shape of the image", "Coordinates of the center of the circle given in (row, column). If not given, it defaults to the center of the image.", "Radius of the circle. If not given, it is set to the 75% of the smallest image dimension.", "Binary level set of the circle with the given radius and center.", "New in version 0.17: This function is deprecated and will be removed in scikit-image 0.19. Please use the function named disk_level_set instead.", "See also"]}, {"name": "segmentation.clear_border()", "path": "api/skimage.segmentation#skimage.segmentation.clear_border", "type": "segmentation", "text": ["Clear objects connected to the label image border.", "Imaging data labels.", "The width of the border examined. By default, only objects that touch the outside of the image are removed.", "Cleared objects are set to this value.", "Whether or not to manipulate the labels array in-place.", "Image data mask. Objects in labels image overlapping with False pixels of mask will be removed. If defined, the argument buffer_size will be ignored.", "Imaging data labels with cleared borders"]}, {"name": "segmentation.disk_level_set()", "path": "api/skimage.segmentation#skimage.segmentation.disk_level_set", "type": "segmentation", "text": ["Create a disk level set with binary values.", "Shape of the image", "Coordinates of the center of the disk given in (row, column). If not given, it defaults to the center of the image.", "Radius of the disk. If not given, it is set to the 75% of the smallest image dimension.", "Binary level set of the disk with the given radius and center.", "See also"]}, {"name": "segmentation.expand_labels()", "path": "api/skimage.segmentation#skimage.segmentation.expand_labels", "type": "segmentation", "text": ["Expand labels in label image by distance pixels without overlapping.", "Given a label image, expand_labels grows label regions (connected components) outwards by up to distance pixels without overflowing into neighboring regions. More specifically, each background pixel that is within Euclidean distance of <= distance pixels of a connected component is assigned the label of that connected component. Where multiple connected components are within distance pixels of a background pixel, the label value of the closest connected component will be assigned (see Notes for the case of multiple labels at equal distance).", "label image", "Euclidean distance in pixels by which to grow the labels. Default is one.", "Labeled array, where all connected regions have been enlarged", "See also", "Where labels are spaced more than distance pixels are apart, this is equivalent to a morphological dilation with a disc or hyperball of radius distance. However, in contrast to a morphological dilation, expand_labels will not expand a label region into a neighboring region.", "This implementation of expand_labels is derived from CellProfiler [1], where it is known as module \u201cIdentifySecondaryObjects (Distance-N)\u201d [2].", "There is an important edge case when a pixel has the same distance to multiple regions, as it is not defined which region expands into that space. Here, the exact behavior depends on the upstream implementation of scipy.ndimage.distance_transform_edt.", "https://cellprofiler.org", "https://github.com/CellProfiler/CellProfiler/blob/082930ea95add7b72243a4fa3d39ae5145995e9c/cellprofiler/modules/identifysecondaryobjects.py#L559", "Labels will not overwrite each other:", "In case of ties, behavior is undefined, but currently resolves to the label closest to (0,) * ndim in lexicographical order."]}, {"name": "segmentation.felzenszwalb()", "path": "api/skimage.segmentation#skimage.segmentation.felzenszwalb", "type": "segmentation", "text": ["Computes Felsenszwalb\u2019s efficient graph based image segmentation.", "Produces an oversegmentation of a multichannel (i.e. RGB) image using a fast, minimum spanning tree based clustering on the image grid. The parameter scale sets an observation level. Higher scale means less and larger segments. sigma is the diameter of a Gaussian kernel, used for smoothing the image prior to segmentation.", "The number of produced segments as well as their size can only be controlled indirectly through scale. Segment size within an image can vary greatly depending on local contrast.", "For RGB images, the algorithm uses the euclidean distance between pixels in color space.", "Input image.", "Free parameter. Higher means larger clusters.", "Width (standard deviation) of Gaussian kernel used in preprocessing.", "Minimum component size. Enforced using postprocessing.", "Whether the last axis of the image is to be interpreted as multiple channels. A value of False, for a 3D image, is not currently supported.", "Integer mask indicating segment labels.", "The k parameter used in the original paper renamed to scale here.", "Efficient graph-based image segmentation, Felzenszwalb, P.F. and Huttenlocher, D.P. International Journal of Computer Vision, 2004"]}, {"name": "segmentation.find_boundaries()", "path": "api/skimage.segmentation#skimage.segmentation.find_boundaries", "type": "segmentation", "text": ["Return bool array where boundaries between labeled regions are True.", "An array in which different regions are labeled with either different integers or boolean values.", "A pixel is considered a boundary pixel if any of its neighbors has a different label. connectivity controls which pixels are considered neighbors. A connectivity of 1 (default) means pixels sharing an edge (in 2D) or a face (in 3D) will be considered neighbors. A connectivity of label_img.ndim means pixels sharing a corner will be considered neighbors.", "How to mark the boundaries:", "For modes \u2018inner\u2019 and \u2018outer\u2019, a definition of a background label is required. See mode for descriptions of these two.", "A bool image where True represents a boundary pixel. For mode equal to \u2018subpixel\u2019, boundaries.shape[i] is equal to 2 * label_img.shape[i] - 1 for all i (a pixel is inserted in between all other pairs of pixels)."]}, {"name": "segmentation.flood()", "path": "api/skimage.segmentation#skimage.segmentation.flood", "type": "segmentation", "text": ["Mask corresponding to a flood fill.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is larger or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the initial value of image at seed_point. This is fastest. If a value is given, a comparison will be done at every point and if within tolerance of the initial value will also be filled (inclusive).", "A Boolean array with the same shape as image is returned, with True values for areas connected to and equal (or within tolerance of) the seed point. All other values are False.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs. This function returns just the mask representing the fill.", "If indices are desired rather than masks for memory reasons, the user can simply run numpy.nonzero on the result, save the indices, and discard this mask.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:"]}, {"name": "segmentation.flood_fill()", "path": "api/skimage.segmentation#skimage.segmentation.flood_fill", "type": "segmentation", "text": ["Perform flood filling on an image.", "Starting at a specific seed_point, connected points equal or within tolerance of the seed value are found, then set to new_value.", "An n-dimensional array.", "The point in image used as the starting point for the flood fill. If the image is 1D, this point may be given as an integer.", "New value to set the entire fill. This must be chosen in agreement with the dtype of image.", "A structuring element used to determine the neighborhood of each evaluated pixel. It must contain only 1\u2019s and 0\u2019s, have the same number of dimensions as image. If not given, all adjacent pixels are considered as part of the neighborhood (fully connected).", "A number used to determine the neighborhood of each evaluated pixel. Adjacent pixels whose squared distance from the center is less than or equal to connectivity are considered neighbors. Ignored if selem is not None.", "If None (default), adjacent values must be strictly equal to the value of image at seed_point to be filled. This is fastest. If a tolerance is provided, adjacent points with values within plus or minus tolerance from the seed point are filled (inclusive).", "If True, flood filling is applied to image in place. If False, the flood filled result is returned without modifying the input image (default).", "This parameter is deprecated and will be removed in version 0.19.0 in favor of in_place. If True, flood filling is applied to image inplace. If False, the flood filled result is returned without modifying the input image (default).", "An array with the same shape as image is returned, with values in areas connected to and equal (or within tolerance of) the seed point replaced with new_value.", "The conceptual analogy of this operation is the \u2018paint bucket\u2019 tool in many raster graphics programs.", "Fill connected ones with 5, with full connectivity (diagonals included):", "Fill connected ones with 5, excluding diagonal points (connectivity 1):", "Fill with a tolerance:"]}, {"name": "segmentation.inverse_gaussian_gradient()", "path": "api/skimage.segmentation#skimage.segmentation.inverse_gaussian_gradient", "type": "segmentation", "text": ["Inverse of gradient magnitude.", "Compute the magnitude of the gradients in the image and then inverts the result in the range [0, 1]. Flat areas are assigned values close to 1, while areas close to borders are assigned values close to 0.", "This function or a similar one defined by the user should be applied over the image as a preprocessing step before calling morphological_geodesic_active_contour.", "Grayscale image or volume.", "Controls the steepness of the inversion. A larger value will make the transition between the flat areas and border areas steeper in the resulting array.", "Standard deviation of the Gaussian filter applied over the image.", "Preprocessed image (or volume) suitable for morphological_geodesic_active_contour."]}, {"name": "segmentation.join_segmentations()", "path": "api/skimage.segmentation#skimage.segmentation.join_segmentations", "type": "segmentation", "text": ["Return the join of the two input segmentations.", "The join J of S1 and S2 is defined as the segmentation in which two voxels are in the same segment if and only if they are in the same segment in both S1 and S2.", "s1 and s2 are label fields of the same shape.", "The join segmentation of s1 and s2."]}, {"name": "segmentation.mark_boundaries()", "path": "api/skimage.segmentation#skimage.segmentation.mark_boundaries", "type": "segmentation", "text": ["Return image with boundaries between labeled regions highlighted.", "Grayscale or RGB image.", "Label array where regions are marked by different integer values.", "RGB color of boundaries in the output image.", "RGB color surrounding boundaries in the output image. If None, no outline is drawn.", "The mode for finding boundaries.", "Which label to consider background (this is only useful for modes inner and outer).", "An image in which the boundaries between labels are superimposed on the original image.", "See also"]}, {"name": "segmentation.morphological_chan_vese()", "path": "api/skimage.segmentation#skimage.segmentation.morphological_chan_vese", "type": "segmentation", "text": ["Morphological Active Contours without Edges (MorphACWE)", "Active contours without edges implemented with morphological operators. It can be used to segment objects in images and volumes without well defined borders. It is required that the inside of the object looks different on average than the outside (i.e., the inner area of the object should be darker or lighter than the outer area on average).", "Grayscale image or volume to be segmented.", "Number of iterations to run", "Initial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.", "Number of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.", "Weight parameter for the outer region. If lambda1 is larger than lambda2, the outer region will contain a larger range of values than the inner region.", "Weight parameter for the inner region. If lambda2 is larger than lambda1, the inner region will contain a larger range of values than the outer region.", "If given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.", "Final segmentation (i.e., the final level set)", "See also", "This is a version of the Chan-Vese algorithm that uses morphological operators instead of solving a partial differential equation (PDE) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the Chan-Vese PDE (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (it is not necessary to find the right time step for the evolution), and are computationally faster.", "The algorithm and its theoretical derivation are described in [1].", "A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106"]}, {"name": "segmentation.morphological_geodesic_active_contour()", "path": "api/skimage.segmentation#skimage.segmentation.morphological_geodesic_active_contour", "type": "segmentation", "text": ["Morphological Geodesic Active Contours (MorphGAC).", "Geodesic active contours implemented with morphological operators. It can be used to segment objects with visible but noisy, cluttered, broken borders.", "Preprocessed image or volume to be segmented. This is very rarely the original image. Instead, this is usually a preprocessed version of the original image that enhances and highlights the borders (or other structures) of the object to segment. morphological_geodesic_active_contour will try to stop the contour evolution in areas where gimage is small. See morphsnakes.inverse_gaussian_gradient as an example function to perform this preprocessing. Note that the quality of morphological_geodesic_active_contour might greatly depend on this preprocessing.", "Number of iterations to run.", "Initial level set. If an array is given, it will be binarized and used as the initial level set. If a string is given, it defines the method to generate a reasonable initial level set with the shape of the image. Accepted values are \u2018checkerboard\u2019 and \u2018circle\u2019. See the documentation of checkerboard_level_set and circle_level_set respectively for details about how these level sets are created.", "Number of times the smoothing operator is applied per iteration. Reasonable values are around 1-4. Larger values lead to smoother segmentations.", "Areas of the image with a value smaller than this threshold will be considered borders. The evolution of the contour will stop in this areas.", "Balloon force to guide the contour in non-informative areas of the image, i.e., areas where the gradient of the image is too small to push the contour towards a border. A negative value will shrink the contour, while a positive value will expand the contour in these areas. Setting this to zero will disable the balloon force.", "If given, this function is called once per iteration with the current level set as the only argument. This is useful for debugging or for plotting intermediate results during the evolution.", "Final segmentation (i.e., the final level set)", "See also", "This is a version of the Geodesic Active Contours (GAC) algorithm that uses morphological operators instead of solving partial differential equations (PDEs) for the evolution of the contour. The set of morphological operators used in this algorithm are proved to be infinitesimally equivalent to the GAC PDEs (see [1]). However, morphological operators are do not suffer from the numerical stability issues typically found in PDEs (e.g., it is not necessary to find the right time step for the evolution), and are computationally faster.", "The algorithm and its theoretical derivation are described in [1].", "A Morphological Approach to Curvature-based Evolution of Curves and Surfaces, Pablo M\u00e1rquez-Neila, Luis Baumela, Luis \u00c1lvarez. In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2014, DOI:10.1109/TPAMI.2013.106"]}, {"name": "segmentation.quickshift()", "path": "api/skimage.segmentation#skimage.segmentation.quickshift", "type": "segmentation", "text": ["Segments image using quickshift clustering in Color-(x,y) space.", "Produces an oversegmentation of the image using the quickshift mode-seeking algorithm.", "Input image.", "Balances color-space proximity and image-space proximity. Higher values give more weight to color-space.", "Width of Gaussian kernel used in smoothing the sample density. Higher means fewer clusters.", "Cut-off point for data distances. Higher means fewer clusters.", "Whether to return the full segmentation hierarchy tree and distances.", "Width for Gaussian smoothing as preprocessing. Zero means no smoothing.", "Whether the input should be converted to Lab colorspace prior to segmentation. For this purpose, the input is assumed to be RGB.", "Random seed used for breaking ties.", "Integer mask indicating segment labels.", "The authors advocate to convert the image to Lab color space prior to segmentation, though this is not strictly necessary. For this to work, the image must be given in RGB format.", "Quick shift and kernel methods for mode seeking, Vedaldi, A. and Soatto, S. European Conference on Computer Vision, 2008"]}, {"name": "segmentation.random_walker()", "path": "api/skimage.segmentation#skimage.segmentation.random_walker", "type": "segmentation", "text": ["Random walker algorithm for segmentation from markers.", "Random walker algorithm is implemented for gray-level or multichannel images.", "Image to be segmented in phases. Gray-level data can be two- or three-dimensional; multichannel data can be three- or four- dimensional (multichannel=True) with the highest dimension denoting channels. Data spacing is assumed isotropic unless the spacing keyword argument is used.", "Array of seed markers labeled with different positive integers for different phases. Zero-labeled pixels are unlabeled pixels. Negative labels correspond to inactive pixels that are not taken into account (they are removed from the graph). If labels are not consecutive integers, the labels array will be transformed so that labels are consecutive. In the multichannel case, labels should have the same shape as a single channel of data, i.e. without the final dimension denoting channels.", "Penalization coefficient for the random walker motion (the greater beta, the more difficult the diffusion).", "Mode for solving the linear system in the random walker algorithm.", "Tolerance to achieve when solving the linear system using the conjugate gradient based modes (\u2018cg\u2019, \u2018cg_j\u2019 and \u2018cg_mg\u2019).", "If copy is False, the labels array will be overwritten with the result of the segmentation. Use copy=False if you want to save on memory.", "If True, input data is parsed as multichannel data (see \u2018data\u2019 above for proper input format in this case).", "If True, the probability that a pixel belongs to each of the labels will be returned, instead of only the most likely label.", "Spacing between voxels in each spatial dimension. If None, then the spacing between pixels/voxels in each dimension is assumed 1.", "Tolerance on the resulting probability to be in the interval [0, 1]. If the tolerance is not satisfied, a warning is displayed.", "See also", "watershed segmentation A segmentation algorithm based on mathematical morphology and \u201cflooding\u201d of regions from markers.", "Multichannel inputs are scaled with all channel data combined. Ensure all channels are separately normalized prior to running this algorithm.", "The spacing argument is specifically for anisotropic datasets, where data points are spaced differently in one or more spatial dimensions. Anisotropic data is commonly encountered in medical imaging.", "The algorithm was first proposed in [1].", "The algorithm solves the diffusion equation at infinite times for sources placed on markers of each phase in turn. A pixel is labeled with the phase that has the greatest probability to diffuse first to the pixel.", "The diffusion equation is solved by minimizing x.T L x for each phase, where L is the Laplacian of the weighted graph of the image, and x is the probability that a marker of the given phase arrives first at a pixel by diffusion (x=1 on markers of the phase, x=0 on the other markers, and the other coefficients are looked for). Each pixel is attributed the label for which it has a maximal value of x. The Laplacian L of the image is defined as:", "The weight w_ij is a decreasing function of the norm of the local gradient. This ensures that diffusion is easier between pixels of similar values.", "When the Laplacian is decomposed into blocks of marked and unmarked pixels:", "with first indices corresponding to marked pixels, and then to unmarked pixels, minimizing x.T L x for one phase amount to solving:", "where x_m = 1 on markers of the given phase, and 0 on other markers. This linear system is solved in the algorithm using a direct method for small images, and an iterative method for larger images.", "Leo Grady, Random walks for image segmentation, IEEE Trans Pattern Anal Mach Intell. 2006 Nov;28(11):1768-83. DOI:10.1109/TPAMI.2006.233."]}, {"name": "segmentation.relabel_sequential()", "path": "api/skimage.segmentation#skimage.segmentation.relabel_sequential", "type": "segmentation", "text": ["Relabel arbitrary labels to {offset, \u2026 offset + number_of_labels}.", "This function also returns the forward map (mapping the original labels to the reduced labels) and the inverse map (mapping the reduced labels back to the original ones).", "An array of labels, which must be non-negative integers.", "The return labels will start at offset, which should be strictly positive.", "The input label field with labels mapped to {offset, \u2026, number_of_labels + offset - 1}. The data type will be the same as label_field, except when offset + number_of_labels causes overflow of the current data type.", "The map from the original label space to the returned label space. Can be used to re-apply the same mapping. See examples for usage. The output data type will be the same as relabeled.", "The map from the new label space to the original space. This can be used to reconstruct the original label field from the relabeled one. The output data type will be the same as label_field.", "The label 0 is assumed to denote the background and is never remapped.", "The forward map can be extremely big for some inputs, since its length is given by the maximum of the label field. However, in most situations, label_field.max() is much smaller than label_field.size, and in these cases the forward map is guaranteed to be smaller than either the input or output images."]}, {"name": "segmentation.slic()", "path": "api/skimage.segmentation#skimage.segmentation.slic", "type": "segmentation", "text": ["Segments image using k-means clustering in Color-(x,y,z) space.", "Input image, which can be 2D or 3D, and grayscale or multichannel (see multichannel parameter). Input image must either be NaN-free or the NaN\u2019s must be masked out", "The (approximate) number of labels in the segmented output image.", "Balances color proximity and space proximity. Higher values give more weight to space proximity, making superpixel shapes more square/cubic. In SLICO mode, this is the initial compactness. This parameter depends strongly on image contrast and on the shapes of objects in the image. We recommend exploring possible values on a log scale, e.g., 0.01, 0.1, 1, 10, 100, before refining around a chosen value.", "Maximum number of iterations of k-means.", "Width of Gaussian smoothing kernel for pre-processing for each dimension of the image. The same sigma is applied to each dimension in case of a scalar value. Zero means no smoothing. Note, that sigma is automatically scaled if it is scalar and a manual voxel spacing is provided (see Notes section).", "The voxel spacing along each image dimension. By default, slic assumes uniform spacing (same voxel resolution along z, y and x). This parameter controls the weights of the distances along z, y, and x during k-means clustering.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether the input should be converted to Lab colorspace prior to segmentation. The input image must be RGB. Highly recommended. This option defaults to True when multichannel=True and image.shape[-1] == 3.", "Whether the generated segments are connected or not", "Proportion of the minimum segment size to be removed with respect to the supposed segment size `depth*width*height/n_segments`", "Proportion of the maximum connected segment size. A value of 3 works in most of the cases.", "Run SLIC-zero, the zero-parameter mode of SLIC. [2]", "The labels\u2019 index start. Should be 0 or 1.", "New in version 0.17: start_label was introduced in 0.17", "If provided, superpixels are computed only where mask is True, and seed points are homogeneously distributed over the mask using a K-means clustering strategy.", "New in version 0.17: mask was introduced in 0.17", "Integer mask indicating segment labels.", "If convert2lab is set to True but the last array dimension is not of length 3.", "If start_label is not 0 or 1.", "Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S\u00fcsstrunk, SLIC Superpixels Compared to State-of-the-art Superpixel Methods, TPAMI, May 2012. DOI:10.1109/TPAMI.2012.120", "https://www.epfl.ch/labs/ivrl/research/slic-superpixels/#SLICO", "Irving, Benjamin. \u201cmaskSLIC: regional superpixel generation with application to local pathology characterisation in medical images.\u201d, 2016, arXiv:1606.09518", "https://github.com/scikit-image/scikit-image/issues/3722", "Increasing the compactness parameter yields more square regions:"]}, {"name": "segmentation.watershed()", "path": "api/skimage.segmentation#skimage.segmentation.watershed", "type": "segmentation", "text": ["Find watershed basins in image flooded from given markers.", "Data array where the lowest value points are labeled first.", "The desired number of markers, or an array marking the basins with the values to be assigned in the label matrix. Zero means not a marker. If None (no markers given), the local minima of the image are used as markers.", "An array with the same number of dimensions as image whose non-zero elements indicate neighbors for connection. Following the scipy convention, default is a one-connected array of the dimension of the image.", "offset of the connectivity (one offset per dimension)", "Array of same shape as image. Only points at which mask == True will be labeled.", "Use compact watershed [3] with given compactness parameter. Higher values result in more regularly-shaped watershed basins.", "If watershed_line is True, a one-pixel wide line separates the regions obtained by the watershed algorithm. The line has the label 0.", "A labeled matrix of the same type and shape as markers", "See also", "random walker segmentation A segmentation algorithm based on anisotropic diffusion, usually slower than the watershed but with good results on noisy data and boundaries with holes.", "This function implements a watershed algorithm [1] [2] that apportions pixels into marked basins. The algorithm uses a priority queue to hold the pixels with the metric for the priority queue being pixel value, then the time of entry into the queue - this settles ties in favor of the closest marker.", "Some ideas taken from Soille, \u201cAutomated Basin Delineation from Digital Elevation Models Using Mathematical Morphology\u201d, Signal Processing 20 (1990) 171-182", "The most important insight in the paper is that entry time onto the queue solves two problems: a pixel should be assigned to the neighbor with the largest gradient or, if there is no gradient, pixels on a plateau should be split between markers on opposite sides.", "This implementation converts all arguments to specific, lowest common denominator types, then passes these to a C algorithm.", "Markers can be determined manually, or automatically using for example the local minima of the gradient of the image, or the local maxima of the distance function to the background for separating overlapping objects (see example).", "https://en.wikipedia.org/wiki/Watershed_%28image_processing%29", "http://cmm.ensmp.fr/~beucher/wtshed.html", "Peer Neubert & Peter Protzel (2014). Compact Watershed and Preemptive SLIC: On Improving Trade-offs of Superpixel Segmentation Algorithms. ICPR 2014, pp 996-1001. DOI:10.1109/ICPR.2014.181 https://www.tu-chemnitz.de/etit/proaut/publications/cws_pSLIC_ICPR.pdf", "The watershed algorithm is useful to separate overlapping objects.", "We first generate an initial image with two overlapping circles:", "Next, we want to separate the two circles. We generate markers at the maxima of the distance to the background:", "Finally, we run the watershed on the image and markers:", "The algorithm works also for 3-D images, and can be used for example to separate overlapping spheres."]}, {"name": "skimage", "path": "api/skimage", "type": "skimage", "text": ["Image Processing for Python", "scikit-image (a.k.a. skimage) is a collection of algorithms for image processing and computer vision.", "The main package of skimage only provides a few utilities for converting between image data types; for most features, you need to import one of the following subpackages:", "Color space conversion.", "Test images and example data.", "Drawing primitives (lines, text, etc.) that operate on NumPy arrays.", "Image intensity adjustment, e.g., histogram equalization, etc.", "Feature detection and extraction, e.g., texture analysis corners, etc.", "Sharpening, edge finding, rank filters, thresholding, etc.", "Graph-theoretic operations, e.g., shortest paths.", "Reading, saving, and displaying images and video.", "Measurement of image properties, e.g., region properties and contours.", "Metrics corresponding to images, e.g. distance metrics, similarity, etc.", "Morphological operations, e.g., opening or skeletonization.", "Restoration algorithms, e.g., deconvolution algorithms, denoising, etc.", "Partitioning an image into multiple regions.", "Geometric and other transforms, e.g., rotation or the Radon transform.", "Generic utilities.", "A simple graphical user interface for visualizing results and exploring parameters.", "Convert an image to floating point format, with values in [0, 1]. Is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.", "Convert an image to single-precision (32-bit) floating point format, with values in [0, 1].", "Convert an image to double-precision (64-bit) floating point format, with values in [0, 1].", "Convert an image to unsigned integer format, with values in [0, 65535].", "Convert an image to signed integer format, with values in [-32768, 32767].", "Convert an image to unsigned byte format, with values in [0, 255].", "Convert an image to boolean format, with values either True or False.", "Return intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.", "skimage.dtype_limits(image[, clip_negative])", "Return intensity limits, i.e.", "skimage.ensure_python_version(min_version)", "skimage.img_as_bool(image[, force_copy])", "Convert an image to boolean format.", "skimage.img_as_float(image[, force_copy])", "Convert an image to floating point format.", "skimage.img_as_float32(image[, force_copy])", "Convert an image to single-precision (32-bit) floating point format.", "skimage.img_as_float64(image[, force_copy])", "Convert an image to double-precision (64-bit) floating point format.", "skimage.img_as_int(image[, force_copy])", "Convert an image to 16-bit signed integer format.", "skimage.img_as_ubyte(image[, force_copy])", "Convert an image to 8-bit unsigned integer format.", "skimage.img_as_uint(image[, force_copy])", "Convert an image to 16-bit unsigned integer format.", "skimage.lookfor(what)", "Do a keyword search on scikit-image docstrings.", "skimage.data", "Standard test images.", "skimage.util", "Return intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.", "Input image.", "If True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.", "Lower and upper intensity limits.", "Convert an image to boolean format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False.", "Convert an image to floating point format.", "This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0].", "Tinting gray-scale images", "3D adaptive histogram equalization", "Phase Unwrapping", "Finding local maxima", "Use rolling-ball algorithm for estimating background intensity", "Explore 3D images (of cells)", "Convert an image to single-precision (32-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0].", "Convert an image to double-precision (64-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0].", "Convert an image to 16-bit signed integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values.", "Convert an image to 8-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 255.", "Local Histogram Equalization", "Entropy", "Markers for watershed transform", "Segment human cells (in mitosis)", "Rank filters", "Convert an image to 16-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 65535.", "Do a keyword search on scikit-image docstrings.", "Words to look for."]}, {"name": "transform", "path": "api/skimage.transform", "type": "transform", "text": ["skimage.transform.downscale_local_mean(\u2026)", "Down-sample N-dimensional image by local averaging.", "skimage.transform.estimate_transform(ttype, \u2026)", "Estimate 2D geometric transformation parameters.", "skimage.transform.frt2(a)", "Compute the 2-dimensional finite radon transform (FRT) for an n x n integer array.", "skimage.transform.hough_circle(image, radius)", "Perform a circular Hough transform.", "skimage.transform.hough_circle_peaks(\u2026[, \u2026])", "Return peaks in a circle Hough transform.", "skimage.transform.hough_ellipse(image[, \u2026])", "Perform an elliptical Hough transform.", "skimage.transform.hough_line(image[, theta])", "Perform a straight line Hough transform.", "skimage.transform.hough_line_peaks(hspace, \u2026)", "Return peaks in a straight line Hough transform.", "skimage.transform.ifrt2(a)", "Compute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x n integer array.", "skimage.transform.integral_image(image)", "Integral image / summed area table.", "skimage.transform.integrate(ii, start, end)", "Use an integral image to integrate over a given window.", "skimage.transform.iradon(radon_image[, \u2026])", "Inverse radon transform.", "skimage.transform.iradon_sart(radon_image[, \u2026])", "Inverse radon transform.", "skimage.transform.matrix_transform(coords, \u2026)", "Apply 2D matrix transform.", "skimage.transform.order_angles_golden_ratio(theta)", "Order angles to reduce the amount of correlated information in subsequent projections.", "skimage.transform.probabilistic_hough_line(image)", "Return lines from a progressive probabilistic line Hough transform.", "skimage.transform.pyramid_expand(image[, \u2026])", "Upsample and then smooth image.", "skimage.transform.pyramid_gaussian(image[, \u2026])", "Yield images of the Gaussian pyramid formed by the input image.", "skimage.transform.pyramid_laplacian(image[, \u2026])", "Yield images of the laplacian pyramid formed by the input image.", "skimage.transform.pyramid_reduce(image[, \u2026])", "Smooth and then downsample image.", "skimage.transform.radon(image[, theta, \u2026])", "Calculates the radon transform of an image given specified projection angles.", "skimage.transform.rescale(image, scale[, \u2026])", "Scale image by a certain factor.", "skimage.transform.resize(image, output_shape)", "Resize image to match a certain size.", "skimage.transform.rotate(image, angle[, \u2026])", "Rotate image by a certain angle around its center.", "skimage.transform.swirl(image[, center, \u2026])", "Perform a swirl transformation.", "skimage.transform.warp(image, inverse_map[, \u2026])", "Warp an image according to a given coordinate transformation.", "skimage.transform.warp_coords(coord_map, shape)", "Build the source coordinates for the output of a 2-D image warp.", "skimage.transform.warp_polar(image[, \u2026])", "Remap image to polar or log-polar coordinates space.", "skimage.transform.AffineTransform([matrix, \u2026])", "2D affine transformation.", "skimage.transform.EssentialMatrixTransform([\u2026])", "Essential matrix transformation.", "skimage.transform.EuclideanTransform([\u2026])", "2D Euclidean transformation.", "skimage.transform.FundamentalMatrixTransform([\u2026])", "Fundamental matrix transformation.", "skimage.transform.PiecewiseAffineTransform()", "2D piecewise affine transformation.", "skimage.transform.PolynomialTransform([params])", "2D polynomial transformation.", "skimage.transform.ProjectiveTransform([matrix])", "Projective transformation.", "skimage.transform.SimilarityTransform([\u2026])", "2D similarity transformation.", "Down-sample N-dimensional image by local averaging.", "The image is padded with cval if it is not perfectly divisible by the integer factors.", "In contrast to interpolation in skimage.transform.resize and skimage.transform.rescale this function calculates the local mean of elements in each block of size factors in the input image.", "N-dimensional input image.", "Array containing down-sampling integer factor along each axis.", "Constant padding value if image is not perfectly divisible by the integer factors.", "Unused, but kept here for API consistency with the other transforms in this module. (The local mean will never fall outside the range of values in the input image, assuming the provided cval also falls within that range.)", "Down-sampled image with same number of dimensions as input image. For integer inputs, the output dtype will be float64. See numpy.mean() for details.", "Estimate 2D geometric transformation parameters.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Type of transform.", "Function parameters (src, dst, n, angle):", "Also see examples below.", "Transform object containing the transformation parameters and providing access to forward and inverse transformation functions.", "Compute the 2-dimensional finite radon transform (FRT) for an n x n integer array.", "A 2-D square n x n integer array.", "Finite Radon Transform array of (n+1) x n integer coefficients.", "See also", "The two-dimensional inverse FRT.", "The FRT has a unique inverse if and only if n is prime. [FRT] The idea for this algorithm is due to Vlad Negnevitski.", "A. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)", "Generate a test image: Use a prime number for the array dimensions", "Apply the Finite Radon Transform:", "Perform a circular Hough transform.", "Input image with nonzero values representing edges.", "Radii at which to compute the Hough transform. Floats are converted to integers.", "Normalize the accumulator with the number of pixels used to draw the radius.", "Extend the output size by twice the largest radius in order to detect centers outside the input picture.", "Hough transform accumulator for each radius. R designates the larger radius if full_output is True. Otherwise, R = 0.", "Return peaks in a circle Hough transform.", "Identifies most prominent circles separated by certain distances in given Hough spaces. Non-maximum suppression with different sizes is applied separately in the first and second dimension of the Hough space to identify peaks. For circles with different radius but close in distance, only the one with highest peak is kept.", "Hough spaces returned by the hough_circle function.", "Radii corresponding to Hough spaces.", "Minimum distance separating centers in the x dimension.", "Minimum distance separating centers in the y dimension.", "Minimum intensity of peaks in each Hough space. Default is 0.5 * max(hspace).", "Maximum number of peaks in each Hough space. When the number of peaks exceeds num_peaks, only num_peaks coordinates based on peak intensity are considered for the corresponding radius.", "Maximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.", "If True, normalize the accumulator by the radius to sort the prominent peaks.", "Peak values in Hough space, x and y center coordinates and radii.", "Circles with bigger radius have higher peaks in Hough space. If larger circles are preferred over smaller ones, normalize should be False. Otherwise, circles will be returned in the order of decreasing voting number.", "Perform an elliptical Hough transform.", "Input image with nonzero values representing edges.", "Accumulator threshold value.", "Bin size on the minor axis used in the accumulator.", "Minimal major axis length.", "Maximal minor axis length. If None, the value is set to the half of the smaller image dimension.", "Where (yc, xc) is the center, (a, b) the major and minor axes, respectively. The orientation value follows skimage.draw.ellipse_perimeter convention.", "The accuracy must be chosen to produce a peak in the accumulator distribution. In other words, a flat accumulator distribution with low values may be caused by a too low bin size.", "Xie, Yonghong, and Qiang Ji. \u201cA new efficient ellipse detection method.\u201d Pattern Recognition, 2002. Proceedings. 16th International Conference on. Vol. 2. IEEE, 2002", "Perform a straight line Hough transform.", "Input image with nonzero values representing edges.", "Angles at which to compute the transform, in radians. Defaults to a vector of 180 angles evenly spaced from -pi/2 to pi/2.", "Hough transform accumulator.", "Angles at which the transform is computed, in radians.", "Distance values.", "The origin is the top left corner of the original image. X and Y axis are horizontal and vertical edges respectively. The distance is the minimal algebraic distance from the origin to the detected line. The angle accuracy can be improved by decreasing the step size in the theta array.", "Generate a test image:", "Apply the Hough transform:", "(Source code, png, pdf)", "Return peaks in a straight line Hough transform.", "Identifies most prominent lines separated by a certain angle and distance in a Hough transform. Non-maximum suppression with different sizes is applied separately in the first (distances) and second (angles) dimension of the Hough space to identify peaks.", "Hough space returned by the hough_line function.", "Angles returned by the hough_line function. Assumed to be continuous. (angles[-1] - angles[0] == PI).", "Distances returned by the hough_line function.", "Minimum distance separating lines (maximum filter size for first dimension of hough space).", "Minimum angle separating lines (maximum filter size for second dimension of hough space).", "Minimum intensity of peaks. Default is 0.5 * max(hspace).", "Maximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.", "Peak values in Hough space, angles and distances.", "Compute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x n integer array.", "A 2-D (n+1) row x n column integer array.", "Inverse Finite Radon Transform array of n x n integer coefficients.", "See also", "The two-dimensional FRT", "The FRT has a unique inverse if and only if n is prime. See [1] for an overview. The idea for this algorithm is due to Vlad Negnevitski.", "A. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)", "Apply the Finite Radon Transform:", "Apply the Inverse Finite Radon Transform to recover the input", "Check that it\u2019s identical to the original", "Integral image / summed area table.", "The integral image contains the sum of all elements above and to the left of it, i.e.:", "Input image.", "Integral image/summed area table of same shape as input image.", "F.C. Crow, \u201cSummed-area tables for texture mapping,\u201d ACM SIGGRAPH Computer Graphics, vol. 18, 1984, pp. 207-212.", "Use an integral image to integrate over a given window.", "Integral image.", "Coordinates of top left corner of window(s). Each tuple in the list contains the starting row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2,\u2026), \u2026].", "Coordinates of bottom right corner of window(s). Each tuple in the list containing the end row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2, \u2026), \u2026].", "Integral (sum) over the given window(s).", "Inverse radon transform.", "Reconstruct an image from the radon transform, using the filtered back projection algorithm.", "Image containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.", "Reconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).", "Number of rows and columns in the reconstruction.", "Filter used in frequency domain filtering. Ramp filter used by default. Filters available: ramp, shepp-logan, cosine, hamming, hann. Assign None to use no filter.", "Interpolation method used in reconstruction. Methods available: \u2018linear\u2019, \u2018nearest\u2019, and \u2018cubic\u2019 (\u2018cubic\u2019 is slow).", "Assume the reconstructed image is zero outside the inscribed circle. Also changes the default output_size to match the behaviour of radon called with circle=True.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Reconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).", "Changed in version 0.19: In iradon, filter argument is deprecated in favor of filter_name.", "It applies the Fourier slice theorem to reconstruct an image by multiplying the frequency domain of the filter with the FFT of the projection data. This algorithm is called filtered back projection.", "AC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.", "B.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989", "Inverse radon transform.", "Reconstruct an image from the radon transform, using a single iteration of the Simultaneous Algebraic Reconstruction Technique (SART) algorithm.", "Image containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.", "Reconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).", "Image containing an initial reconstruction estimate. Shape of this array should be (radon_image.shape[0], radon_image.shape[0]). The default is an array of zeros.", "Shift the projections contained in radon_image (the sinogram) by this many pixels before reconstructing the image. The i\u2019th value defines the shift of the i\u2019th column of radon_image.", "Force all values in the reconstructed tomogram to lie in the range [clip[0], clip[1]]", "Relaxation parameter for the update step. A higher value can improve the convergence rate, but one runs the risk of instabilities. Values close to or higher than 1 are not recommended.", "Output data type, must be floating point. By default, if input data type is not float, input is cast to double, otherwise dtype is set to input data type.", "Reconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).", "Algebraic Reconstruction Techniques are based on formulating the tomography reconstruction problem as a set of linear equations. Along each ray, the projected value is the sum of all the values of the cross section along the ray. A typical feature of SART (and a few other variants of algebraic techniques) is that it samples the cross section at equidistant points along the ray, using linear interpolation between the pixel values of the cross section. The resulting set of linear equations are then solved using a slightly modified Kaczmarz method.", "When using SART, a single iteration is usually sufficient to obtain a good reconstruction. Further iterations will tend to enhance high-frequency information, but will also often increase the noise.", "AC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.", "AH Andersen, AC Kak, \u201cSimultaneous algebraic reconstruction technique (SART): a superior implementation of the ART algorithm\u201d, Ultrasonic Imaging 6 pp 81\u201394 (1984)", "S Kaczmarz, \u201cAngen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen\u201d, Bulletin International de l\u2019Academie Polonaise des Sciences et des Lettres 35 pp 355\u2013357 (1937)", "Kohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.", "Kaczmarz\u2019 method, Wikipedia, https://en.wikipedia.org/wiki/Kaczmarz_method", "Apply 2D matrix transform.", "x, y coordinates to transform", "Homogeneous transformation matrix.", "Transformed coordinates.", "Order angles to reduce the amount of correlated information in subsequent projections.", "Projection angles in degrees. Duplicate angles are not allowed.", "The returned generator yields indices into theta such that theta[indices] gives the approximate golden ratio ordering of the projections. In total, len(theta) indices are yielded. All non-negative integers < len(theta) are yielded exactly once.", "The method used here is that of the golden ratio introduced by T. Kohler.", "Kohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.", "Winkelmann, Stefanie, et al. \u201cAn optimal radial profile order based on the Golden Ratio for time-resolved MRI.\u201d Medical Imaging, IEEE Transactions on 26.1 (2007): 68-76.", "Return lines from a progressive probabilistic line Hough transform.", "Input image with nonzero values representing edges.", "Threshold", "Minimum accepted length of detected lines. Increase the parameter to extract longer lines.", "Maximum gap between pixels to still form a line. Increase the parameter to merge broken lines more aggressively.", "Angles at which to compute the transform, in radians. If None, use a range from -pi/2 to pi/2.", "Seed to initialize the random number generator.", "List of lines identified, lines in format ((x0, y0), (x1, y1)), indicating line start and end.", "C. Galamhos, J. Matas and J. Kittler, \u201cProgressive probabilistic Hough transform for line detection\u201d, in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1999.", "Upsample and then smooth image.", "Input image.", "Upscale factor.", "Sigma for Gaussian filter. Default is 2 * upscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of upsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Upsampled and smoothed float image.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf", "Yield images of the Gaussian pyramid formed by the input image.", "Recursively applies the pyramid_reduce function to the image, and yields the downscaled images.", "Note that the first image of the pyramid will be the original, unscaled image. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.", "Input image.", "Number of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.", "Downscale factor.", "Sigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of downsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Generator yielding pyramid layers as float images.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf", "Yield images of the laplacian pyramid formed by the input image.", "Each layer contains the difference between the downsampled and the downsampled, smoothed image:", "Note that the first image of the pyramid will be the difference between the original, unscaled image and its smoothed version. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.", "Input image.", "Number of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.", "Downscale factor.", "Sigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of downsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Generator yielding pyramid layers as float images.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf", "http://sepwww.stanford.edu/data/media/public/sep/morgan/texturematch/paper_html/node3.html", "Smooth and then downsample image.", "Input image.", "Downscale factor.", "Sigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of downsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Smoothed and downsampled float image.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf", "Calculates the radon transform of an image given specified projection angles.", "Input image. The rotation axis will be located in the pixel with indices (image.shape[0] // 2, image.shape[1] // 2).", "Projection angles (in degrees). If None, the value is set to np.arange(180).", "Assume image is zero outside the inscribed circle, making the width of each projection (the first dimension of the sinogram) equal to min(image.shape).", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Radon transform (sinogram). The tomography rotation axis will lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.", "Based on code of Justin K. Romberg (https://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html)", "AC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.", "B.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989", "Scale image by a certain factor.", "Performs interpolation to up-scale or down-scale N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.", "Input image.", "Scale factors. Separate scale factors can be defined as (rows, cols[, \u2026][, dim]).", "Scaled version of the input.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.", "Standard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor.", "Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2].", "Resize image to match a certain size.", "Performs interpolation to up-size or down-size N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.", "Input image.", "Size of the generated output image (rows, cols[, \u2026][, dim]). If dim is not provided, the number of channels is preserved. In case the number of input channels does not equal the number of output channels a n-dimensional interpolation is applied.", "Resized version of the input.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Whether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.", "Standard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor, where s > 1. For the up-size case, s < 1, no anti-aliasing is performed prior to rescaling.", "Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2].", "Rotate image by a certain angle around its center.", "Input image.", "Rotation angle in degrees in counter-clockwise direction.", "Determine whether the shape of the output image will be automatically calculated, so the complete rotated image exactly fits. Default is False.", "The rotation center. If center=None, the image is rotated around its center, i.e. center=(cols / 2 - 0.5, rows / 2 - 0.5). Please note that this parameter is (cols, rows), contrary to normal skimage ordering.", "Rotated version of the input.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2].", "Different perimeters", "Measure region properties", "Perform a swirl transformation.", "Input image.", "Center coordinate of transformation.", "The amount of swirling applied.", "The extent of the swirl in pixels. The effect dies out rapidly beyond radius.", "Additional rotation applied to the image.", "Swirled version of the input.", "Shape of the output image generated. By default the shape of the input image is preserved.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode, with \u2018constant\u2019 used as the default. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Warp an image according to a given coordinate transformation.", "Input image.", "Inverse coordinate map, which transforms coordinates in the output images into their corresponding coordinates in the input image.", "There are a number of different options to define this map, depending on the dimensionality of the input image. A 2-D image can have 2 dimensions for gray-scale images, or 3 dimensions with color information.", "Note, that a (3, 3) matrix is interpreted as a homogeneous transformation matrix, so you cannot interpolate values from a 3-D input, if the output is of shape (3,).", "See example section for usage.", "Keyword arguments passed to inverse_map.", "Shape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.", "Default is 0 if image.dtype is bool and 1 otherwise.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "The warped input image.", "The following image warps are all equal but differ substantially in execution time. The image is shifted to the bottom.", "Use a geometric transform to warp an image (fast):", "Use a callable (slow):", "Use a transformation matrix to warp an image (fast):", "You can also use the inverse of a geometric transformation (fast):", "For N-D images you can pass a coordinate array, that specifies the coordinates in the input image for every element in the output image. E.g. if you want to rescale a 3-D cube, you can do:", "Setup the coordinate array, that defines the scaling:", "Assume that the cube contains spatial data, where the first array element center is at coordinate (0.5, 0.5, 0.5) in real space, i.e. we have to account for this extra offset when scaling the image:", "Registration using optical flow", "Build the source coordinates for the output of a 2-D image warp.", "Return input coordinates for given output coordinates. Coordinates are in the shape (P, 2), where P is the number of coordinates and each element is a (row, col) pair.", "Shape of output image (rows, cols[, bands]).", "dtype for return value (sane choices: float32 or float64).", "Coordinates for scipy.ndimage.map_coordinates, that will yield an image of shape (orows, ocols, bands) by drawing from source points according to the coord_transform_fn.", "This is a lower-level routine that produces the source coordinates for 2-D images used by warp().", "It is provided separately from warp to give additional flexibility to users who would like, for example, to re-use a particular coordinate mapping, to use specific dtypes at various points along the the image-warping process, or to implement different post-processing logic than warp performs after the call to ndi.map_coordinates.", "Produce a coordinate map that shifts an image up and to the right:", "Remap image to polar or log-polar coordinates space.", "Input image. Only 2-D arrays are accepted by default. If multichannel=True, 3-D arrays are accepted and the last axis is interpreted as multiple channels.", "Point in image that represents the center of the transformation (i.e., the origin in cartesian space). Values can be of type float. If no value is given, the center is assumed to be the center point of the image.", "Radius of the circle that bounds the area to be transformed.", "Specify whether the image warp is polar or log-polar. Defaults to \u2018linear\u2019.", "Whether the image is a 3-D array in which the third axis is to be interpreted as multiple channels. If set to False (default), only 2-D arrays are accepted.", "Passed to transform.warp.", "The polar or log-polar warped image.", "Perform a basic polar warp on a grayscale image:", "Perform a log-polar warp on a grayscale image:", "Perform a log-polar warp on a grayscale image while specifying center, radius, and output shape:", "Perform a log-polar warp on a color image:", "Bases: skimage.transform._geometric.ProjectiveTransform", "2D affine transformation.", "Has the following form:", "where sx and sy are scale factors in the x and y directions, and the homogeneous transformation matrix is:", "Homogeneous transformation matrix.", "Scale factor(s). If a single value, it will be assigned to both sx and sy.", "New in version 0.17: Added support for supplying a single scalar value.", "Rotation angle in counter-clockwise direction as radians.", "Shear angle in counter-clockwise direction as radians.", "Translation parameters.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.transform._geometric.FundamentalMatrixTransform", "Essential matrix transformation.", "The essential matrix relates corresponding points between a pair of calibrated images. The matrix transforms normalized, homogeneous image points in one image to epipolar lines in the other image.", "The essential matrix is only defined for a pair of moving images capturing a non-planar scene. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is unknown, the fundamental matrix describes the projective relation between the two images (FundamentalMatrixTransform).", "Rotation matrix of the relative camera motion.", "Translation vector of the relative camera motion. The vector must have unit length.", "Essential matrix.", "Hartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.", "Essential matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate essential matrix using 8-point algorithm.", "The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Bases: skimage.transform._geometric.ProjectiveTransform", "2D Euclidean transformation.", "Has the following form:", "where the homogeneous transformation matrix is:", "The Euclidean transformation is a rigid transformation with rotation and translation parameters. The similarity transformation extends the Euclidean transformation with a single scaling factor.", "Homogeneous transformation matrix.", "Rotation angle in counter-clockwise direction as radians.", "x, y translation parameters.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Bases: skimage.transform._geometric.GeometricTransform", "Fundamental matrix transformation.", "The fundamental matrix relates corresponding points between a pair of uncalibrated images. The matrix transforms homogeneous image points in one image to epipolar lines in the other image.", "The fundamental matrix is only defined for a pair of moving images. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is known, the essential matrix describes the metric relation between the two images (EssentialMatrixTransform).", "Fundamental matrix.", "Hartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.", "Fundamental matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate fundamental matrix using 8-point algorithm.", "The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Apply inverse transformation.", "Destination coordinates.", "Epipolar lines in the source image.", "Compute the Sampson distance.", "The Sampson distance is the first approximation to the geometric error.", "Source coordinates.", "Destination coordinates.", "Sampson distance.", "Bases: skimage.transform._geometric.GeometricTransform", "2D piecewise affine transformation.", "Control points are used to define the mapping. The transform is based on a Delaunay triangulation of the points to form a mesh. Each triangle is used to find a local affine transform.", "Affine transformations for each triangle in the mesh.", "Inverse affine transformations for each triangle in the mesh.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Apply inverse transformation.", "Coordinates outside of the mesh will be set to - 1.", "Source coordinates.", "Transformed coordinates.", "Bases: skimage.transform._geometric.GeometricTransform", "2D polynomial transformation.", "Has the following form:", "Polynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].", "Polynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "The transformation is defined as:", "These equations can be transformed to the following form:", "which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where:", "In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.", "Source coordinates.", "Destination coordinates.", "Polynomial order (number of coefficients is order + 1).", "True, if model estimation succeeds.", "Apply inverse transformation.", "Destination coordinates.", "Source coordinates.", "Bases: skimage.transform._geometric.GeometricTransform", "Projective transformation.", "Apply a projective transformation (homography) on coordinates.", "For each homogeneous coordinate \\(\\mathbf{x} = [x, y, 1]^T\\), its target position is calculated by multiplying with the given matrix, \\(H\\), to give \\(H \\mathbf{x}\\):", "E.g., to rotate by theta degrees clockwise, the matrix should be:", "or, to translate x by 10 and y by 20:", "Homogeneous transformation matrix.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "The transformation is defined as:", "These equations can be transformed to the following form:", "which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where:", "In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.", "In case of the affine transformation the coefficients c0 and c1 are 0. Thus the system of equations is:", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Apply inverse transformation.", "Destination coordinates.", "Source coordinates.", "Bases: skimage.transform._geometric.EuclideanTransform", "2D similarity transformation.", "Has the following form:", "where s is a scale factor and the homogeneous transformation matrix is:", "The similarity transformation extends the Euclidean transformation with a single scaling factor in addition to the rotation and translation parameters.", "Homogeneous transformation matrix.", "Scale factor.", "Rotation angle in counter-clockwise direction as radians.", "x, y translation parameters.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.AffineTransform", "path": "api/skimage.transform#skimage.transform.AffineTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.ProjectiveTransform", "2D affine transformation.", "Has the following form:", "where sx and sy are scale factors in the x and y directions, and the homogeneous transformation matrix is:", "Homogeneous transformation matrix.", "Scale factor(s). If a single value, it will be assigned to both sx and sy.", "New in version 0.17: Added support for supplying a single scalar value.", "Rotation angle in counter-clockwise direction as radians.", "Shear angle in counter-clockwise direction as radians.", "Translation parameters.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.AffineTransform.rotation()", "path": "api/skimage.transform#skimage.transform.AffineTransform.rotation", "type": "transform", "text": []}, {"name": "transform.AffineTransform.scale()", "path": "api/skimage.transform#skimage.transform.AffineTransform.scale", "type": "transform", "text": []}, {"name": "transform.AffineTransform.shear()", "path": "api/skimage.transform#skimage.transform.AffineTransform.shear", "type": "transform", "text": []}, {"name": "transform.AffineTransform.translation()", "path": "api/skimage.transform#skimage.transform.AffineTransform.translation", "type": "transform", "text": []}, {"name": "transform.AffineTransform.__init__()", "path": "api/skimage.transform#skimage.transform.AffineTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.downscale_local_mean()", "path": "api/skimage.transform#skimage.transform.downscale_local_mean", "type": "transform", "text": ["Down-sample N-dimensional image by local averaging.", "The image is padded with cval if it is not perfectly divisible by the integer factors.", "In contrast to interpolation in skimage.transform.resize and skimage.transform.rescale this function calculates the local mean of elements in each block of size factors in the input image.", "N-dimensional input image.", "Array containing down-sampling integer factor along each axis.", "Constant padding value if image is not perfectly divisible by the integer factors.", "Unused, but kept here for API consistency with the other transforms in this module. (The local mean will never fall outside the range of values in the input image, assuming the provided cval also falls within that range.)", "Down-sampled image with same number of dimensions as input image. For integer inputs, the output dtype will be float64. See numpy.mean() for details."]}, {"name": "transform.EssentialMatrixTransform", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.FundamentalMatrixTransform", "Essential matrix transformation.", "The essential matrix relates corresponding points between a pair of calibrated images. The matrix transforms normalized, homogeneous image points in one image to epipolar lines in the other image.", "The essential matrix is only defined for a pair of moving images capturing a non-planar scene. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is unknown, the fundamental matrix describes the projective relation between the two images (FundamentalMatrixTransform).", "Rotation matrix of the relative camera motion.", "Translation vector of the relative camera motion. The vector must have unit length.", "Essential matrix.", "Hartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.", "Essential matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate essential matrix using 8-point algorithm.", "The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.EssentialMatrixTransform.estimate()", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform.estimate", "type": "transform", "text": ["Estimate essential matrix using 8-point algorithm.", "The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.EssentialMatrixTransform.__init__()", "path": "api/skimage.transform#skimage.transform.EssentialMatrixTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.estimate_transform()", "path": "api/skimage.transform#skimage.transform.estimate_transform", "type": "transform", "text": ["Estimate 2D geometric transformation parameters.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Type of transform.", "Function parameters (src, dst, n, angle):", "Also see examples below.", "Transform object containing the transformation parameters and providing access to forward and inverse transformation functions."]}, {"name": "transform.EuclideanTransform", "path": "api/skimage.transform#skimage.transform.EuclideanTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.ProjectiveTransform", "2D Euclidean transformation.", "Has the following form:", "where the homogeneous transformation matrix is:", "The Euclidean transformation is a rigid transformation with rotation and translation parameters. The similarity transformation extends the Euclidean transformation with a single scaling factor.", "Homogeneous transformation matrix.", "Rotation angle in counter-clockwise direction as radians.", "x, y translation parameters.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.EuclideanTransform.estimate()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.estimate", "type": "transform", "text": ["Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.EuclideanTransform.rotation()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.rotation", "type": "transform", "text": []}, {"name": "transform.EuclideanTransform.translation()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.translation", "type": "transform", "text": []}, {"name": "transform.EuclideanTransform.__init__()", "path": "api/skimage.transform#skimage.transform.EuclideanTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.frt2()", "path": "api/skimage.transform#skimage.transform.frt2", "type": "transform", "text": ["Compute the 2-dimensional finite radon transform (FRT) for an n x n integer array.", "A 2-D square n x n integer array.", "Finite Radon Transform array of (n+1) x n integer coefficients.", "See also", "The two-dimensional inverse FRT.", "The FRT has a unique inverse if and only if n is prime. [FRT] The idea for this algorithm is due to Vlad Negnevitski.", "A. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)", "Generate a test image: Use a prime number for the array dimensions", "Apply the Finite Radon Transform:"]}, {"name": "transform.FundamentalMatrixTransform", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.GeometricTransform", "Fundamental matrix transformation.", "The fundamental matrix relates corresponding points between a pair of uncalibrated images. The matrix transforms homogeneous image points in one image to epipolar lines in the other image.", "The fundamental matrix is only defined for a pair of moving images. In the case of pure rotation or planar scenes, the homography describes the geometric relation between two images (ProjectiveTransform). If the intrinsic calibration of the images is known, the essential matrix describes the metric relation between the two images (EssentialMatrixTransform).", "Fundamental matrix.", "Hartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.", "Fundamental matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate fundamental matrix using 8-point algorithm.", "The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Apply inverse transformation.", "Destination coordinates.", "Epipolar lines in the source image.", "Compute the Sampson distance.", "The Sampson distance is the first approximation to the geometric error.", "Source coordinates.", "Destination coordinates.", "Sampson distance."]}, {"name": "transform.FundamentalMatrixTransform.estimate()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.estimate", "type": "transform", "text": ["Estimate fundamental matrix using 8-point algorithm.", "The 8-point algorithm requires at least 8 corresponding point pairs for a well-conditioned solution, otherwise the over-determined solution is estimated.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.FundamentalMatrixTransform.inverse()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.inverse", "type": "transform", "text": ["Apply inverse transformation.", "Destination coordinates.", "Epipolar lines in the source image."]}, {"name": "transform.FundamentalMatrixTransform.residuals()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.residuals", "type": "transform", "text": ["Compute the Sampson distance.", "The Sampson distance is the first approximation to the geometric error.", "Source coordinates.", "Destination coordinates.", "Sampson distance."]}, {"name": "transform.FundamentalMatrixTransform.__init__()", "path": "api/skimage.transform#skimage.transform.FundamentalMatrixTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.hough_circle()", "path": "api/skimage.transform#skimage.transform.hough_circle", "type": "transform", "text": ["Perform a circular Hough transform.", "Input image with nonzero values representing edges.", "Radii at which to compute the Hough transform. Floats are converted to integers.", "Normalize the accumulator with the number of pixels used to draw the radius.", "Extend the output size by twice the largest radius in order to detect centers outside the input picture.", "Hough transform accumulator for each radius. R designates the larger radius if full_output is True. Otherwise, R = 0."]}, {"name": "transform.hough_circle_peaks()", "path": "api/skimage.transform#skimage.transform.hough_circle_peaks", "type": "transform", "text": ["Return peaks in a circle Hough transform.", "Identifies most prominent circles separated by certain distances in given Hough spaces. Non-maximum suppression with different sizes is applied separately in the first and second dimension of the Hough space to identify peaks. For circles with different radius but close in distance, only the one with highest peak is kept.", "Hough spaces returned by the hough_circle function.", "Radii corresponding to Hough spaces.", "Minimum distance separating centers in the x dimension.", "Minimum distance separating centers in the y dimension.", "Minimum intensity of peaks in each Hough space. Default is 0.5 * max(hspace).", "Maximum number of peaks in each Hough space. When the number of peaks exceeds num_peaks, only num_peaks coordinates based on peak intensity are considered for the corresponding radius.", "Maximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.", "If True, normalize the accumulator by the radius to sort the prominent peaks.", "Peak values in Hough space, x and y center coordinates and radii.", "Circles with bigger radius have higher peaks in Hough space. If larger circles are preferred over smaller ones, normalize should be False. Otherwise, circles will be returned in the order of decreasing voting number."]}, {"name": "transform.hough_ellipse()", "path": "api/skimage.transform#skimage.transform.hough_ellipse", "type": "transform", "text": ["Perform an elliptical Hough transform.", "Input image with nonzero values representing edges.", "Accumulator threshold value.", "Bin size on the minor axis used in the accumulator.", "Minimal major axis length.", "Maximal minor axis length. If None, the value is set to the half of the smaller image dimension.", "Where (yc, xc) is the center, (a, b) the major and minor axes, respectively. The orientation value follows skimage.draw.ellipse_perimeter convention.", "The accuracy must be chosen to produce a peak in the accumulator distribution. In other words, a flat accumulator distribution with low values may be caused by a too low bin size.", "Xie, Yonghong, and Qiang Ji. \u201cA new efficient ellipse detection method.\u201d Pattern Recognition, 2002. Proceedings. 16th International Conference on. Vol. 2. IEEE, 2002"]}, {"name": "transform.hough_line()", "path": "api/skimage.transform#skimage.transform.hough_line", "type": "transform", "text": ["Perform a straight line Hough transform.", "Input image with nonzero values representing edges.", "Angles at which to compute the transform, in radians. Defaults to a vector of 180 angles evenly spaced from -pi/2 to pi/2.", "Hough transform accumulator.", "Angles at which the transform is computed, in radians.", "Distance values.", "The origin is the top left corner of the original image. X and Y axis are horizontal and vertical edges respectively. The distance is the minimal algebraic distance from the origin to the detected line. The angle accuracy can be improved by decreasing the step size in the theta array.", "Generate a test image:", "Apply the Hough transform:", "(Source code, png, pdf)"]}, {"name": "transform.hough_line_peaks()", "path": "api/skimage.transform#skimage.transform.hough_line_peaks", "type": "transform", "text": ["Return peaks in a straight line Hough transform.", "Identifies most prominent lines separated by a certain angle and distance in a Hough transform. Non-maximum suppression with different sizes is applied separately in the first (distances) and second (angles) dimension of the Hough space to identify peaks.", "Hough space returned by the hough_line function.", "Angles returned by the hough_line function. Assumed to be continuous. (angles[-1] - angles[0] == PI).", "Distances returned by the hough_line function.", "Minimum distance separating lines (maximum filter size for first dimension of hough space).", "Minimum angle separating lines (maximum filter size for second dimension of hough space).", "Minimum intensity of peaks. Default is 0.5 * max(hspace).", "Maximum number of peaks. When the number of peaks exceeds num_peaks, return num_peaks coordinates based on peak intensity.", "Peak values in Hough space, angles and distances."]}, {"name": "transform.ifrt2()", "path": "api/skimage.transform#skimage.transform.ifrt2", "type": "transform", "text": ["Compute the 2-dimensional inverse finite radon transform (iFRT) for an (n+1) x n integer array.", "A 2-D (n+1) row x n column integer array.", "Inverse Finite Radon Transform array of n x n integer coefficients.", "See also", "The two-dimensional FRT", "The FRT has a unique inverse if and only if n is prime. See [1] for an overview. The idea for this algorithm is due to Vlad Negnevitski.", "A. Kingston and I. Svalbe, \u201cProjective transforms on periodic discrete image arrays,\u201d in P. Hawkes (Ed), Advances in Imaging and Electron Physics, 139 (2006)", "Apply the Finite Radon Transform:", "Apply the Inverse Finite Radon Transform to recover the input", "Check that it\u2019s identical to the original"]}, {"name": "transform.integral_image()", "path": "api/skimage.transform#skimage.transform.integral_image", "type": "transform", "text": ["Integral image / summed area table.", "The integral image contains the sum of all elements above and to the left of it, i.e.:", "Input image.", "Integral image/summed area table of same shape as input image.", "F.C. Crow, \u201cSummed-area tables for texture mapping,\u201d ACM SIGGRAPH Computer Graphics, vol. 18, 1984, pp. 207-212."]}, {"name": "transform.integrate()", "path": "api/skimage.transform#skimage.transform.integrate", "type": "transform", "text": ["Use an integral image to integrate over a given window.", "Integral image.", "Coordinates of top left corner of window(s). Each tuple in the list contains the starting row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2,\u2026), \u2026].", "Coordinates of bottom right corner of window(s). Each tuple in the list containing the end row, col, \u2026 index i.e [(row_win1, col_win1, \u2026), (row_win2, col_win2, \u2026), \u2026].", "Integral (sum) over the given window(s)."]}, {"name": "transform.iradon()", "path": "api/skimage.transform#skimage.transform.iradon", "type": "transform", "text": ["Inverse radon transform.", "Reconstruct an image from the radon transform, using the filtered back projection algorithm.", "Image containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.", "Reconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).", "Number of rows and columns in the reconstruction.", "Filter used in frequency domain filtering. Ramp filter used by default. Filters available: ramp, shepp-logan, cosine, hamming, hann. Assign None to use no filter.", "Interpolation method used in reconstruction. Methods available: \u2018linear\u2019, \u2018nearest\u2019, and \u2018cubic\u2019 (\u2018cubic\u2019 is slow).", "Assume the reconstructed image is zero outside the inscribed circle. Also changes the default output_size to match the behaviour of radon called with circle=True.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Reconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).", "Changed in version 0.19: In iradon, filter argument is deprecated in favor of filter_name.", "It applies the Fourier slice theorem to reconstruct an image by multiplying the frequency domain of the filter with the FFT of the projection data. This algorithm is called filtered back projection.", "AC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.", "B.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989"]}, {"name": "transform.iradon_sart()", "path": "api/skimage.transform#skimage.transform.iradon_sart", "type": "transform", "text": ["Inverse radon transform.", "Reconstruct an image from the radon transform, using a single iteration of the Simultaneous Algebraic Reconstruction Technique (SART) algorithm.", "Image containing radon transform (sinogram). Each column of the image corresponds to a projection along a different angle. The tomography rotation axis should lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.", "Reconstruction angles (in degrees). Default: m angles evenly spaced between 0 and 180 (if the shape of radon_image is (N, M)).", "Image containing an initial reconstruction estimate. Shape of this array should be (radon_image.shape[0], radon_image.shape[0]). The default is an array of zeros.", "Shift the projections contained in radon_image (the sinogram) by this many pixels before reconstructing the image. The i\u2019th value defines the shift of the i\u2019th column of radon_image.", "Force all values in the reconstructed tomogram to lie in the range [clip[0], clip[1]]", "Relaxation parameter for the update step. A higher value can improve the convergence rate, but one runs the risk of instabilities. Values close to or higher than 1 are not recommended.", "Output data type, must be floating point. By default, if input data type is not float, input is cast to double, otherwise dtype is set to input data type.", "Reconstructed image. The rotation axis will be located in the pixel with indices (reconstructed.shape[0] // 2, reconstructed.shape[1] // 2).", "Algebraic Reconstruction Techniques are based on formulating the tomography reconstruction problem as a set of linear equations. Along each ray, the projected value is the sum of all the values of the cross section along the ray. A typical feature of SART (and a few other variants of algebraic techniques) is that it samples the cross section at equidistant points along the ray, using linear interpolation between the pixel values of the cross section. The resulting set of linear equations are then solved using a slightly modified Kaczmarz method.", "When using SART, a single iteration is usually sufficient to obtain a good reconstruction. Further iterations will tend to enhance high-frequency information, but will also often increase the noise.", "AC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.", "AH Andersen, AC Kak, \u201cSimultaneous algebraic reconstruction technique (SART): a superior implementation of the ART algorithm\u201d, Ultrasonic Imaging 6 pp 81\u201394 (1984)", "S Kaczmarz, \u201cAngen\u00e4herte aufl\u00f6sung von systemen linearer gleichungen\u201d, Bulletin International de l\u2019Academie Polonaise des Sciences et des Lettres 35 pp 355\u2013357 (1937)", "Kohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.", "Kaczmarz\u2019 method, Wikipedia, https://en.wikipedia.org/wiki/Kaczmarz_method"]}, {"name": "transform.matrix_transform()", "path": "api/skimage.transform#skimage.transform.matrix_transform", "type": "transform", "text": ["Apply 2D matrix transform.", "x, y coordinates to transform", "Homogeneous transformation matrix.", "Transformed coordinates."]}, {"name": "transform.order_angles_golden_ratio()", "path": "api/skimage.transform#skimage.transform.order_angles_golden_ratio", "type": "transform", "text": ["Order angles to reduce the amount of correlated information in subsequent projections.", "Projection angles in degrees. Duplicate angles are not allowed.", "The returned generator yields indices into theta such that theta[indices] gives the approximate golden ratio ordering of the projections. In total, len(theta) indices are yielded. All non-negative integers < len(theta) are yielded exactly once.", "The method used here is that of the golden ratio introduced by T. Kohler.", "Kohler, T. \u201cA projection access scheme for iterative reconstruction based on the golden section.\u201d Nuclear Science Symposium Conference Record, 2004 IEEE. Vol. 6. IEEE, 2004.", "Winkelmann, Stefanie, et al. \u201cAn optimal radial profile order based on the Golden Ratio for time-resolved MRI.\u201d Medical Imaging, IEEE Transactions on 26.1 (2007): 68-76."]}, {"name": "transform.PiecewiseAffineTransform", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.GeometricTransform", "2D piecewise affine transformation.", "Control points are used to define the mapping. The transform is based on a Delaunay triangulation of the points to form a mesh. Each triangle is used to find a local affine transform.", "Affine transformations for each triangle in the mesh.", "Inverse affine transformations for each triangle in the mesh.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Apply inverse transformation.", "Coordinates outside of the mesh will be set to - 1.", "Source coordinates.", "Transformed coordinates."]}, {"name": "transform.PiecewiseAffineTransform.estimate()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.estimate", "type": "transform", "text": ["Estimate the transformation from a set of corresponding points.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.PiecewiseAffineTransform.inverse()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.inverse", "type": "transform", "text": ["Apply inverse transformation.", "Coordinates outside of the mesh will be set to - 1.", "Source coordinates.", "Transformed coordinates."]}, {"name": "transform.PiecewiseAffineTransform.__init__()", "path": "api/skimage.transform#skimage.transform.PiecewiseAffineTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.PolynomialTransform", "path": "api/skimage.transform#skimage.transform.PolynomialTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.GeometricTransform", "2D polynomial transformation.", "Has the following form:", "Polynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].", "Polynomial coefficients where N * 2 = (order + 1) * (order + 2). So, a_ji is defined in params[0, :] and b_ji in params[1, :].", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "The transformation is defined as:", "These equations can be transformed to the following form:", "which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where:", "In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.", "Source coordinates.", "Destination coordinates.", "Polynomial order (number of coefficients is order + 1).", "True, if model estimation succeeds.", "Apply inverse transformation.", "Destination coordinates.", "Source coordinates."]}, {"name": "transform.PolynomialTransform.estimate()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.estimate", "type": "transform", "text": ["Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "The transformation is defined as:", "These equations can be transformed to the following form:", "which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where:", "In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.", "Source coordinates.", "Destination coordinates.", "Polynomial order (number of coefficients is order + 1).", "True, if model estimation succeeds."]}, {"name": "transform.PolynomialTransform.inverse()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.inverse", "type": "transform", "text": ["Apply inverse transformation.", "Destination coordinates.", "Source coordinates."]}, {"name": "transform.PolynomialTransform.__init__()", "path": "api/skimage.transform#skimage.transform.PolynomialTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.probabilistic_hough_line()", "path": "api/skimage.transform#skimage.transform.probabilistic_hough_line", "type": "transform", "text": ["Return lines from a progressive probabilistic line Hough transform.", "Input image with nonzero values representing edges.", "Threshold", "Minimum accepted length of detected lines. Increase the parameter to extract longer lines.", "Maximum gap between pixels to still form a line. Increase the parameter to merge broken lines more aggressively.", "Angles at which to compute the transform, in radians. If None, use a range from -pi/2 to pi/2.", "Seed to initialize the random number generator.", "List of lines identified, lines in format ((x0, y0), (x1, y1)), indicating line start and end.", "C. Galamhos, J. Matas and J. Kittler, \u201cProgressive probabilistic Hough transform for line detection\u201d, in IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1999."]}, {"name": "transform.ProjectiveTransform", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.GeometricTransform", "Projective transformation.", "Apply a projective transformation (homography) on coordinates.", "For each homogeneous coordinate \\(\\mathbf{x} = [x, y, 1]^T\\), its target position is calculated by multiplying with the given matrix, \\(H\\), to give \\(H \\mathbf{x}\\):", "E.g., to rotate by theta degrees clockwise, the matrix should be:", "or, to translate x by 10 and y by 20:", "Homogeneous transformation matrix.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "The transformation is defined as:", "These equations can be transformed to the following form:", "which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where:", "In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.", "In case of the affine transformation the coefficients c0 and c1 are 0. Thus the system of equations is:", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds.", "Apply inverse transformation.", "Destination coordinates.", "Source coordinates."]}, {"name": "transform.ProjectiveTransform.estimate()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.estimate", "type": "transform", "text": ["Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "The transformation is defined as:", "These equations can be transformed to the following form:", "which exist for each set of corresponding points, so we have a set of N * 2 equations. The coefficients appear linearly so we can write A x = 0, where:", "In case of total least-squares the solution of this homogeneous system of equations is the right singular vector of A which corresponds to the smallest singular value normed by the coefficient c3.", "In case of the affine transformation the coefficients c0 and c1 are 0. Thus the system of equations is:", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.ProjectiveTransform.inverse()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.inverse", "type": "transform", "text": ["Apply inverse transformation.", "Destination coordinates.", "Source coordinates."]}, {"name": "transform.ProjectiveTransform.__init__()", "path": "api/skimage.transform#skimage.transform.ProjectiveTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.pyramid_expand()", "path": "api/skimage.transform#skimage.transform.pyramid_expand", "type": "transform", "text": ["Upsample and then smooth image.", "Input image.", "Upscale factor.", "Sigma for Gaussian filter. Default is 2 * upscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of upsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Upsampled and smoothed float image.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf"]}, {"name": "transform.pyramid_gaussian()", "path": "api/skimage.transform#skimage.transform.pyramid_gaussian", "type": "transform", "text": ["Yield images of the Gaussian pyramid formed by the input image.", "Recursively applies the pyramid_reduce function to the image, and yields the downscaled images.", "Note that the first image of the pyramid will be the original, unscaled image. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.", "Input image.", "Number of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.", "Downscale factor.", "Sigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of downsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Generator yielding pyramid layers as float images.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf"]}, {"name": "transform.pyramid_laplacian()", "path": "api/skimage.transform#skimage.transform.pyramid_laplacian", "type": "transform", "text": ["Yield images of the laplacian pyramid formed by the input image.", "Each layer contains the difference between the downsampled and the downsampled, smoothed image:", "Note that the first image of the pyramid will be the difference between the original, unscaled image and its smoothed version. The total number of images is max_layer + 1. In case all layers are computed, the last image is either a one-pixel image or the image where the reduction does not change its shape.", "Input image.", "Number of layers for the pyramid. 0th layer is the original image. Default is -1 which builds all possible layers.", "Downscale factor.", "Sigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of downsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Generator yielding pyramid layers as float images.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf", "http://sepwww.stanford.edu/data/media/public/sep/morgan/texturematch/paper_html/node3.html"]}, {"name": "transform.pyramid_reduce()", "path": "api/skimage.transform#skimage.transform.pyramid_reduce", "type": "transform", "text": ["Smooth and then downsample image.", "Input image.", "Downscale factor.", "Sigma for Gaussian filter. Default is 2 * downscale / 6.0 which corresponds to a filter mask twice the size of the scale factor that covers more than 99% of the Gaussian distribution.", "Order of splines used in interpolation of downsampling. See skimage.transform.warp for detail.", "The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to \u2018constant\u2019.", "Value to fill past edges of input if mode is \u2018constant\u2019.", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Smoothed and downsampled float image.", "http://persci.mit.edu/pub_pdfs/pyramid83.pdf"]}, {"name": "transform.radon()", "path": "api/skimage.transform#skimage.transform.radon", "type": "transform", "text": ["Calculates the radon transform of an image given specified projection angles.", "Input image. The rotation axis will be located in the pixel with indices (image.shape[0] // 2, image.shape[1] // 2).", "Projection angles (in degrees). If None, the value is set to np.arange(180).", "Assume image is zero outside the inscribed circle, making the width of each projection (the first dimension of the sinogram) equal to min(image.shape).", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Radon transform (sinogram). The tomography rotation axis will lie at the pixel index radon_image.shape[0] // 2 along the 0th dimension of radon_image.", "Based on code of Justin K. Romberg (https://www.clear.rice.edu/elec431/projects96/DSP/bpanalysis.html)", "AC Kak, M Slaney, \u201cPrinciples of Computerized Tomographic Imaging\u201d, IEEE Press 1988.", "B.R. Ramesh, N. Srinivasa, K. Rajgopal, \u201cAn Algorithm for Computing the Discrete Radon Transform With Some Applications\u201d, Proceedings of the Fourth IEEE Region 10 International Conference, TENCON \u201889, 1989"]}, {"name": "transform.rescale()", "path": "api/skimage.transform#skimage.transform.rescale", "type": "transform", "text": ["Scale image by a certain factor.", "Performs interpolation to up-scale or down-scale N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.", "Input image.", "Scale factors. Separate scale factors can be defined as (rows, cols[, \u2026][, dim]).", "Scaled version of the input.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Whether the last axis of the image is to be interpreted as multiple channels or another spatial dimension.", "Whether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.", "Standard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor.", "Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]."]}, {"name": "transform.resize()", "path": "api/skimage.transform#skimage.transform.resize", "type": "transform", "text": ["Resize image to match a certain size.", "Performs interpolation to up-size or down-size N-dimensional images. Note that anti-aliasing should be enabled when down-sizing images to avoid aliasing artifacts. For down-sampling with an integer factor also see skimage.transform.downscale_local_mean.", "Input image.", "Size of the generated output image (rows, cols[, \u2026][, dim]). If dim is not provided, the number of channels is preserved. In case the number of input channels does not equal the number of output channels a n-dimensional interpolation is applied.", "Resized version of the input.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Whether to apply a Gaussian filter to smooth the image prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. If input image data type is bool, no anti-aliasing is applied.", "Standard deviation for Gaussian filtering to avoid aliasing artifacts. By default, this value is chosen as (s - 1) / 2 where s is the down-scaling factor, where s > 1. For the up-size case, s < 1, no anti-aliasing is performed prior to rescaling.", "Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]."]}, {"name": "transform.rotate()", "path": "api/skimage.transform#skimage.transform.rotate", "type": "transform", "text": ["Rotate image by a certain angle around its center.", "Input image.", "Rotation angle in degrees in counter-clockwise direction.", "Determine whether the shape of the output image will be automatically calculated, so the complete rotated image exactly fits. Default is False.", "The rotation center. If center=None, the image is rotated around its center, i.e. center=(cols / 2 - 0.5, rows / 2 - 0.5). Please note that this parameter is (cols, rows), contrary to normal skimage ordering.", "Rotated version of the input.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "Modes \u2018reflect\u2019 and \u2018symmetric\u2019 are similar, but differ in whether the edge pixels are duplicated during the reflection. As an example, if an array has values [0, 1, 2] and was padded to the right by four values using symmetric, the result would be [0, 1, 2, 2, 1, 0, 0], while for reflect it would be [0, 1, 2, 1, 0, 1, 2]."]}, {"name": "transform.SimilarityTransform", "path": "api/skimage.transform#skimage.transform.SimilarityTransform", "type": "transform", "text": ["Bases: skimage.transform._geometric.EuclideanTransform", "2D similarity transformation.", "Has the following form:", "where s is a scale factor and the homogeneous transformation matrix is:", "The similarity transformation extends the Euclidean transformation with a single scaling factor in addition to the rotation and translation parameters.", "Homogeneous transformation matrix.", "Scale factor.", "Rotation angle in counter-clockwise direction as radians.", "x, y translation parameters.", "Homogeneous transformation matrix.", "Initialize self. See help(type(self)) for accurate signature.", "Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.SimilarityTransform.estimate()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.estimate", "type": "transform", "text": ["Estimate the transformation from a set of corresponding points.", "You can determine the over-, well- and under-determined parameters with the total least-squares method.", "Number of source and destination coordinates must match.", "Source coordinates.", "Destination coordinates.", "True, if model estimation succeeds."]}, {"name": "transform.SimilarityTransform.scale()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.scale", "type": "transform", "text": []}, {"name": "transform.SimilarityTransform.__init__()", "path": "api/skimage.transform#skimage.transform.SimilarityTransform.__init__", "type": "transform", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "transform.swirl()", "path": "api/skimage.transform#skimage.transform.swirl", "type": "transform", "text": ["Perform a swirl transformation.", "Input image.", "Center coordinate of transformation.", "The amount of swirling applied.", "The extent of the swirl in pixels. The effect dies out rapidly beyond radius.", "Additional rotation applied to the image.", "Swirled version of the input.", "Shape of the output image generated. By default the shape of the input image is preserved.", "The order of the spline interpolation, default is 0 if image.dtype is bool and 1 otherwise. The order has to be in the range 0-5. See skimage.transform.warp for detail.", "Points outside the boundaries of the input are filled according to the given mode, with \u2018constant\u2019 used as the default. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html"]}, {"name": "transform.warp()", "path": "api/skimage.transform#skimage.transform.warp", "type": "transform", "text": ["Warp an image according to a given coordinate transformation.", "Input image.", "Inverse coordinate map, which transforms coordinates in the output images into their corresponding coordinates in the input image.", "There are a number of different options to define this map, depending on the dimensionality of the input image. A 2-D image can have 2 dimensions for gray-scale images, or 3 dimensions with color information.", "Note, that a (3, 3) matrix is interpreted as a homogeneous transformation matrix, so you cannot interpolate values from a 3-D input, if the output is of shape (3,).", "See example section for usage.", "Keyword arguments passed to inverse_map.", "Shape of the output image generated. By default the shape of the input image is preserved. Note that, even for multi-band images, only rows and columns need to be specified.", "Default is 0 if image.dtype is bool and 1 otherwise.", "Points outside the boundaries of the input are filled according to the given mode. Modes match the behaviour of numpy.pad.", "Used in conjunction with mode \u2018constant\u2019, the value outside the image boundaries.", "Whether to clip the output to the range of values of the input image. This is enabled by default, since higher order interpolation may produce values outside the given input range.", "Whether to keep the original range of values. Otherwise, the input image is converted according to the conventions of img_as_float. Also see https://scikit-image.org/docs/dev/user_guide/data_types.html", "The warped input image.", "The following image warps are all equal but differ substantially in execution time. The image is shifted to the bottom.", "Use a geometric transform to warp an image (fast):", "Use a callable (slow):", "Use a transformation matrix to warp an image (fast):", "You can also use the inverse of a geometric transformation (fast):", "For N-D images you can pass a coordinate array, that specifies the coordinates in the input image for every element in the output image. E.g. if you want to rescale a 3-D cube, you can do:", "Setup the coordinate array, that defines the scaling:", "Assume that the cube contains spatial data, where the first array element center is at coordinate (0.5, 0.5, 0.5) in real space, i.e. we have to account for this extra offset when scaling the image:"]}, {"name": "transform.warp_coords()", "path": "api/skimage.transform#skimage.transform.warp_coords", "type": "transform", "text": ["Build the source coordinates for the output of a 2-D image warp.", "Return input coordinates for given output coordinates. Coordinates are in the shape (P, 2), where P is the number of coordinates and each element is a (row, col) pair.", "Shape of output image (rows, cols[, bands]).", "dtype for return value (sane choices: float32 or float64).", "Coordinates for scipy.ndimage.map_coordinates, that will yield an image of shape (orows, ocols, bands) by drawing from source points according to the coord_transform_fn.", "This is a lower-level routine that produces the source coordinates for 2-D images used by warp().", "It is provided separately from warp to give additional flexibility to users who would like, for example, to re-use a particular coordinate mapping, to use specific dtypes at various points along the the image-warping process, or to implement different post-processing logic than warp performs after the call to ndi.map_coordinates.", "Produce a coordinate map that shifts an image up and to the right:"]}, {"name": "transform.warp_polar()", "path": "api/skimage.transform#skimage.transform.warp_polar", "type": "transform", "text": ["Remap image to polar or log-polar coordinates space.", "Input image. Only 2-D arrays are accepted by default. If multichannel=True, 3-D arrays are accepted and the last axis is interpreted as multiple channels.", "Point in image that represents the center of the transformation (i.e., the origin in cartesian space). Values can be of type float. If no value is given, the center is assumed to be the center point of the image.", "Radius of the circle that bounds the area to be transformed.", "Specify whether the image warp is polar or log-polar. Defaults to \u2018linear\u2019.", "Whether the image is a 3-D array in which the third axis is to be interpreted as multiple channels. If set to False (default), only 2-D arrays are accepted.", "Passed to transform.warp.", "The polar or log-polar warped image.", "Perform a basic polar warp on a grayscale image:", "Perform a log-polar warp on a grayscale image:", "Perform a log-polar warp on a grayscale image while specifying center, radius, and output shape:", "Perform a log-polar warp on a color image:"]}, {"name": "Tutorials", "path": "user_guide/tutorials", "type": "Guide", "text": []}, {"name": "User Guide", "path": "user_guide", "type": "Guide", "text": []}, {"name": "util", "path": "api/skimage.util", "type": "util", "text": ["skimage.util.apply_parallel(function, array)", "Map a function in parallel across an array.", "skimage.util.compare_images(image1, image2)", "Return an image showing the differences between two images.", "skimage.util.crop(ar, crop_width[, copy, order])", "Crop array ar by crop_width along each dimension.", "skimage.util.dtype_limits(image[, clip_negative])", "Return intensity limits, i.e.", "skimage.util.img_as_bool(image[, force_copy])", "Convert an image to boolean format.", "skimage.util.img_as_float(image[, force_copy])", "Convert an image to floating point format.", "skimage.util.img_as_float32(image[, force_copy])", "Convert an image to single-precision (32-bit) floating point format.", "skimage.util.img_as_float64(image[, force_copy])", "Convert an image to double-precision (64-bit) floating point format.", "skimage.util.img_as_int(image[, force_copy])", "Convert an image to 16-bit signed integer format.", "skimage.util.img_as_ubyte(image[, force_copy])", "Convert an image to 8-bit unsigned integer format.", "skimage.util.img_as_uint(image[, force_copy])", "Convert an image to 16-bit unsigned integer format.", "skimage.util.invert(image[, signed_float])", "Invert an image.", "skimage.util.map_array(input_arr, \u2026[, out])", "Map values from input array from input_vals to output_vals.", "skimage.util.montage(arr_in[, fill, \u2026])", "Create a montage of several single- or multichannel images.", "skimage.util.pad(array, pad_width[, mode])", "Pad an array.", "skimage.util.random_noise(image[, mode, \u2026])", "Function to add random noise of various types to a floating-point image.", "skimage.util.regular_grid(ar_shape, n_points)", "Find n_points regularly spaced along ar_shape.", "skimage.util.regular_seeds(ar_shape, n_points)", "Return an image with ~`n_points` regularly-spaced nonzero pixels.", "skimage.util.unique_rows(ar)", "Remove repeated rows from a 2D array.", "skimage.util.view_as_blocks(arr_in, block_shape)", "Block view of the input n-dimensional array (using re-striding).", "skimage.util.view_as_windows(arr_in, \u2026[, step])", "Rolling window view of the input n-dimensional array.", "Map a function in parallel across an array.", "Split an array into possibly overlapping chunks of a given depth and boundary type, call the given function in parallel on the chunks, combine the chunks and return the resulting array.", "Function to be mapped which takes an array as an argument.", "Array which the function will be applied to.", "A single integer is interpreted as the length of one side of a square chunk that should be tiled across the array. One tuple of length array.ndim represents the shape of a chunk, and it is tiled across the array. A list of tuples of length ndim, where each sub-tuple is a sequence of chunk sizes along the corresponding dimension. If None, the array is broken up into chunks based on the number of available cpus. More information about chunks is in the documentation here.", "Integer equal to the depth of the added boundary cells. Defaults to zero.", "type of external boundary padding.", "Tuple of arguments to be passed to the function.", "Dictionary of keyword arguments to be passed to the function.", "The data-type of the function output. If None, Dask will attempt to infer this by calling the function on data of shape (1,) * ndim. For functions expecting RGB or multichannel data this may be problematic. In such cases, the user should manually specify this dtype argument instead.", "New in version 0.18: dtype was added in 0.18.", "If chunks is None and multichannel is True, this function will keep only a single chunk along the channels axis. When depth is specified as a scalar value, that depth will be applied only to the non-channels axes (a depth of 0 will be used along the channels axis). If the user manually specified both chunks and a depth tuple, then this argument will have no effect.", "New in version 0.18: multichannel was added in 0.18.", "If True, compute eagerly returning a NumPy Array. If False, compute lazily returning a Dask Array. If None (default), compute based on array type provided (eagerly for NumPy Arrays and lazily for Dask Arrays).", "Returns the result of the applying the operation. Type is dependent on the compute argument.", "Numpy edge modes \u2018symmetric\u2019, \u2018wrap\u2019, and \u2018edge\u2019 are converted to the equivalent dask boundary modes \u2018reflect\u2019, \u2018periodic\u2019 and \u2018nearest\u2019, respectively. Setting compute=False can be useful for chaining later operations. For example region selection to preview a result or storing large data to disk instead of loading in memory.", "Return an image showing the differences between two images.", "New in version 0.16.", "Images to process, must be of the same shape.", "Method used for the comparison. Valid values are {\u2018diff\u2019, \u2018blend\u2019, \u2018checkerboard\u2019}. Details are provided in the note section.", "Used only for the checkerboard method. Specifies the number of tiles (row, column) to divide the image.", "Image showing the differences.", "'diff' computes the absolute difference between the two images. 'blend' computes the mean value. 'checkerboard' makes tiles of dimension n_tiles that display alternatively the first and the second image.", "Crop array ar by crop_width along each dimension.", "Input array.", "Number of values to remove from the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) specifies unique crop widths at the start and end of each axis. ((before, after),) or (before, after) specifies a fixed start and end crop for every axis. (n,) or n for integer n is a shortcut for before = after = n for all axes.", "If True, ensure the returned array is a contiguous copy. Normally, a crop operation will return a discontiguous view of the underlying input array.", "If copy==True, control the memory layout of the copy. See np.copy.", "The cropped array. If copy=False (default), this is a sliced view of the input array.", "Return intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.", "Input image.", "If True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.", "Lower and upper intensity limits.", "Convert an image to boolean format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False.", "Convert an image to floating point format.", "This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0].", "Convert an image to single-precision (32-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0].", "Convert an image to double-precision (64-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0].", "Convert an image to 16-bit signed integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values.", "Convert an image to 8-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 255.", "Convert an image to 16-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 65535.", "Invert an image.", "Invert the intensity range of the input image, so that the dtype maximum is now the dtype minimum, and vice-versa. This operation is slightly different depending on the input dtype:", "See the examples for clarification.", "Input image.", "If True and the image is of type float, the range is assumed to be [-1, 1]. If False and the image is of type float, the range is assumed to be [0, 1].", "Inverted image.", "Ideally, for signed integers we would simply multiply by -1. However, signed integer ranges are asymmetric. For example, for np.int8, the range of possible values is [-128, 127], so that -128 * -1 equals -128! By subtracting from -1, we correctly map the maximum dtype value to the minimum.", "Use rolling-ball algorithm for estimating background intensity", "Map values from input array from input_vals to output_vals.", "The input label image.", "The values to map from.", "The values to map to.", "The output array. Will be created if not provided. It should have the same dtype as output_vals.", "The array of mapped values.", "Create a montage of several single- or multichannel images.", "Create a rectangular montage from an input array representing an ensemble of equally shaped single- (gray) or multichannel (color) images.", "For example, montage(arr_in) called with the following arr_in", "1", "2", "3", "will return", "1", "2", "3", "where the \u2018*\u2019 patch will be determined by the fill parameter.", "An array representing an ensemble of K images of equal shape.", "Value to fill the padding areas and/or the extra tiles in the output array. Has to be float for single channel collections. For multichannel collections has to be an array-like of shape of number of channels. If mean, uses the mean value over all images.", "Whether to rescale the intensity of each image to [0, 1].", "The desired grid shape for the montage (ntiles_row, ntiles_column). The default aspect ratio is square.", "The size of the spacing between the tiles and between the tiles and the borders. If non-zero, makes the boundaries of individual images easier to perceive.", "If True, the last arr_in dimension is threated as a color channel, otherwise as spatial.", "Output array with input images glued together (including padding p).", "Pad an array.", "The array to pad.", "Number of values padded to the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) unique pad widths for each axis. ((before, after),) yields same before and after pad for each axis. (pad,) or int is a shortcut for before = after = pad width for all axes.", "One of the following string values or a user supplied function.", "Pads with a constant value.", "Pads with the edge values of array.", "Pads with the linear ramp between end_value and the array edge value.", "Pads with the maximum value of all or part of the vector along each axis.", "Pads with the mean value of all or part of the vector along each axis.", "Pads with the median value of all or part of the vector along each axis.", "Pads with the minimum value of all or part of the vector along each axis.", "Pads with the reflection of the vector mirrored on the first and last values of the vector along each axis.", "Pads with the reflection of the vector mirrored along the edge of the array.", "Pads with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.", "Pads with undefined values.", "New in version 1.17.", "Padding function, see Notes.", "Used in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge of each axis used to calculate the statistic value.", "((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each axis.", "((before, after),) yields same before and after statistic lengths for each axis.", "(stat_length,) or int is a shortcut for before = after = statistic length for all axes.", "Default is None, to use the entire axis.", "Used in \u2018constant\u2019. The values to set the padded values for each axis.", "((before_1, after_1), ... (before_N, after_N)) unique pad constants for each axis.", "((before, after),) yields same before and after constants for each axis.", "(constant,) or constant is a shortcut for before = after = constant for all axes.", "Default is 0.", "Used in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp and that will form the edge of the padded array.", "((before_1, after_1), ... (before_N, after_N)) unique end values for each axis.", "((before, after),) yields same before and after end values for each axis.", "(constant,) or constant is a shortcut for before = after = constant for all axes.", "Default is 0.", "Used in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an unaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended part of the array is created by subtracting the reflected values from two times the edge value.", "Padded array of rank equal to array with shape increased according to pad_width.", "New in version 1.7.0.", "For an array with rank greater than 1, some of the padding of later axes is calculated from padding of previous axes. This is easiest to think about with a rank 2 array where the corners of the padded array are calculated by using padded values from the first axis.", "The padding function, if used, should modify a rank 1 array in-place. It has the following signature:", "where", "A rank 1 array already padded with zeros. Padded values are vector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].", "A 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded at the beginning of vector where iaxis_pad_width[1] represents the number of values padded at the end of vector.", "The axis currently being calculated.", "Any keyword arguments the function requires.", "Function to add random noise of various types to a floating-point image.", "Input image data. Will be converted to float.", "One of the following strings, selecting the type of noise to add:", "local variance at each point of image.", "-1 (for signed images).", "low_val is 0 for unsigned images or -1 for signed images.", "n is Gaussian noise with specified mean & variance.", "If provided, this will set the random seed before generating noise, for valid pseudo-random comparisons.", "If True (default), the output will be clipped after noise applied for modes \u2018speckle\u2019, \u2018poisson\u2019, and \u2018gaussian\u2019. This is needed to maintain the proper image data range. If False, clipping is not applied, and the output may extend beyond the range [-1, 1].", "Mean of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Default : 0.", "Variance of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Note: variance = (standard deviation) ** 2. Default : 0.01", "Array of positive floats, same shape as image, defining the local variance at every image point. Used in \u2018localvar\u2019.", "Proportion of image pixels to replace with noise on range [0, 1]. Used in \u2018salt\u2019, \u2018pepper\u2019, and \u2018salt & pepper\u2019. Default : 0.05", "Proportion of salt vs. pepper noise for \u2018s&p\u2019 on range [0, 1]. Higher values represent more salt. Default : 0.5 (equal amounts)", "Output floating-point image data on range [0, 1] or [-1, 1] if the input image was unsigned or signed, respectively.", "Speckle, Poisson, Localvar, and Gaussian noise may generate noise outside the valid image range. The default is to clip (not alias) these values, but they may be preserved by setting clip=False. Note that in this case the output may contain values outside the ranges [0, 1] or [-1, 1]. Use this option with care.", "Because of the prevalence of exclusively positive floating-point images in intermediate calculations, it is not possible to intuit if an input is signed based on dtype alone. Instead, negative values are explicitly searched for. Only if found does this function assume signed input. Unexpected results only occur in rare, poorly exposes cases (e.g. if all values are above 50 percent gray in a signed image). In this event, manually scaling the input to the positive domain will solve the problem.", "The Poisson distribution is only defined for positive integers. To apply this noise type, the number of unique values in the image is found and the next round power of two is used to scale up the floating-point result, after which it is scaled back down to the floating-point image range.", "To generate Poisson noise against a signed image, the signed image is temporarily converted to an unsigned image in the floating point domain, Poisson noise is generated, then it is returned to the original range.", "Find n_points regularly spaced along ar_shape.", "The returned points (as slices) should be as close to cubically-spaced as possible. Essentially, the points are spaced by the Nth root of the input array size, where N is the number of dimensions. However, if an array dimension cannot fit a full step size, it is \u201cdiscarded\u201d, and the computation is done for only the remaining dimensions.", "The shape of the space embedding the grid. len(ar_shape) is the number of dimensions.", "The (approximate) number of points to embed in the space.", "A slice along each dimension of ar_shape, such that the intersection of all the slices give the coordinates of regularly spaced points.", "Changed in version 0.14.1: In scikit-image 0.14.1 and 0.15, the return type was changed from a list to a tuple to ensure compatibility with Numpy 1.15 and higher. If your code requires the returned result to be a list, you may convert the output of this function to a list with:", "Return an image with ~`n_points` regularly-spaced nonzero pixels.", "The shape of the desired output image.", "The desired number of nonzero points.", "The desired data type of the output.", "The desired image.", "Remove repeated rows from a 2D array.", "In particular, if given an array of coordinates of shape (Npoints, Ndim), it will remove repeated points.", "The input array.", "A copy of the input array with repeated rows removed.", "The function will generate a copy of ar if it is not C-contiguous, which will negatively affect performance for large input arrays.", "Block view of the input n-dimensional array (using re-striding).", "Blocks are non-overlapping views of the input array.", "N-d input array.", "The shape of the block. Each dimension must divide evenly into the corresponding dimensions of arr_in.", "Block view of the input array.", "Rolling window view of the input n-dimensional array.", "Windows are overlapping views of the input array, with adjacent windows shifted by a single row or column (or an index of a higher dimension).", "N-d input array.", "Defines the shape of the elementary n-dimensional orthotope (better know as hyperrectangle [1]) of the rolling window view. If an integer is given, the shape will be a hypercube of sidelength given by its value.", "Indicates step size at which extraction shall be performed. If integer is given, then the step is uniform in all dimensions.", "(rolling) window view of the input array.", "One should be very careful with rolling views when it comes to memory usage. Indeed, although a \u2018view\u2019 has the same memory footprint as its base array, the actual array that emerges when this \u2018view\u2019 is used in a computation is generally a (much) larger array than the original, especially for 2-dimensional arrays and above.", "For example, let us consider a 3 dimensional array of size (100, 100, 100) of float64. This array takes about 8*100**3 Bytes for storage which is just 8 MB. If one decides to build a rolling view on this array with a window of (3, 3, 3) the hypothetical size of the rolling view (if one was to reshape the view for example) would be 8*(100-3+1)**3*3**3 which is about 203 MB! The scaling becomes even worse as the dimension of the input array becomes larger.", "https://en.wikipedia.org/wiki/Hyperrectangle"]}, {"name": "util.apply_parallel()", "path": "api/skimage.util#skimage.util.apply_parallel", "type": "util", "text": ["Map a function in parallel across an array.", "Split an array into possibly overlapping chunks of a given depth and boundary type, call the given function in parallel on the chunks, combine the chunks and return the resulting array.", "Function to be mapped which takes an array as an argument.", "Array which the function will be applied to.", "A single integer is interpreted as the length of one side of a square chunk that should be tiled across the array. One tuple of length array.ndim represents the shape of a chunk, and it is tiled across the array. A list of tuples of length ndim, where each sub-tuple is a sequence of chunk sizes along the corresponding dimension. If None, the array is broken up into chunks based on the number of available cpus. More information about chunks is in the documentation here.", "Integer equal to the depth of the added boundary cells. Defaults to zero.", "type of external boundary padding.", "Tuple of arguments to be passed to the function.", "Dictionary of keyword arguments to be passed to the function.", "The data-type of the function output. If None, Dask will attempt to infer this by calling the function on data of shape (1,) * ndim. For functions expecting RGB or multichannel data this may be problematic. In such cases, the user should manually specify this dtype argument instead.", "New in version 0.18: dtype was added in 0.18.", "If chunks is None and multichannel is True, this function will keep only a single chunk along the channels axis. When depth is specified as a scalar value, that depth will be applied only to the non-channels axes (a depth of 0 will be used along the channels axis). If the user manually specified both chunks and a depth tuple, then this argument will have no effect.", "New in version 0.18: multichannel was added in 0.18.", "If True, compute eagerly returning a NumPy Array. If False, compute lazily returning a Dask Array. If None (default), compute based on array type provided (eagerly for NumPy Arrays and lazily for Dask Arrays).", "Returns the result of the applying the operation. Type is dependent on the compute argument.", "Numpy edge modes \u2018symmetric\u2019, \u2018wrap\u2019, and \u2018edge\u2019 are converted to the equivalent dask boundary modes \u2018reflect\u2019, \u2018periodic\u2019 and \u2018nearest\u2019, respectively. Setting compute=False can be useful for chaining later operations. For example region selection to preview a result or storing large data to disk instead of loading in memory."]}, {"name": "util.compare_images()", "path": "api/skimage.util#skimage.util.compare_images", "type": "util", "text": ["Return an image showing the differences between two images.", "New in version 0.16.", "Images to process, must be of the same shape.", "Method used for the comparison. Valid values are {\u2018diff\u2019, \u2018blend\u2019, \u2018checkerboard\u2019}. Details are provided in the note section.", "Used only for the checkerboard method. Specifies the number of tiles (row, column) to divide the image.", "Image showing the differences.", "'diff' computes the absolute difference between the two images. 'blend' computes the mean value. 'checkerboard' makes tiles of dimension n_tiles that display alternatively the first and the second image."]}, {"name": "util.crop()", "path": "api/skimage.util#skimage.util.crop", "type": "util", "text": ["Crop array ar by crop_width along each dimension.", "Input array.", "Number of values to remove from the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) specifies unique crop widths at the start and end of each axis. ((before, after),) or (before, after) specifies a fixed start and end crop for every axis. (n,) or n for integer n is a shortcut for before = after = n for all axes.", "If True, ensure the returned array is a contiguous copy. Normally, a crop operation will return a discontiguous view of the underlying input array.", "If copy==True, control the memory layout of the copy. See np.copy.", "The cropped array. If copy=False (default), this is a sliced view of the input array."]}, {"name": "util.dtype_limits()", "path": "api/skimage.util#skimage.util.dtype_limits", "type": "util", "text": ["Return intensity limits, i.e. (min, max) tuple, of the image\u2019s dtype.", "Input image.", "If True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values.", "Lower and upper intensity limits."]}, {"name": "util.img_as_bool()", "path": "api/skimage.util#skimage.util.img_as_bool", "type": "util", "text": ["Convert an image to boolean format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The upper half of the input dtype\u2019s positive range is True, and the lower half is False. All negative values (if present) are False."]}, {"name": "util.img_as_float()", "path": "api/skimage.util#skimage.util.img_as_float", "type": "util", "text": ["Convert an image to floating point format.", "This function is similar to img_as_float64, but will not convert lower-precision floating point arrays to float64.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]."]}, {"name": "util.img_as_float32()", "path": "api/skimage.util#skimage.util.img_as_float32", "type": "util", "text": ["Convert an image to single-precision (32-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]."]}, {"name": "util.img_as_float64()", "path": "api/skimage.util#skimage.util.img_as_float64", "type": "util", "text": ["Convert an image to double-precision (64-bit) floating point format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] when converting from unsigned or signed datatypes, respectively. If the input image has a float type, intensity values are not modified and can be outside the ranges [0.0, 1.0] or [-1.0, 1.0]."]}, {"name": "util.img_as_int()", "path": "api/skimage.util#skimage.util.img_as_int", "type": "util", "text": ["Convert an image to 16-bit signed integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "The values are scaled between -32768 and 32767. If the input data-type is positive-only (e.g., uint8), then the output image will still only have positive values."]}, {"name": "util.img_as_ubyte()", "path": "api/skimage.util#skimage.util.img_as_ubyte", "type": "util", "text": ["Convert an image to 8-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 255."]}, {"name": "util.img_as_uint()", "path": "api/skimage.util#skimage.util.img_as_uint", "type": "util", "text": ["Convert an image to 16-bit unsigned integer format.", "Input image.", "Force a copy of the data, irrespective of its current dtype.", "Output image.", "Negative input values will be clipped. Positive values are scaled between 0 and 65535."]}, {"name": "util.invert()", "path": "api/skimage.util#skimage.util.invert", "type": "util", "text": ["Invert an image.", "Invert the intensity range of the input image, so that the dtype maximum is now the dtype minimum, and vice-versa. This operation is slightly different depending on the input dtype:", "See the examples for clarification.", "Input image.", "If True and the image is of type float, the range is assumed to be [-1, 1]. If False and the image is of type float, the range is assumed to be [0, 1].", "Inverted image.", "Ideally, for signed integers we would simply multiply by -1. However, signed integer ranges are asymmetric. For example, for np.int8, the range of possible values is [-128, 127], so that -128 * -1 equals -128! By subtracting from -1, we correctly map the maximum dtype value to the minimum."]}, {"name": "util.map_array()", "path": "api/skimage.util#skimage.util.map_array", "type": "util", "text": ["Map values from input array from input_vals to output_vals.", "The input label image.", "The values to map from.", "The values to map to.", "The output array. Will be created if not provided. It should have the same dtype as output_vals.", "The array of mapped values."]}, {"name": "util.montage()", "path": "api/skimage.util#skimage.util.montage", "type": "util", "text": ["Create a montage of several single- or multichannel images.", "Create a rectangular montage from an input array representing an ensemble of equally shaped single- (gray) or multichannel (color) images.", "For example, montage(arr_in) called with the following arr_in", "1", "2", "3", "will return", "1", "2", "3", "where the \u2018*\u2019 patch will be determined by the fill parameter.", "An array representing an ensemble of K images of equal shape.", "Value to fill the padding areas and/or the extra tiles in the output array. Has to be float for single channel collections. For multichannel collections has to be an array-like of shape of number of channels. If mean, uses the mean value over all images.", "Whether to rescale the intensity of each image to [0, 1].", "The desired grid shape for the montage (ntiles_row, ntiles_column). The default aspect ratio is square.", "The size of the spacing between the tiles and between the tiles and the borders. If non-zero, makes the boundaries of individual images easier to perceive.", "If True, the last arr_in dimension is threated as a color channel, otherwise as spatial.", "Output array with input images glued together (including padding p)."]}, {"name": "util.pad()", "path": "api/skimage.util#skimage.util.pad", "type": "util", "text": ["Pad an array.", "The array to pad.", "Number of values padded to the edges of each axis. ((before_1, after_1), \u2026 (before_N, after_N)) unique pad widths for each axis. ((before, after),) yields same before and after pad for each axis. (pad,) or int is a shortcut for before = after = pad width for all axes.", "One of the following string values or a user supplied function.", "Pads with a constant value.", "Pads with the edge values of array.", "Pads with the linear ramp between end_value and the array edge value.", "Pads with the maximum value of all or part of the vector along each axis.", "Pads with the mean value of all or part of the vector along each axis.", "Pads with the median value of all or part of the vector along each axis.", "Pads with the minimum value of all or part of the vector along each axis.", "Pads with the reflection of the vector mirrored on the first and last values of the vector along each axis.", "Pads with the reflection of the vector mirrored along the edge of the array.", "Pads with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.", "Pads with undefined values.", "New in version 1.17.", "Padding function, see Notes.", "Used in \u2018maximum\u2019, \u2018mean\u2019, \u2018median\u2019, and \u2018minimum\u2019. Number of values at edge of each axis used to calculate the statistic value.", "((before_1, after_1), \u2026 (before_N, after_N)) unique statistic lengths for each axis.", "((before, after),) yields same before and after statistic lengths for each axis.", "(stat_length,) or int is a shortcut for before = after = statistic length for all axes.", "Default is None, to use the entire axis.", "Used in \u2018constant\u2019. The values to set the padded values for each axis.", "((before_1, after_1), ... (before_N, after_N)) unique pad constants for each axis.", "((before, after),) yields same before and after constants for each axis.", "(constant,) or constant is a shortcut for before = after = constant for all axes.", "Default is 0.", "Used in \u2018linear_ramp\u2019. The values used for the ending value of the linear_ramp and that will form the edge of the padded array.", "((before_1, after_1), ... (before_N, after_N)) unique end values for each axis.", "((before, after),) yields same before and after end values for each axis.", "(constant,) or constant is a shortcut for before = after = constant for all axes.", "Default is 0.", "Used in \u2018reflect\u2019, and \u2018symmetric\u2019. The \u2018even\u2019 style is the default with an unaltered reflection around the edge value. For the \u2018odd\u2019 style, the extended part of the array is created by subtracting the reflected values from two times the edge value.", "Padded array of rank equal to array with shape increased according to pad_width.", "New in version 1.7.0.", "For an array with rank greater than 1, some of the padding of later axes is calculated from padding of previous axes. This is easiest to think about with a rank 2 array where the corners of the padded array are calculated by using padded values from the first axis.", "The padding function, if used, should modify a rank 1 array in-place. It has the following signature:", "where", "A rank 1 array already padded with zeros. Padded values are vector[:iaxis_pad_width[0]] and vector[-iaxis_pad_width[1]:].", "A 2-tuple of ints, iaxis_pad_width[0] represents the number of values padded at the beginning of vector where iaxis_pad_width[1] represents the number of values padded at the end of vector.", "The axis currently being calculated.", "Any keyword arguments the function requires."]}, {"name": "util.random_noise()", "path": "api/skimage.util#skimage.util.random_noise", "type": "util", "text": ["Function to add random noise of various types to a floating-point image.", "Input image data. Will be converted to float.", "One of the following strings, selecting the type of noise to add:", "local variance at each point of image.", "-1 (for signed images).", "low_val is 0 for unsigned images or -1 for signed images.", "n is Gaussian noise with specified mean & variance.", "If provided, this will set the random seed before generating noise, for valid pseudo-random comparisons.", "If True (default), the output will be clipped after noise applied for modes \u2018speckle\u2019, \u2018poisson\u2019, and \u2018gaussian\u2019. This is needed to maintain the proper image data range. If False, clipping is not applied, and the output may extend beyond the range [-1, 1].", "Mean of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Default : 0.", "Variance of random distribution. Used in \u2018gaussian\u2019 and \u2018speckle\u2019. Note: variance = (standard deviation) ** 2. Default : 0.01", "Array of positive floats, same shape as image, defining the local variance at every image point. Used in \u2018localvar\u2019.", "Proportion of image pixels to replace with noise on range [0, 1]. Used in \u2018salt\u2019, \u2018pepper\u2019, and \u2018salt & pepper\u2019. Default : 0.05", "Proportion of salt vs. pepper noise for \u2018s&p\u2019 on range [0, 1]. Higher values represent more salt. Default : 0.5 (equal amounts)", "Output floating-point image data on range [0, 1] or [-1, 1] if the input image was unsigned or signed, respectively.", "Speckle, Poisson, Localvar, and Gaussian noise may generate noise outside the valid image range. The default is to clip (not alias) these values, but they may be preserved by setting clip=False. Note that in this case the output may contain values outside the ranges [0, 1] or [-1, 1]. Use this option with care.", "Because of the prevalence of exclusively positive floating-point images in intermediate calculations, it is not possible to intuit if an input is signed based on dtype alone. Instead, negative values are explicitly searched for. Only if found does this function assume signed input. Unexpected results only occur in rare, poorly exposes cases (e.g. if all values are above 50 percent gray in a signed image). In this event, manually scaling the input to the positive domain will solve the problem.", "The Poisson distribution is only defined for positive integers. To apply this noise type, the number of unique values in the image is found and the next round power of two is used to scale up the floating-point result, after which it is scaled back down to the floating-point image range.", "To generate Poisson noise against a signed image, the signed image is temporarily converted to an unsigned image in the floating point domain, Poisson noise is generated, then it is returned to the original range."]}, {"name": "util.regular_grid()", "path": "api/skimage.util#skimage.util.regular_grid", "type": "util", "text": ["Find n_points regularly spaced along ar_shape.", "The returned points (as slices) should be as close to cubically-spaced as possible. Essentially, the points are spaced by the Nth root of the input array size, where N is the number of dimensions. However, if an array dimension cannot fit a full step size, it is \u201cdiscarded\u201d, and the computation is done for only the remaining dimensions.", "The shape of the space embedding the grid. len(ar_shape) is the number of dimensions.", "The (approximate) number of points to embed in the space.", "A slice along each dimension of ar_shape, such that the intersection of all the slices give the coordinates of regularly spaced points.", "Changed in version 0.14.1: In scikit-image 0.14.1 and 0.15, the return type was changed from a list to a tuple to ensure compatibility with Numpy 1.15 and higher. If your code requires the returned result to be a list, you may convert the output of this function to a list with:"]}, {"name": "util.regular_seeds()", "path": "api/skimage.util#skimage.util.regular_seeds", "type": "util", "text": ["Return an image with ~`n_points` regularly-spaced nonzero pixels.", "The shape of the desired output image.", "The desired number of nonzero points.", "The desired data type of the output.", "The desired image."]}, {"name": "util.unique_rows()", "path": "api/skimage.util#skimage.util.unique_rows", "type": "util", "text": ["Remove repeated rows from a 2D array.", "In particular, if given an array of coordinates of shape (Npoints, Ndim), it will remove repeated points.", "The input array.", "A copy of the input array with repeated rows removed.", "The function will generate a copy of ar if it is not C-contiguous, which will negatively affect performance for large input arrays."]}, {"name": "util.view_as_blocks()", "path": "api/skimage.util#skimage.util.view_as_blocks", "type": "util", "text": ["Block view of the input n-dimensional array (using re-striding).", "Blocks are non-overlapping views of the input array.", "N-d input array.", "The shape of the block. Each dimension must divide evenly into the corresponding dimensions of arr_in.", "Block view of the input array."]}, {"name": "util.view_as_windows()", "path": "api/skimage.util#skimage.util.view_as_windows", "type": "util", "text": ["Rolling window view of the input n-dimensional array.", "Windows are overlapping views of the input array, with adjacent windows shifted by a single row or column (or an index of a higher dimension).", "N-d input array.", "Defines the shape of the elementary n-dimensional orthotope (better know as hyperrectangle [1]) of the rolling window view. If an integer is given, the shape will be a hypercube of sidelength given by its value.", "Indicates step size at which extraction shall be performed. If integer is given, then the step is uniform in all dimensions.", "(rolling) window view of the input array.", "One should be very careful with rolling views when it comes to memory usage. Indeed, although a \u2018view\u2019 has the same memory footprint as its base array, the actual array that emerges when this \u2018view\u2019 is used in a computation is generally a (much) larger array than the original, especially for 2-dimensional arrays and above.", "For example, let us consider a 3 dimensional array of size (100, 100, 100) of float64. This array takes about 8*100**3 Bytes for storage which is just 8 MB. If one decides to build a rolling view on this array with a window of (3, 3, 3) the hypothetical size of the rolling view (if one was to reshape the view for example) would be 8*(100-3+1)**3*3**3 which is about 203 MB! The scaling becomes even worse as the dimension of the input array becomes larger.", "https://en.wikipedia.org/wiki/Hyperrectangle"]}, {"name": "viewer", "path": "api/skimage.viewer", "type": "viewer", "text": ["skimage.viewer.CollectionViewer(image_collection)", "Viewer for displaying image collections.", "skimage.viewer.ImageViewer(image[, useblit])", "Viewer for displaying images.", "skimage.viewer.canvastools", "skimage.viewer.plugins", "skimage.viewer.qt", "skimage.viewer.utils", "skimage.viewer.viewers", "skimage.viewer.widgets", "Widgets for interacting with ImageViewer.", "Bases: skimage.viewer.viewers.core.ImageViewer", "Viewer for displaying image collections.", "Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:", "Previous/next image in collection.", "0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.", "First/last image in collection.", "List of images to be displayed.", "Control whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.", "Initialize self. See help(type(self)) for accurate signature.", "Select image on display using index into image collection.", "Bases: object", "Viewer for displaying images.", "This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions.", "Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.", "Image being viewed.", "Matplotlib canvas, figure, and axes used to display image.", "Image being viewed. Setting this value will update the displayed frame.", "Plugins typically operate on (but don\u2019t change) the original image.", "List of attached plugins.", "Initialize self. See help(type(self)) for accurate signature.", "Connect callback function to matplotlib event and return id.", "Disconnect callback by its id (returned by connect_event).", "Open image file and display in viewer.", "Save current image to file.", "The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image).", "Show ImageViewer and attached plugins.", "This behaves much like matplotlib.pyplot.show and QWidget.show.", "Update displayed image.", "This method can be overridden or extended in subclasses and plugins to react to image changes."]}, {"name": "viewer.canvastools", "path": "api/skimage.viewer.canvastools", "type": "viewer", "text": ["skimage.viewer.canvastools.LineTool(manager)", "Widget for line selection in a plot.", "skimage.viewer.canvastools.PaintTool(\u2026[, \u2026])", "Widget for painting on top of a plot.", "skimage.viewer.canvastools.RectangleTool(manager)", "Widget for selecting a rectangular region in a plot.", "skimage.viewer.canvastools.ThickLineTool(manager)", "Widget for line selection in a plot.", "skimage.viewer.canvastools.base", "skimage.viewer.canvastools.linetool", "skimage.viewer.canvastools.painttool", "skimage.viewer.canvastools.recttool", "Bases: skimage.viewer.canvastools.base.CanvasToolBase", "Widget for line selection in a plot.", "Skimage viewer or plot plugin object.", "Function called whenever a control handle is moved. This function must accept the end points of line as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Maximum pixel distance allowed when selecting control handle.", "Properties for matplotlib.lines.Line2D.", "Marker properties for the handles (also see matplotlib.lines.Line2D).", "End points of line ((x1, y1), (x2, y2)).", "Initialize self. See help(type(self)) for accurate signature.", "Geometry information that gets passed to callback functions.", "Bases: skimage.viewer.canvastools.base.CanvasToolBase", "Widget for painting on top of a plot.", "Skimage viewer or plot plugin object.", "2D shape tuple used to initialize overlay image.", "The size of the paint cursor.", "Opacity of overlay.", "Function called whenever a control handle is moved. This function must accept the end points of line as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Properties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.", "Overlay of painted labels displayed on top of image.", "Current paint color.", "Initialize self. See help(type(self)) for accurate signature.", "Geometry information that gets passed to callback functions.", "Bases: skimage.viewer.canvastools.base.CanvasToolBase, matplotlib.widgets.RectangleSelector", "Widget for selecting a rectangular region in a plot.", "After making the desired selection, press \u201cEnter\u201d to accept the selection and call the on_enter callback function.", "Skimage viewer or plot plugin object.", "Function called whenever a control handle is moved. This function must accept the rectangle extents as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Maximum pixel distance allowed when selecting control handle.", "Properties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.", "Return (xmin, xmax, ymin, ymax).", "The parent axes for the widget.", "A callback function that is called after a selection is completed. It must have the signature:", "where eclick and erelease are the mouse click and release MouseEvents that start and complete the selection.", "Whether to draw the full rectangle box, the diagonal line of the rectangle, or nothing at all.", "Selections with an x-span less than minspanx are ignored.", "Selections with an y-span less than minspany are ignored.", "Whether to use blitting for faster drawing (if supported by the backend).", "Properties with which the line is drawn, if drawtype == \"line\". Default:", "Properties with which the rectangle is drawn, if drawtype ==\n\"box\". Default:", "Whether to interpret minspanx and minspany in data or in pixel coordinates.", "Button(s) that trigger rectangle selection.", "Distance in pixels within which the interactive tool handles can be activated.", "Properties with which the interactive handles are drawn. Currently not implemented and ignored.", "Whether to draw a set of handles that allow interaction with the widget after it is drawn.", "Keyboard modifiers which affect the widget\u2019s behavior. Values amend the defaults.", "\u201csquare\u201d and \u201ccenter\u201d can be combined.", "Corners of rectangle from lower left, moving clockwise.", "Midpoint of rectangle edges from left, moving clockwise.", "Return (xmin, xmax, ymin, ymax).", "Geometry information that gets passed to callback functions.", "Bases: skimage.viewer.canvastools.linetool.LineTool", "Widget for line selection in a plot.", "The thickness of the line can be varied using the mouse scroll wheel, or with the \u2018+\u2019 and \u2018-\u2018 keys.", "Skimage viewer or plot plugin object.", "Function called whenever a control handle is moved. This function must accept the end points of line as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Function called whenever the line thickness is changed.", "Maximum pixel distance allowed when selecting control handle.", "Properties for matplotlib.lines.Line2D.", "Marker properties for the handles (also see matplotlib.lines.Line2D).", "End points of line ((x1, y1), (x2, y2)).", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.canvastools.LineTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool", "type": "viewer", "text": ["Bases: skimage.viewer.canvastools.base.CanvasToolBase", "Widget for line selection in a plot.", "Skimage viewer or plot plugin object.", "Function called whenever a control handle is moved. This function must accept the end points of line as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Maximum pixel distance allowed when selecting control handle.", "Properties for matplotlib.lines.Line2D.", "Marker properties for the handles (also see matplotlib.lines.Line2D).", "End points of line ((x1, y1), (x2, y2)).", "Initialize self. See help(type(self)) for accurate signature.", "Geometry information that gets passed to callback functions."]}, {"name": "viewer.canvastools.LineTool.end_points()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.end_points", "type": "viewer", "text": []}, {"name": "viewer.canvastools.LineTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.geometry", "type": "viewer", "text": ["Geometry information that gets passed to callback functions."]}, {"name": "viewer.canvastools.LineTool.hit_test()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.hit_test", "type": "viewer", "text": []}, {"name": "viewer.canvastools.LineTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_mouse_press", "type": "viewer", "text": []}, {"name": "viewer.canvastools.LineTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_mouse_release", "type": "viewer", "text": []}, {"name": "viewer.canvastools.LineTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.on_move", "type": "viewer", "text": []}, {"name": "viewer.canvastools.LineTool.update()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.update", "type": "viewer", "text": []}, {"name": "viewer.canvastools.LineTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.LineTool.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.canvastools.PaintTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool", "type": "viewer", "text": ["Bases: skimage.viewer.canvastools.base.CanvasToolBase", "Widget for painting on top of a plot.", "Skimage viewer or plot plugin object.", "2D shape tuple used to initialize overlay image.", "The size of the paint cursor.", "Opacity of overlay.", "Function called whenever a control handle is moved. This function must accept the end points of line as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Properties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.", "Overlay of painted labels displayed on top of image.", "Current paint color.", "Initialize self. See help(type(self)) for accurate signature.", "Geometry information that gets passed to callback functions."]}, {"name": "viewer.canvastools.PaintTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.geometry", "type": "viewer", "text": ["Geometry information that gets passed to callback functions."]}, {"name": "viewer.canvastools.PaintTool.label()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.label", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.on_key_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_key_press", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_mouse_press", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_mouse_release", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.on_move", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.overlay()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.overlay", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.radius()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.radius", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.shape()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.shape", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.update_cursor()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.update_cursor", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.update_overlay()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.update_overlay", "type": "viewer", "text": []}, {"name": "viewer.canvastools.PaintTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.PaintTool.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.canvastools.RectangleTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool", "type": "viewer", "text": ["Bases: skimage.viewer.canvastools.base.CanvasToolBase, matplotlib.widgets.RectangleSelector", "Widget for selecting a rectangular region in a plot.", "After making the desired selection, press \u201cEnter\u201d to accept the selection and call the on_enter callback function.", "Skimage viewer or plot plugin object.", "Function called whenever a control handle is moved. This function must accept the rectangle extents as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Maximum pixel distance allowed when selecting control handle.", "Properties for matplotlib.patches.Rectangle. This class redefines defaults in matplotlib.widgets.RectangleSelector.", "Return (xmin, xmax, ymin, ymax).", "The parent axes for the widget.", "A callback function that is called after a selection is completed. It must have the signature:", "where eclick and erelease are the mouse click and release MouseEvents that start and complete the selection.", "Whether to draw the full rectangle box, the diagonal line of the rectangle, or nothing at all.", "Selections with an x-span less than minspanx are ignored.", "Selections with an y-span less than minspany are ignored.", "Whether to use blitting for faster drawing (if supported by the backend).", "Properties with which the line is drawn, if drawtype == \"line\". Default:", "Properties with which the rectangle is drawn, if drawtype ==\n\"box\". Default:", "Whether to interpret minspanx and minspany in data or in pixel coordinates.", "Button(s) that trigger rectangle selection.", "Distance in pixels within which the interactive tool handles can be activated.", "Properties with which the interactive handles are drawn. Currently not implemented and ignored.", "Whether to draw a set of handles that allow interaction with the widget after it is drawn.", "Keyboard modifiers which affect the widget\u2019s behavior. Values amend the defaults.", "\u201csquare\u201d and \u201ccenter\u201d can be combined.", "Corners of rectangle from lower left, moving clockwise.", "Midpoint of rectangle edges from left, moving clockwise.", "Return (xmin, xmax, ymin, ymax).", "Geometry information that gets passed to callback functions."]}, {"name": "viewer.canvastools.RectangleTool.corners()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.corners", "type": "viewer", "text": ["Corners of rectangle from lower left, moving clockwise."]}, {"name": "viewer.canvastools.RectangleTool.edge_centers()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.edge_centers", "type": "viewer", "text": ["Midpoint of rectangle edges from left, moving clockwise."]}, {"name": "viewer.canvastools.RectangleTool.extents()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.extents", "type": "viewer", "text": ["Return (xmin, xmax, ymin, ymax)."]}, {"name": "viewer.canvastools.RectangleTool.geometry()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.geometry", "type": "viewer", "text": ["Geometry information that gets passed to callback functions."]}, {"name": "viewer.canvastools.RectangleTool.on_mouse_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_mouse_press", "type": "viewer", "text": []}, {"name": "viewer.canvastools.RectangleTool.on_mouse_release()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_mouse_release", "type": "viewer", "text": []}, {"name": "viewer.canvastools.RectangleTool.on_move()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.on_move", "type": "viewer", "text": []}, {"name": "viewer.canvastools.RectangleTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.RectangleTool.__init__", "type": "viewer", "text": ["The parent axes for the widget.", "A callback function that is called after a selection is completed. It must have the signature:", "where eclick and erelease are the mouse click and release MouseEvents that start and complete the selection.", "Whether to draw the full rectangle box, the diagonal line of the rectangle, or nothing at all.", "Selections with an x-span less than minspanx are ignored.", "Selections with an y-span less than minspany are ignored.", "Whether to use blitting for faster drawing (if supported by the backend).", "Properties with which the line is drawn, if drawtype == \"line\". Default:", "Properties with which the rectangle is drawn, if drawtype ==\n\"box\". Default:", "Whether to interpret minspanx and minspany in data or in pixel coordinates.", "Button(s) that trigger rectangle selection.", "Distance in pixels within which the interactive tool handles can be activated.", "Properties with which the interactive handles are drawn. Currently not implemented and ignored.", "Whether to draw a set of handles that allow interaction with the widget after it is drawn.", "Keyboard modifiers which affect the widget\u2019s behavior. Values amend the defaults.", "\u201csquare\u201d and \u201ccenter\u201d can be combined."]}, {"name": "viewer.canvastools.ThickLineTool", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool", "type": "viewer", "text": ["Bases: skimage.viewer.canvastools.linetool.LineTool", "Widget for line selection in a plot.", "The thickness of the line can be varied using the mouse scroll wheel, or with the \u2018+\u2019 and \u2018-\u2018 keys.", "Skimage viewer or plot plugin object.", "Function called whenever a control handle is moved. This function must accept the end points of line as the only argument.", "Function called whenever the control handle is released.", "Function called whenever the \u201center\u201d key is pressed.", "Function called whenever the line thickness is changed.", "Maximum pixel distance allowed when selecting control handle.", "Properties for matplotlib.lines.Line2D.", "Marker properties for the handles (also see matplotlib.lines.Line2D).", "End points of line ((x1, y1), (x2, y2)).", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.canvastools.ThickLineTool.on_key_press()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.on_key_press", "type": "viewer", "text": []}, {"name": "viewer.canvastools.ThickLineTool.on_scroll()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.on_scroll", "type": "viewer", "text": []}, {"name": "viewer.canvastools.ThickLineTool.__init__()", "path": "api/skimage.viewer.canvastools#skimage.viewer.canvastools.ThickLineTool.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.CollectionViewer", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer", "type": "viewer", "text": ["Bases: skimage.viewer.viewers.core.ImageViewer", "Viewer for displaying image collections.", "Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:", "Previous/next image in collection.", "0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.", "First/last image in collection.", "List of images to be displayed.", "Control whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.", "Initialize self. See help(type(self)) for accurate signature.", "Select image on display using index into image collection."]}, {"name": "viewer.CollectionViewer.keyPressEvent()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.keyPressEvent", "type": "viewer", "text": []}, {"name": "viewer.CollectionViewer.update_index()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.update_index", "type": "viewer", "text": ["Select image on display using index into image collection."]}, {"name": "viewer.CollectionViewer.__init__()", "path": "api/skimage.viewer#skimage.viewer.CollectionViewer.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.ImageViewer", "path": "api/skimage.viewer#skimage.viewer.ImageViewer", "type": "viewer", "text": ["Bases: object", "Viewer for displaying images.", "This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions.", "Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.", "Image being viewed.", "Matplotlib canvas, figure, and axes used to display image.", "Image being viewed. Setting this value will update the displayed frame.", "Plugins typically operate on (but don\u2019t change) the original image.", "List of attached plugins.", "Initialize self. See help(type(self)) for accurate signature.", "Connect callback function to matplotlib event and return id.", "Disconnect callback by its id (returned by connect_event).", "Open image file and display in viewer.", "Save current image to file.", "The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image).", "Show ImageViewer and attached plugins.", "This behaves much like matplotlib.pyplot.show and QWidget.show.", "Update displayed image.", "This method can be overridden or extended in subclasses and plugins to react to image changes."]}, {"name": "viewer.ImageViewer.add_tool()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.add_tool", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.closeEvent()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.closeEvent", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.connect_event()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.connect_event", "type": "viewer", "text": ["Connect callback function to matplotlib event and return id."]}, {"name": "viewer.ImageViewer.disconnect_event()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.disconnect_event", "type": "viewer", "text": ["Disconnect callback by its id (returned by connect_event)."]}, {"name": "viewer.ImageViewer.dock_areas", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.dock_areas", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.image", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.open_file()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.open_file", "type": "viewer", "text": ["Open image file and display in viewer."]}, {"name": "viewer.ImageViewer.original_image_changed", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.original_image_changed", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.redraw()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.redraw", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.remove_tool()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.remove_tool", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.reset_image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.reset_image", "type": "viewer", "text": []}, {"name": "viewer.ImageViewer.save_to_file()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.save_to_file", "type": "viewer", "text": ["Save current image to file.", "The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image)."]}, {"name": "viewer.ImageViewer.show()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.show", "type": "viewer", "text": ["Show ImageViewer and attached plugins.", "This behaves much like matplotlib.pyplot.show and QWidget.show."]}, {"name": "viewer.ImageViewer.update_image()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.update_image", "type": "viewer", "text": ["Update displayed image.", "This method can be overridden or extended in subclasses and plugins to react to image changes."]}, {"name": "viewer.ImageViewer.__init__()", "path": "api/skimage.viewer#skimage.viewer.ImageViewer.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins", "path": "api/skimage.viewer.plugins", "type": "viewer", "text": ["skimage.viewer.plugins.CannyPlugin(*args, \u2026)", "Canny filter plugin to show edges of an image.", "skimage.viewer.plugins.ColorHistogram([max_pct])", "skimage.viewer.plugins.Crop([maxdist])", "skimage.viewer.plugins.LabelPainter([max_radius])", "skimage.viewer.plugins.LineProfile([\u2026])", "Plugin to compute interpolated intensity under a scan line on an image.", "skimage.viewer.plugins.Measure([maxdist])", "skimage.viewer.plugins.OverlayPlugin(**kwargs)", "Plugin for ImageViewer that displays an overlay on top of main image.", "skimage.viewer.plugins.PlotPlugin([\u2026])", "Plugin for ImageViewer that contains a plot canvas.", "skimage.viewer.plugins.Plugin([\u2026])", "Base class for plugins that interact with an ImageViewer.", "skimage.viewer.plugins.base", "Base class for Plugins that interact with ImageViewer.", "skimage.viewer.plugins.canny", "skimage.viewer.plugins.color_histogram", "skimage.viewer.plugins.crop", "skimage.viewer.plugins.labelplugin", "skimage.viewer.plugins.lineprofile", "skimage.viewer.plugins.measure", "skimage.viewer.plugins.overlayplugin", "skimage.viewer.plugins.plotplugin", "Bases: skimage.viewer.plugins.overlayplugin.OverlayPlugin", "Canny filter plugin to show edges of an image.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Bases: skimage.viewer.plugins.plotplugin.PlotPlugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Return the image mask and the histogram data.", "The selected pixels.", "The data describing the histogram and the selected region. The dictionary contains:", "Bases: skimage.viewer.plugins.base.Plugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Bases: skimage.viewer.plugins.base.Plugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Bases: skimage.viewer.plugins.plotplugin.PlotPlugin", "Plugin to compute interpolated intensity under a scan line on an image.", "See PlotPlugin and Plugin classes for additional details.", "Maximum pixel distance allowed when selecting end point of scan line.", "(minimum, maximum) intensity limits for plotted profile. The following special values are defined:", "None : rescale based on min/max intensity along selected scan line. \u2018image\u2019 : fixed scale based on min/max intensity in image. \u2018dtype\u2019 : fixed scale based on min/max intensity of image dtype.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Return intensity profile of the selected line.", "The positions ((x1, y1), (x2, y2)) of the line ends.", "Profile of intensity values. Length 1 (grayscale) or 3 (rgb).", "Return the drawn line and the resulting scan.", "An array of 0s with the scanned line set to 255. If the linewidth of the line tool is greater than 1, sets the values within the profiled polygon to 128.", "The line scan values across the image.", "Bases: skimage.viewer.plugins.base.Plugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Bases: skimage.viewer.plugins.base.Plugin", "Plugin for ImageViewer that displays an overlay on top of main image.", "The base Plugin class displays the filtered image directly on the viewer. OverlayPlugin will instead overlay an image with a transparent colormap.", "See base Plugin class for additional details.", "Overlay displayed on top of image. This overlay defaults to a color map with alpha values varying linearly from 0 to 1.", "Color of overlay.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "On close disconnect all artists and events from ImageViewer.", "Note that artists must be appended to self.artists.", "Display filtered image as an overlay on top of image in viewer.", "Return filtered image.", "This \u201cfiltered image\u201d is used when saving from the plugin.", "Return the overlaid image.", "The overlay currently displayed.", "Bases: skimage.viewer.plugins.base.Plugin", "Plugin for ImageViewer that contains a plot canvas.", "Base class for plugins that contain a Matplotlib plot canvas, which can, for example, display an image histogram.", "See base Plugin class for additional details.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Redraw plot.", "Bases: object", "Base class for plugins that interact with an ImageViewer.", "A plugin connects an image filter (or another function) to an image viewer. Note that a Plugin is initialized without an image viewer and attached in a later step. See example below for details.", "Window containing image used in measurement/manipulation.", "Function that gets called to update image in image viewer. This value can be None if, for example, you have a plugin that extracts information from an image and doesn\u2019t manipulate it. Alternatively, this function can be defined as a method in a Plugin subclass.", "Size of plugin window in pixels. Note that Qt will automatically resize a window to fit components. So if you\u2019re adding rows of components, you can leave height = 0 and just let Qt determine the final height.", "If True, use blitting to speed up animation. Only available on some Matplotlib backends. If None, set to True when using Agg backend. This only has an effect if you draw on top of an image viewer.", "The plugin will automatically delegate parameters to image_filter based on its parameter type, i.e., ptype (widgets for required arguments must be added in the order they appear in the function). The image attached to the viewer is automatically passed as the first argument to the filter function.", "#TODO: Add flag so image is not passed to filter function by default.", "ptype = \u2018kwarg\u2019 is the default for most widgets so it\u2019s unnecessary here.", "Window containing image used in measurement.", "Name of plugin. This is displayed as the window title.", "List of Matplotlib artists and canvastools. Any artists created by the plugin should be added to this list so that it gets cleaned up on close.", "Initialize self. See help(type(self)) for accurate signature.", "Add widget to plugin.", "Alternatively, Plugin\u2019s __add__ method is overloaded to add widgets:", "Widgets can adjust required or optional arguments of filter function or parameters for the plugin. This is specified by the Widget\u2019s ptype.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "On close disconnect all artists and events from ImageViewer.", "Note that artists must be appended to self.artists.", "Display the filtered image on image viewer.", "If you don\u2019t want to simply replace the displayed image with the filtered image (e.g., you want to display a transparent overlay), you can override this method.", "Call image_filter with widget args and kwargs", "Note: display_filtered_image is automatically called.", "Return filtered image.", "Return the plugin\u2019s representation and data.", "The filtered image.", "Any data associated with the plugin.", "Derived classes should override this method to return a tuple containing an overlay of the same shape of the image, and a data object. Either of these is optional: return None if you don\u2019t want to return a value.", "Remove artists that are connected to the image viewer.", "Show plugin.", "Update keyword parameters of the plugin itself.", "These parameters will typically be implemented as class properties so that they update the image or some other component."]}, {"name": "viewer.plugins.CannyPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.overlayplugin.OverlayPlugin", "Canny filter plugin to show edges of an image.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.CannyPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.CannyPlugin.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.CannyPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.CannyPlugin.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.ColorHistogram", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.plotplugin.PlotPlugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Return the image mask and the histogram data.", "The selected pixels.", "The data describing the histogram and the selected region. The dictionary contains:"]}, {"name": "viewer.plugins.ColorHistogram.ab_selected()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.ab_selected", "type": "viewer", "text": []}, {"name": "viewer.plugins.ColorHistogram.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.ColorHistogram.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.help", "type": "viewer", "text": []}, {"name": "viewer.plugins.ColorHistogram.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.ColorHistogram.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.output", "type": "viewer", "text": ["Return the image mask and the histogram data.", "The selected pixels.", "The data describing the histogram and the selected region. The dictionary contains:"]}, {"name": "viewer.plugins.ColorHistogram.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.ColorHistogram.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.Crop", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.base.Plugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.Crop.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.Crop.crop()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.crop", "type": "viewer", "text": []}, {"name": "viewer.plugins.Crop.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.help", "type": "viewer", "text": []}, {"name": "viewer.plugins.Crop.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.Crop.reset()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.reset", "type": "viewer", "text": []}, {"name": "viewer.plugins.Crop.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Crop.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.LabelPainter", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.base.Plugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.LabelPainter.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.LabelPainter.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.help", "type": "viewer", "text": []}, {"name": "viewer.plugins.LabelPainter.label()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.label", "type": "viewer", "text": []}, {"name": "viewer.plugins.LabelPainter.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.LabelPainter.on_enter()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.on_enter", "type": "viewer", "text": []}, {"name": "viewer.plugins.LabelPainter.radius()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.radius", "type": "viewer", "text": []}, {"name": "viewer.plugins.LabelPainter.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LabelPainter.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.LineProfile", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.plotplugin.PlotPlugin", "Plugin to compute interpolated intensity under a scan line on an image.", "See PlotPlugin and Plugin classes for additional details.", "Maximum pixel distance allowed when selecting end point of scan line.", "(minimum, maximum) intensity limits for plotted profile. The following special values are defined:", "None : rescale based on min/max intensity along selected scan line. \u2018image\u2019 : fixed scale based on min/max intensity in image. \u2018dtype\u2019 : fixed scale based on min/max intensity of image dtype.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Return intensity profile of the selected line.", "The positions ((x1, y1), (x2, y2)) of the line ends.", "Profile of intensity values. Length 1 (grayscale) or 3 (rgb).", "Return the drawn line and the resulting scan.", "An array of 0s with the scanned line set to 255. If the linewidth of the line tool is greater than 1, sets the values within the profiled polygon to 128.", "The line scan values across the image."]}, {"name": "viewer.plugins.LineProfile.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.LineProfile.get_profiles()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.get_profiles", "type": "viewer", "text": ["Return intensity profile of the selected line.", "The positions ((x1, y1), (x2, y2)) of the line ends.", "Profile of intensity values. Length 1 (grayscale) or 3 (rgb)."]}, {"name": "viewer.plugins.LineProfile.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.help", "type": "viewer", "text": []}, {"name": "viewer.plugins.LineProfile.line_changed()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.line_changed", "type": "viewer", "text": []}, {"name": "viewer.plugins.LineProfile.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.LineProfile.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.output", "type": "viewer", "text": ["Return the drawn line and the resulting scan.", "An array of 0s with the scanned line set to 255. If the linewidth of the line tool is greater than 1, sets the values within the profiled polygon to 128.", "The line scan values across the image."]}, {"name": "viewer.plugins.LineProfile.reset_axes()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.reset_axes", "type": "viewer", "text": []}, {"name": "viewer.plugins.LineProfile.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.LineProfile.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.Measure", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.base.Plugin", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.Measure.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.Measure.help()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.help", "type": "viewer", "text": []}, {"name": "viewer.plugins.Measure.line_changed()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.line_changed", "type": "viewer", "text": []}, {"name": "viewer.plugins.Measure.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.Measure.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Measure.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.OverlayPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.base.Plugin", "Plugin for ImageViewer that displays an overlay on top of main image.", "The base Plugin class displays the filtered image directly on the viewer. OverlayPlugin will instead overlay an image with a transparent colormap.", "See base Plugin class for additional details.", "Overlay displayed on top of image. This overlay defaults to a color map with alpha values varying linearly from 0 to 1.", "Color of overlay.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "On close disconnect all artists and events from ImageViewer.", "Note that artists must be appended to self.artists.", "Display filtered image as an overlay on top of image in viewer.", "Return filtered image.", "This \u201cfiltered image\u201d is used when saving from the plugin.", "Return the overlaid image.", "The overlay currently displayed."]}, {"name": "viewer.plugins.OverlayPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.OverlayPlugin.closeEvent()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.closeEvent", "type": "viewer", "text": ["On close disconnect all artists and events from ImageViewer.", "Note that artists must be appended to self.artists."]}, {"name": "viewer.plugins.OverlayPlugin.color()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.color", "type": "viewer", "text": []}, {"name": "viewer.plugins.OverlayPlugin.colors", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.colors", "type": "viewer", "text": []}, {"name": "viewer.plugins.OverlayPlugin.display_filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.display_filtered_image", "type": "viewer", "text": ["Display filtered image as an overlay on top of image in viewer."]}, {"name": "viewer.plugins.OverlayPlugin.filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.filtered_image", "type": "viewer", "text": ["Return filtered image.", "This \u201cfiltered image\u201d is used when saving from the plugin."]}, {"name": "viewer.plugins.OverlayPlugin.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.output", "type": "viewer", "text": ["Return the overlaid image.", "The overlay currently displayed."]}, {"name": "viewer.plugins.OverlayPlugin.overlay()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.overlay", "type": "viewer", "text": []}, {"name": "viewer.plugins.OverlayPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.OverlayPlugin.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.PlotPlugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin", "type": "viewer", "text": ["Bases: skimage.viewer.plugins.base.Plugin", "Plugin for ImageViewer that contains a plot canvas.", "Base class for plugins that contain a Matplotlib plot canvas, which can, for example, display an image histogram.", "See base Plugin class for additional details.", "Initialize self. See help(type(self)) for accurate signature.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "Redraw plot."]}, {"name": "viewer.plugins.PlotPlugin.add_plot()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.add_plot", "type": "viewer", "text": []}, {"name": "viewer.plugins.PlotPlugin.add_tool()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.add_tool", "type": "viewer", "text": []}, {"name": "viewer.plugins.PlotPlugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.PlotPlugin.redraw()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.redraw", "type": "viewer", "text": ["Redraw plot."]}, {"name": "viewer.plugins.PlotPlugin.remove_tool()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.remove_tool", "type": "viewer", "text": []}, {"name": "viewer.plugins.PlotPlugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.PlotPlugin.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.plugins.Plugin", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin", "type": "viewer", "text": ["Bases: object", "Base class for plugins that interact with an ImageViewer.", "A plugin connects an image filter (or another function) to an image viewer. Note that a Plugin is initialized without an image viewer and attached in a later step. See example below for details.", "Window containing image used in measurement/manipulation.", "Function that gets called to update image in image viewer. This value can be None if, for example, you have a plugin that extracts information from an image and doesn\u2019t manipulate it. Alternatively, this function can be defined as a method in a Plugin subclass.", "Size of plugin window in pixels. Note that Qt will automatically resize a window to fit components. So if you\u2019re adding rows of components, you can leave height = 0 and just let Qt determine the final height.", "If True, use blitting to speed up animation. Only available on some Matplotlib backends. If None, set to True when using Agg backend. This only has an effect if you draw on top of an image viewer.", "The plugin will automatically delegate parameters to image_filter based on its parameter type, i.e., ptype (widgets for required arguments must be added in the order they appear in the function). The image attached to the viewer is automatically passed as the first argument to the filter function.", "#TODO: Add flag so image is not passed to filter function by default.", "ptype = \u2018kwarg\u2019 is the default for most widgets so it\u2019s unnecessary here.", "Window containing image used in measurement.", "Name of plugin. This is displayed as the window title.", "List of Matplotlib artists and canvastools. Any artists created by the plugin should be added to this list so that it gets cleaned up on close.", "Initialize self. See help(type(self)) for accurate signature.", "Add widget to plugin.", "Alternatively, Plugin\u2019s __add__ method is overloaded to add widgets:", "Widgets can adjust required or optional arguments of filter function or parameters for the plugin. This is specified by the Widget\u2019s ptype.", "Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets.", "On close disconnect all artists and events from ImageViewer.", "Note that artists must be appended to self.artists.", "Display the filtered image on image viewer.", "If you don\u2019t want to simply replace the displayed image with the filtered image (e.g., you want to display a transparent overlay), you can override this method.", "Call image_filter with widget args and kwargs", "Note: display_filtered_image is automatically called.", "Return filtered image.", "Return the plugin\u2019s representation and data.", "The filtered image.", "Any data associated with the plugin.", "Derived classes should override this method to return a tuple containing an overlay of the same shape of the image, and a data object. Either of these is optional: return None if you don\u2019t want to return a value.", "Remove artists that are connected to the image viewer.", "Show plugin.", "Update keyword parameters of the plugin itself.", "These parameters will typically be implemented as class properties so that they update the image or some other component."]}, {"name": "viewer.plugins.Plugin.add_widget()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.add_widget", "type": "viewer", "text": ["Add widget to plugin.", "Alternatively, Plugin\u2019s __add__ method is overloaded to add widgets:", "Widgets can adjust required or optional arguments of filter function or parameters for the plugin. This is specified by the Widget\u2019s ptype."]}, {"name": "viewer.plugins.Plugin.attach()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.attach", "type": "viewer", "text": ["Attach the plugin to an ImageViewer.", "Note that the ImageViewer will automatically call this method when the plugin is added to the ImageViewer. For example:", "Also note that attach automatically calls the filter function so that the image matches the filtered value specified by attached widgets."]}, {"name": "viewer.plugins.Plugin.clean_up()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.clean_up", "type": "viewer", "text": []}, {"name": "viewer.plugins.Plugin.closeEvent()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.closeEvent", "type": "viewer", "text": ["On close disconnect all artists and events from ImageViewer.", "Note that artists must be appended to self.artists."]}, {"name": "viewer.plugins.Plugin.display_filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.display_filtered_image", "type": "viewer", "text": ["Display the filtered image on image viewer.", "If you don\u2019t want to simply replace the displayed image with the filtered image (e.g., you want to display a transparent overlay), you can override this method."]}, {"name": "viewer.plugins.Plugin.filtered_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.filtered_image", "type": "viewer", "text": ["Return filtered image."]}, {"name": "viewer.plugins.Plugin.filter_image()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.filter_image", "type": "viewer", "text": ["Call image_filter with widget args and kwargs", "Note: display_filtered_image is automatically called."]}, {"name": "viewer.plugins.Plugin.image_changed", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.image_changed", "type": "viewer", "text": []}, {"name": "viewer.plugins.Plugin.image_viewer", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.image_viewer", "type": "viewer", "text": []}, {"name": "viewer.plugins.Plugin.name", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.name", "type": "viewer", "text": []}, {"name": "viewer.plugins.Plugin.output()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.output", "type": "viewer", "text": ["Return the plugin\u2019s representation and data.", "The filtered image.", "Any data associated with the plugin.", "Derived classes should override this method to return a tuple containing an overlay of the same shape of the image, and a data object. Either of these is optional: return None if you don\u2019t want to return a value."]}, {"name": "viewer.plugins.Plugin.remove_image_artists()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.remove_image_artists", "type": "viewer", "text": ["Remove artists that are connected to the image viewer."]}, {"name": "viewer.plugins.Plugin.show()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.show", "type": "viewer", "text": ["Show plugin."]}, {"name": "viewer.plugins.Plugin.update_plugin()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.update_plugin", "type": "viewer", "text": ["Update keyword parameters of the plugin itself.", "These parameters will typically be implemented as class properties so that they update the image or some other component."]}, {"name": "viewer.plugins.Plugin.__init__()", "path": "api/skimage.viewer.plugins#skimage.viewer.plugins.Plugin.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.utils", "path": "api/skimage.viewer.utils", "type": "viewer", "text": ["skimage.viewer.utils.figimage(image[, \u2026])", "Return figure and axes with figure tightly surrounding image.", "skimage.viewer.utils.init_qtapp()", "Initialize QAppliction.", "skimage.viewer.utils.new_plot([parent, \u2026])", "Return new figure and axes.", "skimage.viewer.utils.start_qtapp([app])", "Start Qt mainloop", "skimage.viewer.utils.update_axes_image(\u2026)", "Update the image displayed by an image plot.", "skimage.viewer.utils.ClearColormap(rgb[, \u2026])", "Color map that varies linearly from alpha = 0 to 1", "skimage.viewer.utils.FigureCanvas(figure, \u2026)", "Canvas for displaying images.", "skimage.viewer.utils.LinearColormap(name, \u2026)", "LinearSegmentedColormap in which color varies smoothly.", "skimage.viewer.utils.RequiredAttr([init_val])", "A class attribute that must be set before use.", "skimage.viewer.utils.canvas", "skimage.viewer.utils.core", "skimage.viewer.utils.dialogs", "Return figure and axes with figure tightly surrounding image.", "Unlike pyplot.figimage, this actually plots onto an axes object, which fills the figure. Plotting the image onto an axes allows for subsequent overlays of axes artists.", "image to plot", "If scale is 1, the figure and axes have the same dimension as the image. Smaller values of scale will shrink the figure.", "Dots per inch for figure. If None, use the default rcParam.", "Initialize QAppliction.", "The QApplication needs to be initialized before creating any QWidgets", "Return new figure and axes.", "Qt widget that displays the plot objects. If None, you must manually call canvas.setParent and pass the parent widget.", "Keyword arguments passed matplotlib.figure.Figure.add_subplot.", "Keyword arguments passed matplotlib.figure.Figure.", "Start Qt mainloop", "Update the image displayed by an image plot.", "This sets the image plot\u2019s array and updates its shape appropriately", "Image axes to update.", "Image array.", "Bases: skimage.viewer.utils.core.LinearColormap", "Color map that varies linearly from alpha = 0 to 1", "Create color map from linear mapping segments", "segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional.", "Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use:", "Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]:", "Hence y0 in the first row and y1 in the last row are never used.", "See also", "Static method; factory function for generating a smoothly-varying LinearSegmentedColormap.", "For information about making a mapping array.", "Bases: object", "Canvas for displaying images.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: matplotlib.colors.LinearSegmentedColormap", "LinearSegmentedColormap in which color varies smoothly.", "This class is a simplification of LinearSegmentedColormap, which doesn\u2019t support jumps in color intensities.", "Name of colormap.", "Dictionary of \u2018red\u2019, \u2018green\u2019, \u2018blue\u2019, and (optionally) \u2018alpha\u2019 values. Each color key contains a list of x, y tuples. x must increase monotonically from 0 to 1 and corresponds to input values for a mappable object (e.g. an image). y corresponds to the color intensity.", "Create color map from linear mapping segments", "segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional.", "Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use:", "Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]:", "Hence y0 in the first row and y1 in the last row are never used.", "See also", "Static method; factory function for generating a smoothly-varying LinearSegmentedColormap.", "For information about making a mapping array.", "Bases: object", "A class attribute that must be set before use.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.utils.ClearColormap", "path": "api/skimage.viewer.utils#skimage.viewer.utils.ClearColormap", "type": "viewer", "text": ["Bases: skimage.viewer.utils.core.LinearColormap", "Color map that varies linearly from alpha = 0 to 1", "Create color map from linear mapping segments", "segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional.", "Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use:", "Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]:", "Hence y0 in the first row and y1 in the last row are never used.", "See also", "Static method; factory function for generating a smoothly-varying LinearSegmentedColormap.", "For information about making a mapping array."]}, {"name": "viewer.utils.ClearColormap.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.ClearColormap.__init__", "type": "viewer", "text": ["Create color map from linear mapping segments", "segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional.", "Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use:", "Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]:", "Hence y0 in the first row and y1 in the last row are never used.", "See also", "Static method; factory function for generating a smoothly-varying LinearSegmentedColormap.", "For information about making a mapping array."]}, {"name": "viewer.utils.figimage()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.figimage", "type": "viewer", "text": ["Return figure and axes with figure tightly surrounding image.", "Unlike pyplot.figimage, this actually plots onto an axes object, which fills the figure. Plotting the image onto an axes allows for subsequent overlays of axes artists.", "image to plot", "If scale is 1, the figure and axes have the same dimension as the image. Smaller values of scale will shrink the figure.", "Dots per inch for figure. If None, use the default rcParam."]}, {"name": "viewer.utils.FigureCanvas", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas", "type": "viewer", "text": ["Bases: object", "Canvas for displaying images.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.utils.FigureCanvas.resizeEvent()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas.resizeEvent", "type": "viewer", "text": []}, {"name": "viewer.utils.FigureCanvas.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.FigureCanvas.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.utils.init_qtapp()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.init_qtapp", "type": "viewer", "text": ["Initialize QAppliction.", "The QApplication needs to be initialized before creating any QWidgets"]}, {"name": "viewer.utils.LinearColormap", "path": "api/skimage.viewer.utils#skimage.viewer.utils.LinearColormap", "type": "viewer", "text": ["Bases: matplotlib.colors.LinearSegmentedColormap", "LinearSegmentedColormap in which color varies smoothly.", "This class is a simplification of LinearSegmentedColormap, which doesn\u2019t support jumps in color intensities.", "Name of colormap.", "Dictionary of \u2018red\u2019, \u2018green\u2019, \u2018blue\u2019, and (optionally) \u2018alpha\u2019 values. Each color key contains a list of x, y tuples. x must increase monotonically from 0 to 1 and corresponds to input values for a mappable object (e.g. an image). y corresponds to the color intensity.", "Create color map from linear mapping segments", "segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional.", "Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use:", "Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]:", "Hence y0 in the first row and y1 in the last row are never used.", "See also", "Static method; factory function for generating a smoothly-varying LinearSegmentedColormap.", "For information about making a mapping array."]}, {"name": "viewer.utils.LinearColormap.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.LinearColormap.__init__", "type": "viewer", "text": ["Create color map from linear mapping segments", "segmentdata argument is a dictionary with a red, green and blue entries. Each entry should be a list of x, y0, y1 tuples, forming rows in a table. Entries for alpha are optional.", "Example: suppose you want red to increase from 0 to 1 over the bottom half, green to do the same over the middle half, and blue over the top half. Then you would use:", "Each row in the table for a given color is a sequence of x, y0, y1 tuples. In each sequence, x must increase monotonically from 0 to 1. For any input value z falling between x[i] and x[i+1], the output value of a given color will be linearly interpolated between y1[i] and y0[i+1]:", "Hence y0 in the first row and y1 in the last row are never used.", "See also", "Static method; factory function for generating a smoothly-varying LinearSegmentedColormap.", "For information about making a mapping array."]}, {"name": "viewer.utils.new_plot()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.new_plot", "type": "viewer", "text": ["Return new figure and axes.", "Qt widget that displays the plot objects. If None, you must manually call canvas.setParent and pass the parent widget.", "Keyword arguments passed matplotlib.figure.Figure.add_subplot.", "Keyword arguments passed matplotlib.figure.Figure."]}, {"name": "viewer.utils.RequiredAttr", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr", "type": "viewer", "text": ["Bases: object", "A class attribute that must be set before use.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.utils.RequiredAttr.instances", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr.instances", "type": "viewer", "text": []}, {"name": "viewer.utils.RequiredAttr.__init__()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.RequiredAttr.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.utils.start_qtapp()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.start_qtapp", "type": "viewer", "text": ["Start Qt mainloop"]}, {"name": "viewer.utils.update_axes_image()", "path": "api/skimage.viewer.utils#skimage.viewer.utils.update_axes_image", "type": "viewer", "text": ["Update the image displayed by an image plot.", "This sets the image plot\u2019s array and updates its shape appropriately", "Image axes to update.", "Image array."]}, {"name": "viewer.viewers", "path": "api/skimage.viewer.viewers", "type": "viewer", "text": ["skimage.viewer.viewers.CollectionViewer(\u2026)", "Viewer for displaying image collections.", "skimage.viewer.viewers.ImageViewer(image[, \u2026])", "Viewer for displaying images.", "skimage.viewer.viewers.core", "ImageViewer class for viewing and interacting with images.", "Bases: skimage.viewer.viewers.core.ImageViewer", "Viewer for displaying image collections.", "Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:", "Previous/next image in collection.", "0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.", "First/last image in collection.", "List of images to be displayed.", "Control whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.", "Initialize self. See help(type(self)) for accurate signature.", "Select image on display using index into image collection.", "Bases: object", "Viewer for displaying images.", "This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions.", "Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.", "Image being viewed.", "Matplotlib canvas, figure, and axes used to display image.", "Image being viewed. Setting this value will update the displayed frame.", "Plugins typically operate on (but don\u2019t change) the original image.", "List of attached plugins.", "Initialize self. See help(type(self)) for accurate signature.", "Connect callback function to matplotlib event and return id.", "Disconnect callback by its id (returned by connect_event).", "Open image file and display in viewer.", "Save current image to file.", "The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image).", "Show ImageViewer and attached plugins.", "This behaves much like matplotlib.pyplot.show and QWidget.show.", "Update displayed image.", "This method can be overridden or extended in subclasses and plugins to react to image changes."]}, {"name": "viewer.viewers.CollectionViewer", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer", "type": "viewer", "text": ["Bases: skimage.viewer.viewers.core.ImageViewer", "Viewer for displaying image collections.", "Select the displayed frame of the image collection using the slider or with the following keyboard shortcuts:", "Previous/next image in collection.", "0% to 90% of collection. For example, \u201c5\u201d goes to the image in the middle (i.e. 50%) of the collection.", "First/last image in collection.", "List of images to be displayed.", "Control whether image is updated on slide or release of the image slider. Using \u2018on_release\u2019 will give smoother behavior when displaying large images or when writing a plugin/subclass that requires heavy computation.", "Initialize self. See help(type(self)) for accurate signature.", "Select image on display using index into image collection."]}, {"name": "viewer.viewers.CollectionViewer.keyPressEvent()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.keyPressEvent", "type": "viewer", "text": []}, {"name": "viewer.viewers.CollectionViewer.update_index()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.update_index", "type": "viewer", "text": ["Select image on display using index into image collection."]}, {"name": "viewer.viewers.CollectionViewer.__init__()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.CollectionViewer.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.viewers.ImageViewer", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer", "type": "viewer", "text": ["Bases: object", "Viewer for displaying images.", "This viewer is a simple container object that holds a Matplotlib axes for showing images. ImageViewer doesn\u2019t subclass the Matplotlib axes (or figure) because of the high probability of name collisions.", "Subclasses and plugins will likely extend the update_image method to add custom overlays or filter the displayed image.", "Image being viewed.", "Matplotlib canvas, figure, and axes used to display image.", "Image being viewed. Setting this value will update the displayed frame.", "Plugins typically operate on (but don\u2019t change) the original image.", "List of attached plugins.", "Initialize self. See help(type(self)) for accurate signature.", "Connect callback function to matplotlib event and return id.", "Disconnect callback by its id (returned by connect_event).", "Open image file and display in viewer.", "Save current image to file.", "The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image).", "Show ImageViewer and attached plugins.", "This behaves much like matplotlib.pyplot.show and QWidget.show.", "Update displayed image.", "This method can be overridden or extended in subclasses and plugins to react to image changes."]}, {"name": "viewer.viewers.ImageViewer.add_tool()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.add_tool", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.closeEvent()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.closeEvent", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.connect_event()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.connect_event", "type": "viewer", "text": ["Connect callback function to matplotlib event and return id."]}, {"name": "viewer.viewers.ImageViewer.disconnect_event()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.disconnect_event", "type": "viewer", "text": ["Disconnect callback by its id (returned by connect_event)."]}, {"name": "viewer.viewers.ImageViewer.dock_areas", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.dock_areas", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.image", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.open_file()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.open_file", "type": "viewer", "text": ["Open image file and display in viewer."]}, {"name": "viewer.viewers.ImageViewer.original_image_changed", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.original_image_changed", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.redraw()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.redraw", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.remove_tool()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.remove_tool", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.reset_image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.reset_image", "type": "viewer", "text": []}, {"name": "viewer.viewers.ImageViewer.save_to_file()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.save_to_file", "type": "viewer", "text": ["Save current image to file.", "The current behavior is not ideal: It saves the image displayed on screen, so all images will be converted to RGB, and the image size is not preserved (resizing the viewer window will alter the size of the saved image)."]}, {"name": "viewer.viewers.ImageViewer.show()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.show", "type": "viewer", "text": ["Show ImageViewer and attached plugins.", "This behaves much like matplotlib.pyplot.show and QWidget.show."]}, {"name": "viewer.viewers.ImageViewer.update_image()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.update_image", "type": "viewer", "text": ["Update displayed image.", "This method can be overridden or extended in subclasses and plugins to react to image changes."]}, {"name": "viewer.viewers.ImageViewer.__init__()", "path": "api/skimage.viewer.viewers#skimage.viewer.viewers.ImageViewer.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets", "path": "api/skimage.viewer.widgets", "type": "viewer", "text": ["Widgets for interacting with ImageViewer.", "These widgets should be added to a Plugin subclass using its add_widget method or calling:", "on a Plugin instance. The Plugin will delegate action based on the widget\u2019s parameter type specified by its ptype attribute, which can be:", "skimage.viewer.widgets.BaseWidget(name[, \u2026])", "skimage.viewer.widgets.Button(name, callback)", "Button which calls callback upon click.", "skimage.viewer.widgets.CheckBox(name[, \u2026])", "CheckBox widget", "skimage.viewer.widgets.ComboBox(name, items)", "ComboBox widget for selecting among a list of choices.", "skimage.viewer.widgets.OKCancelButtons([\u2026])", "Buttons that close the parent plugin.", "skimage.viewer.widgets.SaveButtons([name, \u2026])", "Buttons to save image to io.stack or to a file.", "skimage.viewer.widgets.Slider(name[, low, \u2026])", "Slider widget for adjusting numeric parameters.", "skimage.viewer.widgets.Text([name, text])", "skimage.viewer.widgets.core", "skimage.viewer.widgets.history", "Bases: object", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "Button which calls callback upon click.", "Name of button.", "Function to call when button is clicked.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "CheckBox widget", "Name of CheckBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the CheckBox.", "Initial state of the CheckBox.", "Checkbox alignment", "Parameter type", "Callback function called in response to checkbox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "ComboBox widget for selecting among a list of choices.", "Name of ComboBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the ComboBox.", "Allowed parameter values.", "Parameter type.", "Callback function called in response to combobox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "Buttons that close the parent plugin.", "OK will replace the original image with the current (filtered) image. Cancel will just close the plugin.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "Buttons to save image to io.stack or to a file.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "Slider widget for adjusting numeric parameters.", "Name of slider parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the slider.", "Range of slider values.", "Default slider value. If None, use midpoint between low and high.", "Numeric type of slider value.", "Parameter type.", "Callback function called in response to slider changes. Note: This function is typically set (overridden) when the widget is added to a plugin.", "Slider orientation.", "Control when callback function is called: on slider move or release.", "Initialize self. See help(type(self)) for accurate signature.", "Bases: skimage.viewer.widgets.core.BaseWidget", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.BaseWidget", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget", "type": "viewer", "text": ["Bases: object", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.BaseWidget.plugin", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.plugin", "type": "viewer", "text": []}, {"name": "viewer.widgets.BaseWidget.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.val", "type": "viewer", "text": []}, {"name": "viewer.widgets.BaseWidget.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.BaseWidget.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.Button", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Button", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "Button which calls callback upon click.", "Name of button.", "Function to call when button is clicked.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.Button.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Button.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.CheckBox", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "CheckBox widget", "Name of CheckBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the CheckBox.", "Initial state of the CheckBox.", "Checkbox alignment", "Parameter type", "Callback function called in response to checkbox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.CheckBox.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox.val", "type": "viewer", "text": []}, {"name": "viewer.widgets.CheckBox.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.CheckBox.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.ComboBox", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "ComboBox widget for selecting among a list of choices.", "Name of ComboBox parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the ComboBox.", "Allowed parameter values.", "Parameter type.", "Callback function called in response to combobox changes. Note: This function is typically set (overridden) when the widget is added to a plugin.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.ComboBox.index()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.index", "type": "viewer", "text": []}, {"name": "viewer.widgets.ComboBox.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.val", "type": "viewer", "text": []}, {"name": "viewer.widgets.ComboBox.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.ComboBox.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.OKCancelButtons", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "Buttons that close the parent plugin.", "OK will replace the original image with the current (filtered) image. Cancel will just close the plugin.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.OKCancelButtons.close_plugin()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.close_plugin", "type": "viewer", "text": []}, {"name": "viewer.widgets.OKCancelButtons.update_original_image()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.update_original_image", "type": "viewer", "text": []}, {"name": "viewer.widgets.OKCancelButtons.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.OKCancelButtons.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.SaveButtons", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "Buttons to save image to io.stack or to a file.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.SaveButtons.save_to_file()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.save_to_file", "type": "viewer", "text": []}, {"name": "viewer.widgets.SaveButtons.save_to_stack()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.save_to_stack", "type": "viewer", "text": []}, {"name": "viewer.widgets.SaveButtons.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.SaveButtons.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.Slider", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "Slider widget for adjusting numeric parameters.", "Name of slider parameter. If this parameter is passed as a keyword argument, it must match the name of that keyword argument (spaces are replaced with underscores). In addition, this name is displayed as the name of the slider.", "Range of slider values.", "Default slider value. If None, use midpoint between low and high.", "Numeric type of slider value.", "Parameter type.", "Callback function called in response to slider changes. Note: This function is typically set (overridden) when the widget is added to a plugin.", "Slider orientation.", "Control when callback function is called: on slider move or release.", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.Slider.val()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider.val", "type": "viewer", "text": []}, {"name": "viewer.widgets.Slider.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Slider.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.Text", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text", "type": "viewer", "text": ["Bases: skimage.viewer.widgets.core.BaseWidget", "Initialize self. See help(type(self)) for accurate signature."]}, {"name": "viewer.widgets.Text.text()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text.text", "type": "viewer", "text": []}, {"name": "viewer.widgets.Text.__init__()", "path": "api/skimage.viewer.widgets#skimage.viewer.widgets.Text.__init__", "type": "viewer", "text": ["Initialize self. See help(type(self)) for accurate signature."]}]