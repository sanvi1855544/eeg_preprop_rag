[{"name": "clear()", "path": "backends#clear", "type": "torch.backends", "text": " \nclear()  \nClears the cuFFT plan cache. \n"}, {"name": "max_size", "path": "backends#max_size", "type": "torch.backends", "text": " \nmax_size  \nA int that controls cache capacity of cuFFT plan. \n"}, {"name": "torch", "path": "torch", "type": "torch", "text": "torch The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0 Tensors  \n\nis_tensor\n Returns True if obj is a PyTorch tensor.  \n\nis_storage\n Returns True if obj is a PyTorch storage object.  \n\nis_complex\n Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.  \n\nis_floating_point\n Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.  \n\nis_nonzero\n Returns True if the input is a single element tensor which is not equal to zero after type conversions.  \n\nset_default_dtype\n Sets the default floating point dtype to d.  \n\nget_default_dtype\n Get the current default floating point torch.dtype.  \n\nset_default_tensor_type\n Sets the default torch.Tensor type to floating point tensor type t.  \n\nnumel\n Returns the total number of elements in the input tensor.  \n\nset_printoptions\n Set options for printing.  \n\nset_flush_denormal\n Disables denormal floating numbers on CPU.   Creation Ops  Note Random sampling creation ops are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() You may also use torch.empty() with the In-place random sampling methods to create torch.Tensor s with values sampled from a broader range of distributions.   \n\ntensor\n Constructs a tensor with data.  \n\nsparse_coo_tensor\n Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.  \n\nas_tensor\n Convert the data into a torch.Tensor.  \n\nas_strided\n Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.  \n\nfrom_numpy\n Creates a Tensor from a numpy.ndarray.  \n\nzeros\n Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.  \n\nzeros_like\n Returns a tensor filled with the scalar value 0, with the same size as input.  \n\nones\n Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.  \n\nones_like\n Returns a tensor filled with the scalar value 1, with the same size as input.  \n\narange\n Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil  with values from the interval [start, end) taken with common difference step beginning from start.  \n\nrange\n Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1  with values from start to end with step step.  \n\nlinspace\n Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.  \n\nlogspace\n Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}  to baseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale with base base.  \n\neye\n Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.  \n\nempty\n Returns a tensor filled with uninitialized data.  \n\nempty_like\n Returns an uninitialized tensor with the same size as input.  \n\nempty_strided\n Returns a tensor filled with uninitialized data.  \n\nfull\n Creates a tensor of size size filled with fill_value.  \n\nfull_like\n Returns a tensor with the same size as input filled with fill_value.  \n\nquantize_per_tensor\n Converts a float tensor to a quantized tensor with given scale and zero point.  \n\nquantize_per_channel\n Converts a float tensor to a per-channel quantized tensor with given scales and zero points.  \n\ndequantize\n Returns an fp32 Tensor by dequantizing a quantized Tensor  \n\ncomplex\n Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.  \n\npolar\n Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.  \n\nheaviside\n Computes the Heaviside step function for each element in input.   Indexing, Slicing, Joining, Mutating Ops  \n\ncat\n Concatenates the given sequence of seq tensors in the given dimension.  \n\nchunk\n Splits a tensor into a specific number of chunks.  \n\ncolumn_stack\n Creates a new tensor by horizontally stacking the tensors in tensors.  \n\ndstack\n Stack tensors in sequence depthwise (along third axis).  \n\ngather\n Gathers values along an axis specified by dim.  \n\nhstack\n Stack tensors in sequence horizontally (column wise).  \n\nindex_select\n Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.  \n\nmasked_select\n Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.  \n\nmovedim\n Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.  \n\nmoveaxis\n Alias for torch.movedim().  \n\nnarrow\n Returns a new tensor that is a narrowed version of input tensor.  \n\nnonzero\n   \n\nreshape\n Returns a tensor with the same data and number of elements as input, but with the specified shape.  \n\nrow_stack\n Alias of torch.vstack().  \n\nscatter\n Out-of-place version of torch.Tensor.scatter_()  \n\nscatter_add\n Out-of-place version of torch.Tensor.scatter_add_()  \n\nsplit\n Splits the tensor into chunks.  \n\nsqueeze\n Returns a tensor with all the dimensions of input of size 1 removed.  \n\nstack\n Concatenates a sequence of tensors along a new dimension.  \n\nswapaxes\n Alias for torch.transpose().  \n\nswapdims\n Alias for torch.transpose().  \n\nt\n Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.  \n\ntake\n Returns a new tensor with the elements of input at the given indices.  \n\ntensor_split\n Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.  \n\ntile\n Constructs a tensor by repeating the elements of input.  \n\ntranspose\n Returns a tensor that is a transposed version of input.  \n\nunbind\n Removes a tensor dimension.  \n\nunsqueeze\n Returns a new tensor with a dimension of size one inserted at the specified position.  \n\nvstack\n Stack tensors in sequence vertically (row wise).  \n\nwhere\n Return a tensor of elements selected from either x or y, depending on condition.   Generators  \n\nGenerator\n Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.   Random sampling  \n\nseed\n Sets the seed for generating random numbers to a non-deterministic random number.  \n\nmanual_seed\n Sets the seed for generating random numbers.  \n\ninitial_seed\n Returns the initial seed for generating random numbers as a Python long.  \n\nget_rng_state\n Returns the random number generator state as a torch.ByteTensor.  \n\nset_rng_state\n Sets the random number generator state.    \ntorch.default_generator Returns the default CPU torch.Generator \n  \n\nbernoulli\n Draws binary random numbers (0 or 1) from a Bernoulli distribution.  \n\nmultinomial\n Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.  \n\nnormal\n Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.  \n\npoisson\n Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,  \n\nrand\n Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)   \n\nrand_like\n Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) .  \n\nrandint\n Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).  \n\nrandint_like\n Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).  \n\nrandn\n Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).  \n\nrandn_like\n Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.  \n\nrandperm\n Returns a random permutation of integers from 0 to n - 1.   In-place random sampling There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:  \ntorch.Tensor.bernoulli_() - in-place version of torch.bernoulli()\n \ntorch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution \ntorch.Tensor.exponential_() - numbers drawn from the exponential distribution \ntorch.Tensor.geometric_() - elements drawn from the geometric distribution \ntorch.Tensor.log_normal_() - samples from the log-normal distribution \ntorch.Tensor.normal_() - in-place version of torch.normal()\n \ntorch.Tensor.random_() - numbers sampled from the discrete uniform distribution \ntorch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution  Quasi-random sampling  \nquasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.   Serialization  \n\nsave\n Saves an object to a disk file.  \n\nload\n Loads an object saved with torch.save() from a file.   Parallelism  \n\nget_num_threads\n Returns the number of threads used for parallelizing CPU operations  \n\nset_num_threads\n Sets the number of threads used for intraop parallelism on CPU.  \n\nget_num_interop_threads\n Returns the number of threads used for inter-op parallelism on CPU (e.g.  \n\nset_num_interop_threads\n Sets the number of threads used for interop parallelism (e.g.   Locally disabling gradient computation The context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled() are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. These context managers are thread local, so they won\u2019t work if you send work to another thread using the threading module, etc. Examples: >>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n  \n\nno_grad\n Context-manager that disabled gradient calculation.  \n\nenable_grad\n Context-manager that enables gradient calculation.  \n\nset_grad_enabled\n Context-manager that sets gradient calculation to on or off.   Math operations Pointwise Ops  \n\nabs\n Computes the absolute value of each element in input.  \n\nabsolute\n Alias for torch.abs()  \n\nacos\n Computes the inverse cosine of each element in input.  \n\narccos\n Alias for torch.acos().  \n\nacosh\n Returns a new tensor with the inverse hyperbolic cosine of the elements of input.  \n\narccosh\n Alias for torch.acosh().  \n\nadd\n Adds the scalar other to each element of the input input and returns a new resulting tensor.  \n\naddcdiv\n Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  \n\naddcmul\n Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  \n\nangle\n Computes the element-wise angle (in radians) of the given input tensor.  \n\nasin\n Returns a new tensor with the arcsine of the elements of input.  \n\narcsin\n Alias for torch.asin().  \n\nasinh\n Returns a new tensor with the inverse hyperbolic sine of the elements of input.  \n\narcsinh\n Alias for torch.asinh().  \n\natan\n Returns a new tensor with the arctangent of the elements of input.  \n\narctan\n Alias for torch.atan().  \n\natanh\n Returns a new tensor with the inverse hyperbolic tangent of the elements of input.  \n\narctanh\n Alias for torch.atanh().  \n\natan2\n Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}  with consideration of the quadrant.  \n\nbitwise_not\n Computes the bitwise NOT of the given input tensor.  \n\nbitwise_and\n Computes the bitwise AND of input and other.  \n\nbitwise_or\n Computes the bitwise OR of input and other.  \n\nbitwise_xor\n Computes the bitwise XOR of input and other.  \n\nceil\n Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.  \n\nclamp\n Clamp all elements in input into the range [ min, max ].  \n\nclip\n Alias for torch.clamp().  \n\nconj\n Computes the element-wise conjugate of the given input tensor.  \n\ncopysign\n Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.  \n\ncos\n Returns a new tensor with the cosine of the elements of input.  \n\ncosh\n Returns a new tensor with the hyperbolic cosine of the elements of input.  \n\ndeg2rad\n Returns a new tensor with each of the elements of input converted from angles in degrees to radians.  \n\ndiv\n Divides each element of the input input by the corresponding element of other.  \n\ndivide\n Alias for torch.div().  \n\ndigamma\n Computes the logarithmic derivative of the gamma function on input.  \n\nerf\n Computes the error function of each element.  \n\nerfc\n Computes the complementary error function of each element of input.  \n\nerfinv\n Computes the inverse error function of each element of input.  \n\nexp\n Returns a new tensor with the exponential of the elements of the input tensor input.  \n\nexp2\n Computes the base two exponential function of input.  \n\nexpm1\n Returns a new tensor with the exponential of the elements minus 1 of input.  \n\nfake_quantize_per_channel_affine\n Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.  \n\nfake_quantize_per_tensor_affine\n Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.  \n\nfix\n Alias for torch.trunc()  \n\nfloat_power\n Raises input to the power of exponent, elementwise, in double precision.  \n\nfloor\n Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.  \n\nfloor_divide\n   \n\nfmod\n Computes the element-wise remainder of division.  \n\nfrac\n Computes the fractional portion of each element in input.  \n\nimag\n Returns a new tensor containing imaginary values of the self tensor.  \n\nldexp\n Multiplies input by 2**:attr:other.  \n\nlerp\n Does a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.  \n\nlgamma\n Computes the logarithm of the gamma function on input.  \n\nlog\n Returns a new tensor with the natural logarithm of the elements of input.  \n\nlog10\n Returns a new tensor with the logarithm to the base 10 of the elements of input.  \n\nlog1p\n Returns a new tensor with the natural logarithm of (1 + input).  \n\nlog2\n Returns a new tensor with the logarithm to the base 2 of the elements of input.  \n\nlogaddexp\n Logarithm of the sum of exponentiations of the inputs.  \n\nlogaddexp2\n Logarithm of the sum of exponentiations of the inputs in base-2.  \n\nlogical_and\n Computes the element-wise logical AND of the given input tensors.  \n\nlogical_not\n Computes the element-wise logical NOT of the given input tensor.  \n\nlogical_or\n Computes the element-wise logical OR of the given input tensors.  \n\nlogical_xor\n Computes the element-wise logical XOR of the given input tensors.  \n\nlogit\n Returns a new tensor with the logit of the elements of input.  \n\nhypot\n Given the legs of a right triangle, return its hypotenuse.  \n\ni0\n Computes the zeroth order modified Bessel function of the first kind for each element of input.  \n\nigamma\n Computes the regularized lower incomplete gamma function:  \n\nigammac\n Computes the regularized upper incomplete gamma function:  \n\nmul\n Multiplies each element of the input input with the scalar other and returns a new resulting tensor.  \n\nmultiply\n Alias for torch.mul().  \n\nmvlgamma\n Computes the multivariate log-gamma function) with dimension pp  element-wise, given by  \n\nnan_to_num\n Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively.  \n\nneg\n Returns a new tensor with the negative of the elements of input.  \n\nnegative\n Alias for torch.neg()  \n\nnextafter\n Return the next floating-point value after input towards other, elementwise.  \n\npolygamma\n Computes the nthn^{th}  derivative of the digamma function on input.  \n\npow\n Takes the power of each element in input with exponent and returns a tensor with the result.  \n\nrad2deg\n Returns a new tensor with each of the elements of input converted from angles in radians to degrees.  \n\nreal\n Returns a new tensor containing real values of the self tensor.  \n\nreciprocal\n Returns a new tensor with the reciprocal of the elements of input  \n\nremainder\n Computes the element-wise remainder of division.  \n\nround\n Returns a new tensor with each of the elements of input rounded to the closest integer.  \n\nrsqrt\n Returns a new tensor with the reciprocal of the square-root of each of the elements of input.  \n\nsigmoid\n Returns a new tensor with the sigmoid of the elements of input.  \n\nsign\n Returns a new tensor with the signs of the elements of input.  \n\nsgn\n For complex tensors, this function returns a new tensor whose elemants have the same angle as that of the elements of input and absolute value 1.  \n\nsignbit\n Tests if each element of input has its sign bit set (is less than zero) or not.  \n\nsin\n Returns a new tensor with the sine of the elements of input.  \n\nsinc\n Computes the normalized sinc of input.  \n\nsinh\n Returns a new tensor with the hyperbolic sine of the elements of input.  \n\nsqrt\n Returns a new tensor with the square-root of the elements of input.  \n\nsquare\n Returns a new tensor with the square of the elements of input.  \n\nsub\n Subtracts other, scaled by alpha, from input.  \n\nsubtract\n Alias for torch.sub().  \n\ntan\n Returns a new tensor with the tangent of the elements of input.  \n\ntanh\n Returns a new tensor with the hyperbolic tangent of the elements of input.  \n\ntrue_divide\n Alias for torch.div() with rounding_mode=None.  \n\ntrunc\n Returns a new tensor with the truncated integer values of the elements of input.  \n\nxlogy\n Computes input * log(other) with the following cases.   Reduction Ops  \n\nargmax\n Returns the indices of the maximum value of all elements in the input tensor.  \n\nargmin\n Returns the indices of the minimum value(s) of the flattened tensor or along a dimension  \n\namax\n Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.  \n\namin\n Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.  \n\nall\n Tests if all elements in input evaluate to True.  \n\nany\n \n param input \nthe input tensor.     \n\nmax\n Returns the maximum value of all elements in the input tensor.  \n\nmin\n Returns the minimum value of all elements in the input tensor.  \n\ndist\n Returns the p-norm of (input - other)  \n\nlogsumexp\n Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.  \n\nmean\n Returns the mean value of all elements in the input tensor.  \n\nmedian\n Returns the median of the values in input.  \n\nnanmedian\n Returns the median of the values in input, ignoring NaN values.  \n\nmode\n Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.  \n\nnorm\n Returns the matrix norm or vector norm of a given tensor.  \n\nnansum\n Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.  \n\nprod\n Returns the product of all elements in the input tensor.  \n\nquantile\n Returns the q-th quantiles of all elements in the input tensor, doing a linear interpolation when the q-th quantile lies between two data points.  \n\nnanquantile\n This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.  \n\nstd\n Returns the standard-deviation of all elements in the input tensor.  \n\nstd_mean\n Returns the standard-deviation and mean of all elements in the input tensor.  \n\nsum\n Returns the sum of all elements in the input tensor.  \n\nunique\n Returns the unique elements of the input tensor.  \n\nunique_consecutive\n Eliminates all but the first element from every consecutive group of equivalent elements.  \n\nvar\n Returns the variance of all elements in the input tensor.  \n\nvar_mean\n Returns the variance and mean of all elements in the input tensor.  \n\ncount_nonzero\n Counts the number of non-zero values in the tensor input along the given dim.   Comparison Ops  \n\nallclose\n This function checks if all input and other satisfy the condition:  \n\nargsort\n Returns the indices that sort a tensor along a given dimension in ascending order by value.  \n\neq\n Computes element-wise equality  \n\nequal\n True if two tensors have the same size and elements, False otherwise.  \n\nge\n Computes input\u2265other\\text{input} \\geq \\text{other}  element-wise.  \n\ngreater_equal\n Alias for torch.ge().  \n\ngt\n Computes input>other\\text{input} > \\text{other}  element-wise.  \n\ngreater\n Alias for torch.gt().  \n\nisclose\n Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.  \n\nisfinite\n Returns a new tensor with boolean elements representing if each element is finite or not.  \n\nisinf\n Tests if each element of input is infinite (positive or negative infinity) or not.  \n\nisposinf\n Tests if each element of input is positive infinity or not.  \n\nisneginf\n Tests if each element of input is negative infinity or not.  \n\nisnan\n Returns a new tensor with boolean elements representing if each element of input is NaN or not.  \n\nisreal\n Returns a new tensor with boolean elements representing if each element of input is real-valued or not.  \n\nkthvalue\n Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.  \n\nle\n Computes input\u2264other\\text{input} \\leq \\text{other}  element-wise.  \n\nless_equal\n Alias for torch.le().  \n\nlt\n Computes input<other\\text{input} < \\text{other}  element-wise.  \n\nless\n Alias for torch.lt().  \n\nmaximum\n Computes the element-wise maximum of input and other.  \n\nminimum\n Computes the element-wise minimum of input and other.  \n\nfmax\n Computes the element-wise maximum of input and other.  \n\nfmin\n Computes the element-wise minimum of input and other.  \n\nne\n Computes input\u2260other\\text{input} \\neq \\text{other}  element-wise.  \n\nnot_equal\n Alias for torch.ne().  \n\nsort\n Sorts the elements of the input tensor along a given dimension in ascending order by value.  \n\ntopk\n Returns the k largest elements of the given input tensor along a given dimension.  \n\nmsort\n Sorts the elements of the input tensor along its first dimension in ascending order by value.   Spectral Ops  \n\nstft\n Short-time Fourier transform (STFT).  \n\nistft\n Inverse short time Fourier Transform.  \n\nbartlett_window\n Bartlett window function.  \n\nblackman_window\n Blackman window function.  \n\nhamming_window\n Hamming window function.  \n\nhann_window\n Hann window function.  \n\nkaiser_window\n Computes the Kaiser window with window length window_length and shape parameter beta.   Other Operations  \n\natleast_1d\n Returns a 1-dimensional view of each input tensor with zero dimensions.  \n\natleast_2d\n Returns a 2-dimensional view of each input tensor with zero dimensions.  \n\natleast_3d\n Returns a 3-dimensional view of each input tensor with zero dimensions.  \n\nbincount\n Count the frequency of each value in an array of non-negative ints.  \n\nblock_diag\n Create a block diagonal matrix from provided tensors.  \n\nbroadcast_tensors\n Broadcasts the given tensors according to Broadcasting semantics.  \n\nbroadcast_to\n Broadcasts input to the shape shape.  \n\nbroadcast_shapes\n Similar to broadcast_tensors() but for shapes.  \n\nbucketize\n Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.  \n\ncartesian_prod\n Do cartesian product of the given sequence of tensors.  \n\ncdist\n Computes batched the p-norm distance between each pair of the two collections of row vectors.  \n\nclone\n Returns a copy of input.  \n\ncombinations\n Compute combinations of length rr  of the given tensor.  \n\ncross\n Returns the cross product of vectors in dimension dim of input and other.  \n\ncummax\n Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.  \n\ncummin\n Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.  \n\ncumprod\n Returns the cumulative product of elements of input in the dimension dim.  \n\ncumsum\n Returns the cumulative sum of elements of input in the dimension dim.  \n\ndiag\n \n If input is a vector (1-D tensor), then returns a 2-D square tensor    \n\ndiag_embed\n Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.  \n\ndiagflat\n \n If input is a vector (1-D tensor), then returns a 2-D square tensor    \n\ndiagonal\n Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.  \n\ndiff\n Computes the n-th forward difference along the given dimension.  \n\neinsum\n Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.  \n\nflatten\n Flattens input by reshaping it into a one-dimensional tensor.  \n\nflip\n Reverse the order of a n-D tensor along given axis in dims.  \n\nfliplr\n Flip tensor in the left/right direction, returning a new tensor.  \n\nflipud\n Flip tensor in the up/down direction, returning a new tensor.  \n\nkron\n Computes the Kronecker product, denoted by \u2297\\otimes , of input and other.  \n\nrot90\n Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.  \n\ngcd\n Computes the element-wise greatest common divisor (GCD) of input and other.  \n\nhistc\n Computes the histogram of a tensor.  \n\nmeshgrid\n Take NN  tensors, each of which can be either scalar or 1-dimensional vector, and create NN  N-dimensional grids, where the ii  th grid is defined by expanding the ii  th input over dimensions defined by other inputs.  \n\nlcm\n Computes the element-wise least common multiple (LCM) of input and other.  \n\nlogcumsumexp\n Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.  \n\nravel\n Return a contiguous flattened tensor.  \n\nrenorm\n Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm  \n\nrepeat_interleave\n Repeat elements of a tensor.  \n\nroll\n Roll the tensor along the given dimension(s).  \n\nsearchsorted\n Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, the order of the corresponding innermost dimension within sorted_sequence would be preserved.  \n\ntensordot\n Returns a contraction of a and b over multiple dimensions.  \n\ntrace\n Returns the sum of the elements of the diagonal of the input 2-D matrix.  \n\ntril\n Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.  \n\ntril_indices\n Returns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \n\ntriu\n Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.  \n\ntriu_indices\n Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \n\nvander\n Generates a Vandermonde matrix.  \n\nview_as_real\n Returns a view of input as a real tensor.  \n\nview_as_complex\n Returns a view of input as a complex tensor.   BLAS and LAPACK Operations  \n\naddbmm\n Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).  \n\naddmm\n Performs a matrix multiplication of the matrices mat1 and mat2.  \n\naddmv\n Performs a matrix-vector product of the matrix mat and the vector vec.  \n\naddr\n Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.  \n\nbaddbmm\n Performs a batch matrix-matrix product of matrices in batch1 and batch2.  \n\nbmm\n Performs a batch matrix-matrix product of matrices stored in input and mat2.  \n\nchain_matmul\n Returns the matrix product of the NN  2-D tensors.  \n\ncholesky\n Computes the Cholesky decomposition of a symmetric positive-definite matrix AA  or for batches of symmetric positive-definite matrices.  \n\ncholesky_inverse\n Computes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv.  \n\ncholesky_solve\n Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu .  \n\ndot\n Computes the dot product of two 1D tensors.  \n\neig\n Computes the eigenvalues and eigenvectors of a real square matrix.  \n\ngeqrf\n This is a low-level function for calling LAPACK directly.  \n\nger\n Alias of torch.outer().  \n\ninner\n Computes the dot product for 1D tensors.  \n\ninverse\n Takes the inverse of the square matrix input.  \n\ndet\n Calculates determinant of a square matrix or batches of square matrices.  \n\nlogdet\n Calculates log determinant of a square matrix or batches of square matrices.  \n\nslogdet\n Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.  \n\nlstsq\n Computes the solution to the least squares and least norm problems for a full rank matrix AA  of size (m\u00d7n)(m \\times n)  and a matrix BB  of size (m\u00d7k)(m \\times k) .  \n\nlu\n Computes the LU factorization of a matrix or batches of matrices A.  \n\nlu_solve\n Returns the LU solve of the linear system Ax=bAx = b  using the partially pivoted LU factorization of A from torch.lu().  \n\nlu_unpack\n Unpacks the data and pivots from a LU factorization of a tensor.  \n\nmatmul\n Matrix product of two tensors.  \n\nmatrix_power\n Returns the matrix raised to the power n for square matrices.  \n\nmatrix_rank\n Returns the numerical rank of a 2-D tensor.  \n\nmatrix_exp\n Returns the matrix exponential.  \n\nmm\n Performs a matrix multiplication of the matrices input and mat2.  \n\nmv\n Performs a matrix-vector product of the matrix input and the vector vec.  \n\norgqr\n Computes the orthogonal matrix Q of a QR factorization, from the (input, input2) tuple returned by torch.geqrf().  \n\normqr\n Multiplies mat (given by input3) by the orthogonal Q matrix of the QR factorization formed by torch.geqrf() that is represented by (a, tau) (given by (input, input2)).  \n\nouter\n Outer product of input and vec2.  \n\npinverse\n Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.  \n\nqr\n Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices.  \n\nsolve\n This function returns the solution to the system of linear equations represented by AX=BAX = B  and the LU factorization of A, in order as a namedtuple solution, LU.  \n\nsvd\n Computes the singular value decomposition of either a matrix or batch of matrices input.  \n\nsvd_lowrank\n Return the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix AA  such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T .  \n\npca_lowrank\n Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.  \n\nsymeig\n This function returns eigenvalues and eigenvectors of a real symmetric matrix input or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).  \n\nlobpcg\n Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.  \n\ntrapz\n Estimate \u222bydx\\int y\\,dx  along dim, using the trapezoid rule.  \n\ntriangular_solve\n Solves a system of equations with a triangular coefficient matrix AA  and multiple right-hand sides bb .  \n\nvdot\n Computes the dot product of two 1D tensors.   Utilities  \n\ncompiled_with_cxx11_abi\n Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1  \n\nresult_type\n Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.  \n\ncan_cast\n Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  \n\npromote_types\n Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.  \n\nuse_deterministic_algorithms\n Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.  \n\nare_deterministic_algorithms_enabled\n Returns True if the global deterministic flag is turned on.  \n\n_assert\n A wrapper around Python\u2019s assert which is symbolically traceable.  \n"}, {"name": "torch.abs()", "path": "generated/torch.abs#torch.abs", "type": "torch", "text": " \ntorch.abs(input, *, out=None) \u2192 Tensor  \nComputes the absolute value of each element in input.  outi=\u2223inputi\u2223\\text{out}_{i} = |\\text{input}_{i}|  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n \n"}, {"name": "torch.absolute()", "path": "generated/torch.absolute#torch.absolute", "type": "torch", "text": " \ntorch.absolute(input, *, out=None) \u2192 Tensor  \nAlias for torch.abs() \n"}, {"name": "torch.acos()", "path": "generated/torch.acos#torch.acos", "type": "torch", "text": " \ntorch.acos(input, *, out=None) \u2192 Tensor  \nComputes the inverse cosine of each element in input.  outi=cos\u2061\u22121(inputi)\\text{out}_{i} = \\cos^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n \n"}, {"name": "torch.acosh()", "path": "generated/torch.acosh#torch.acosh", "type": "torch", "text": " \ntorch.acosh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic cosine of the elements of input.  Note The domain of the inverse hyperbolic cosine is [1, inf) and values outside this range will be mapped to NaN, except for + INF for which the output is mapped to + INF.   outi=cosh\u2061\u22121(inputi)\\text{out}_{i} = \\cosh^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])\n \n"}, {"name": "torch.add()", "path": "generated/torch.add#torch.add", "type": "torch", "text": " \ntorch.add(input, other, *, out=None)  \nAdds the scalar other to each element of the input input and returns a new resulting tensor.  out=input+other\\text{out} = \\text{input} + \\text{other}  \nIf input is of type FloatTensor or DoubleTensor, other must be a real number, otherwise it should be an integer.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nvalue (Number) \u2013 the number to be added to each element of input\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n  \ntorch.add(input, other, *, alpha=1, out=None) \n Each element of the tensor other is multiplied by the scalar alpha and added to each element of the tensor input. The resulting tensor is returned. The shapes of input and other must be broadcastable.  out=input+alpha\u00d7other\\text{out} = \\text{input} + \\text{alpha} \\times \\text{other}  \nIf other is of type FloatTensor or DoubleTensor, alpha must be a real number, otherwise it should be an integer.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \n \nalpha (Number) \u2013 the scalar multiplier for other\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(a, b, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n \n"}, {"name": "torch.addbmm()", "path": "generated/torch.addbmm#torch.addbmm", "type": "torch", "text": " \ntorch.addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, input must be broadcastable with a (n\u00d7p)(n \\times p)  tensor and out will be a (n\u00d7p)(n \\times p)  tensor.  out=\u03b2 input+\u03b1(\u2211i=0b\u22121batch1i@batch2i)out = \\beta\\ \\text{input} + \\alpha\\ (\\sum_{i=0}^{b-1} \\text{batch1}_i \\mathbin{@} \\text{batch2}_i)  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32.  Parameters \n \nbatch1 (Tensor) \u2013 the first batch of matrices to be multiplied \nbatch2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \ninput (Tensor) \u2013 matrix to be added \nalpha (Number, optional) \u2013 multiplier for batch1 @ batch2 (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n \n"}, {"name": "torch.addcdiv()", "path": "generated/torch.addcdiv#torch.addcdiv", "type": "torch", "text": " \ntorch.addcdiv(input, tensor1, tensor2, *, value=1, out=None) \u2192 Tensor  \nPerforms the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  Warning Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.   outi=inputi+value\u00d7tensor1itensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}  \nThe shapes of input, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \ntensor1 (Tensor) \u2013 the numerator tensor \ntensor2 (Tensor) \u2013 the denominator tensor   Keyword Arguments \n \nvalue (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2} \n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])\n \n"}, {"name": "torch.addcmul()", "path": "generated/torch.addcmul#torch.addcmul", "type": "torch", "text": " \ntorch.addcmul(input, tensor1, tensor2, *, value=1, out=None) \u2192 Tensor  \nPerforms the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  outi=inputi+value\u00d7tensor1i\u00d7tensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\text{tensor1}_i \\times \\text{tensor2}_i  \nThe shapes of tensor, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \ntensor1 (Tensor) \u2013 the tensor to be multiplied \ntensor2 (Tensor) \u2013 the tensor to be multiplied   Keyword Arguments \n \nvalue (Number, optional) \u2013 multiplier for tensor1.\u2217tensor2tensor1 .* tensor2 \n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])\n \n"}, {"name": "torch.addmm()", "path": "generated/torch.addmm#torch.addmm", "type": "torch", "text": " \ntorch.addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result. If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p)  tensor and out will be a (n\u00d7p)(n \\times p)  tensor. alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.  out=\u03b2 input+\u03b1(mat1i@mat2i)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat1}_i \\mathbin{@} \\text{mat2}_i)  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32.  Parameters \n \ninput (Tensor) \u2013 matrix to be added \nmat1 (Tensor) \u2013 the first matrix to be matrix multiplied \nmat2 (Tensor) \u2013 the second matrix to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])\n \n"}, {"name": "torch.addmv()", "path": "generated/torch.addmv#torch.addmv", "type": "torch", "text": " \ntorch.addmv(input, mat, vec, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result. If mat is a (n\u00d7m)(n \\times m)  tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n. alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.  out=\u03b2 input+\u03b1(mat@vec)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat} \\mathbin{@} \\text{vec})  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers  Parameters \n \ninput (Tensor) \u2013 vector to be added \nmat (Tensor) \u2013 matrix to be matrix multiplied \nvec (Tensor) \u2013 vector to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat@vecmat @ vec  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n \n"}, {"name": "torch.addr()", "path": "generated/torch.addr#torch.addr", "type": "torch", "text": " \ntorch.addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms the outer-product of vectors vec1 and vec2 and adds it to the matrix input. Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.  out=\u03b2 input+\u03b1(vec1\u2297vec2)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{vec1} \\otimes \\text{vec2})  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m)  and out will be a matrix of size (n\u00d7m)(n \\times m) .  Parameters \n \ninput (Tensor) \u2013 matrix to be added \nvec1 (Tensor) \u2013 the first vector of the outer product \nvec2 (Tensor) \u2013 the second vector of the outer product   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for vec1\u2297vec2\\text{vec1} \\otimes \\text{vec2}  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])\n \n"}, {"name": "torch.all()", "path": "generated/torch.all#torch.all", "type": "torch", "text": " \ntorch.all(input) \u2192 Tensor  \nTests if all elements in input evaluate to True.  Note This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.  Example: >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)\n  \ntorch.all(input, dim, keepdim=False, *, out=None) \u2192 Tensor \n For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)\n \n"}, {"name": "torch.allclose()", "path": "generated/torch.allclose#torch.allclose", "type": "torch", "text": " \ntorch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 bool  \nThis function checks if all input and other satisfy the condition:  \u2223input\u2212other\u2223\u2264atol+rtol\u00d7\u2223other\u2223\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert  \nelementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose  Parameters \n \ninput (Tensor) \u2013 first tensor to compare \nother (Tensor) \u2013 second tensor to compare \natol (float, optional) \u2013 absolute tolerance. Default: 1e-08 \nrtol (float, optional) \u2013 relative tolerance. Default: 1e-05 \nequal_nan (bool, optional) \u2013 if True, then two NaN s will be considered equal. Default: False\n    Example: >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n \n"}, {"name": "torch.amax()", "path": "generated/torch.amax#torch.amax", "type": "torch", "text": " \ntorch.amax(input, dim, keepdim=False, *, out=None) \u2192 Tensor  \nReturns the maximum value of each slice of the input tensor in the given dimension(s) dim.  Note  \nThe difference between max/min and amax/amin is: \n\n \namax/amin supports reducing on multiple dimensions, \namax/amin does not return indices, \namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.     If keepdim is ``True`, the output tensors are of the same size as input except in the dimension(s) dim where they are of size 1. Otherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in the output tensors having fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])\n \n"}, {"name": "torch.amin()", "path": "generated/torch.amin#torch.amin", "type": "torch", "text": " \ntorch.amin(input, dim, keepdim=False, *, out=None) \u2192 Tensor  \nReturns the minimum value of each slice of the input tensor in the given dimension(s) dim.  Note  \nThe difference between max/min and amax/amin is: \n\n \namax/amin supports reducing on multiple dimensions, \namax/amin does not return indices, \namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.     If keepdim is True, the output tensors are of the same size as input except in the dimension(s) dim where they are of size 1. Otherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in the output tensors having fewer dimensions than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])\n \n"}, {"name": "torch.angle()", "path": "generated/torch.angle#torch.angle", "type": "torch", "text": " \ntorch.angle(input, *, out=None) \u2192 Tensor  \nComputes the element-wise angle (in radians) of the given input tensor.  outi=angle(inputi)\\text{out}_{i} = angle(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Note Starting in PyTorch 1.8, angle returns pi for negative real numbers, zero for non-negative real numbers, and propagates NaNs. Previously the function would return zero for all real numbers and not propagate floating-point NaNs.  Example: >>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])\n \n"}, {"name": "torch.any()", "path": "generated/torch.any#torch.any", "type": "torch", "text": " \ntorch.any(input) \u2192 Tensor  \n Parameters \ninput (Tensor) \u2013 the input tensor.   Tests if any element in input evaluates to True.  Note This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.  Example: >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)\n  \ntorch.any(input, dim, keepdim=False, *, out=None) \u2192 Tensor \n For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 2) < 0\n>>> a\ntensor([[ True,  True],\n        [False,  True],\n        [ True,  True],\n        [False, False]])\n>>> torch.any(a, 1)\ntensor([ True,  True,  True, False])\n>>> torch.any(a, 0)\ntensor([True, True])\n \n"}, {"name": "torch.arange()", "path": "generated/torch.arange#torch.arange", "type": "torch", "text": " \ntorch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil  with values from the interval [start, end) taken with common difference step beginning from start. Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise adding a small epsilon to end in such cases.  outi+1=outi+step\\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}  \n Parameters \n \nstart (Number) \u2013 the starting value for the set of points. Default: 0. \nend (Number) \u2013 the ending value for the set of points \nstep (Number) \u2013 the gap between each pair of adjacent points. Default: 1.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64. \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n \n"}, {"name": "torch.arccos()", "path": "generated/torch.arccos#torch.arccos", "type": "torch", "text": " \ntorch.arccos(input, *, out=None) \u2192 Tensor  \nAlias for torch.acos(). \n"}, {"name": "torch.arccosh()", "path": "generated/torch.arccosh#torch.arccosh", "type": "torch", "text": " \ntorch.arccosh(input, *, out=None) \u2192 Tensor  \nAlias for torch.acosh(). \n"}, {"name": "torch.arcsin()", "path": "generated/torch.arcsin#torch.arcsin", "type": "torch", "text": " \ntorch.arcsin(input, *, out=None) \u2192 Tensor  \nAlias for torch.asin(). \n"}, {"name": "torch.arcsinh()", "path": "generated/torch.arcsinh#torch.arcsinh", "type": "torch", "text": " \ntorch.arcsinh(input, *, out=None) \u2192 Tensor  \nAlias for torch.asinh(). \n"}, {"name": "torch.arctan()", "path": "generated/torch.arctan#torch.arctan", "type": "torch", "text": " \ntorch.arctan(input, *, out=None) \u2192 Tensor  \nAlias for torch.atan(). \n"}, {"name": "torch.arctanh()", "path": "generated/torch.arctanh#torch.arctanh", "type": "torch", "text": " \ntorch.arctanh(input, *, out=None) \u2192 Tensor  \nAlias for torch.atanh(). \n"}, {"name": "torch.are_deterministic_algorithms_enabled()", "path": "generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled", "type": "torch", "text": " \ntorch.are_deterministic_algorithms_enabled() [source]\n \nReturns True if the global deterministic flag is turned on. Refer to torch.use_deterministic_algorithms() documentation for more details. \n"}, {"name": "torch.argmax()", "path": "generated/torch.argmax#torch.argmax", "type": "torch", "text": " \ntorch.argmax(input) \u2192 LongTensor  \nReturns the indices of the maximum value of all elements in the input tensor. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Note If there are multiple minimal values then the indices of the first minimal value are returned.   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)\n  \ntorch.argmax(input, dim, keepdim=False) \u2192 LongTensor \n Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. If None, the argmax of the flattened input is returned. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])\n \n"}, {"name": "torch.argmin()", "path": "generated/torch.argmin#torch.argmin", "type": "torch", "text": " \ntorch.argmin(input, dim=None, keepdim=False) \u2192 LongTensor  \nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.  Note If there are multiple minimal values then the indices of the first minimal value are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. If None, the argmin of the flattened input is returned. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])\n \n"}, {"name": "torch.argsort()", "path": "generated/torch.argsort#torch.argsort", "type": "torch", "text": " \ntorch.argsort(input, dim=-1, descending=False) \u2192 LongTensor  \nReturns the indices that sort a tensor along a given dimension in ascending order by value. This is the second value returned by torch.sort(). See its documentation for the exact semantics of this method.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int, optional) \u2013 the dimension to sort along \ndescending (bool, optional) \u2013 controls the sorting order (ascending or descending)    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])\n \n"}, {"name": "torch.asin()", "path": "generated/torch.asin#torch.asin", "type": "torch", "text": " \ntorch.asin(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the arcsine of the elements of input.  outi=sin\u2061\u22121(inputi)\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n \n"}, {"name": "torch.asinh()", "path": "generated/torch.asinh#torch.asinh", "type": "torch", "text": " \ntorch.asinh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic sine of the elements of input.  outi=sinh\u2061\u22121(inputi)\\text{out}_{i} = \\sinh^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])\n \n"}, {"name": "torch.as_strided()", "path": "generated/torch.as_strided#torch.as_strided", "type": "torch", "text": " \ntorch.as_strided(input, size, stride, storage_offset=0) \u2192 Tensor  \nCreate a view of an existing torch.Tensor input with specified size, stride and storage_offset.  Warning More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like torch.Tensor.expand(), are easier to read and are therefore more advisable to use.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nsize (tuple or ints) \u2013 the shape of the output tensor \nstride (tuple or ints) \u2013 the stride of the output tensor \nstorage_offset (int, optional) \u2013 the offset in the underlying storage of the output tensor    Example: >>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])\n \n"}, {"name": "torch.as_tensor()", "path": "generated/torch.as_tensor#torch.as_tensor", "type": "torch", "text": " \ntorch.as_tensor(data, dtype=None, device=None) \u2192 Tensor  \nConvert the data into a torch.Tensor. If the data is already a Tensor with the same dtype and device, no copy will be performed, otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True. Similarly, if the data is an ndarray of the corresponding dtype and the device is the cpu, no copy will be performed.  Parameters \n \ndata (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from data. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.    Example: >>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])\n \n"}, {"name": "torch.atan()", "path": "generated/torch.atan#torch.atan", "type": "torch", "text": " \ntorch.atan(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the arctangent of the elements of input.  outi=tan\u2061\u22121(inputi)\\text{out}_{i} = \\tan^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n \n"}, {"name": "torch.atan2()", "path": "generated/torch.atan2#torch.atan2", "type": "torch", "text": " \ntorch.atan2(input, other, *, out=None) \u2192 Tensor  \nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}  with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})  and vector (1,0)(1, 0) . (Note that otheri\\text{other}_{i} , the second parameter, is the x-coordinate, while inputi\\text{input}_{i} , the first parameter, is the y-coordinate.) The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n \n"}, {"name": "torch.atanh()", "path": "generated/torch.atanh#torch.atanh", "type": "torch", "text": " \ntorch.atanh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic tangent of the elements of input.  Note The domain of the inverse hyperbolic tangent is (-1, 1) and values outside this range will be mapped to NaN, except for the values 1 and -1 for which the output is mapped to +/-INF respectively.   outi=tanh\u2061\u22121(inputi)\\text{out}_{i} = \\tanh^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])\n \n"}, {"name": "torch.atleast_1d()", "path": "generated/torch.atleast_1d#torch.atleast_1d", "type": "torch", "text": " \ntorch.atleast_1d(*tensors) [source]\n \nReturns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.  Parameters \ninput (Tensor or list of Tensors) \u2013   Returns \noutput (Tensor or tuple of Tensors)    Example::\n\n>>> x = torch.randn(2)\n>>> x\ntensor([1.4584, 0.7583])\n>>> torch.atleast_1d(x)\ntensor([1.4584, 0.7583])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x,y))\n(tensor([0.5000]), tensor([1.]))\n   \n"}, {"name": "torch.atleast_2d()", "path": "generated/torch.atleast_2d#torch.atleast_2d", "type": "torch", "text": " \ntorch.atleast_2d(*tensors) [source]\n \nReturns a 2-dimensional view of each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns \noutput (Tensor or tuple of Tensors)    Example::\n\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.randn(2,2)\n>>> x\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> torch.atleast_2d(x)\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x,y))\n(tensor([[0.5000]]), tensor([[1.]]))\n   \n"}, {"name": "torch.atleast_3d()", "path": "generated/torch.atleast_3d#torch.atleast_3d", "type": "torch", "text": " \ntorch.atleast_3d(*tensors) [source]\n \nReturns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns \noutput (Tensor or tuple of Tensors)   Example >>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))\n \n"}, {"name": "torch.autograd", "path": "autograd", "type": "torch.autograd", "text": "Automatic differentiation package - torch.autograd torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).  \ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None) [source]\n \nComputes the sum of gradients of given tensors w.r.t. graph leaves. The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors). This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.   Note If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ntensors (sequence of Tensor) \u2013 Tensors of which the derivative will be computed. \ngrad_tensors (sequence of (Tensor or None)) \u2013 The \u201cvector\u201d in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n  \ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) [source]\n \nComputes and returns the sum of gradients of outputs w.r.t. the inputs. grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None). If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.  Note If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \noutputs (sequence of Tensor) \u2013 outputs of the differentiated function. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad). \ngrad_outputs (sequence of Tensor) \u2013 The \u201cvector\u201d in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False. \nallow_unused (bool, optional) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.    \n Functional higher level API  Warning This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.  This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set, you can use a lambda to capture them. For example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as f(input, constant, flag=flag) you can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input).  \ntorch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Jacobian of a given function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Jacobian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the jacobian, usually we invoke autograd.grad once per row of the jacobian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.  Return type \nJacobian (Tensor or nested tuple of Tensors)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(2, 2)\n>>> jacobian(exp_reducer, inputs)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]])\n >>> jacobian(exp_reducer, inputs, create_graph=True)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]], grad_fn=<ViewBackward>)\n >>> def exp_adder(x, y):\n...   return 2 * x.exp() + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> jacobian(exp_adder, inputs)\n(tensor([[2.8052, 0.0000],\n        [0.0000, 3.3963]]),\n tensor([[3., 0.],\n         [0., 3.]]))\n \n  \ntorch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Hessian of a given scalar function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Hessian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the hessian, usually we invoke autograd.grad once per row of the hessian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.  Return type \nHessian (Tensor or a tuple of tuple of Tensors)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> hessian(pow_reducer, inputs)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]])\n >>> hessian(pow_reducer, inputs, create_graph=True)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]], grad_fn=<ViewBackward>)\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> hessian(pow_adder_reducer, inputs)\n((tensor([[4., 0.],\n          [0., 4.]]),\n  tensor([[0., 0.],\n          [0., 0.]])),\n (tensor([[0., 0.],\n          [0., 0.]]),\n  tensor([[6., 0.],\n          [0., 6.]])))\n \n  \ntorch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Jacobian product is computed. Must be the same size as the output of func. This argument is optional when the output of func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4)\n>>> vjp(exp_reducer, inputs, v)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782]),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]]))\n >>> vjp(exp_reducer, inputs, v, create_graph=True)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=<SumBackward1>),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=<MulBackward0>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = torch.ones(2)\n>>> vjp(adder, inputs, v)\n(tensor([2.4225, 2.3340]),\n (tensor([2., 2.]), tensor([3., 3.])))\n \n  \ntorch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Jacobian vector product is computed. Must be the same size as the input of func. This argument is optional when the input to func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4, 4)\n>>> jvp(exp_reducer, inputs, v)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106]),\n tensor([6.3090, 4.6742, 7.9114, 8.2106]))\n >>> jvp(exp_reducer, inputs, v, create_graph=True)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SumBackward1>),\n tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SqueezeBackward1>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.ones(2), torch.ones(2))\n>>> jvp(adder, inputs, v)\n(tensor([2.2399, 2.5005]),\n tensor([5., 5.]))\n  Note The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don\u2019t have support for forward mode AD in PyTorch at the moment.  \n  \ntorch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Hessian product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> vhp(pow_reducer, inputs, v)\n(tensor(0.5591),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]]))\n>>> vhp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.5591, grad_fn=<SumBackward0>),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]], grad_fn=<MulBackward0>))\n>>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> vhp(pow_adder_reducer, inputs, v)\n(tensor(4.8053),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n \n  \ntorch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Hessian vector product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> hvp(pow_reducer, inputs, v)\n(tensor(0.1448),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]]))\n >>> hvp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.1448, grad_fn=<SumBackward0>),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]], grad_fn=<MulBackward0>))\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> hvp(pow_adder_reducer, inputs, v)\n(tensor(2.3030),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n  Note This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.  \n Locally disabling gradient computation  \nclass torch.autograd.no_grad [source]\n \nContext-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n \n  \nclass torch.autograd.enable_grad [source]\n \nContext-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n \n  \nclass torch.autograd.set_grad_enabled(mode) [source]\n \nContext-manager that sets gradient calculation to on or off. set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation in other threads.  Parameters \nmode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.   Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n \n Default gradient layouts When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows. If param.grad is initially None:  If param\u2019s memory is non-overlapping and dense, .grad is created with strides matching param (thus matching param\u2019s layout). Otherwise, .grad is created with rowmajor-contiguous strides.  If param already has a non-sparse .grad attribute:  If create_graph=False, backward() accumulates into .grad in-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad\u2019s strides.  The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. In fact, resetting all .grads to None before each accumulation phase, e.g.: for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n such that they\u2019re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks. Manual gradient layouts If you need manual control over .grad\u2019s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True. In-place operations on Tensors Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you\u2019re operating under heavy memory pressure, you might never need to use them. In-place correctness checks All Tensor s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you\u2019re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct. Variable (deprecated)  Warning The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True. Below please find a quick guide on what has changed:  \nVariable(tensor) and Variable(tensor, requires_grad) still work as expected, but they return Tensors instead of Variables. \nvar.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors with the same method names.  In addition, one can now create tensors with requires_grad=True using factory methods such as torch.randn(), torch.zeros(), torch.ones(), and others like the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)  Tensor autograd functions  \nclass torch.Tensor  \n \ngrad  \nThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it. \n  \nrequires_grad  \nIs True if gradients need to be computed for this Tensor, False otherwise.  Note The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.  \n  \nis_leaf  \nAll Tensors that have requires_grad which is False will be leaf Tensors by convention. For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Example: >>> a = torch.rand(10, requires_grad=True)\n>>> a.is_leaf\nTrue\n>>> b = torch.rand(10, requires_grad=True).cuda()\n>>> b.is_leaf\nFalse\n# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n>>> c = torch.rand(10, requires_grad=True) + 2\n>>> c.is_leaf\nFalse\n# c was created by the addition operation\n>>> d = torch.rand(10).cuda()\n>>> d.is_leaf\nTrue\n# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n>>> e = torch.rand(10).cuda().requires_grad_()\n>>> e.is_leaf\nTrue\n# e requires gradients and has no operations creating it\n>>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n>>> f.is_leaf\nTrue\n# f requires grad, has no operation creating it\n \n  \nbackward(gradient=None, retain_graph=None, create_graph=False, inputs=None) [source]\n \nComputes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ngradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n  \ndetach()  \nReturns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.  \n  \ndetach_()  \nDetaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place. \n  \nregister_hook(hook) [source]\n \nRegisters a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -> Tensor or None\n The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n>>> v.backward(torch.tensor([1., 2., 3.]))\n>>> v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n>>> h.remove()  # removes the hook\n \n  \nretain_grad() [source]\n \nEnables .grad attribute for non-leaf Tensors. \n \n Function  \nclass torch.autograd.Function [source]\n \nRecords operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: >>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)\n  \nstatic backward(ctx, *grad_outputs) [source]\n \nDefines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. \n  \nstatic forward(ctx, *args, **kwargs) [source]\n \nPerforms the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \n \n Context method mixins When creating a new Function, the following methods are available to ctx.  \nclass torch.autograd.function._ContextMethodMixin [source]\n \n \nmark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. \n  \nmark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. \n  \nsave_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. \n  \nset_materialize_grads(value) [source]\n \nSets whether to materialize output grad tensors. Default is true. This should be called only from inside the forward() method If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method. \n \n Numerical gradient checking  \ntorch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=False, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point or complex type and with requires_grad=True. The check between numerical and analytical gradients uses allclose(). For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of grad_output: 1 and 1j. For more details, check out Autograd for Complex Numbers.  Note The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.   Warning If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \ncheck_sparse_nnz (bool, optional) \u2013 if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros, for Tensor outputs. \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n  \ntorch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True. This function checks that backpropagating through the gradients computed to the given grad_outputs are correct. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.   Warning If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \ngrad_outputs (tuple of Tensor or Tensor, optional) \u2013 The gradients with respect to the function\u2019s outputs. \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \ngen_non_contig_grad_outputs (bool, optional) \u2013 if grad_outputs is None and gen_non_contig_grad_outputs is True, the randomly generated gradient outputs are made to be noncontiguous \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n Profiler Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using profile. and nvprof based (registers both CPU and GPU activity) using emit_nvtx.  \nclass torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, use_kineto=False, use_cpu=True) [source]\n \nContext manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks  Parameters \n \nenabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. \nuse_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation. \nrecord_shapes (bool, optional) \u2013 If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection. \nwith_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators. \nprofile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. \nwith_stack (bool, optional) \u2013 record source information (file and line number) for the ops. \nuse_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. \nuse_cpu (bool, optional) \u2013 profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling.    Example >>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n  \nexport_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n  \nkey_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by \nname, input shapes) rather than just event name. ((event) \u2013  \nis useful to see which input shapes contribute to the runtime (This) \u2013  \nmost and may help with size-specific optimizations or (the) \u2013  \nthe best candidates for quantization (choosing) \u2013  \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n  \nproperty self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n  \ntable(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False) [source]\n \nPrints an EventList as a nicely formatted table.  Parameters \n \nsort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage, cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count. \ntop_level_events_only (bool, optional) \u2013 Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python lstm, python add or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.   Returns \nA string containing the table.   \n  \ntotal_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n \n  \nclass torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False) [source]\n \nContext manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.  Parameters \n \nenabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op. Default: True. \nrecord_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation.    Example >>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates. During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function. Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass. Double-backward If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass. \n  \ntorch.autograd.profiler.load_nvprof(path) [source]\n \nOpens an nvprof trace file and parses autograd annotations.  Parameters \npath (str) \u2013 path to nvprof trace   \n Anomaly detection  \nclass torch.autograd.detect_anomaly [source]\n \nContext-manager that enable anomaly detection for the autograd engine. This does two things:  Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. Any backward computation that generate \u201cnan\u201d value will raise an error.   Warning This mode should be enabled only for debugging as the different tests will slow down your program execution.  Example >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n \n  \nclass torch.autograd.set_detect_anomaly(mode) [source]\n \nContext-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.  Parameters \nmode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False).   \n\n"}, {"name": "torch.autograd.backward()", "path": "autograd#torch.autograd.backward", "type": "torch.autograd", "text": " \ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None) [source]\n \nComputes the sum of gradients of given tensors w.r.t. graph leaves. The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors). This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.   Note If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ntensors (sequence of Tensor) \u2013 Tensors of which the derivative will be computed. \ngrad_tensors (sequence of (Tensor or None)) \u2013 The \u201cvector\u201d in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n"}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "torch.autograd", "text": " \nclass torch.autograd.detect_anomaly [source]\n \nContext-manager that enable anomaly detection for the autograd engine. This does two things:  Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. Any backward computation that generate \u201cnan\u201d value will raise an error.   Warning This mode should be enabled only for debugging as the different tests will slow down your program execution.  Example >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n \n"}, {"name": "torch.autograd.enable_grad", "path": "autograd#torch.autograd.enable_grad", "type": "torch.autograd", "text": " \nclass torch.autograd.enable_grad [source]\n \nContext-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n \n"}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "torch.autograd", "text": " \nclass torch.autograd.Function [source]\n \nRecords operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: >>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)\n  \nstatic backward(ctx, *grad_outputs) [source]\n \nDefines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. \n  \nstatic forward(ctx, *args, **kwargs) [source]\n \nPerforms the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \n \n"}, {"name": "torch.autograd.Function.backward()", "path": "autograd#torch.autograd.Function.backward", "type": "torch.autograd", "text": " \nstatic backward(ctx, *grad_outputs) [source]\n \nDefines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. \n"}, {"name": "torch.autograd.Function.forward()", "path": "autograd#torch.autograd.Function.forward", "type": "torch.autograd", "text": " \nstatic forward(ctx, *args, **kwargs) [source]\n \nPerforms the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \n"}, {"name": "torch.autograd.function._ContextMethodMixin", "path": "autograd#torch.autograd.function._ContextMethodMixin", "type": "torch.autograd", "text": " \nclass torch.autograd.function._ContextMethodMixin [source]\n \n \nmark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. \n  \nmark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. \n  \nsave_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. \n  \nset_materialize_grads(value) [source]\n \nSets whether to materialize output grad tensors. Default is true. This should be called only from inside the forward() method If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method. \n \n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_dirty()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_dirty", "type": "torch.autograd", "text": " \nmark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. \n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "type": "torch.autograd", "text": " \nmark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. \n"}, {"name": "torch.autograd.function._ContextMethodMixin.save_for_backward()", "path": "autograd#torch.autograd.function._ContextMethodMixin.save_for_backward", "type": "torch.autograd", "text": " \nsave_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. \n"}, {"name": "torch.autograd.function._ContextMethodMixin.set_materialize_grads()", "path": "autograd#torch.autograd.function._ContextMethodMixin.set_materialize_grads", "type": "torch.autograd", "text": " \nset_materialize_grads(value) [source]\n \nSets whether to materialize output grad tensors. Default is true. This should be called only from inside the forward() method If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method. \n"}, {"name": "torch.autograd.functional.hessian()", "path": "autograd#torch.autograd.functional.hessian", "type": "torch.autograd", "text": " \ntorch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Hessian of a given scalar function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Hessian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the hessian, usually we invoke autograd.grad once per row of the hessian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.  Return type \nHessian (Tensor or a tuple of tuple of Tensors)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> hessian(pow_reducer, inputs)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]])\n >>> hessian(pow_reducer, inputs, create_graph=True)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]], grad_fn=<ViewBackward>)\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> hessian(pow_adder_reducer, inputs)\n((tensor([[4., 0.],\n          [0., 4.]]),\n  tensor([[0., 0.],\n          [0., 0.]])),\n (tensor([[0., 0.],\n          [0., 0.]]),\n  tensor([[6., 0.],\n          [0., 6.]])))\n \n"}, {"name": "torch.autograd.functional.hvp()", "path": "autograd#torch.autograd.functional.hvp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Hessian vector product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> hvp(pow_reducer, inputs, v)\n(tensor(0.1448),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]]))\n >>> hvp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.1448, grad_fn=<SumBackward0>),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]], grad_fn=<MulBackward0>))\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> hvp(pow_adder_reducer, inputs, v)\n(tensor(2.3030),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n  Note This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.  \n"}, {"name": "torch.autograd.functional.jacobian()", "path": "autograd#torch.autograd.functional.jacobian", "type": "torch.autograd", "text": " \ntorch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Jacobian of a given function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Jacobian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the jacobian, usually we invoke autograd.grad once per row of the jacobian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.  Return type \nJacobian (Tensor or nested tuple of Tensors)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(2, 2)\n>>> jacobian(exp_reducer, inputs)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]])\n >>> jacobian(exp_reducer, inputs, create_graph=True)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]], grad_fn=<ViewBackward>)\n >>> def exp_adder(x, y):\n...   return 2 * x.exp() + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> jacobian(exp_adder, inputs)\n(tensor([[2.8052, 0.0000],\n        [0.0000, 3.3963]]),\n tensor([[3., 0.],\n         [0., 3.]]))\n \n"}, {"name": "torch.autograd.functional.jvp()", "path": "autograd#torch.autograd.functional.jvp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Jacobian vector product is computed. Must be the same size as the input of func. This argument is optional when the input to func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4, 4)\n>>> jvp(exp_reducer, inputs, v)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106]),\n tensor([6.3090, 4.6742, 7.9114, 8.2106]))\n >>> jvp(exp_reducer, inputs, v, create_graph=True)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SumBackward1>),\n tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SqueezeBackward1>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.ones(2), torch.ones(2))\n>>> jvp(adder, inputs, v)\n(tensor([2.2399, 2.5005]),\n tensor([5., 5.]))\n  Note The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don\u2019t have support for forward mode AD in PyTorch at the moment.  \n"}, {"name": "torch.autograd.functional.vhp()", "path": "autograd#torch.autograd.functional.vhp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Hessian product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> vhp(pow_reducer, inputs, v)\n(tensor(0.5591),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]]))\n>>> vhp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.5591, grad_fn=<SumBackward0>),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]], grad_fn=<MulBackward0>))\n>>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> vhp(pow_adder_reducer, inputs, v)\n(tensor(4.8053),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n \n"}, {"name": "torch.autograd.functional.vjp()", "path": "autograd#torch.autograd.functional.vjp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Jacobian product is computed. Must be the same size as the output of func. This argument is optional when the output of func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4)\n>>> vjp(exp_reducer, inputs, v)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782]),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]]))\n >>> vjp(exp_reducer, inputs, v, create_graph=True)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=<SumBackward1>),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=<MulBackward0>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = torch.ones(2)\n>>> vjp(adder, inputs, v)\n(tensor([2.4225, 2.3340]),\n (tensor([2., 2.]), tensor([3., 3.])))\n \n"}, {"name": "torch.autograd.grad()", "path": "autograd#torch.autograd.grad", "type": "torch.autograd", "text": " \ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) [source]\n \nComputes and returns the sum of gradients of outputs w.r.t. the inputs. grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None). If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.  Note If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \noutputs (sequence of Tensor) \u2013 outputs of the differentiated function. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad). \ngrad_outputs (sequence of Tensor) \u2013 The \u201cvector\u201d in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False. \nallow_unused (bool, optional) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.    \n"}, {"name": "torch.autograd.gradcheck()", "path": "autograd#torch.autograd.gradcheck", "type": "torch.autograd", "text": " \ntorch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=False, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point or complex type and with requires_grad=True. The check between numerical and analytical gradients uses allclose(). For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of grad_output: 1 and 1j. For more details, check out Autograd for Complex Numbers.  Note The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.   Warning If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \ncheck_sparse_nnz (bool, optional) \u2013 if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros, for Tensor outputs. \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n"}, {"name": "torch.autograd.gradgradcheck()", "path": "autograd#torch.autograd.gradgradcheck", "type": "torch.autograd", "text": " \ntorch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True. This function checks that backpropagating through the gradients computed to the given grad_outputs are correct. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.   Warning If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \ngrad_outputs (tuple of Tensor or Tensor, optional) \u2013 The gradients with respect to the function\u2019s outputs. \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \ngen_non_contig_grad_outputs (bool, optional) \u2013 if grad_outputs is None and gen_non_contig_grad_outputs is True, the randomly generated gradient outputs are made to be noncontiguous \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n"}, {"name": "torch.autograd.no_grad", "path": "autograd#torch.autograd.no_grad", "type": "torch.autograd", "text": " \nclass torch.autograd.no_grad [source]\n \nContext-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n \n"}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "torch.autograd", "text": " \nclass torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False) [source]\n \nContext manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.  Parameters \n \nenabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op. Default: True. \nrecord_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation.    Example >>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates. During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function. Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass. Double-backward If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass. \n"}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "autograd#torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "text": " \ntorch.autograd.profiler.load_nvprof(path) [source]\n \nOpens an nvprof trace file and parses autograd annotations.  Parameters \npath (str) \u2013 path to nvprof trace   \n"}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "torch.autograd", "text": " \nclass torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, use_kineto=False, use_cpu=True) [source]\n \nContext manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks  Parameters \n \nenabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. \nuse_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation. \nrecord_shapes (bool, optional) \u2013 If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection. \nwith_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators. \nprofile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. \nwith_stack (bool, optional) \u2013 record source information (file and line number) for the ops. \nuse_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. \nuse_cpu (bool, optional) \u2013 profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling.    Example >>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n  \nexport_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n  \nkey_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by \nname, input shapes) rather than just event name. ((event) \u2013  \nis useful to see which input shapes contribute to the runtime (This) \u2013  \nmost and may help with size-specific optimizations or (the) \u2013  \nthe best candidates for quantization (choosing) \u2013  \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n  \nproperty self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n  \ntable(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False) [source]\n \nPrints an EventList as a nicely formatted table.  Parameters \n \nsort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage, cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count. \ntop_level_events_only (bool, optional) \u2013 Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python lstm, python add or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.   Returns \nA string containing the table.   \n  \ntotal_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n \n"}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "autograd#torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "text": " \nexport_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n"}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "autograd#torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "text": " \nkey_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by \nname, input shapes) rather than just event name. ((event) \u2013  \nis useful to see which input shapes contribute to the runtime (This) \u2013  \nmost and may help with size-specific optimizations or (the) \u2013  \nthe best candidates for quantization (choosing) \u2013  \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n"}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total()", "path": "autograd#torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "text": " \nproperty self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n"}, {"name": "torch.autograd.profiler.profile.table()", "path": "autograd#torch.autograd.profiler.profile.table", "type": "torch.autograd", "text": " \ntable(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False) [source]\n \nPrints an EventList as a nicely formatted table.  Parameters \n \nsort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage, cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count. \ntop_level_events_only (bool, optional) \u2013 Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python lstm, python add or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.   Returns \nA string containing the table.   \n"}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "autograd#torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "text": " \ntotal_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n"}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "torch.autograd", "text": " \nclass torch.autograd.set_detect_anomaly(mode) [source]\n \nContext-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.  Parameters \nmode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False).   \n"}, {"name": "torch.autograd.set_grad_enabled", "path": "autograd#torch.autograd.set_grad_enabled", "type": "torch.autograd", "text": " \nclass torch.autograd.set_grad_enabled(mode) [source]\n \nContext-manager that sets gradient calculation to on or off. set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation in other threads.  Parameters \nmode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.   Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n \n"}, {"name": "torch.backends", "path": "backends", "type": "torch.backends", "text": "torch.backends torch.backends controls the behavior of various backends that PyTorch supports. These backends include:  torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp  torch.backends.cuda  \ntorch.backends.cuda.is_built() [source]\n \nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it. \n  \ntorch.backends.cuda.matmul.allow_tf32  \nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n  \ntorch.backends.cuda.cufft_plan_cache  \ncufft_plan_cache caches the cuFFT plans  \nsize  \nA readonly int that shows the number of plans currently in the cuFFT plan cache. \n  \nmax_size  \nA int that controls cache capacity of cuFFT plan. \n  \nclear()  \nClears the cuFFT plan cache. \n \n torch.backends.cudnn  \ntorch.backends.cudnn.version() [source]\n \nReturns the version of cuDNN \n  \ntorch.backends.cudnn.is_available() [source]\n \nReturns a bool indicating if CUDNN is currently available. \n  \ntorch.backends.cudnn.enabled  \nA bool that controls whether cuDNN is enabled. \n  \ntorch.backends.cudnn.allow_tf32  \nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n  \ntorch.backends.cudnn.deterministic  \nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms(). \n  \ntorch.backends.cudnn.benchmark  \nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. \n torch.backends.mkl  \ntorch.backends.mkl.is_available() [source]\n \nReturns whether PyTorch is built with MKL support. \n torch.backends.mkldnn  \ntorch.backends.mkldnn.is_available() [source]\n \nReturns whether PyTorch is built with MKL-DNN support. \n torch.backends.openmp  \ntorch.backends.openmp.is_available() [source]\n \nReturns whether PyTorch is built with OpenMP support. \n\n"}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "torch.backends", "text": " \ntorch.backends.cuda.cufft_plan_cache  \ncufft_plan_cache caches the cuFFT plans  \nsize  \nA readonly int that shows the number of plans currently in the cuFFT plan cache. \n  \nmax_size  \nA int that controls cache capacity of cuFFT plan. \n  \nclear()  \nClears the cuFFT plan cache. \n \n"}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "torch.backends", "text": " \ntorch.backends.cuda.is_built() [source]\n \nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it. \n"}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "torch.backends", "text": " \ntorch.backends.cuda.matmul.allow_tf32  \nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n"}, {"name": "torch.backends.cuda.size", "path": "backends#torch.backends.cuda.size", "type": "torch.backends", "text": " \nsize  \nA readonly int that shows the number of plans currently in the cuFFT plan cache. \n"}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "torch.backends", "text": " \ntorch.backends.cudnn.allow_tf32  \nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n"}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "torch.backends", "text": " \ntorch.backends.cudnn.benchmark  \nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. \n"}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "torch.backends", "text": " \ntorch.backends.cudnn.deterministic  \nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms(). \n"}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "torch.backends", "text": " \ntorch.backends.cudnn.enabled  \nA bool that controls whether cuDNN is enabled. \n"}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "torch.backends", "text": " \ntorch.backends.cudnn.is_available() [source]\n \nReturns a bool indicating if CUDNN is currently available. \n"}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "torch.backends", "text": " \ntorch.backends.cudnn.version() [source]\n \nReturns the version of cuDNN \n"}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "torch.backends", "text": " \ntorch.backends.mkl.is_available() [source]\n \nReturns whether PyTorch is built with MKL support. \n"}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "torch.backends", "text": " \ntorch.backends.mkldnn.is_available() [source]\n \nReturns whether PyTorch is built with MKL-DNN support. \n"}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "torch.backends", "text": " \ntorch.backends.openmp.is_available() [source]\n \nReturns whether PyTorch is built with OpenMP support. \n"}, {"name": "torch.baddbmm()", "path": "generated/torch.baddbmm#torch.baddbmm", "type": "torch", "text": " \ntorch.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().  outi=\u03b2inputi+\u03b1(batch1i@batch2i)\\text{out}_i = \\beta\\ \\text{input}_i + \\alpha\\ (\\text{batch1}_i \\mathbin{@} \\text{batch2}_i)  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \nbatch1 (Tensor) \u2013 the first batch of matrices to be multiplied \nbatch2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n \n"}, {"name": "torch.bartlett_window()", "path": "generated/torch.bartlett_window#torch.bartlett_window", "type": "torch", "text": " \ntorch.bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nBartlett window function.  w[n]=1\u2212\u22232nN\u22121\u22121\u2223={2nN\u22121if 0\u2264n\u2264N\u2212122\u22122nN\u22121if N\u221212<n<N,w[n] = 1 - \\left| \\frac{2n}{N-1} - 1 \\right| = \\begin{cases} \\frac{2n}{N - 1} & \\text{if } 0 \\leq n \\leq \\frac{N - 1}{2} \\\\ 2 - \\frac{2n}{N - 1} & \\text{if } \\frac{N - 1}{2} < n < N \\\\ \\end{cases},  \nwhere NN  is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1 , the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window  Return type \nTensor   \n"}, {"name": "torch.bernoulli()", "path": "generated/torch.bernoulli#torch.bernoulli", "type": "torch", "text": " \ntorch.bernoulli(input, *, generator=None, out=None) \u2192 Tensor  \nDraws binary random numbers (0 or 1) from a Bernoulli distribution. The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1 . The ith\\text{i}^{th}  element of the output tensor will draw a value 11  according to the ith\\text{i}^{th}  probability value given in input.  outi\u223cBernoulli(p=inputi)\\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i})  \nThe returned out tensor only has values 0 or 1 and is of the same shape as input. out can have integral dtype, but input must have floating point dtype.  Parameters \ninput (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution  Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n \n"}, {"name": "torch.bincount()", "path": "generated/torch.bincount#torch.bincount", "type": "torch", "text": " \ntorch.bincount(input, weights=None, minlength=0) \u2192 Tensor  \nCount the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.  Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ninput (Tensor) \u2013 1-d int tensor \nweights (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input tensor. \nminlength (int) \u2013 optional, minimum number of bins. Should be non-negative.   Returns \na tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)  Return type \noutput (Tensor)   Example: >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n \n"}, {"name": "torch.bitwise_and()", "path": "generated/torch.bitwise_and#torch.bitwise_and", "type": "torch", "text": " \ntorch.bitwise_and(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])\n \n"}, {"name": "torch.bitwise_not()", "path": "generated/torch.bitwise_not#torch.bitwise_not", "type": "torch", "text": " \ntorch.bitwise_not(input, *, out=None) \u2192 Tensor  \nComputes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)\n \n"}, {"name": "torch.bitwise_or()", "path": "generated/torch.bitwise_or#torch.bitwise_or", "type": "torch", "text": " \ntorch.bitwise_or(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise OR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])\n \n"}, {"name": "torch.bitwise_xor()", "path": "generated/torch.bitwise_xor#torch.bitwise_xor", "type": "torch", "text": " \ntorch.bitwise_xor(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])\n \n"}, {"name": "torch.blackman_window()", "path": "generated/torch.blackman_window#torch.blackman_window", "type": "torch", "text": " \ntorch.blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nBlackman window function.  w[n]=0.42\u22120.5cos\u2061(2\u03c0nN\u22121)+0.08cos\u2061(4\u03c0nN\u22121)w[n] = 0.42 - 0.5 \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right) + 0.08 \\cos \\left( \\frac{4 \\pi n}{N - 1} \\right)  \nwhere NN  is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1 , the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window  Return type \nTensor   \n"}, {"name": "torch.block_diag()", "path": "generated/torch.block_diag#torch.block_diag", "type": "torch", "text": " \ntorch.block_diag(*tensors) [source]\n \nCreate a block diagonal matrix from provided tensors.  Parameters \n*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.  Returns \n A 2 dimensional tensor with all the input tensors arranged in\n\norder such that their upper left and lower right corners are diagonally adjacent. All other elements are set to 0.    Return type \nTensor   Example: >>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n \n"}, {"name": "torch.bmm()", "path": "generated/torch.bmm#torch.bmm", "type": "torch", "text": " \ntorch.bmm(input, mat2, *, deterministic=False, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices stored in input and mat2. input and mat2 must be 3-D tensors each containing the same number of matrices. If input is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor.  outi=inputi@mat2i\\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i  \nThis operator supports TensorFloat32.  Note This function does not broadcast. For broadcasting matrix products, see torch.matmul().   Parameters \n \ninput (Tensor) \u2013 the first batch of matrices to be multiplied \nmat2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \ndeterministic (bool, optional) \u2013 flag to choose between a faster non-deterministic calculation, or a slower deterministic calculation. This argument is only available for sparse-dense CUDA bmm. Default: False\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])\n \n"}, {"name": "torch.broadcast_shapes()", "path": "generated/torch.broadcast_shapes#torch.broadcast_shapes", "type": "torch", "text": " \ntorch.broadcast_shapes(*shapes) \u2192 Size [source]\n \nSimilar to broadcast_tensors() but for shapes. This is equivalent to torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape but avoids the need create to intermediate tensors. This is useful for broadcasting tensors of common batch shape but different rightmost shape, e.g. to broadcast mean vectors with covariance matrices. Example: >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])\n  Parameters \n*shapes (torch.Size) \u2013 Shapes of tensors.  Returns \nA shape compatible with all input shapes.  Return type \nshape (torch.Size)  Raises \nRuntimeError \u2013 If shapes are incompatible.   \n"}, {"name": "torch.broadcast_tensors()", "path": "generated/torch.broadcast_tensors#torch.broadcast_tensors", "type": "torch", "text": " \ntorch.broadcast_tensors(*tensors) \u2192 List of Tensors [source]\n \nBroadcasts the given tensors according to Broadcasting semantics.  Parameters \n*tensors \u2013 any number of tensors of the same type    Warning More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: >>> x = torch.arange(3).view(1, 3)\n>>> y = torch.arange(2).view(2, 1)\n>>> a, b = torch.broadcast_tensors(x, y)\n>>> a.size()\ntorch.Size([2, 3])\n>>> a\ntensor([[0, 1, 2],\n        [0, 1, 2]])\n \n"}, {"name": "torch.broadcast_to()", "path": "generated/torch.broadcast_to#torch.broadcast_to", "type": "torch", "text": " \ntorch.broadcast_to(input, shape) \u2192 Tensor  \nBroadcasts input to the shape shape. Equivalent to calling input.expand(shape). See expand() for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nshape (list, tuple, or torch.Size) \u2013 the new shape.    Example: >>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n \n"}, {"name": "torch.bucketize()", "path": "generated/torch.bucketize#torch.bucketize", "type": "torch", "text": " \ntorch.bucketize(input, boundaries, *, out_int32=False, right=False, out=None) \u2192 Tensor  \nReturns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Return a new tensor with the same size as input. If right is False (default), then the left boundary is closed. More formally, the returned index satisfies the following rules:   \nright returned index satisfies   \nFalse boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]  \nTrue boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]    Parameters \n \ninput (Tensor or Scalar) \u2013 N-D tensor or a Scalar containing the search value(s). \nboundaries (Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence.   Keyword Arguments \n \nout_int32 (bool, optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise. Default value is False, i.e. default output data type is torch.int64. \nright (bool, optional) \u2013 if False, return the first suitable location that is found. If True, return the last such index. If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or the size of boundaries (one pass the last index). In other words, if False, gets the lower bound index for each value in input from boundaries. If True, gets the upper bound index instead. Default value is False. \nout (Tensor, optional) \u2013 the output tensor, must be the same size as input if provided.    Example: >>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])\n \n"}, {"name": "torch.can_cast()", "path": "generated/torch.can_cast#torch.can_cast", "type": "torch", "text": " \ntorch.can_cast(from, to) \u2192 bool  \nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  Parameters \n \nfrom (dpython:type) \u2013 The original torch.dtype. \nto (dpython:type) \u2013 The target torch.dtype.    Example: >>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse\n \n"}, {"name": "torch.cartesian_prod()", "path": "generated/torch.cartesian_prod#torch.cartesian_prod", "type": "torch", "text": " \ntorch.cartesian_prod(*tensors) [source]\n \nDo cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.  Parameters \n*tensors \u2013 any number of 1 dimensional tensors.  Returns \n A tensor equivalent to converting all the input tensors into lists,\n\ndo itertools.product on these lists, and finally convert the resulting list into tensor.    Return type \nTensor   Example: >>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])\n \n"}, {"name": "torch.cat()", "path": "generated/torch.cat#torch.cat", "type": "torch", "text": " \ntorch.cat(tensors, dim=0, *, out=None) \u2192 Tensor  \nConcatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk(). torch.cat() can be best understood via examples.  Parameters \n \ntensors (sequence of Tensors) \u2013 any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension. \ndim (int, optional) \u2013 the dimension over which the tensors are concatenated   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])\n \n"}, {"name": "torch.cdist()", "path": "generated/torch.cdist#torch.cdist", "type": "torch", "text": " \ntorch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary') [source]\n \nComputes batched the p-norm distance between each pair of the two collections of row vectors.  Parameters \n \nx1 (Tensor) \u2013 input tensor of shape B\u00d7P\u00d7MB \\times P \\times M . \nx2 (Tensor) \u2013 input tensor of shape B\u00d7R\u00d7MB \\times R \\times M . \np \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty] . \ncompute_mode \u2013 \u2018use_mm_for_euclid_dist_if_necessary\u2019 - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 \u2018use_mm_for_euclid_dist\u2019 - will always use matrix multiplication approach to calculate euclidean distance (p = 2) \u2018donot_use_mm_for_euclid_dist\u2019 - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.    If x1 has shape B\u00d7P\u00d7MB \\times P \\times M  and x2 has shape B\u00d7R\u00d7MB \\times R \\times M  then the output will have shape B\u00d7P\u00d7RB \\times P \\times R . This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()). Example >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])\n \n"}, {"name": "torch.ceil()", "path": "generated/torch.ceil#torch.ceil", "type": "torch", "text": " \ntorch.ceil(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.  outi=\u2308inputi\u2309=\u230ainputi\u230b+1\\text{out}_{i} = \\left\\lceil \\text{input}_{i} \\right\\rceil = \\left\\lfloor \\text{input}_{i} \\right\\rfloor + 1  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n \n"}, {"name": "torch.chain_matmul()", "path": "generated/torch.chain_matmul#torch.chain_matmul", "type": "torch", "text": " \ntorch.chain_matmul(*matrices) [source]\n \nReturns the matrix product of the NN  2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NN  needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NN  is 1, then this is a no-op - the original matrix is returned as is.  Parameters \nmatrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined.  Returns \nif the ithi^{th}  tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1} , then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1} .  Return type \nTensor   Example: >>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n \n"}, {"name": "torch.cholesky()", "path": "generated/torch.cholesky#torch.cholesky", "type": "torch", "text": " \ntorch.cholesky(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA  or for batches of symmetric positive-definite matrices. If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:  A=UTUA = U^TU \nIf upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:  A=LLTA = LL^T \nIf upper is True, and AA  is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.  Note torch.linalg.cholesky() should be used over torch.cholesky when possible. Note however that torch.linalg.cholesky() does not yet support the upper parameter and instead always returns the lower triangular matrix.   Parameters \n \ninput (Tensor) \u2013 the input tensor AA  of size (\u2217,n,n)(*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices. \nupper (bool, optional) \u2013 flag that indicates whether to return a upper or lower triangular matrix. Default: False\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output matrix   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2)\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = torch.matmul(l, l.transpose(-1, -2))\n>>> torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)\n \n"}, {"name": "torch.cholesky_inverse()", "path": "generated/torch.cholesky_inverse#torch.cholesky_inverse", "type": "torch", "text": " \ntorch.cholesky_inverse(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uu  is lower triangular such that the returned tensor is  inv=(uuT)\u22121inv = (uu^{{T}})^{{-1}}  \nIf upper is True or not provided, uu  is upper triangular such that the returned tensor is  inv=(uTu)\u22121inv = (u^T u)^{{-1}}  \n Parameters \n \ninput (Tensor) \u2013 the input 2-D tensor uu , a upper or lower triangular Cholesky factor \nupper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for inv   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n \n"}, {"name": "torch.cholesky_solve()", "path": "generated/torch.cholesky_solve#torch.cholesky_solve", "type": "torch", "text": " \ntorch.cholesky_solve(input, input2, upper=False, *, out=None) \u2192 Tensor  \nSolves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu . If upper is False, uu  is and lower triangular and c is returned such that:  c=(uuT)\u22121bc = (u u^T)^{{-1}} b  \nIf upper is True or not provided, uu  is upper triangular and c is returned such that:  c=(uTu)\u22121bc = (u^T u)^{{-1}} b  \ntorch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c Supports real-valued and complex-valued inputs. For the complex-valued inputs the transpose operator above is the conjugate transpose.  Parameters \n \ninput (Tensor) \u2013 input matrix bb  of size (\u2217,m,k)(*, m, k) , where \u2217*  is zero or more batch dimensions \ninput2 (Tensor) \u2013 input matrix uu  of size (\u2217,m,m)(*, m, m) , where \u2217*  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor \nupper (bool, optional) \u2013 whether to consider the Cholesky factor as a lower or upper triangular matrix. Default: False.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for c   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n \n"}, {"name": "torch.chunk()", "path": "generated/torch.chunk#torch.chunk", "type": "torch", "text": " \ntorch.chunk(input, chunks, dim=0) \u2192 List of Tensors  \nSplits a tensor into a specific number of chunks. Each chunk is a view of the input tensor. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.  Parameters \n \ninput (Tensor) \u2013 the tensor to split \nchunks (int) \u2013 number of chunks to return \ndim (int) \u2013 dimension along which to split the tensor    \n"}, {"name": "torch.clamp()", "path": "generated/torch.clamp#torch.clamp", "type": "torch", "text": " \ntorch.clamp(input, min, max, *, out=None) \u2192 Tensor  \nClamp all elements in input into the range [ min, max ]. Let min_value and max_value be min and max, respectively, this returns:  yi=min\u2061(max\u2061(xi,min_value),max_value)y_i = \\min(\\max(x_i, \\text{min\\_value}), \\text{max\\_value})  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \nmin (Number) \u2013 lower-bound of the range to be clamped to \nmax (Number) \u2013 upper-bound of the range to be clamped to   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n  \ntorch.clamp(input, *, min, out=None) \u2192 Tensor \n Clamps all elements in input to be larger or equal min.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \n \nmin (Number) \u2013 minimal value of each element in the output \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.0299, -2.3184,  2.1593, -0.8883])\n>>> torch.clamp(a, min=0.5)\ntensor([ 0.5000,  0.5000,  2.1593,  0.5000])\n  \ntorch.clamp(input, *, max, out=None) \u2192 Tensor \n Clamps all elements in input to be smaller or equal max.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \n \nmax (Number) \u2013 maximal value of each element in the output \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.7753, -0.4702, -0.4599,  1.1899])\n>>> torch.clamp(a, max=0.5)\ntensor([ 0.5000, -0.4702, -0.4599,  0.5000])\n \n"}, {"name": "torch.clip()", "path": "generated/torch.clip#torch.clip", "type": "torch", "text": " \ntorch.clip(input, min, max, *, out=None) \u2192 Tensor  \nAlias for torch.clamp(). \n"}, {"name": "torch.clone()", "path": "generated/torch.clone#torch.clone", "type": "torch", "text": " \ntorch.clone(input, *, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of input.  Note This function is differentiable, so gradients will flow back from the result of this operation to input. To create a tensor without an autograd relationship to input see detach().   Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.column_stack()", "path": "generated/torch.column_stack#torch.column_stack", "type": "torch", "text": " \ntorch.column_stack(tensors, *, out=None) \u2192 Tensor  \nCreates a new tensor by horizontally stacking the tensors in tensors. Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])\n \n"}, {"name": "torch.combinations()", "path": "generated/torch.combinations#torch.combinations", "type": "torch", "text": " \ntorch.combinations(input, r=2, with_replacement=False) \u2192 seq  \nCompute combinations of length rr  of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.  Parameters \n \ninput (Tensor) \u2013 1D vector. \nr (int, optional) \u2013 number of elements to combine \nwith_replacement (boolean, optional) \u2013 whether to allow duplication in combination   Returns \nA tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.  Return type \nTensor   Example: >>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n \n"}, {"name": "torch.compiled_with_cxx11_abi()", "path": "generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi", "type": "torch", "text": " \ntorch.compiled_with_cxx11_abi() [source]\n \nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1 \n"}, {"name": "torch.complex()", "path": "generated/torch.complex#torch.complex", "type": "torch", "text": " \ntorch.complex(real, imag, *, out=None) \u2192 Tensor  \nConstructs a complex tensor with its real part equal to real and its imaginary part equal to imag.  Parameters \n \nreal (Tensor) \u2013 The real part of the complex tensor. Must be float or double. \nimag (Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype as real.   Keyword Arguments \nout (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.    Example::\n\n>>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64\n   \n"}, {"name": "torch.conj()", "path": "generated/torch.conj#torch.conj", "type": "torch", "text": " \ntorch.conj(input, *, out=None) \u2192 Tensor  \nComputes the element-wise conjugate of the given input tensor. If :attr`input` has a non-complex dtype, this function just returns input.  Warning In the future, torch.conj() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj() when input is of non-complex dtype to be compatible with this change.   outi=conj(inputi)\\text{out}_{i} = conj(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\n \n"}, {"name": "torch.copysign()", "path": "generated/torch.copysign#torch.copysign", "type": "torch", "text": " \ntorch.copysign(input, other, *, out=None) \u2192 Tensor  \nCreate a new floating-point tensor with the magnitude of input and the sign of other, elementwise.  outi={\u2212\u2223inputi\u2223ifotheri\u2264\u22120.0\u2223inputi\u2223ifotheri\u22650.0\\text{out}_{i} = \\begin{cases} -|\\text{input}_{i}| & \\text{if} \\text{other}_{i} \\leq -0.0 \\\\ |\\text{input}_{i}| & \\text{if} \\text{other}_{i} \\geq 0.0 \\\\ \\end{cases}  \nSupports broadcasting to a common shape, and integer and float inputs.  Parameters \n \ninput (Tensor) \u2013 magnitudes. \nother (Tensor or Number) \u2013 contains value(s) whose signbit(s) are applied to the magnitudes in input.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])\n \n"}, {"name": "torch.cos()", "path": "generated/torch.cos#torch.cos", "type": "torch", "text": " \ntorch.cos(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the cosine of the elements of input.  outi=cos\u2061(inputi)\\text{out}_{i} = \\cos(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n \n"}, {"name": "torch.cosh()", "path": "generated/torch.cosh#torch.cosh", "type": "torch", "text": " \ntorch.cosh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the hyperbolic cosine of the elements of input.  outi=cosh\u2061(inputi)\\text{out}_{i} = \\cosh(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n  Note When input is on the CPU, the implementation of torch.cosh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details.  \n"}, {"name": "torch.count_nonzero()", "path": "generated/torch.count_nonzero#torch.count_nonzero", "type": "torch", "text": " \ntorch.count_nonzero(input, dim=None) \u2192 Tensor  \nCounts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros.    Example: >>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])\n \n"}, {"name": "torch.cross()", "path": "generated/torch.cross#torch.cross", "type": "torch", "text": " \ntorch.cross(input, other, dim=None, *, out=None) \u2192 Tensor  \nReturns the cross product of vectors in dimension dim of input and other. input and other must have the same size, and the size of their dim dimension should be 3. If dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor \ndim (int, optional) \u2013 the dimension to take the cross-product in.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n \n"}, {"name": "torch.cuda", "path": "cuda", "type": "torch.cuda", "text": "torch.cuda This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.  \ntorch.cuda.can_device_access_peer(device, peer_device) [source]\n \nChecks if peer access between two devices is possible. \n  \ntorch.cuda.current_blas_handle() [source]\n \nReturns cublasHandle_t pointer to current cuBLAS handle \n  \ntorch.cuda.current_device() [source]\n \nReturns the index of a currently selected device. \n  \ntorch.cuda.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).   \n  \ntorch.cuda.default_stream(device=None) [source]\n \nReturns the default Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).   \n  \nclass torch.cuda.device(device) [source]\n \nContext-manager that changes the selected device.  Parameters \ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.   \n  \ntorch.cuda.device_count() [source]\n \nReturns the number of GPUs available. \n  \nclass torch.cuda.device_of(obj) [source]\n \nContext-manager that changes the current device to that of given object. You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.  Parameters \nobj (Tensor or Storage) \u2013 object allocated on the selected device.   \n  \ntorch.cuda.get_arch_list() [source]\n \nReturns list CUDA architectures this library was compiled for. \n  \ntorch.cuda.get_device_capability(device=None) [source]\n \nGets the cuda capability of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe major and minor cuda capability of the device  Return type \ntuple(int, int)   \n  \ntorch.cuda.get_device_name(device=None) [source]\n \nGets the name of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe name of the device  Return type \nstr   \n  \ntorch.cuda.get_device_properties(device) [source]\n \nGets the properties of a device.  Parameters \ndevice (torch.device or int or str) \u2013 device for which to return the properties of the device.  Returns \nthe properties of the device  Return type \n_CudaDeviceProperties   \n  \ntorch.cuda.get_gencode_flags() [source]\n \nReturns NVCC gencode flags this library were compiled with. \n  \ntorch.cuda.init() [source]\n \nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. \n  \ntorch.cuda.ipc_collect() [source]\n \nForce collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  \n  \ntorch.cuda.is_available() [source]\n \nReturns a bool indicating if CUDA is currently available. \n  \ntorch.cuda.is_initialized() [source]\n \nReturns whether PyTorch\u2019s CUDA state has been initialized. \n  \ntorch.cuda.set_device(device) [source]\n \nSets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters \ndevice (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.   \n  \ntorch.cuda.stream(stream) [source]\n \nContext-manager that selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.  Parameters \nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.    Note Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.  \n  \ntorch.cuda.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on a CUDA device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   \n Random Number Generator  \ntorch.cuda.get_rng_state(device='cuda') [source]\n \nReturns the random number generator state of the specified GPU as a ByteTensor.  Parameters \ndevice (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    Warning This function eagerly initializes CUDA.  \n  \ntorch.cuda.get_rng_state_all() [source]\n \nReturns a list of ByteTensor representing the random number states of all devices. \n  \ntorch.cuda.set_rng_state(new_state, device='cuda') [source]\n \nSets the random number generator state of the specified GPU.  Parameters \n \nnew_state (torch.ByteTensor) \u2013 The desired state \ndevice (torch.device or int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    \n  \ntorch.cuda.set_rng_state_all(new_states) [source]\n \nSets the random number generator state of all devices.  Parameters \nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device   \n  \ntorch.cuda.manual_seed(seed) [source]\n \nSets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().  \n  \ntorch.cuda.manual_seed_all(seed) [source]\n \nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.   \n  \ntorch.cuda.seed() [source]\n \nSets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().  \n  \ntorch.cuda.seed_all() [source]\n \nSets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. \n  \ntorch.cuda.initial_seed() [source]\n \nReturns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.  \n Communication collectives  \ntorch.cuda.comm.broadcast(tensor, devices=None, *, out=None) [source]\n \nBroadcasts a tensor to specified GPU devices.  Parameters \n \ntensor (Tensor) \u2013 tensor to broadcast. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to broadcast. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results.     Note Exactly one of devices and out must be specified.   Returns \n \n \nIf devices is specified, \n\na tuple containing copies of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a copy of tensor.       \n  \ntorch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) [source]\n \nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters \n \ntensors (sequence) \u2013 tensors to broadcast. Must be on the same device, either CPU or GPU. \ndevices (Iterable[torch.device, str or int]) \u2013 an iterable of GPU devices, among which to broadcast. \nbuffer_size (int) \u2013 maximum size of the buffer used for coalescing   Returns \nA tuple containing copies of tensor, placed on devices.   \n  \ntorch.cuda.comm.reduce_add(inputs, destination=None) [source]\n \nSums tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.  Parameters \n \ninputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. \ndestination (int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns \nA tensor containing an elementwise sum of all inputs, placed on the destination device.   \n  \ntorch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None) [source]\n \nScatters tensor across multiple GPUs.  Parameters \n \ntensor (Tensor) \u2013 tensor to scatter. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to scatter. \nchunk_sizes (Iterable[int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sums to tensor.size(dim). If not specified, tensor will be divided into equal chunks. \ndim (int, optional) \u2013 A dimension along which to chunk tensor. Default: 0. \nstreams (Iterable[Stream], optional) \u2013 an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results. Sizes of these tensors must match that of tensor, except for dim, where the total size must sum to tensor.size(dim).     Note Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.   Returns \n \n \nIf devices is specified, \n\na tuple containing chunks of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a chunk of tensor.       \n  \ntorch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None) [source]\n \nGathers tensors from multiple GPU devices.  Parameters \n \ntensors (Iterable[Tensor]) \u2013 an iterable of tensors to gather. Tensor sizes in all dimensions other than dim have to match. \ndim (int, optional) \u2013 a dimension along which the tensors will be concatenated. Default: 0. \ndestination (torch.device, str, or int, optional) \u2013 the output device. Can be CPU or CUDA. Default: the current CUDA device. \nout (Tensor, optional, keyword-only) \u2013 the tensor to store gather result. Its sizes must match those of tensors, except for dim, where the size must equal sum(tensor.size(dim) for tensor in tensors). Can be on CPU or CUDA.     Note destination must not be specified when out is specified.   Returns \n \n \nIf destination is specified, \n\na tensor located on destination device, that is a result of concatenating tensors along dim.    \n \nIf out is specified, \n\nthe out tensor, now containing results of concatenating tensors along dim.       \n Streams and events  \nclass torch.cuda.Stream [source]\n \nWrapper around a CUDA stream. A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.  Parameters \n \ndevice (torch.device or int, optional) \u2013 a device on which to allocate the stream. If device is None (default) or a negative integer, this will use the current device. \npriority (int, optional) \u2013 priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.     Note Although CUDA versions >= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.   \nquery() [source]\n \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n  \nrecord_event(event=None) [source]\n \nRecords an event.  Parameters \nevent (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n  \nsynchronize() [source]\n \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n  \nwait_event(event) [source]\n \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n  \nwait_stream(stream) [source]\n \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n \n  \nclass torch.cuda.Event [source]\n \nWrapper around a CUDA event. CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams. The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.  Parameters \n \nenable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False) \nblocking (bool, optional) \u2013 if True, wait() will be blocking (default: False) \ninterprocess (bool) \u2013 if True, the event can be shared between processes (default: False)     \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n  \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n  \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n  \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n  \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n  \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n  \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. \n \n Memory management  \ntorch.cuda.empty_cache() [source]\n \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).   \n  \ntorch.cuda.memory_stats(device=None) [source]\n \nReturns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \n\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \n\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \n\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \n\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \n\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \n\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \n\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \n\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  \nall: combined statistics across all memory pools. \nlarge_pool: statistics for the large allocation pool (as of October 2019, for size >= 1MB allocations). \nsmall_pool: statistics for the small allocation pool (as of October 2019, for size < 1MB allocations).  Metric type:  \ncurrent: current value of this metric. \npeak: maximum value of this metric. \nallocated: historical total increase in this metric. \nfreed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \n\"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \n\"num_ooms\": number of out-of-memory errors thrown.   Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_summary(device=None, abbreviated=False) [source]\n \nReturns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \n \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). \nabbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).     Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_snapshot() [source]\n \nReturns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_allocated(device=None) [source]\n \nReturns the current GPU memory occupied by tensors in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.max_memory_allocated(device=None) [source]\n \nReturns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.reset_max_memory_allocated(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_reserved(device=None) [source]\n \nReturns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.max_memory_reserved(device=None) [source]\n \nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.set_per_process_memory_fraction(fraction, device=None) [source]\n \nSet memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.  Parameters \n \nfraction (float) \u2013 Range: 0~1. Allowed memory equals total_memory * fraction. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used.     Note In general, the total available free memory is less than the total capacity.  \n  \ntorch.cuda.memory_cached(device=None) [source]\n \nDeprecated; see memory_reserved(). \n  \ntorch.cuda.max_memory_cached(device=None) [source]\n \nDeprecated; see max_memory_reserved(). \n  \ntorch.cuda.reset_max_memory_cached(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n NVIDIA Tools Extension (NVTX)  \ntorch.cuda.nvtx.mark(msg) [source]\n \nDescribe an instantaneous event that occurred at some point.  Parameters \nmsg (string) \u2013 ASCII message to associate with the event.   \n  \ntorch.cuda.nvtx.range_push(msg) [source]\n \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.  Parameters \nmsg (string) \u2013 ASCII message to associate with range   \n  \ntorch.cuda.nvtx.range_pop() [source]\n \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended. \n\n"}, {"name": "torch.cuda.amp", "path": "amp", "type": "torch.cuda.amp", "text": "Automatic Mixed Precision package - torch.cuda.amp torch.cuda.amp provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half). Some ops, like linear layers and convolutions, are much faster in float16. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype. Ordinarily, \u201cautomatic mixed precision training\u201d uses torch.cuda.amp.autocast and torch.cuda.amp.GradScaler together, as shown in the Automatic Mixed Precision examples and Automatic Mixed Precision recipe. However, autocast and GradScaler are modular, and may be used separately if desired.  Autocasting Gradient Scaling \nAutocast Op Reference  Op Eligibility \nOp-Specific Behavior  Ops that can autocast to float16 Ops that can autocast to float32 Ops that promote to the widest input type Prefer binary_cross_entropy_with_logits over binary_cross_entropy      Autocasting  \nclass torch.cuda.amp.autocast(enabled=True) [source]\n \nInstances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details. When entering an autocast-enabled region, Tensors may be any type. You should not call .half() on your model(s) or inputs when using autocasting. autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops. Example: # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions). autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. Example: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue. autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).  Parameters \nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled in the region.   \n  \ntorch.cuda.amp.custom_fwd(fwd=None, **kwargs) [source]\n \nHelper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.  Parameters \ncast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.    Note If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.  \n  \ntorch.cuda.amp.custom_bwd(bwd) [source]\n \nHelper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail. \n Gradient Scaling If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost. To prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don\u2019t flush to zero. Each parameter\u2019s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.  \nclass torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True) [source]\n \n \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n  \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n  \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n  \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n  \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n  \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n  \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n  \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n  \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n  \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n  \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n  \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n  \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n  \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.  \n \n Autocast Op Reference Op Eligibility Only CUDA ops are eligible for autocasting. Ops that run in float64 or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled. Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an out=... Tensor are allowed in autocast-enabled regions, but won\u2019t go through autocasting. For example, in an autocast-enabled region a.addmm(b, c) can autocast, but a.addmm_(b, c) and a.addmm(b, c, out=d) cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions. Ops called with an explicit dtype=... argument are not eligible, and will produce output that respects the dtype argument. Op-Specific Behavior The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace. Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they\u2019re downstream from autocasted ops. If an op is unlisted, we assume it\u2019s numerically stable in float16. If you believe an unlisted op is numerically unstable in float16, please file an issue. Ops that can autocast to float16\n __matmul__, addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, prelu, RNNCell Ops that can autocast to float32\n __pow__, __rdiv__, __rpow__, __rtruediv__, acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, gelu, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss Ops that promote to the widest input type These ops don\u2019t require a particular dtype for stability, but take multiple inputs and require that the inputs\u2019 dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32. addcdiv, addcmul, atan2, bilinear, cat, cross, dot, equal, index_put, stack, tensordot Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting\u2019s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled. Prefer binary_cross_entropy_with_logits over binary_cross_entropy\n The backward passes of torch.nn.functional.binary_cross_entropy() (and torch.nn.BCELoss, which wraps it) can produce gradients that aren\u2019t representable in float16. In autocast-enabled regions, the forward input may be float16, which means the backward gradient must be representable in float16 (autocasting float16 forward inputs to float32 doesn\u2019t help, because that cast must be reversed in backward). Therefore, binary_cross_entropy and BCELoss raise an error in autocast-enabled regions. Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits() or torch.nn.BCEWithLogitsLoss. binary_cross_entropy_with_logits and BCEWithLogits are safe to autocast.\n"}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "torch.cuda.amp", "text": " \nclass torch.cuda.amp.autocast(enabled=True) [source]\n \nInstances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details. When entering an autocast-enabled region, Tensors may be any type. You should not call .half() on your model(s) or inputs when using autocasting. autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops. Example: # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions). autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. Example: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue. autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).  Parameters \nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled in the region.   \n"}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "torch.cuda.amp", "text": " \ntorch.cuda.amp.custom_bwd(bwd) [source]\n \nHelper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail. \n"}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "torch.cuda.amp", "text": " \ntorch.cuda.amp.custom_fwd(fwd=None, **kwargs) [source]\n \nHelper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.  Parameters \ncast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.    Note If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.  \n"}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "torch.cuda.amp", "text": " \nclass torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True) [source]\n \n \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n  \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n  \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n  \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n  \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n  \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n  \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n  \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n  \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n  \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n  \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n  \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n  \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n  \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.  \n \n"}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "torch.cuda.amp", "text": " \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "torch.cuda.amp", "text": " \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "torch.cuda.amp", "text": " \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n"}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "torch.cuda.amp", "text": " \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n"}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "torch.cuda.amp", "text": " \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n"}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "torch.cuda.amp", "text": " \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n"}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "torch.cuda.amp", "text": " \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "torch.cuda.amp", "text": " \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "torch.cuda.amp", "text": " \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "torch.cuda.amp", "text": " \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n"}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "torch.cuda.amp", "text": " \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n"}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "torch.cuda.amp", "text": " \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n"}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "torch.cuda.amp", "text": " \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n"}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "torch.cuda.amp", "text": " \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.  \n"}, {"name": "torch.cuda.can_device_access_peer()", "path": "cuda#torch.cuda.can_device_access_peer", "type": "torch.cuda", "text": " \ntorch.cuda.can_device_access_peer(device, peer_device) [source]\n \nChecks if peer access between two devices is possible. \n"}, {"name": "torch.cuda.comm.broadcast()", "path": "cuda#torch.cuda.comm.broadcast", "type": "torch.cuda", "text": " \ntorch.cuda.comm.broadcast(tensor, devices=None, *, out=None) [source]\n \nBroadcasts a tensor to specified GPU devices.  Parameters \n \ntensor (Tensor) \u2013 tensor to broadcast. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to broadcast. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results.     Note Exactly one of devices and out must be specified.   Returns \n \n \nIf devices is specified, \n\na tuple containing copies of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a copy of tensor.       \n"}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "cuda#torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "text": " \ntorch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) [source]\n \nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters \n \ntensors (sequence) \u2013 tensors to broadcast. Must be on the same device, either CPU or GPU. \ndevices (Iterable[torch.device, str or int]) \u2013 an iterable of GPU devices, among which to broadcast. \nbuffer_size (int) \u2013 maximum size of the buffer used for coalescing   Returns \nA tuple containing copies of tensor, placed on devices.   \n"}, {"name": "torch.cuda.comm.gather()", "path": "cuda#torch.cuda.comm.gather", "type": "torch.cuda", "text": " \ntorch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None) [source]\n \nGathers tensors from multiple GPU devices.  Parameters \n \ntensors (Iterable[Tensor]) \u2013 an iterable of tensors to gather. Tensor sizes in all dimensions other than dim have to match. \ndim (int, optional) \u2013 a dimension along which the tensors will be concatenated. Default: 0. \ndestination (torch.device, str, or int, optional) \u2013 the output device. Can be CPU or CUDA. Default: the current CUDA device. \nout (Tensor, optional, keyword-only) \u2013 the tensor to store gather result. Its sizes must match those of tensors, except for dim, where the size must equal sum(tensor.size(dim) for tensor in tensors). Can be on CPU or CUDA.     Note destination must not be specified when out is specified.   Returns \n \n \nIf destination is specified, \n\na tensor located on destination device, that is a result of concatenating tensors along dim.    \n \nIf out is specified, \n\nthe out tensor, now containing results of concatenating tensors along dim.       \n"}, {"name": "torch.cuda.comm.reduce_add()", "path": "cuda#torch.cuda.comm.reduce_add", "type": "torch.cuda", "text": " \ntorch.cuda.comm.reduce_add(inputs, destination=None) [source]\n \nSums tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.  Parameters \n \ninputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. \ndestination (int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns \nA tensor containing an elementwise sum of all inputs, placed on the destination device.   \n"}, {"name": "torch.cuda.comm.scatter()", "path": "cuda#torch.cuda.comm.scatter", "type": "torch.cuda", "text": " \ntorch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None) [source]\n \nScatters tensor across multiple GPUs.  Parameters \n \ntensor (Tensor) \u2013 tensor to scatter. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to scatter. \nchunk_sizes (Iterable[int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sums to tensor.size(dim). If not specified, tensor will be divided into equal chunks. \ndim (int, optional) \u2013 A dimension along which to chunk tensor. Default: 0. \nstreams (Iterable[Stream], optional) \u2013 an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results. Sizes of these tensors must match that of tensor, except for dim, where the total size must sum to tensor.size(dim).     Note Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.   Returns \n \n \nIf devices is specified, \n\na tuple containing chunks of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a chunk of tensor.       \n"}, {"name": "torch.cuda.current_blas_handle()", "path": "cuda#torch.cuda.current_blas_handle", "type": "torch.cuda", "text": " \ntorch.cuda.current_blas_handle() [source]\n \nReturns cublasHandle_t pointer to current cuBLAS handle \n"}, {"name": "torch.cuda.current_device()", "path": "cuda#torch.cuda.current_device", "type": "torch.cuda", "text": " \ntorch.cuda.current_device() [source]\n \nReturns the index of a currently selected device. \n"}, {"name": "torch.cuda.current_stream()", "path": "cuda#torch.cuda.current_stream", "type": "torch.cuda", "text": " \ntorch.cuda.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.default_stream()", "path": "cuda#torch.cuda.default_stream", "type": "torch.cuda", "text": " \ntorch.cuda.default_stream(device=None) [source]\n \nReturns the default Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.device", "path": "cuda#torch.cuda.device", "type": "torch.cuda", "text": " \nclass torch.cuda.device(device) [source]\n \nContext-manager that changes the selected device.  Parameters \ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.   \n"}, {"name": "torch.cuda.device_count()", "path": "cuda#torch.cuda.device_count", "type": "torch.cuda", "text": " \ntorch.cuda.device_count() [source]\n \nReturns the number of GPUs available. \n"}, {"name": "torch.cuda.device_of", "path": "cuda#torch.cuda.device_of", "type": "torch.cuda", "text": " \nclass torch.cuda.device_of(obj) [source]\n \nContext-manager that changes the current device to that of given object. You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.  Parameters \nobj (Tensor or Storage) \u2013 object allocated on the selected device.   \n"}, {"name": "torch.cuda.empty_cache()", "path": "cuda#torch.cuda.empty_cache", "type": "torch.cuda", "text": " \ntorch.cuda.empty_cache() [source]\n \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.Event", "path": "cuda#torch.cuda.Event", "type": "torch.cuda", "text": " \nclass torch.cuda.Event [source]\n \nWrapper around a CUDA event. CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams. The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.  Parameters \n \nenable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False) \nblocking (bool, optional) \u2013 if True, wait() will be blocking (default: False) \ninterprocess (bool) \u2013 if True, the event can be shared between processes (default: False)     \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n  \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n  \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n  \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n  \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n  \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n  \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. \n \n"}, {"name": "torch.cuda.Event.elapsed_time()", "path": "cuda#torch.cuda.Event.elapsed_time", "type": "torch.cuda", "text": " \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n"}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "cuda#torch.cuda.Event.from_ipc_handle", "type": "torch.cuda", "text": " \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n"}, {"name": "torch.cuda.Event.ipc_handle()", "path": "cuda#torch.cuda.Event.ipc_handle", "type": "torch.cuda", "text": " \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n"}, {"name": "torch.cuda.Event.query()", "path": "cuda#torch.cuda.Event.query", "type": "torch.cuda", "text": " \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n"}, {"name": "torch.cuda.Event.record()", "path": "cuda#torch.cuda.Event.record", "type": "torch.cuda", "text": " \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n"}, {"name": "torch.cuda.Event.synchronize()", "path": "cuda#torch.cuda.Event.synchronize", "type": "torch.cuda", "text": " \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n"}, {"name": "torch.cuda.Event.wait()", "path": "cuda#torch.cuda.Event.wait", "type": "torch.cuda", "text": " \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. \n"}, {"name": "torch.cuda.get_arch_list()", "path": "cuda#torch.cuda.get_arch_list", "type": "torch.cuda", "text": " \ntorch.cuda.get_arch_list() [source]\n \nReturns list CUDA architectures this library was compiled for. \n"}, {"name": "torch.cuda.get_device_capability()", "path": "cuda#torch.cuda.get_device_capability", "type": "torch.cuda", "text": " \ntorch.cuda.get_device_capability(device=None) [source]\n \nGets the cuda capability of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe major and minor cuda capability of the device  Return type \ntuple(int, int)   \n"}, {"name": "torch.cuda.get_device_name()", "path": "cuda#torch.cuda.get_device_name", "type": "torch.cuda", "text": " \ntorch.cuda.get_device_name(device=None) [source]\n \nGets the name of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe name of the device  Return type \nstr   \n"}, {"name": "torch.cuda.get_device_properties()", "path": "cuda#torch.cuda.get_device_properties", "type": "torch.cuda", "text": " \ntorch.cuda.get_device_properties(device) [source]\n \nGets the properties of a device.  Parameters \ndevice (torch.device or int or str) \u2013 device for which to return the properties of the device.  Returns \nthe properties of the device  Return type \n_CudaDeviceProperties   \n"}, {"name": "torch.cuda.get_gencode_flags()", "path": "cuda#torch.cuda.get_gencode_flags", "type": "torch.cuda", "text": " \ntorch.cuda.get_gencode_flags() [source]\n \nReturns NVCC gencode flags this library were compiled with. \n"}, {"name": "torch.cuda.get_rng_state()", "path": "cuda#torch.cuda.get_rng_state", "type": "torch.cuda", "text": " \ntorch.cuda.get_rng_state(device='cuda') [source]\n \nReturns the random number generator state of the specified GPU as a ByteTensor.  Parameters \ndevice (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    Warning This function eagerly initializes CUDA.  \n"}, {"name": "torch.cuda.get_rng_state_all()", "path": "cuda#torch.cuda.get_rng_state_all", "type": "torch.cuda", "text": " \ntorch.cuda.get_rng_state_all() [source]\n \nReturns a list of ByteTensor representing the random number states of all devices. \n"}, {"name": "torch.cuda.init()", "path": "cuda#torch.cuda.init", "type": "torch.cuda", "text": " \ntorch.cuda.init() [source]\n \nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. \n"}, {"name": "torch.cuda.initial_seed()", "path": "cuda#torch.cuda.initial_seed", "type": "torch.cuda", "text": " \ntorch.cuda.initial_seed() [source]\n \nReturns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.  \n"}, {"name": "torch.cuda.ipc_collect()", "path": "cuda#torch.cuda.ipc_collect", "type": "torch.cuda", "text": " \ntorch.cuda.ipc_collect() [source]\n \nForce collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  \n"}, {"name": "torch.cuda.is_available()", "path": "cuda#torch.cuda.is_available", "type": "torch.cuda", "text": " \ntorch.cuda.is_available() [source]\n \nReturns a bool indicating if CUDA is currently available. \n"}, {"name": "torch.cuda.is_initialized()", "path": "cuda#torch.cuda.is_initialized", "type": "torch.cuda", "text": " \ntorch.cuda.is_initialized() [source]\n \nReturns whether PyTorch\u2019s CUDA state has been initialized. \n"}, {"name": "torch.cuda.list_gpu_processes()", "path": "cuda#torch.cuda.list_gpu_processes", "type": "torch.cuda", "text": " \ntorch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.manual_seed()", "path": "cuda#torch.cuda.manual_seed", "type": "torch.cuda", "text": " \ntorch.cuda.manual_seed(seed) [source]\n \nSets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().  \n"}, {"name": "torch.cuda.manual_seed_all()", "path": "cuda#torch.cuda.manual_seed_all", "type": "torch.cuda", "text": " \ntorch.cuda.manual_seed_all(seed) [source]\n \nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.   \n"}, {"name": "torch.cuda.max_memory_allocated()", "path": "cuda#torch.cuda.max_memory_allocated", "type": "torch.cuda", "text": " \ntorch.cuda.max_memory_allocated(device=None) [source]\n \nReturns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.max_memory_cached()", "path": "cuda#torch.cuda.max_memory_cached", "type": "torch.cuda", "text": " \ntorch.cuda.max_memory_cached(device=None) [source]\n \nDeprecated; see max_memory_reserved(). \n"}, {"name": "torch.cuda.max_memory_reserved()", "path": "cuda#torch.cuda.max_memory_reserved", "type": "torch.cuda", "text": " \ntorch.cuda.max_memory_reserved(device=None) [source]\n \nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_allocated()", "path": "cuda#torch.cuda.memory_allocated", "type": "torch.cuda", "text": " \ntorch.cuda.memory_allocated(device=None) [source]\n \nReturns the current GPU memory occupied by tensors in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_cached()", "path": "cuda#torch.cuda.memory_cached", "type": "torch.cuda", "text": " \ntorch.cuda.memory_cached(device=None) [source]\n \nDeprecated; see memory_reserved(). \n"}, {"name": "torch.cuda.memory_reserved()", "path": "cuda#torch.cuda.memory_reserved", "type": "torch.cuda", "text": " \ntorch.cuda.memory_reserved(device=None) [source]\n \nReturns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_snapshot()", "path": "cuda#torch.cuda.memory_snapshot", "type": "torch.cuda", "text": " \ntorch.cuda.memory_snapshot() [source]\n \nReturns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_stats()", "path": "cuda#torch.cuda.memory_stats", "type": "torch.cuda", "text": " \ntorch.cuda.memory_stats(device=None) [source]\n \nReturns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \n\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \n\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \n\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \n\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \n\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \n\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \n\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \n\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  \nall: combined statistics across all memory pools. \nlarge_pool: statistics for the large allocation pool (as of October 2019, for size >= 1MB allocations). \nsmall_pool: statistics for the small allocation pool (as of October 2019, for size < 1MB allocations).  Metric type:  \ncurrent: current value of this metric. \npeak: maximum value of this metric. \nallocated: historical total increase in this metric. \nfreed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \n\"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \n\"num_ooms\": number of out-of-memory errors thrown.   Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_summary()", "path": "cuda#torch.cuda.memory_summary", "type": "torch.cuda", "text": " \ntorch.cuda.memory_summary(device=None, abbreviated=False) [source]\n \nReturns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \n \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). \nabbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).     Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.nvtx.mark()", "path": "cuda#torch.cuda.nvtx.mark", "type": "torch.cuda", "text": " \ntorch.cuda.nvtx.mark(msg) [source]\n \nDescribe an instantaneous event that occurred at some point.  Parameters \nmsg (string) \u2013 ASCII message to associate with the event.   \n"}, {"name": "torch.cuda.nvtx.range_pop()", "path": "cuda#torch.cuda.nvtx.range_pop", "type": "torch.cuda", "text": " \ntorch.cuda.nvtx.range_pop() [source]\n \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended. \n"}, {"name": "torch.cuda.nvtx.range_push()", "path": "cuda#torch.cuda.nvtx.range_push", "type": "torch.cuda", "text": " \ntorch.cuda.nvtx.range_push(msg) [source]\n \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.  Parameters \nmsg (string) \u2013 ASCII message to associate with range   \n"}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "cuda#torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "text": " \ntorch.cuda.reset_max_memory_allocated(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "cuda#torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "text": " \ntorch.cuda.reset_max_memory_cached(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.seed()", "path": "cuda#torch.cuda.seed", "type": "torch.cuda", "text": " \ntorch.cuda.seed() [source]\n \nSets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().  \n"}, {"name": "torch.cuda.seed_all()", "path": "cuda#torch.cuda.seed_all", "type": "torch.cuda", "text": " \ntorch.cuda.seed_all() [source]\n \nSets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. \n"}, {"name": "torch.cuda.set_device()", "path": "cuda#torch.cuda.set_device", "type": "torch.cuda", "text": " \ntorch.cuda.set_device(device) [source]\n \nSets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters \ndevice (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.   \n"}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "cuda#torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "text": " \ntorch.cuda.set_per_process_memory_fraction(fraction, device=None) [source]\n \nSet memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.  Parameters \n \nfraction (float) \u2013 Range: 0~1. Allowed memory equals total_memory * fraction. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used.     Note In general, the total available free memory is less than the total capacity.  \n"}, {"name": "torch.cuda.set_rng_state()", "path": "cuda#torch.cuda.set_rng_state", "type": "torch.cuda", "text": " \ntorch.cuda.set_rng_state(new_state, device='cuda') [source]\n \nSets the random number generator state of the specified GPU.  Parameters \n \nnew_state (torch.ByteTensor) \u2013 The desired state \ndevice (torch.device or int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    \n"}, {"name": "torch.cuda.set_rng_state_all()", "path": "cuda#torch.cuda.set_rng_state_all", "type": "torch.cuda", "text": " \ntorch.cuda.set_rng_state_all(new_states) [source]\n \nSets the random number generator state of all devices.  Parameters \nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device   \n"}, {"name": "torch.cuda.Stream", "path": "cuda#torch.cuda.Stream", "type": "torch.cuda", "text": " \nclass torch.cuda.Stream [source]\n \nWrapper around a CUDA stream. A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.  Parameters \n \ndevice (torch.device or int, optional) \u2013 a device on which to allocate the stream. If device is None (default) or a negative integer, this will use the current device. \npriority (int, optional) \u2013 priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.     Note Although CUDA versions >= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.   \nquery() [source]\n \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n  \nrecord_event(event=None) [source]\n \nRecords an event.  Parameters \nevent (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n  \nsynchronize() [source]\n \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n  \nwait_event(event) [source]\n \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n  \nwait_stream(stream) [source]\n \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n \n"}, {"name": "torch.cuda.stream()", "path": "cuda#torch.cuda.stream", "type": "torch.cuda", "text": " \ntorch.cuda.stream(stream) [source]\n \nContext-manager that selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.  Parameters \nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.    Note Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.  \n"}, {"name": "torch.cuda.Stream.query()", "path": "cuda#torch.cuda.Stream.query", "type": "torch.cuda", "text": " \nquery() [source]\n \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n"}, {"name": "torch.cuda.Stream.record_event()", "path": "cuda#torch.cuda.Stream.record_event", "type": "torch.cuda", "text": " \nrecord_event(event=None) [source]\n \nRecords an event.  Parameters \nevent (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n"}, {"name": "torch.cuda.Stream.synchronize()", "path": "cuda#torch.cuda.Stream.synchronize", "type": "torch.cuda", "text": " \nsynchronize() [source]\n \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n"}, {"name": "torch.cuda.Stream.wait_event()", "path": "cuda#torch.cuda.Stream.wait_event", "type": "torch.cuda", "text": " \nwait_event(event) [source]\n \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n"}, {"name": "torch.cuda.Stream.wait_stream()", "path": "cuda#torch.cuda.Stream.wait_stream", "type": "torch.cuda", "text": " \nwait_stream(stream) [source]\n \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n"}, {"name": "torch.cuda.synchronize()", "path": "cuda#torch.cuda.synchronize", "type": "torch.cuda", "text": " \ntorch.cuda.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on a CUDA device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cummax()", "path": "generated/torch.cummax#torch.cummax", "type": "torch", "text": " \ntorch.cummax(input, dim, *, out=None) -> (Tensor, LongTensor)  \nReturns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.  yi=max(x1,x2,x3,\u2026,xi)y_i = max(x_1, x_2, x_3, \\dots, x_i)  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))\n \n"}, {"name": "torch.cummin()", "path": "generated/torch.cummin#torch.cummin", "type": "torch", "text": " \ntorch.cummin(input, dim, *, out=None) -> (Tensor, LongTensor)  \nReturns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.  yi=min(x1,x2,x3,\u2026,xi)y_i = min(x_1, x_2, x_3, \\dots, x_i)  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))\n \n"}, {"name": "torch.cumprod()", "path": "generated/torch.cumprod#torch.cumprod", "type": "torch", "text": " \ntorch.cumprod(input, dim, *, dtype=None, out=None) \u2192 Tensor  \nReturns the cumulative product of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1\u00d7x2\u00d7x3\u00d7\u22ef\u00d7xiy_i = x_1 \\times x_2\\times x_3\\times \\dots \\times x_i  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])\n \n"}, {"name": "torch.cumsum()", "path": "generated/torch.cumsum#torch.cumsum", "type": "torch", "text": " \ntorch.cumsum(input, dim, *, dtype=None, out=None) \u2192 Tensor  \nReturns the cumulative sum of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1+x2+x3+\u22ef+xiy_i = x_1 + x_2 + x_3 + \\dots + x_i  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])\n \n"}, {"name": "torch.deg2rad()", "path": "generated/torch.deg2rad#torch.deg2rad", "type": "torch", "text": " \ntorch.deg2rad(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with each of the elements of input converted from angles in degrees to radians.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])\n \n"}, {"name": "torch.dequantize()", "path": "generated/torch.dequantize#torch.dequantize", "type": "torch", "text": " \ntorch.dequantize(tensor) \u2192 Tensor  \nReturns an fp32 Tensor by dequantizing a quantized Tensor  Parameters \ntensor (Tensor) \u2013 A quantized Tensor    \ntorch.dequantize(tensors) \u2192 sequence of Tensors \n Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors  Parameters \ntensors (sequence of Tensors) \u2013 A list of quantized Tensors   \n"}, {"name": "torch.det()", "path": "generated/torch.det#torch.det", "type": "torch", "text": " \ntorch.det(input) \u2192 Tensor  \nCalculates determinant of a square matrix or batches of square matrices.  Note torch.det() is deprecated. Please use torch.linalg.det() instead.   Note Backward through detdet  internally uses SVD results when input is not invertible. In this case, double backward through detdet  will be unstable when input doesn\u2019t have distinct singular values. See torch.svd~torch.svd  for details.   Parameters \ninput (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.   Example: >>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(3.7641)\n\n>>> A = torch.randn(3, 2, 2)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n \n"}, {"name": "torch.diag()", "path": "generated/torch.diag#torch.diag", "type": "torch", "text": " \ntorch.diag(input, diagonal=0, *, out=None) \u2192 Tensor  \n If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input.  The argument diagonal controls which diagonal to consider:  If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndiagonal (int, optional) \u2013 the diagonal to consider   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    See also torch.diagonal() always returns the diagonal of its input. torch.diagflat() always constructs a tensor with diagonal elements specified by the input.  Examples: Get the square matrix where the input vector is the diagonal: >>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\n Get the k-th diagonal of a given matrix: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])\n \n"}, {"name": "torch.diagflat()", "path": "generated/torch.diagflat#torch.diagflat", "type": "torch", "text": " \ntorch.diagflat(input, offset=0) \u2192 Tensor  \n If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened input.  The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \noffset (int, optional) \u2013 the diagonal to consider. Default: 0 (main diagonal).    Examples: >>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\n \n"}, {"name": "torch.diagonal()", "path": "generated/torch.diagonal#torch.diagonal", "type": "torch", "text": " \ntorch.diagonal(input, offset=0, dim1=0, dim2=1) \u2192 Tensor  \nReturns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.  Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.  Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 2-dimensional. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: 0. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: 1.     Note To take a batch diagonal, pass in dim1=-2, dim2=-1.  Examples: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n \n"}, {"name": "torch.diag_embed()", "path": "generated/torch.diag_embed#torch.diag_embed", "type": "torch", "text": " \ntorch.diag_embed(input, offset=0, dim1=-2, dim2=-1) \u2192 Tensor  \nCreates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.  The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 00 , the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset. Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.  Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 1-dimensional. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: -2. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: -1.    Example: >>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n \n"}, {"name": "torch.diff()", "path": "generated/torch.diff#torch.diff", "type": "torch", "text": " \ntorch.diff(input, n=1, dim=-1, prepend=None, append=None) \u2192 Tensor  \nComputes the n-th forward difference along the given dimension. The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order differences are calculated by using torch.diff() recursively.  Note Only n = 1 is currently supported   Parameters \n \ninput (Tensor) \u2013 the tensor to compute the differences on \nn (int, optional) \u2013 the number of times to recursively compute the difference \ndim (int, optional) \u2013 the dimension to compute the difference along. Default is the last dimension. \nappend (prepend,) \u2013 values to prepend or append to input along dim before computing the difference. Their dimensions must be equivalent to that of input, and their shapes must match input\u2019s shape except on dim.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])\n \n"}, {"name": "torch.digamma()", "path": "generated/torch.digamma#torch.digamma", "type": "torch", "text": " \ntorch.digamma(input, *, out=None) \u2192 Tensor  \nComputes the logarithmic derivative of the gamma function on input.  \u03c8(x)=ddxln\u2061(\u0393(x))=\u0393\u2032(x)\u0393(x)\\psi(x) = \\frac{d}{dx} \\ln\\left(\\Gamma\\left(x\\right)\\right) = \\frac{\\Gamma'(x)}{\\Gamma(x)}  \n Parameters \ninput (Tensor) \u2013 the tensor to compute the digamma function on  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Note This function is similar to SciPy\u2019s scipy.special.digamma.   Note From PyTorch 1.8 onwards, the digamma function returns -Inf for 0. Previously it returned NaN for 0.  Example: >>> a = torch.tensor([1, 0.5])\n>>> torch.digamma(a)\ntensor([-0.5772, -1.9635])\n \n"}, {"name": "torch.dist()", "path": "generated/torch.dist#torch.dist", "type": "torch", "text": " \ntorch.dist(input, other, p=2) \u2192 Tensor  \nReturns the p-norm of (input - other) The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the Right-hand-side input tensor \np (float, optional) \u2013 the norm to be computed    Example: >>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(inf)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)\n \n"}, {"name": "torch.distributed", "path": "distributed", "type": "torch.distributed", "text": "Distributed communication package - torch.distributed  Note Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.  Backends torch.distributed supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.   \nBackend gloo mpi nccl  \nDevice CPU GPU CPU GPU CPU GPU   \nsend \u2713 \u2718 \u2713 ? \u2718 \u2718  \nrecv \u2713 \u2718 \u2713 ? \u2718 \u2718  \nbroadcast \u2713 \u2713 \u2713 ? \u2718 \u2713  \nall_reduce \u2713 \u2713 \u2713 ? \u2718 \u2713  \nreduce \u2713 \u2718 \u2713 ? \u2718 \u2713  \nall_gather \u2713 \u2718 \u2713 ? \u2718 \u2713  \ngather \u2713 \u2718 \u2713 ? \u2718 \u2718  \nscatter \u2713 \u2718 \u2713 ? \u2718 \u2718  \nreduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713  \nall_to_all \u2718 \u2718 \u2713 ? \u2718 \u2718  \nbarrier \u2713 \u2718 \u2713 ? \u2718 \u2713   Backends that come with PyTorch PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)  Note As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the init_method argument of init_process_group() points to a file it must adhere to the following schema:  Local file system, init_method=\"file:///d:/tmp/some_file\"\n Shared file system, init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"\n  Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.  Which backend to use? In the past, we were often asked: \u201cwhich backend should I use?\u201d.  \nRule of thumb  Use the NCCL backend for distributed GPU training Use the Gloo backend for distributed CPU training.   \nGPU hosts with InfiniBand interconnect  Use NCCL, since it\u2019s the only backend that currently supports InfiniBand and GPUDirect.   \nGPU hosts with Ethernet interconnect  Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)   \nCPU hosts with InfiniBand interconnect  If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.   \nCPU hosts with Ethernet interconnect  Use Gloo, unless you have specific reasons to use MPI.    Common environment variables Choosing the network interface to use By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):  \nNCCL_SOCKET_IFNAME, for example export NCCL_SOCKET_IFNAME=eth0\n \nGLOO_SOCKET_IFNAME, for example export GLOO_SOCKET_IFNAME=eth0\n  If you\u2019re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable. Other NCCL environment variables NCCL has also provided a number of environment variables for fine-tuning purposes. Commonly used ones include the following for debugging purposes:  export NCCL_DEBUG=INFO export NCCL_DEBUG_SUBSYS=ALL  For the full list of NCCL environment variables, please refer to NVIDIA NCCL\u2019s official documentation Basics The torch.distributed package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class torch.nn.parallel.DistributedDataParallel() builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and torch.nn.DataParallel() in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process. In the single-machine synchronous case, torch.distributed or the torch.nn.parallel.DistributedDataParallel() wrapper may still have advantages over other approaches to data-parallelism, including torch.nn.DataParallel():  Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes. Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and \u201cGIL-thrashing\u201d that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.  Initialization The package needs to be initialized using the torch.distributed.init_process_group() function before calling any other methods. This blocks until all processes have joined.  \ntorch.distributed.is_available() [source]\n \nReturns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS. \n  \ntorch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='') [source]\n \nInitializes the default distributed process group, and this will also initialize the distributed package.  There are 2 main ways to initialize a process group:\n\n Specify store, rank, and world_size explicitly. Specify init_method (a URL string) which indicates where/how to discover peers. Optionally specify rank and world_size, or encode all required parameters in the URL and omit them.    If neither is specified, init_method is assumed to be \u201cenv://\u201d.  Parameters \n \nbackend (str or Backend) \u2013 The backend to use. Depending on build-time configurations, valid values include mpi, gloo, and nccl. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If using multiple processes per machine with nccl backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. \ninit_method (str, optional) \u2013 URL specifying how to initialize the process group. Default is \u201cenv://\u201d if no init_method or store is specified. Mutually exclusive with store. \nworld_size (int, optional) \u2013 Number of processes participating in the job. Required if store is specified. \nrank (int, optional) \u2013 Rank of the current process (it should be a number between 0 and world_size-1). Required if store is specified. \nstore (Store, optional) \u2013 Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with init_method. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1. When NCCL_BLOCKING_WAIT is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When NCCL_ASYNC_ERROR_HANDLING is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. NCCL_BLOCKING_WAIT will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, NCCL_ASYNC_ERROR_HANDLING has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set. \ngroup_name (str, optional, deprecated) \u2013 Group name.    To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI. \n  \nclass torch.distributed.Backend [source]\n \nAn enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends. The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL. This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".  Note The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.  \n  \ntorch.distributed.get_backend(group=None) [source]\n \nReturns the backend of the given process group.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.  Returns \nThe backend of the given process group as a lower case string.   \n  \ntorch.distributed.get_rank(group=None) [source]\n \nReturns the rank of current process group Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe rank of the process group -1, if not part of the group   \n  \ntorch.distributed.get_world_size(group=None) [source]\n \nReturns the number of processes in the current process group  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe world size of the process group -1, if not part of the group   \n  \ntorch.distributed.is_initialized() [source]\n \nChecking if the default process group has been initialized \n  \ntorch.distributed.is_mpi_available() [source]\n \nChecks if the MPI backend is available. \n  \ntorch.distributed.is_nccl_available() [source]\n \nChecks if the NCCL backend is available. \n Currently three initialization methods are supported: TCP initialization There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired world_size. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks. Note that multicast address is not supported anymore in the latest distributed package. group_name is deprecated as well. import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n Shared file-system initialization Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired world_size. The URL should start with file:// and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn\u2019t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next init_process_group() call on the same file path/name. Note that automatic rank assignment is not supported anymore in the latest distributed package and group_name is deprecated as well.  Warning This method assumes that the file system supports locking using fcntl - most local systems and NFS support it.   Warning This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call init_process_group() multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call init_process_group() again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time init_process_group() is called.  import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n Environment variable initialization This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:  \nMASTER_PORT - required; has to be a free port on machine with rank 0 \nMASTER_ADDR - required (except for rank 0); address of rank 0 node \nWORLD_SIZE - required; can be set either here, or in a call to init function \nRANK - required; can be set either here, or in a call to init function  The machine with rank 0 will be used to set up all connections. This is the default method, meaning that init_method does not have to be specified (or can be env://). Distributed Key-Value Store The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in torch.distributed.init_process_group() (by explicitly creating the store as an alternative to specifying init_method.) There are 3 choices for Key-Value Stores: TCPStore, FileStore, and HashStore.  \nclass torch.distributed.Store  \nBase class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore). \n  \nclass torch.distributed.TCPStore  \nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc.  Parameters \n \nhost_name (str) \u2013 The hostname or IP Address the server store should run on. \nport (int) \u2013 The port on which the server store should listen for incoming requests. \nworld_size (int) \u2013 The total number of store users (number of clients + 1 for the server). \nis_master (bool) \u2013 True when initializing the server store, False for client stores. \ntimeout (timedelta) \u2013 Timeout used by the store during initialization and for methods such as get() and wait().     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n   \n  \nclass torch.distributed.HashStore  \nA thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.  Example::\n\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n   \n  \nclass torch.distributed.FileStore  \nA store implementation that uses a file to store the underlying key-value pairs.  Parameters \n \nfile_name (str) \u2013 path of the file in which to store the key-value pairs \nworld_size (int) \u2013 The total number of processes using the store     Example::\n\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n   \n  \nclass torch.distributed.PrefixStore  \nA wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store.  Parameters \n \nprefix (str) \u2013 The prefix string that is prepended to each key before being inserted into the store. \nstore (torch.distributed.store) \u2013 A store object that forms the underlying key-value store.    \n  \ntorch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) \u2192 None  \nInserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value.  Parameters \n \nkey (str) \u2013 The key to be added to the store. \nvalue (str) \u2013 The value associated with key to be added to the store.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n  \ntorch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bytes  \nRetrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.  Parameters \nkey (str) \u2013 The function will return the value associated with this key.  Returns \nValue associated with key if key is in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n  \ntorch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) \u2192 int  \nThe first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception.  Parameters \n \nkey (str) \u2013 The key in the store whose counter will be incremented. \namount (int) \u2013 The quantity by which the counter will be incremented.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n   \n  \ntorch.distributed.Store.wait(*args, **kwargs)  \nOverloaded function.  wait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -> None  Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.  Parameters \nkeys (list) \u2013 List of keys on which to wait until they are set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n    wait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -> None  Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout.  Parameters \n \nkeys (list) \u2013 List of keys on which to wait until they are set in the store. \ntimeout (timedelta) \u2013 Time to wait for the keys to be added before throwing an exception.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n   \n  \ntorch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) \u2192 int  \nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.  Warning When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.   Returns \nThe number of keys present in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n   \n  \ntorch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bool  \nDeletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.  Warning The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.   Parameters \nkey (str) \u2013 The key to be deleted from the store  Returns \nTrue if key was deleted, otherwise False.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n   \n  \ntorch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) \u2192 None  \nSets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().  Parameters \ntimeout (timedelta) \u2013 timeout to be set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n   \n Groups By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).  \ntorch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None) [source]\n \nCreates a new distributed group. This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.  Warning Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.   Parameters \n \nranks (list[int]) \u2013 List of ranks of group members. If None, will be set to all ranks. Default is None. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the gloo backend. \nbackend (str or Backend, optional) \u2013 The backend to use. Depending on build-time configurations, valid values are gloo and nccl. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO).   Returns \nA handle of distributed group that can be given to collective calls.   \n Point-to-point communication  \ntorch.distributed.send(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv    \n  \ntorch.distributed.recv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nSender rank -1, if not part of the group   \n isend() and irecv() return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:  \nis_completed() - returns True if the operation has finished \nwait() - will block the process until the operation is finished. is_completed() is guaranteed to return True once it returns.   \ntorch.distributed.isend(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv   Returns \nA distributed request object. None, if not part of the group   \n  \ntorch.distributed.irecv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nA distributed request object. None, if not part of the group   \n Synchronous and asynchronous collective operations Every collective operation function supports the following two kinds of operations, depending on the setting of the async_op flag passed into the collective: Synchronous operation - the default mode, when async_op is set to False. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see CUDA Semantics. See the below script to see examples of differences in these semantics for CPU and CUDA operations. Asynchronous operation - when async_op is set to True. The collective operation function returns a distributed request object. In general, you don\u2019t need to create it manually and it is guaranteed to support two methods:  \nis_completed() - in the case of CPU collectives, returns True if completed. In the case of CUDA operations, returns True if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization. \nwait() - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block until the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.  Example The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams: # Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n Collective functions  \ntorch.distributed.broadcast(tensor, src, group=None, async_op=False) [source]\n \nBroadcasts the tensor to the whole group. tensor must have the same number of elements in all processes participating in the collective.  Parameters \n \ntensor (Tensor) \u2013 Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.broadcast_object_list(object_list, src=0, group=None) [source]\n \nBroadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.  Parameters \n \nobject_list (List[Any]) \u2013 List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes. \nsrc (int) \u2013 Source rank from which to broadcast object_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, object_list will contain the broadcasted objects from src rank.    Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Warning broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]\n   \n  \ntorch.distributed.all_reduce(tensor, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. After the call tensor is going to be bitwise identical in all processes. Complex tensors are supported.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n >>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n \n  \ntorch.distributed.reduce(tensor, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines. Only the process with rank dst is going to receive the final result.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Complex tensors are supported.  Parameters \n \ntensor_list (list[Tensor]) \u2013 Output list. It should contain correctly-sized tensors to be used for output of the collective. \ntensor (Tensor) \u2013 Tensor to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n \n  \ntorch.distributed.all_gather_object(object_list, obj, group=None) [source]\n \nGathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobject_list (list[Any]) \u2013 Output list. It should be correctly sized as the size of the group for this collective and will contain the output. \nobject (Any) \u2013 Pickable Python object to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.    Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Warning all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n   \n  \ntorch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False) [source]\n \nGathers a list of tensors in a single process.  Parameters \n \ntensor (Tensor) \u2013 Input tensor. \ngather_list (list[Tensor], optional) \u2013 List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank) \ndst (int, optional) \u2013 Destination rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None) [source]\n \nGathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobj (Any) \u2013 Input object. Must be picklable. \nobject_gather_list (list[Any]) \u2013 Output list. On the dst rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be None on non-dst ranks. (default is None) \ndst (int, optional) \u2013 Destination rank. (default is 0) \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. On the dst rank, object_gather_list will contain the output of the collective.    Note Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.   Note Note that this API is not supported when using the NCCL backend.   Warning gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n   \n  \ntorch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False) [source]\n \nScatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the tensor argument.  Parameters \n \ntensor (Tensor) \u2013 Output tensor. \nscatter_list (list[Tensor]) \u2013 List of tensors to scatter (default is None, must be specified on the source rank) \nsrc (int) \u2013 Source rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None) [source]\n \nScatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.  Parameters \n \nscatter_object_output_list (List[Any]) \u2013 Non-empty list whose first element will store the object scattered to this rank. \nscatter_object_input_list (List[Any]) \u2013 List of input objects to scatter. Each object must be picklable. Only objects on the src rank will be scattered, and the argument can be None for non-src ranks. \nsrc (int) \u2013 Source rank from which to scatter scatter_object_input_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.    Note Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.   Warning scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n   \n  \ntorch.distributed.reduce_scatter(output, input_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces, then scatters a list of tensors to all processes in a group.  Parameters \n \noutput (Tensor) \u2013 Output tensor. \ninput_list (list[Tensor]) \u2013 List of tensors to reduce and scatter. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n  \ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False) [source]\n \nEach process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.  Parameters \n \noutput_tensor_list (list[Tensor]) \u2013 List of tensors to be gathered one per rank. \ninput_tensor_list (list[Tensor]) \u2013 List of tensors to scatter one per rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.    Warning all_to_all is experimental and subject to change.  Examples >>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n >>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n >>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n \n  \ntorch.distributed.barrier(group=None, async_op=False, device_ids=None) [source]\n \nSynchronizes all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().  Parameters \n \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndevice_ids ([int], optional) \u2013 List of device/GPU ids. Valid only for NCCL backend.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \nclass torch.distributed.ReduceOp  \nAn enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR. Note that BAND, BOR, and BXOR reductions are not available when using the NCCL backend. Additionally, MAX, MIN and PRODUCT are not supported for complex tensors. The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc. Members: SUM PRODUCT MIN MAX BAND BOR BXOR \n  \nclass torch.distributed.reduce_op  \nDeprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX. ReduceOp is recommended to use instead. \n Autograd-enabled communication primitives If you want to use collective communication functions supporting autograd you can find an implementation of those in the torch.distributed.nn.* module. Functions here are synchronous and will be inserted in the autograd graph, so you need to ensure that all the processes that participated in the collective operation will do the backward pass for the backward communication to effectively happen and don\u2019t cause a deadlock. Please notice that currently the only backend where all the functions are guaranteed to work is gloo. .. autofunction:: torch.distributed.nn.broadcast .. autofunction:: torch.distributed.nn.gather .. autofunction:: torch.distributed.nn.scatter .. autofunction:: torch.distributed.nn.reduce .. autofunction:: torch.distributed.nn.all_gather .. autofunction:: torch.distributed.nn.all_to_all .. autofunction:: torch.distributed.nn.all_reduce Multi-GPU collective functions If you have more than one GPU on each node, when using the NCCL and Gloo backend, broadcast_multigpu() all_reduce_multigpu() reduce_multigpu() all_gather_multigpu() and reduce_scatter_multigpu() support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend. For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference: Code running on Node 0 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n Code running on Node 1 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n After the call, all 16 tensors on the two nodes will have the all-reduced value of 16  \ntorch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0) [source]\n \nBroadcasts the tensor to the whole group with multiple GPU tensors per node. tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU Only nccl and gloo backend are currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \nsrc_tensor (int, optional) \u2013 Source tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.all_reduce_multigpu(tensor_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensor in tensor_list is going to be bitwise identical in all processes. Complex tensors are supported. Only nccl and gloo backend is currently supported tensors should only be GPU tensors  Parameters \n \nlist (tensor) \u2013 List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.reduce_multigpu(tensor_list, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False, dst_tensor=0) [source]\n \nReduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result. Only nccl backend is currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndst_tensor (int, optional) \u2013 Destination tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, otherwise   \n  \ntorch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU Only nccl backend is currently supported tensors should only be GPU tensors Complex tensors are supported.  Parameters \n \noutput_tensor_lists (List[List[Tensor]]) \u2013 \nOutput lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i]. Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j] Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ninput_tensor_list (List[Tensor]) \u2013 List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported. Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.  Parameters \n \noutput_tensor_list (List[Tensor]) \u2013 \nOutput tensors (on different GPUs) to receive the result of the operation. Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.  \ninput_tensor_lists (List[List[Tensor]]) \u2013 \nInput lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i]. Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j] Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n Third-party backends Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to Tutorials - Custom C++ and CUDA Extensions and test/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party backends are decided by their own implementations. The new backend derives from c10d.ProcessGroup and registers the backend name and the instantiating interface through torch.distributed.Backend.register_backend() when imported. When manually importing this backend and invoking torch.distributed.init_process_group() with the corresponding backend name, the torch.distributed package runs on the new backend.  Warning The support of third-party backend is experimental and subject to change.  Launch utility The torch.distributed package also provides a launch utility in torch.distributed.launch. This helper utility can be used to launch multiple processes per node for distributed training. torch.distributed.launch is a module that spawns up multiple distributed training processes on each of the training nodes. The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth. In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (--nproc_per_node). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (nproc_per_node), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1). How to use this module:  Single-Node multi-process distributed training  >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n  Multi-Node multi-process distributed training: (e.g. two nodes)  Node 1: (IP: 192.168.1.1, and has a free port: 1234) >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n Node 2: >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n  To look up what optional arguments this module offers:  >>> python -m torch.distributed.launch --help\n Important Notices: 1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training. 2. In your training program, you must parse the command-line argument: --local_rank=LOCAL_PROCESS_RANK, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by: Parsing the local_rank argument >>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local_rank\", type=int)\n>>> args = parser.parse_args()\n Set your device to local rank using either >>> torch.cuda.set_device(args.local_rank)  # before your code runs\n or >>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n 3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. You need to make sure that the init_method uses env://, which is the only supported init_method by this module. torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')\n 4. In your training program, you can either use regular distributed functions or use torch.nn.parallel.DistributedDataParallel() module. If your training program uses GPUs for training and you would like to use torch.nn.parallel.DistributedDataParallel() module, here is how to configure it. model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[args.local_rank],\n                                                  output_device=args.local_rank)\n Please ensure that device_ids argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the device_ids needs to be [args.local_rank], and output_device needs to be args.local_rank in order to use this utility 5. Another way to pass local_rank to the subprocesses via environment variable LOCAL_RANK. This behavior is enabled when you launch the script with --use_env=True. You must adjust the subprocess example above to replace args.local_rank with os.environ['LOCAL_RANK']; the launcher will not pass --local_rank when you specify this flag.  Warning local_rank is NOT globally unique: it is only unique per process on a machine. Thus, don\u2019t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don\u2019t do this correctly.  Spawn utility The Multiprocessing package - torch.multiprocessing package also provides a spawn function in torch.multiprocessing.spawn(). This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well. For references on how to use it, please refer to PyTorch example - ImageNet implementation Note that this function requires Python 3.4 or higher.\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group, bucket) [source]\n \nThis DDP communication hook just calls allreduce using GradBucket tensors. Once gradient tensors are aggregated across all workers, its then callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won\u2019t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.  Example::\n\n>>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group, bucket) [source]\n \nThis DDP communication hook implements a simple gradient compression approach that converts GradBucket tensors whose type is assumed to be torch.float32 to half-precision floating point format (torch.float16). It allreduces those float16 gradient tensors. Once compressed gradient tensors are allreduced, its then callback called decompress converts the aggregated result back to float32 and takes the mean.  Example::\n\n>>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state, bucket) [source]\n \nThis DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is faster than powerSGD_hook(), but usually results in a much lower accuracy, unless matrix_approximation_rank is 1.  Warning Increasing matrix_approximation_rank here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider powerSGD_hook() first, and only consider this variant when a satisfactory accuracy can be achieved when matrix_approximation_rank is 1.  Once gradient tensors are aggregated across all workers, this hook applies compression as follows:  Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings; Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; Computes P, which is equal to MQ; Allreduces P; Orthogonalizes P; Computes Q, which is approximately equal to M^TP; Allreduces Q; Computes M, which is approximately equal to PQ^T. Truncates the input tensor to the original length.  Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.  Parameters \n \nstate (PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune matrix_approximation_rank and start_powerSGD_iter. \nbucket (dist._GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode at this time, only exactly one tensor is stored in this bucket.   Returns \nFuture handler of the communication, which updates the gradients in place.    Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)\n>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": " \nclass torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group, matrix_approximation_rank=1, start_powerSGD_iter=10, use_error_feedback=True, warm_start=True, random_seed=0) [source]\n \nStores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training. Particularly, matrix_approximation_rank and start_powerSGD_iter are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters use_error_feedback and warm_start on.  \nmatrix_approximation_rank controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression. 1.1. If matrix_approximation_rank is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy. 1.2. The increase of matrix_approximation_rank can substantially increase the computation costs of the compression, and the accuracy may not be futher improved beyond a certain matrix_approximation_rank threshold.   To tune matrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an expoential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.  \nstart_powerSGD_iter defers PowerSGD compression util step start_powerSGD_iter, and vanilla allreduce runs prior to step start_powerSGD_iter. This hybrid scheme of vanilla allreduce + PowerSGD can effectively improve the accuracy, even a relatively small matrix_approximation_rank is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.  To tune start_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached.  Warning If error feedback or warm-up is enabled, the minimum value of start_powerSGD_iter allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.  \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state, bucket) [source]\n \nThis DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:  Views the input flattened 1D gradient tensor as two groups of per-parameter tensors: high-rank tensors and vector-like rank-1 tensors (for biases). \nHandles rank-1 tensors by allreducing them without compression: 2.1. Allocate contiguous memory for those rank-1 tensors, and allreduces all the rank-1 tensors as a batch, without compression; 2.2. Copies the individual rank-1 tensors from the contiguous memory back to the input tensor.  \nHandles high-rank tensors by PowerSGD compression: 3.1. For each high-rank tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; 3.2. Computes each P in Ps, which is equal to MQ; 3.3. Allreduces Ps as a batch; 3.4. Orthogonalizes each P in Ps; 3.5. Computes each Q in Qs, which is approximately equal to M^TP; 3.6. Allreduces Qs as a batch; 3.7. Computes each M among all the high-rank tensors, which is approximately equal to PQ^T.   Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.  Parameters \n \nstate (PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune matrix_approximation_rank` and start_powerSGD_iter. \nbucket (dist._GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode at this time, only exactly one tensor is stored in this bucket.   Returns \nFuture handler of the communication, which updates the gradients in place.    Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, powerSGD_hook)\n   \n"}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "torch.distributed", "text": " \ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Complex tensors are supported.  Parameters \n \ntensor_list (list[Tensor]) \u2013 Output list. It should contain correctly-sized tensors to be used for output of the collective. \ntensor (Tensor) \u2013 Tensor to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n \n"}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU Only nccl backend is currently supported tensors should only be GPU tensors Complex tensors are supported.  Parameters \n \noutput_tensor_lists (List[List[Tensor]]) \u2013 \nOutput lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i]. Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j] Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ninput_tensor_list (List[Tensor]) \u2013 List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "torch.distributed", "text": " \ntorch.distributed.all_gather_object(object_list, obj, group=None) [source]\n \nGathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobject_list (list[Any]) \u2013 Output list. It should be correctly sized as the size of the group for this collective and will contain the output. \nobject (Any) \u2013 Pickable Python object to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.    Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Warning all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "torch.distributed", "text": " \ntorch.distributed.all_reduce(tensor, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. After the call tensor is going to be bitwise identical in all processes. Complex tensors are supported.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n >>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n \n"}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.all_reduce_multigpu(tensor_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensor in tensor_list is going to be bitwise identical in all processes. Complex tensors are supported. Only nccl and gloo backend is currently supported tensors should only be GPU tensors  Parameters \n \nlist (tensor) \u2013 List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "torch.distributed", "text": " \ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False) [source]\n \nEach process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.  Parameters \n \noutput_tensor_list (list[Tensor]) \u2013 List of tensors to be gathered one per rank. \ninput_tensor_list (list[Tensor]) \u2013 List of tensors to scatter one per rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.    Warning all_to_all is experimental and subject to change.  Examples >>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n >>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n >>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n \n"}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.autograd.backward(context_id: int, roots: List[Tensor], retain_graph = False) \u2192 None  \nKicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass. We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done. We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context to be used is looked up given the context_id that is passed in when torch.distributed.autograd.backward() is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API.  Parameters \n \ncontext_id (int) \u2013 The autograd context id for which we should retrieve the gradients. \nroots (list) \u2013 Tensors which represent the roots of the autograd computation. All the tensors should be scalars. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n   \n"}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.autograd.context [source]\n \nContext object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass.  Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>   t1 = torch.rand((3, 3), requires_grad=True)\n>>>   t2 = torch.rand((3, 3), requires_grad=True)\n>>>   loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>   dist_autograd.backward(context_id, [loss])\n   \n"}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.autograd.get_gradients(context_id: int) \u2192 Dict[Tensor, Tensor]  \nRetrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass.  Parameters \ncontext_id (int) \u2013 The autograd context id for which we should retrieve the gradients.  Returns \nA map where the key is the Tensor and the value is the associated gradient for that Tensor.    Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2])\n   \n"}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "torch.distributed", "text": " \nclass torch.distributed.Backend [source]\n \nAn enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends. The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL. This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".  Note The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.  \n"}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "torch.distributed", "text": " \ntorch.distributed.barrier(group=None, async_op=False, device_ids=None) [source]\n \nSynchronizes all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().  Parameters \n \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndevice_ids ([int], optional) \u2013 List of device/GPU ids. Valid only for NCCL backend.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "torch.distributed", "text": " \ntorch.distributed.broadcast(tensor, src, group=None, async_op=False) [source]\n \nBroadcasts the tensor to the whole group. tensor must have the same number of elements in all processes participating in the collective.  Parameters \n \ntensor (Tensor) \u2013 Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0) [source]\n \nBroadcasts the tensor to the whole group with multiple GPU tensors per node. tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU Only nccl and gloo backend are currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \nsrc_tensor (int, optional) \u2013 Source tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "torch.distributed", "text": " \ntorch.distributed.broadcast_object_list(object_list, src=0, group=None) [source]\n \nBroadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.  Parameters \n \nobject_list (List[Any]) \u2013 List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes. \nsrc (int) \u2013 Source rank from which to broadcast object_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, object_list will contain the broadcasted objects from src rank.    Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Warning broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "torch.distributed", "text": " \nclass torch.distributed.FileStore  \nA store implementation that uses a file to store the underlying key-value pairs.  Parameters \n \nfile_name (str) \u2013 path of the file in which to store the key-value pairs \nworld_size (int) \u2013 The total number of processes using the store     Example::\n\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "torch.distributed", "text": " \ntorch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False) [source]\n \nGathers a list of tensors in a single process.  Parameters \n \ntensor (Tensor) \u2013 Input tensor. \ngather_list (list[Tensor], optional) \u2013 List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank) \ndst (int, optional) \u2013 Destination rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "torch.distributed", "text": " \ntorch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None) [source]\n \nGathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobj (Any) \u2013 Input object. Must be picklable. \nobject_gather_list (list[Any]) \u2013 Output list. On the dst rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be None on non-dst ranks. (default is None) \ndst (int, optional) \u2013 Destination rank. (default is 0) \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. On the dst rank, object_gather_list will contain the output of the collective.    Note Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.   Note Note that this API is not supported when using the NCCL backend.   Warning gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "torch.distributed", "text": " \ntorch.distributed.get_backend(group=None) [source]\n \nReturns the backend of the given process group.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.  Returns \nThe backend of the given process group as a lower case string.   \n"}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "torch.distributed", "text": " \ntorch.distributed.get_rank(group=None) [source]\n \nReturns the rank of current process group Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe rank of the process group -1, if not part of the group   \n"}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "torch.distributed", "text": " \ntorch.distributed.get_world_size(group=None) [source]\n \nReturns the number of processes in the current process group  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe world size of the process group -1, if not part of the group   \n"}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "torch.distributed", "text": " \nclass torch.distributed.HashStore  \nA thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.  Example::\n\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n   \n"}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "torch.distributed", "text": " \ntorch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='') [source]\n \nInitializes the default distributed process group, and this will also initialize the distributed package.  There are 2 main ways to initialize a process group:\n\n Specify store, rank, and world_size explicitly. Specify init_method (a URL string) which indicates where/how to discover peers. Optionally specify rank and world_size, or encode all required parameters in the URL and omit them.    If neither is specified, init_method is assumed to be \u201cenv://\u201d.  Parameters \n \nbackend (str or Backend) \u2013 The backend to use. Depending on build-time configurations, valid values include mpi, gloo, and nccl. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If using multiple processes per machine with nccl backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. \ninit_method (str, optional) \u2013 URL specifying how to initialize the process group. Default is \u201cenv://\u201d if no init_method or store is specified. Mutually exclusive with store. \nworld_size (int, optional) \u2013 Number of processes participating in the job. Required if store is specified. \nrank (int, optional) \u2013 Rank of the current process (it should be a number between 0 and world_size-1). Required if store is specified. \nstore (Store, optional) \u2013 Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with init_method. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1. When NCCL_BLOCKING_WAIT is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When NCCL_ASYNC_ERROR_HANDLING is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. NCCL_BLOCKING_WAIT will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, NCCL_ASYNC_ERROR_HANDLING has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set. \ngroup_name (str, optional, deprecated) \u2013 Group name.    To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI. \n"}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "torch.distributed", "text": " \ntorch.distributed.irecv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nA distributed request object. None, if not part of the group   \n"}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "torch.distributed", "text": " \ntorch.distributed.isend(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv   Returns \nA distributed request object. None, if not part of the group   \n"}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "torch.distributed", "text": " \ntorch.distributed.is_available() [source]\n \nReturns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS. \n"}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "torch.distributed", "text": " \ntorch.distributed.is_initialized() [source]\n \nChecking if the default process group has been initialized \n"}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "torch.distributed", "text": " \ntorch.distributed.is_mpi_available() [source]\n \nChecks if the MPI backend is available. \n"}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "torch.distributed", "text": " \ntorch.distributed.is_nccl_available() [source]\n \nChecks if the NCCL backend is available. \n"}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "torch.distributed", "text": " \ntorch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None) [source]\n \nCreates a new distributed group. This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.  Warning Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.   Parameters \n \nranks (list[int]) \u2013 List of ranks of group members. If None, will be set to all ranks. Default is None. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the gloo backend. \nbackend (str or Backend, optional) \u2013 The backend to use. Depending on build-time configurations, valid values are gloo and nccl. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO).   Returns \nA handle of distributed group that can be given to collective calls.   \n"}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "rpc#torch.distributed.optim.DistributedOptimizer", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs) [source]\n \nDistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter. This class uses get_gradients() in order to retrieve the gradients for specific parameters. Concurrent calls to step(), either from the same or different clients, will be serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers. DistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) during multithreaded training (e.g. Distributed Model Parallel). This feature is currently in beta stage, enabled for optimizers including Adagrad, Adam, SGD, RMSprop, AdamW and Adadelta. We are increasing the coverage to all optimizers in future releases.  Parameters \n \noptimizer_class (optim.Optimizer) \u2013 the class of optimizer to instantiate on each worker. \nparams_rref (list[RRef]) \u2013 list of RRefs to local or remote parameters to optimize. \nargs \u2013 arguments to pass to the optimizer constructor on each worker. \nkwargs \u2013 arguments to pass to the optimizer constructor on each worker.     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id)\n    \nstep(context_id) [source]\n \nPerforms a single optimization step. This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.  Parameters \ncontext_id \u2013 the autograd context id for which we should run the optimizer step.   \n \n"}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "rpc#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed RPC Framework", "text": " \nstep(context_id) [source]\n \nPerforms a single optimization step. This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.  Parameters \ncontext_id \u2013 the autograd context id for which we should run the optimizer step.   \n"}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Pipeline Parallelism", "text": " \nclass torch.distributed.pipeline.sync.Pipe(module, chunks=1, checkpoint='except_last', deferred_batch_norm=False) [source]\n \nWraps an arbitrary nn.Sequential module to train on using synchronous pipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on a single GPU, pipeline parallelism is a useful technique to employ for training. The implementation is based on the torchgpipe paper. Pipe combines pipeline parallelism with checkpointing to reduce peak memory required to train while minimizing device under-utilization. You should place all the modules on the appropriate devices and wrap them into an nn.Sequential module defining the desired order of execution.  Parameters \n \nmodule (nn.Sequential) \u2013 sequential module to be parallelized using pipelining. Each module in the sequence has to have all of its parameters on a single device. Each module in the sequence has to either be an nn.Module or nn.Sequential (to combine multiple sequential modules on a single device) \nchunks (int) \u2013 number of micro-batches (default: 1) \ncheckpoint (str) \u2013 when to enable checkpointing, one of 'always', 'except_last', or 'never' (default: 'except_last'). 'never' disables checkpointing completely, 'except_last' enables checkpointing for all micro-batches except the last one and 'always' enables checkpointing for all micro-batches. \ndeferred_batch_norm (bool) \u2013 whether to use deferred BatchNorm moving statistics (default: False). If set to True, we track statistics across multiple micro-batches to update the running statistics per mini-batch.   Raises \n \nTypeError \u2013 the module is not a nn.Sequential. \nValueError \u2013 invalid arguments     Example::\n\nPipeline of two FC layers across GPUs 0 and 1. >>> fc1 = nn.Linear(16, 8).cuda(0)\n>>> fc2 = nn.Linear(8, 4).cuda(1)\n>>> model = nn.Sequential(fc1, fc2)\n>>> model = Pipe(model, chunks=8)\n>>> input = torch.rand(16, 16).cuda(0)\n>>> output_rref = model(input)\n    Note You can wrap a Pipe model with torch.nn.parallel.DistributedDataParallel only when the checkpoint parameter of Pipe is 'never'.   Note Pipe only supports intra-node pipelining currently, but will be expanded to support inter-node pipelining in the future. The forward function returns an RRef to allow for inter-node pipelining in the future, where the output might be on a remote host. For intra-node pipelinining you can use local_value() to retrieve the output locally.   Warning Pipe is experimental and subject to change.   \nforward(input) [source]\n \nProcesses a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to be a Tensor or a sequence of tensors. This restriction is applied at partition boundaries too. The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.  Parameters \ninput (torch.Tensor or sequence of Tensor) \u2013 input mini-batch  Returns \nRRef to the output of the mini-batch  Raises \nTypeError \u2013 input is not a tensor or sequence of tensors.   \n \n"}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Pipeline Parallelism", "text": " \nforward(input) [source]\n \nProcesses a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to be a Tensor or a sequence of tensors. This restriction is applied at partition boundaries too. The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.  Parameters \ninput (torch.Tensor or sequence of Tensor) \u2013 input mini-batch  Returns \nRRef to the output of the mini-batch  Raises \nTypeError \u2013 input is not a tensor or sequence of tensors.   \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Pipeline Parallelism", "text": " \nclass torch.distributed.pipeline.sync.skip.skippable.pop(name) [source]\n \nThe command to pop a skip tensor. def forward(self, input):\n    skip = yield pop('name')\n    return f(input) + skip\n  Parameters \nname (str) \u2013 name of skip tensor  Returns \nthe skip tensor previously stashed by another layer under the same name   \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Pipeline Parallelism", "text": " \ntorch.distributed.pipeline.sync.skip.skippable.skippable(stash=(), pop=()) [source]\n \nThe decorator to define a nn.Module with skip connections. Decorated modules are called \u201cskippable\u201d. This functionality works perfectly fine even when the module is not wrapped by Pipe. Each skip tensor is managed by its name. Before manipulating skip tensors, a skippable module must statically declare the names for skip tensors by stash and/or pop parameters. Skip tensors with pre-declared name can be stashed by yield stash(name, tensor) or popped by tensor = yield\npop(name). Here is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed and popped at the first and last layer, respectively: @skippable(stash=['1to3'])\nclass Layer1(nn.Module):\n    def forward(self, input):\n        yield stash('1to3', input)\n        return f1(input)\n\nclass Layer2(nn.Module):\n    def forward(self, input):\n        return f2(input)\n\n@skippable(pop=['1to3'])\nclass Layer3(nn.Module):\n    def forward(self, input):\n        skip_1to3 = yield pop('1to3')\n        return f3(input) + skip_1to3\n\nmodel = nn.Sequential(Layer1(), Layer2(), Layer3())\n One skippable module can stash or pop multiple skip tensors: @skippable(stash=['alice', 'bob'], pop=['carol'])\nclass StashStashPop(nn.Module):\n    def forward(self, input):\n        yield stash('alice', f_alice(input))\n        yield stash('bob', f_bob(input))\n        carol = yield pop('carol')\n        return input + carol\n Every skip tensor must be associated with exactly one pair of stash and pop. Pipe checks this restriction automatically when wrapping a module. You can also check the restriction by verify_skippables() without Pipe. \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Pipeline Parallelism", "text": " \nclass torch.distributed.pipeline.sync.skip.skippable.stash(name, tensor) [source]\n \nThe command to stash a skip tensor. def forward(self, input):\n    yield stash('name', input)\n    return f(input)\n  Parameters \n \nname (str) \u2013 name of skip tensor \ninput (torch.Tensor or None) \u2013 tensor to pass to the skip connection    \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Pipeline Parallelism", "text": " \ntorch.distributed.pipeline.sync.skip.skippable.verify_skippables(module) [source]\n \nVerifies if the underlying skippable modules satisfy integrity. Every skip tensor must have only one pair of stash and pop. If there are one or more unmatched pairs, it will raise TypeError with the detailed messages. Here are a few failure cases. verify_skippables() will report failure for these cases: # Layer1 stashes \"1to3\".\n# Layer3 pops \"1to3\".\n\nnn.Sequential(Layer1(), Layer2())\n#               \u2514\u2500\u2500\u2500\u2500 ?\n\nnn.Sequential(Layer2(), Layer3())\n#                   ? \u2500\u2500\u2500\u2500\u2518\n\nnn.Sequential(Layer1(), Layer2(), Layer3(), Layer3())\n#               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       ^^^^^^\n\nnn.Sequential(Layer1(), Layer1(), Layer2(), Layer3())\n#             ^^^^^^      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n To use the same name for multiple skip tensors, they must be isolated by different namespaces. See isolate().  Raises \nTypeError \u2013 one or more pairs of stash and pop are not matched.   \n"}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "torch.distributed", "text": " \nclass torch.distributed.PrefixStore  \nA wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store.  Parameters \n \nprefix (str) \u2013 The prefix string that is prepended to each key before being inserted into the store. \nstore (torch.distributed.store) \u2013 A store object that forms the underlying key-value store.    \n"}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "torch.distributed", "text": " \ntorch.distributed.recv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nSender rank -1, if not part of the group   \n"}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "torch.distributed", "text": " \ntorch.distributed.reduce(tensor, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines. Only the process with rank dst is going to receive the final result.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "torch.distributed", "text": " \nclass torch.distributed.ReduceOp  \nAn enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR. Note that BAND, BOR, and BXOR reductions are not available when using the NCCL backend. Additionally, MAX, MIN and PRODUCT are not supported for complex tensors. The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc. Members: SUM PRODUCT MIN MAX BAND BOR BXOR \n"}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.reduce_multigpu(tensor_list, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False, dst_tensor=0) [source]\n \nReduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result. Only nccl backend is currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndst_tensor (int, optional) \u2013 Destination tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, otherwise   \n"}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "torch.distributed", "text": " \nclass torch.distributed.reduce_op  \nDeprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX. ReduceOp is recommended to use instead. \n"}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "torch.distributed", "text": " \ntorch.distributed.reduce_scatter(output, input_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces, then scatters a list of tensors to all processes in a group.  Parameters \n \noutput (Tensor) \u2013 Output tensor. \ninput_list (list[Tensor]) \u2013 List of tensors to reduce and scatter. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n"}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported. Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.  Parameters \n \noutput_tensor_list (List[Tensor]) \u2013 \nOutput tensors (on different GPUs) to receive the result of the operation. Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.  \ninput_tensor_lists (List[List[Tensor]]) \u2013 \nInput lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i]. Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j] Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n"}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.BackendType  \nAn enum class of available backends. PyTorch ships with two builtin backends: BackendType.TENSORPIPE and BackendType.PROCESS_GROUP. Additional ones can be registered using the register_backend() function. \n"}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.functions.async_execution(fn) [source]\n \nA decorator for a function indicating that the return value of the function is guaranteed to be a Future object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the Future returned by the wrapped function and installs subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. That also means the returned Future only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function\u2019s (fn) execution needs to pause and resume due to, e.g., containing rpc_async() or waiting for other signals.  Note To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a Future object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with @staticmethod or @classmethod, @rpc.functions.async_execution needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by @rpc.functions.async_execution.   Example::\n\nThe returned Future object can come from rpc_async(), then(), or Future constructor. The example below shows directly using the Future returned by then(). >>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @rpc.functions.async_execution\n>>> def async_add_chained(to, x, y, z):\n>>>     # This function runs on \"worker1\" and returns immediately when\n>>>     # the callback is installed through the `then(cb)` API. In the\n>>>     # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n>>>     # When the return value of that `rpc_async` arrives at\n>>>     # \"worker1\", \"worker1\" will run the lambda function accordingly\n>>>     # and set the value for the previously returned `Future`, which\n>>>     # will then trigger RPC to send the result back to \"worker0\".\n>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>         lambda fut: fut.wait() + z\n>>>     )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add_chained,\n>>>     args=(\"worker2\", torch.ones(2), 1, 1)\n>>> )\n>>> print(ret)  # prints tensor([3., 3.])\n When combined with TorchScript decorators, this decorator must be the outmost one. >>> from torch import Tensor\n>>> from torch.futures import Future\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @torch.jit.script\n>>> def script_add(x: Tensor, y: Tensor) -> Tensor:\n>>>     return x + y\n>>>\n>>> @rpc.functions.async_execution\n>>> @torch.jit.script\n>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:\n>>>     return rpc.rpc_async(to, script_add, (x, y))\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1)\n>>> )\n>>> print(ret)  # prints tensor([2., 2.])\n When combined with static or class method, this decorator must be the inner one. >>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> class AsyncExecutionClass:\n>>>\n>>>     @staticmethod\n>>>     @rpc.functions.async_execution\n>>>     def static_async_add(to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>>     @classmethod\n>>>     @rpc.functions.async_execution\n>>>     def class_async_add(cls, to, x, y, z):\n>>>         ret_fut = torch.futures.Future()\n>>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: ret_fut.set_result(fut.wait() + z)\n>>>         )\n>>>         return ret_fut\n>>>\n>>>     @rpc.functions.async_execution\n>>>     def bound_async_add(self, to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.static_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.class_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n This decorator also works with RRef helpers, i.e., . torch.distributed.rpc.RRef.rpc_sync(), torch.distributed.rpc.RRef.rpc_async(), and torch.distributed.rpc.RRef.remote(). >>> from torch.distributed import rpc\n>>>\n>>> # reuse the AsyncExecutionClass class above\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_sync().static_async_add(\"worker2\", torch.ones(2), 1, 2)\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_async().static_async_add(\"worker2\", torch.ones(2), 1, 2).wait()\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.remote().static_async_add(\"worker2\", torch.ones(2), 1, 2).to_here()\n>>> print(ret)  # prints tensor([4., 4.])\n   \n"}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.get_worker_info(worker_name=None) [source]\n \nGet WorkerInfo of a given worker name. Use this WorkerInfo to avoid passing an expensive string on every invocation.  Parameters \nworker_name (str) \u2013 the string name of a worker. If None, return the the id of the current worker. (default None)  Returns \nWorkerInfo instance for the given worker_name or WorkerInfo of the current worker if worker_name is None.   \n"}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None) [source]\n \nInitializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs.  Parameters \n \nname (str) \u2013 a globally unique name of this node. (e.g., Trainer3, ParameterServer2, Master, Worker1) Name can only contain number, alphabet, underscore, colon, and/or dash, and must be shorter than 128 characters. \nbackend (BackendType, optional) \u2013 The type of RPC backend implementation. Supported values include BackendType.TENSORPIPE (the default) and BackendType.PROCESS_GROUP. See Backends for more information. \nrank (int) \u2013 a globally unique id/rank of this node. \nworld_size (int) \u2013 The number of workers in the group. \nrpc_backend_options (RpcBackendOptions, optional) \u2013 The options passed to the RpcAgent constructor. It must be an agent-specific subclass of RpcBackendOptions and contains agent-specific initialization configurations. By default, for all agents, it sets the default timeout to 60 seconds and performs the rendezvous with an underlying process group initialized using init_method = \"env://\", meaning that environment variables MASTER_ADDR and MASTER_PORT need to be set properly. See Backends for more information and find which options are available.    \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.ProcessGroupRpcBackendOptions  \nThe backend options class for ProcessGroupAgent, which is derived from RpcBackendOptions.  Parameters \n \nnum_send_recv_threads (int, optional) \u2013 The number of threads in the thread-pool used by ProcessGroupAgent (default: 4). \nrpc_timeout (float, optional) \u2013 The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in rpc_sync() and rpc_async() if necessary. \ninit_method (str, optional) \u2013 The URL to initialize ProcessGroupGloo (default: env://).     \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n  \nproperty num_send_recv_threads  \nThe number of threads in the thread-pool used by ProcessGroupAgent. \n  \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": " \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads", "type": "Distributed RPC Framework", "text": " \nproperty num_send_recv_threads  \nThe number of threads in the thread-pool used by ProcessGroupAgent. \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": " \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n"}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0) [source]\n \nMake a remote call to run func on worker to and return an RRef to the result value immediately. Worker to will be the owner of the returned RRef, and the worker calling remote is a user. The owner manages the global reference count of its RRef, and the owner RRef is only destructed when globally there are no living references to it.  Parameters \n \nto (str or WorkerInfo or int) \u2013 name/rank/WorkerInfo of the destination worker. \nfunc (callable) \u2013 a callable function, such as Python callables, builtin operators (e.g. add()) and annotated TorchScript functions. \nargs (tuple) \u2013 the argument tuple for the func invocation. \nkwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation. \ntimeout (float, optional) \u2013 timeout in seconds for this remote call. If the creation of this RRef on worker to is not successfully processed on this worker within this timeout, then the next time there is an attempt to use the RRef (such as to_here()), a timeout will be raised indicating this failure. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with _set_rpc_timeout is used.   Returns \nA user RRef instance to the result value. Use the blocking API torch.distributed.rpc.RRef.to_here() to retrieve the result value locally.    Warning Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.   Warning The remote API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the torch.distributed.rpc.RRef.confirmed_by_owner() API.   Warning Errors such as timeouts for the remote API are handled on a best-effort basis. This means that when remote calls initiated by remote fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as to_here or fork call), then future uses of the RRef will appropriately raise errors. However, it is possible that the user application will use the RRef before the errors are handled. In this case, errors may not be raised as they have not yet been handled.   Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>> x = rref1.to_here() + rref2.to_here()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n Below is an example of running a TorchScript function using RPC. >>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(t1, t2):\n>>>    return torch.add(t1, t2)\n >>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref = rpc.remote(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rref.to_here()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.RpcBackendOptions  \nAn abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to init_rpc() in order to initialize RPC with specific configurations, such as the RPC timeout and init_method to be used.  \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n  \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n \n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": " \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": " \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n"}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0) [source]\n \nMake a non-blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a Future that can be awaited on.  Parameters \n \nto (str or WorkerInfo or int) \u2013 name/rank/WorkerInfo of the destination worker. \nfunc (callable) \u2013 a callable function, such as Python callables, builtin operators (e.g. add()) and annotated TorchScript functions. \nargs (tuple) \u2013 the argument tuple for the func invocation. \nkwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation. \ntimeout (float, optional) \u2013 timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with _set_rpc_timeout is used.   Returns \nReturns a Future object that can be waited on. When completed, the return value of func on args and kwargs can be retrieved from the Future object.    Warning Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.   Warning The rpc_async API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned Future completes.   Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n>>> result = fut1.wait() + fut2.wait()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n Below is an example of running a TorchScript function using RPC. >>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(t1, t2):\n>>>    return torch.add(t1, t2)\n >>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut = rpc.rpc_async(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> ret = fut.wait()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0) [source]\n \nMake a blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.  Parameters \n \nto (str or WorkerInfo or int) \u2013 name/rank/WorkerInfo of the destination worker. \nfunc (callable) \u2013 a callable function, such as Python callables, builtin operators (e.g. add()) and annotated TorchScript functions. \nargs (tuple) \u2013 the argument tuple for the func invocation. \nkwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation. \ntimeout (float, optional) \u2013 timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with _set_rpc_timeout is used.   Returns \nReturns the result of running func with args and kwargs.    Warning Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.   Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n Below is an example of running a TorchScript function using RPC. >>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(t1, t2):\n>>>    return torch.add(t1, t2)\n >>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.RRef", "path": "rpc#torch.distributed.rpc.RRef", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.RRef [source]\n \n \nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters \n \ndist_autograd_ctx_id (int, optional) \u2013 The distributed autograd context id for which we should retrieve the gradients (default: -1). \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n   \n  \nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. \n  \nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether or not the current node is the owner of this RRef. \n  \nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object  \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. \n  \nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo  \nReturns worker information of the node that owns this RRef. \n  \nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str  \nReturns worker name of the node that owns this RRef. \n  \nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.   \n \n"}, {"name": "torch.distributed.rpc.RRef.backward()", "path": "rpc#torch.distributed.rpc.RRef.backward", "type": "Distributed RPC Framework", "text": " \nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters \n \ndist_autograd_ctx_id (int, optional) \u2013 The distributed autograd context id for which we should retrieve the gradients (default: -1). \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n   \n"}, {"name": "torch.distributed.rpc.RRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.RRef.confirmed_by_owner", "type": "Distributed RPC Framework", "text": " \nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. \n"}, {"name": "torch.distributed.rpc.RRef.is_owner()", "path": "rpc#torch.distributed.rpc.RRef.is_owner", "type": "Distributed RPC Framework", "text": " \nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether or not the current node is the owner of this RRef. \n"}, {"name": "torch.distributed.rpc.RRef.local_value()", "path": "rpc#torch.distributed.rpc.RRef.local_value", "type": "Distributed RPC Framework", "text": " \nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object  \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. \n"}, {"name": "torch.distributed.rpc.RRef.owner()", "path": "rpc#torch.distributed.rpc.RRef.owner", "type": "Distributed RPC Framework", "text": " \nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo  \nReturns worker information of the node that owns this RRef. \n"}, {"name": "torch.distributed.rpc.RRef.owner_name()", "path": "rpc#torch.distributed.rpc.RRef.owner_name", "type": "Distributed RPC Framework", "text": " \nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str  \nReturns worker name of the node that owns this RRef. \n"}, {"name": "torch.distributed.rpc.RRef.remote()", "path": "rpc#torch.distributed.rpc.RRef.remote", "type": "Distributed RPC Framework", "text": " \nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n   \n"}, {"name": "torch.distributed.rpc.RRef.rpc_async()", "path": "rpc#torch.distributed.rpc.RRef.rpc_async", "type": "Distributed RPC Framework", "text": " \nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n   \n"}, {"name": "torch.distributed.rpc.RRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.RRef.rpc_sync", "type": "Distributed RPC Framework", "text": " \nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n   \n"}, {"name": "torch.distributed.rpc.RRef.to_here()", "path": "rpc#torch.distributed.rpc.RRef.to_here", "type": "Distributed RPC Framework", "text": " \nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.   \n"}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.shutdown(graceful=True) [source]\n \nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If graceful=True, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if graceful=False, this is a local shutdown, and it does not wait for other RPC processes to reach this method.  Warning For Future objects returned by rpc_async(), future.wait() should not be called after shutdown().   Parameters \ngraceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for UserRRefs and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.    Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> # do some work\n>>> result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n>>> # ready to shutdown\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> # wait for worker 0 to finish work, and then shutdown.\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.TensorPipeRpcBackendOptions(*, num_worker_threads=16, rpc_timeout=60.0, init_method='env://', device_maps=None, _transports=None, _channels=None) [source]\n \nThe backend options for TensorPipeAgent, derived from RpcBackendOptions.  Parameters \n \nnum_worker_threads (int, optional) \u2013 The number of threads in the thread-pool used by TensorPipeAgent to execute requests (default: 16). \nrpc_timeout (float, optional) \u2013 The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in rpc_sync() and rpc_async() if necessary. \ninit_method (str, optional) \u2013 The URL to initialize the distributed store used for rendezvous. It takes any value accepted for the same argument of init_process_group() (default: env://). \ndevice_maps (Dict[str, Dict]) \u2013 Device placement mappings from this worker to the callee. Key is the callee worker name and value the dictionary (Dict of int, str, or torch.device) that maps this worker\u2019s devices to the callee worker\u2019s devices. (default: None)     \nproperty device_maps  \nThe device map locations. \n  \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n  \nproperty num_worker_threads  \nThe number of threads in the thread-pool used by TensorPipeAgent to execute requests. \n  \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n  \nset_device_map(to, device_map) [source]\n \nSet device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.  Parameters \n \nworker_name (str) \u2013 Callee name. \ndevice_map (Dict of python:int, str, or torch.device) \u2013 Device placement mappings from this worker to the callee. This map must be invertible.     Example::\n\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0, 1}}\n>>>     # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1, 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:1')\n   \n \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC Framework", "text": " \nproperty device_maps  \nThe device map locations. \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": " \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC Framework", "text": " \nproperty num_worker_threads  \nThe number of threads in the thread-pool used by TensorPipeAgent to execute requests. \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": " \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC Framework", "text": " \nset_device_map(to, device_map) [source]\n \nSet device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.  Parameters \n \nworker_name (str) \u2013 Callee name. \ndevice_map (Dict of python:int, str, or torch.device) \u2013 Device placement mappings from this worker to the callee. This map must be invertible.     Example::\n\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0, 1}}\n>>>     # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1, 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:1')\n   \n"}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.WorkerInfo  \nA structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through get_worker_info() and the result can be passed in to functions such as rpc_sync(), rpc_async(), remote() to avoid copying a string on every invocation.  \nproperty id  \nGlobally unique id to identify the worker. \n  \nproperty name  \nThe name of the worker. \n \n"}, {"name": "torch.distributed.rpc.WorkerInfo.id()", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC Framework", "text": " \nproperty id  \nGlobally unique id to identify the worker. \n"}, {"name": "torch.distributed.rpc.WorkerInfo.name()", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC Framework", "text": " \nproperty name  \nThe name of the worker. \n"}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "torch.distributed", "text": " \ntorch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False) [source]\n \nScatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the tensor argument.  Parameters \n \ntensor (Tensor) \u2013 Output tensor. \nscatter_list (list[Tensor]) \u2013 List of tensors to scatter (default is None, must be specified on the source rank) \nsrc (int) \u2013 Source rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "torch.distributed", "text": " \ntorch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None) [source]\n \nScatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.  Parameters \n \nscatter_object_output_list (List[Any]) \u2013 Non-empty list whose first element will store the object scattered to this rank. \nscatter_object_input_list (List[Any]) \u2013 List of input objects to scatter. Each object must be picklable. Only objects on the src rank will be scattered, and the argument can be None for non-src ranks. \nsrc (int) \u2013 Source rank from which to scatter scatter_object_input_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.    Note Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.   Warning scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n   \n"}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "torch.distributed", "text": " \ntorch.distributed.send(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv    \n"}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "torch.distributed", "text": " \nclass torch.distributed.Store  \nBase class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore). \n"}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "torch.distributed", "text": " \ntorch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) \u2192 int  \nThe first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception.  Parameters \n \nkey (str) \u2013 The key in the store whose counter will be incremented. \namount (int) \u2013 The quantity by which the counter will be incremented.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "torch.distributed", "text": " \ntorch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bool  \nDeletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.  Warning The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.   Parameters \nkey (str) \u2013 The key to be deleted from the store  Returns \nTrue if key was deleted, otherwise False.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n   \n"}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "torch.distributed", "text": " \ntorch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bytes  \nRetrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.  Parameters \nkey (str) \u2013 The function will return the value associated with this key.  Returns \nValue associated with key if key is in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "torch.distributed", "text": " \ntorch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) \u2192 int  \nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.  Warning When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.   Returns \nThe number of keys present in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n   \n"}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "torch.distributed", "text": " \ntorch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) \u2192 None  \nInserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value.  Parameters \n \nkey (str) \u2013 The key to be added to the store. \nvalue (str) \u2013 The value associated with key to be added to the store.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "torch.distributed", "text": " \ntorch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) \u2192 None  \nSets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().  Parameters \ntimeout (timedelta) \u2013 timeout to be set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n   \n"}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "torch.distributed", "text": " \ntorch.distributed.Store.wait(*args, **kwargs)  \nOverloaded function.  wait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -> None  Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.  Parameters \nkeys (list) \u2013 List of keys on which to wait until they are set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n    wait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -> None  Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout.  Parameters \n \nkeys (list) \u2013 List of keys on which to wait until they are set in the store. \ntimeout (timedelta) \u2013 Time to wait for the keys to be added before throwing an exception.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n   \n"}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "torch.distributed", "text": " \nclass torch.distributed.TCPStore  \nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc.  Parameters \n \nhost_name (str) \u2013 The hostname or IP Address the server store should run on. \nport (int) \u2013 The port on which the server store should listen for incoming requests. \nworld_size (int) \u2013 The total number of store users (number of clients + 1 for the server). \nis_master (bool) \u2013 True when initializing the server store, False for client stores. \ntimeout (timedelta) \u2013 Timeout used by the store during initialization and for methods such as get() and wait().     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n   \n"}, {"name": "torch.distributions", "path": "distributions", "type": "torch.distributions", "text": "Probability distributions - torch.distributions The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package. It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples f(x)f(x) , the pathwise derivative requires the derivative f\u2032(x)f'(x) . The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs . Score function When the probability density function is differentiable with respect to its parameters, we only need sample() and log_prob() to implement REINFORCE:  \u0394\u03b8=\u03b1r\u2202log\u2061p(a\u2223\u03c0\u03b8(s))\u2202\u03b8\\Delta\\theta = \\alpha r \\frac{\\partial\\log p(a|\\pi^\\theta(s))}{\\partial\\theta} \nwhere \u03b8\\theta  are the parameters, \u03b1\\alpha  is the learning rate, rr  is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))  is the probability of taking action aa  in state ss  given policy \u03c0\u03b8\\pi^\\theta . In practice we would sample an action from the output of a network, apply this action in an environment, and then use log_prob to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows: probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n Pathwise derivative The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the rsample() method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows: params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n Distribution  \nclass torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: object Distribution is the abstract base class for probability distributions.  \nproperty arg_constraints  \nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. \n  \nproperty batch_shape  \nReturns the shape over which parameters are batched. \n  \ncdf(value) [source]\n \nReturns the cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nentropy() [source]\n \nReturns entropy of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nenumerate_support(expand=True) [source]\n \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters \nexpand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns \nTensor iterating over dimension 0.   \n  \nproperty event_shape  \nReturns the shape of a single sample (without batching). \n  \nexpand(batch_shape, _instance=None) [source]\n \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters \n \nbatch_shape (torch.Size) \u2013 the desired expanded size. \n_instance \u2013 new instance provided by subclasses that need to override .expand.   Returns \nNew distribution instance with batch dimensions expanded to batch_size.   \n  \nicdf(value) [source]\n \nReturns the inverse cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nlog_prob(value) [source]\n \nReturns the log of the probability density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nproperty mean  \nReturns the mean of the distribution. \n  \nperplexity() [source]\n \nReturns perplexity of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. \n  \nsample_n(n) [source]\n \nGenerates n samples or n batches of samples if the distribution parameters are batched. \n  \nstatic set_default_validate_args(value) [source]\n \nSets whether validation is enabled or disabled. The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.  Parameters \nvalue (bool) \u2013 Whether to enable validation.   \n  \nproperty stddev  \nReturns the standard deviation of the distribution. \n  \nproperty support  \nReturns a Constraint object representing this distribution\u2019s support. \n  \nproperty variance  \nReturns the variance of the distribution. \n \n ExponentialFamily  \nclass torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below  pF(x;\u03b8)=exp\u2061(\u27e8t(x),\u03b8\u27e9\u2212F(\u03b8)+k(x))p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x)) \nwhere \u03b8\\theta  denotes the natural parameters, t(x)t(x)  denotes the sufficient statistic, F(\u03b8)F(\\theta)  is the log normalizer function for a given family and k(x)k(x)  is the carrier measure.  Note This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).   \nentropy() [source]\n \nMethod to compute the entropy using Bregman divergence of the log normalizer. \n \n Bernoulli  \nclass torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Bernoulli distribution parameterized by probs or logits (but not both). Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p. Example: >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Boolean() \n  \nproperty variance \n \n Beta  \nclass torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Beta distribution parameterized by concentration1 and concentration0. Example: >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nproperty concentration0 \n  \nproperty concentration1 \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n Binomial  \nclass torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits. Example: >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n  Parameters \n \ntotal_count (int or Tensor) \u2013 number of Bernoulli trials \nprobs (Tensor) \u2013 Event probabilities \nlogits (Tensor) \u2013 Event log-odds     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} \n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Categorical  \nclass torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a categorical distribution parameterized by either probs or logits (but not both).  Note It is equivalent to the distribution that torch.multinomial() samples from.  Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}  where K is probs.size(-1). If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index. If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.multinomial() Example: >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Cauchy  \nclass torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution. Example: >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n  Parameters \n \nloc (float or Tensor) \u2013 mode or median of the distribution. \nscale (float or Tensor) \u2013 half width at half maximum.     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n Chi2  \nclass torch.distributions.chi2.Chi2(df, validate_args=None) [source]\n \nBases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n  Parameters \ndf (float or Tensor) \u2013 shape parameter of the distribution    \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n  \nproperty df \n  \nexpand(batch_shape, _instance=None) [source]\n\n \n ContinuousBernoulli  \nclass torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both). The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details. Example: >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n  Parameters \n \nprobs (Number, Tensor) \u2013 (0,1) valued parameters \nlogits (Number, Tensor) \u2013 real valued parameters whose sigmoid matches \u2018probs\u2019    [1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n Dirichlet  \nclass torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Dirichlet distribution parameterized by concentration concentration. Example: >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n  Parameters \nconcentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)    \narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Simplex() \n  \nproperty variance \n \n Exponential  \nclass torch.distributions.exponential.Exponential(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Exponential distribution parameterized by rate. Example: >>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n  Parameters \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n FisherSnedecor  \nclass torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Fisher-Snedecor distribution parameterized by df1 and df2. Example: >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n  Parameters \n \ndf1 (float or Tensor) \u2013 degrees of freedom parameter 1 \ndf2 (float or Tensor) \u2013 degrees of freedom parameter 2     \narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n Gamma  \nclass torch.distributions.gamma.Gamma(concentration, rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Gamma distribution parameterized by shape concentration and rate. Example: >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n  Parameters \n \nconcentration (float or Tensor) \u2013 shape parameter of the distribution (often referred to as alpha) \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution (often referred to as beta)     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n Geometric  \nclass torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1  Bernoulli trials, the first kk  trials failed, before seeing a success. Samples are non-negative integers [0, inf\u2061\\inf ). Example: >>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1. Must be in range (0, 1] \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1.     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n Gumbel  \nclass torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Gumbel Distribution. Examples: >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n  Parameters \n \nloc (float or Tensor) \u2013 Location parameter of the distribution \nscale (float or Tensor) \u2013 Scale parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n HalfCauchy  \nclass torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-Cauchy distribution parameterized by scale where: X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n Example: >>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Cauchy distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n HalfNormal  \nclass torch.distributions.half_normal.HalfNormal(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n Example: >>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Normal distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n Independent  \nclass torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Reinterprets some of the batch dims of a distribution as event dims. This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can: >>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n  Parameters \n \nbase_distribution (torch.distributions.distribution.Distribution) \u2013 a base distribution \nreinterpreted_batch_ndims (int) \u2013 the number of batch dims to reinterpret as event dims     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_enumerate_support \n  \nproperty has_rsample \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Kumaraswamy  \nclass torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Kumaraswamy distribution. Example: >>> m = Kumaraswamy(torch.Tensor([1.0]), torch.Tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty mean \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n LKJCholesky  \nclass torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta  to make the probability of the correlation matrix MM  generated from a Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3. L ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration) Example: >>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n  Parameters \n \ndimension (dim) \u2013 dimension of the matrices \nconcentration (float or Tensor) \u2013 concentration/shape parameter of the distribution (often referred to as eta)    References [1] Generating random correlation matrices based on vines and extended onion method, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.  \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = CorrCholesky() \n \n Laplace  \nclass torch.distributions.laplace.Laplace(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Laplace distribution parameterized by loc and scale. Example: >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n LogNormal  \nclass torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a log-normal distribution parameterized by loc and scale where: X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n Example: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of log of distribution \nscale (float or Tensor) \u2013 standard deviation of log of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty loc \n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n LowRankMultivariateNormal  \nclass torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n Example >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution with shape batch_shape + event_shape\n \ncov_factor (Tensor) \u2013 factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,)\n \ncov_diag (Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape\n     Note The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n   \narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nvariance [source]\n\n \n MixtureSameFamily  \nclass torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component. Examples: # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n  Parameters \n \nmixture_distribution \u2013 torch.distributions.Categorical-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the component_distribution. Must have either scalar batch_shape or batch_shape matching component_distribution.batch_shape[:-1]\n \ncomponent_distribution \u2013 torch.distributions.Distribution-like instance. Right-most batch dimension indexes component.     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(x) [source]\n\n  \nproperty component_distribution \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(x) [source]\n\n  \nproperty mean \n  \nproperty mixture_distribution \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Multinomial  \nclass torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches. Note that total_count need not be specified if only log_prob() is called (see example below)  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.   \nsample() requires a single shared total_count for all parameters and samples. \nlog_prob() allows different total_count for each parameter and sample.  Example: >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n  Parameters \n \ntotal_count (int) \u2013 number of trials \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \ntotal_count: int = None \n  \nproperty variance \n \n MultivariateNormal  \nclass torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix. The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}  or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}  or a lower-triangular matrix L\\mathbf{L}  with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance. Example >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution \ncovariance_matrix (Tensor) \u2013 positive-definite covariance matrix \nprecision_matrix (Tensor) \u2013 positive-definite precision matrix \nscale_tril (Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal     Note Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.   \narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nproperty variance \n \n NegativeBinomial  \nclass torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of failure of each Bernoulli trial is probs.  Parameters \n \ntotal_count (float or Tensor) \u2013 non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count \nprobs (Tensor) \u2013 Event probabilities of failure in the half open interval [0, 1) \nlogits (Tensor) \u2013 Event log-odds for probabilities of failure     \narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n Normal  \nclass torch.distributions.normal.Normal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a normal (also called Gaussian) distribution parameterized by loc and scale. Example: >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution (often referred to as mu) \nscale (float or Tensor) \u2013 standard deviation of the distribution (often referred to as sigma)     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n OneHotCategorical  \nclass torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a one-hot categorical distribution parameterized by probs or logits. Samples are one-hot coded vectors of size probs.size(-1).  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.distributions.Categorical() for specifications of probs and logits. Example: >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = OneHot() \n  \nproperty variance \n \n Pareto  \nclass torch.distributions.pareto.Pareto(scale, alpha, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Pareto Type 1 distribution. Example: >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of the distribution \nalpha (float or Tensor) \u2013 Shape parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nproperty support \n  \nproperty variance \n \n Poisson  \nclass torch.distributions.poisson.Poisson(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Poisson distribution parameterized by rate, the rate parameter. Samples are nonnegative integers, with a pmf given by  rateke\u2212ratek!\\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}  \nExample: >>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n  Parameters \nrate (Number, Tensor) \u2013 the rate parameter    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n RelaxedBernoulli  \nclass torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples. Example: >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty temperature \n \n LogitRelaxedBernoulli  \nclass torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution. Samples are logits of values in (0, 1). See [1] for more details.  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017) [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n \n RelaxedOneHotCategorical  \nclass torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable. Example: >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 unnormalized log probability for each event     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Simplex() \n  \nproperty temperature \n \n StudentT  \nclass torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n TransformedDistribution  \nclass torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied: X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events. An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical  \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(value) [source]\n \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_rsample \n  \nicdf(value) [source]\n \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. \n  \nlog_prob(value) [source]\n \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nproperty support \n \n Uniform  \nclass torch.distributions.uniform.Uniform(low, high, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Generates uniformly distributed random samples from the half-open interval [low, high). Example: >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n  Parameters \n \nlow (float or Tensor) \u2013 lower range (inclusive). \nhigh (float or Tensor) \u2013 upper range (exclusive).     \narg_constraints = {'high': Dependent(), 'low': Dependent()} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nproperty support \n  \nproperty variance \n \n VonMises  \nclass torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution A circular von Mises distribution. This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.  Example::\n\n>>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n    Parameters \n \nloc (torch.Tensor) \u2013 an angle in radians. \nconcentration (torch.Tensor) \u2013 concentration parameter     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()} \n  \nexpand(batch_shape) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(value) [source]\n\n  \nproperty mean  \nThe provided mean is the circular one. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157. \n  \nsupport = Real() \n  \nvariance [source]\n \nThe provided variance is the circular one. \n \n Weibull  \nclass torch.distributions.weibull.Weibull(scale, concentration, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a two-parameter Weibull distribution. Example >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of distribution (lambda). \nconcentration (float or Tensor) \u2013 Concentration parameter of distribution (k/shape).     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n KL Divergence  \ntorch.distributions.kl.kl_divergence(p, q) [source]\n \nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)  between two distributions.  KL(p\u2225q)=\u222bp(x)log\u2061p(x)q(x)dxKL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx \n Parameters \n \np (Distribution) \u2013 A Distribution object. \nq (Distribution) \u2013 A Distribution object.   Returns \nA batch of KL divergences of shape batch_shape.  Return type \nTensor  Raises \nNotImplementedError \u2013 If the distribution types have not been registered via register_kl().   \n  \ntorch.distributions.kl.register_kl(type_p, type_q) [source]\n \nDecorator to register a pairwise function with kl_divergence(). Usage: @register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation: @register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n  Parameters \n \ntype_p (type) \u2013 A subclass of Distribution. \ntype_q (type) \u2013 A subclass of Distribution.    \n Transforms  \nclass torch.distributions.transforms.Transform(cache_size=0) [source]\n \nAbstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution. Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching: y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n However the following will error when caching due to dependency reversal: y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().  Parameters \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.  Variables \n \n~Transform.domain (Constraint) \u2013 The constraint representing valid inputs to this transform. \n~Transform.codomain (Constraint) \u2013 The constraint representing valid outputs to this transform which are inputs to the inverse transform. \n~Transform.bijective (bool) \u2013 Whether this transform is bijective. A transform t is bijective iff t.inv(t(x)) == x and t(t.inv(y)) == y for every x in the domain and y in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). \n~Transform.sign (int or Tensor) \u2013 For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.     \nproperty inv  \nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t. \n  \nproperty sign  \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. \n  \nlog_abs_det_jacobian(x, y) [source]\n \nComputes the log det jacobian log |dy/dx| given input and output. \n  \nforward_shape(shape) [source]\n \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape. \n  \ninverse_shape(shape) [source]\n \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape. \n \n  \nclass torch.distributions.transforms.ComposeTransform(parts, cache_size=0) [source]\n \nComposes multiple transforms in a chain. The transforms being composed are responsible for caching.  Parameters \n \nparts (list of Transform) \u2013 A list of transforms to compose. \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.    \n  \nclass torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0) [source]\n \nWrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian().  Parameters \n \nbase_transform (Transform) \u2013 A base transform. \nreinterpreted_batch_ndims (int) \u2013 The number of extra rightmost dimensions to treat as dependent.    \n  \nclass torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0) [source]\n \nUnit Jacobian transform to reshape the rightmost part of a tensor. Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape().  Parameters \n \nin_shape (torch.Size) \u2013 The input event shape. \nout_shape (torch.Size) \u2013 The output event shape.    \n  \nclass torch.distributions.transforms.ExpTransform(cache_size=0) [source]\n \nTransform via the mapping y=exp\u2061(x)y = \\exp(x) . \n  \nclass torch.distributions.transforms.PowerTransform(exponent, cache_size=0) [source]\n \nTransform via the mapping y=xexponenty = x^{\\text{exponent}} . \n  \nclass torch.distributions.transforms.SigmoidTransform(cache_size=0) [source]\n \nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}  and x=logit(y)x = \\text{logit}(y) . \n  \nclass torch.distributions.transforms.TanhTransform(cache_size=0) [source]\n \nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) . It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead. Note that one should use cache_size=1 when it comes to NaN/Inf values. \n  \nclass torch.distributions.transforms.AbsTransform(cache_size=0) [source]\n \nTransform via the mapping y=\u2223x\u2223y = |x| . \n  \nclass torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0) [source]\n \nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x .  Parameters \n \nloc (Tensor or float) \u2013 Location parameter. \nscale (Tensor or float) \u2013 Scale parameter. \nevent_dim (int) \u2013 Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.    \n  \nclass torch.distributions.transforms.CorrCholeskyTransform(cache_size=0) [source]\n \nTransforms an uncontrained real vector xx  with length D\u2217(D\u22121)/2D*(D-1)/2  into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:  First we convert x into a lower triangular matrix in row order. For each row XiX_i  of the lower triangular part, we apply a signed version of class StickBreakingTransform to transform XiX_i  into a unit Euclidean length vector using the following steps: - Scales into the interval (\u22121,1)(-1, 1)  domain: ri=tanh\u2061(Xi)r_i = \\tanh(X_i) . - Transforms into an unsigned domain: zi=ri2z_i = r_i^2 . - Applies si=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i) . - Transforms back into signed domain: yi=sign(ri)\u2217siy_i = sign(r_i) * \\sqrt{s_i} .  \n  \nclass torch.distributions.transforms.SoftmaxTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)  then normalizing. This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. \n  \nclass torch.distributions.transforms.StickBreakingTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex of one additional dimension via a stick-breaking process. This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses. This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. \n  \nclass torch.distributions.transforms.LowerCholeskyTransform(cache_size=0) [source]\n \nTransform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries. This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. \n  \nclass torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0) [source]\n \nTransform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().  Example::\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)   \n Constraints The following constraints are implemented:  constraints.boolean constraints.cat constraints.corr_cholesky constraints.dependent constraints.greater_than(lower_bound) constraints.greater_than_eq(lower_bound) constraints.independent(constraint, reinterpreted_batch_ndims) constraints.integer_interval(lower_bound, upper_bound) constraints.interval(lower_bound, upper_bound) constraints.less_than(upper_bound) constraints.lower_cholesky constraints.lower_triangular constraints.multinomial constraints.nonnegative_integer constraints.one_hot constraints.positive_definite constraints.positive_integer constraints.positive constraints.real_vector constraints.real constraints.simplex constraints.stack constraints.unit_interval   \nclass torch.distributions.constraints.Constraint [source]\n \nAbstract base class for constraints. A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.  Variables \n \n~Constraint.is_discrete (bool) \u2013 Whether constrained space is discrete. Defaults to False. \n~Constraint.event_dim (int) \u2013 Number of rightmost dimensions that together define an event. The check() method will remove this many dimensions when computing validity.     \ncheck(value) [source]\n \nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. \n \n  \ntorch.distributions.constraints.dependent_property  \nalias of torch.distributions.constraints._DependentProperty \n  \ntorch.distributions.constraints.independent  \nalias of torch.distributions.constraints._IndependentConstraint \n  \ntorch.distributions.constraints.integer_interval  \nalias of torch.distributions.constraints._IntegerInterval \n  \ntorch.distributions.constraints.greater_than  \nalias of torch.distributions.constraints._GreaterThan \n  \ntorch.distributions.constraints.greater_than_eq  \nalias of torch.distributions.constraints._GreaterThanEq \n  \ntorch.distributions.constraints.less_than  \nalias of torch.distributions.constraints._LessThan \n  \ntorch.distributions.constraints.multinomial  \nalias of torch.distributions.constraints._Multinomial \n  \ntorch.distributions.constraints.interval  \nalias of torch.distributions.constraints._Interval \n  \ntorch.distributions.constraints.half_open_interval  \nalias of torch.distributions.constraints._HalfOpenInterval \n  \ntorch.distributions.constraints.cat  \nalias of torch.distributions.constraints._Cat \n  \ntorch.distributions.constraints.stack  \nalias of torch.distributions.constraints._Stack \n Constraint Registry PyTorch provides two global ConstraintRegistry objects that link Constraint objects to Transform objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.  \nbiject_to(constraint) looks up a bijective Transform from constraints.real to the given constraint. The returned transform is guaranteed to have .bijective = True and should implement .log_abs_det_jacobian(). \ntransform_to(constraint) looks up a not-necessarily bijective Transform from constraints.real to the given constraint. The returned transform is not guaranteed to implement .log_abs_det_jacobian().  The transform_to() registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution\u2019s .arg_constraints dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam: loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n The biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant.: dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n  Note An example where transform_to and biject_to differ is constraints.simplex: transform_to(constraints.simplex) returns a SoftmaxTransform that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, biject_to(constraints.simplex) returns a StickBreakingTransform that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.  The biject_to and transform_to objects can be extended by user-defined constraints and transforms using their .register() method either as a function on singleton constraints: transform_to.register(my_constraint, my_transform)\n or as a decorator on parameterized constraints: @transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n You can create your own registry by creating a new ConstraintRegistry object.  \nclass torch.distributions.constraint_registry.ConstraintRegistry [source]\n \nRegistry to link constraints to transforms.  \nregister(constraint, factory=None) [source]\n \nRegisters a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n  Parameters \n \nconstraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. \nfactory (callable) \u2013 A callable that inputs a constraint object and returns a Transform object.    \n \n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Bernoulli distribution parameterized by probs or logits (but not both). Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p. Example: >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Boolean() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.mean()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "torch.distributions", "text": " \nsupport = Boolean() \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.variance()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "torch.distributions", "text": " \nclass torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Beta distribution parameterized by concentration1 and concentration0. Example: >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nproperty concentration0 \n  \nproperty concentration1 \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.beta.Beta.concentration0()", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "torch.distributions", "text": " \nproperty concentration0 \n"}, {"name": "torch.distributions.beta.Beta.concentration1()", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "torch.distributions", "text": " \nproperty concentration1 \n"}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.beta.Beta.mean()", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=()) [source]\n\n"}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.beta.Beta.variance()", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "torch.distributions", "text": " \nclass torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits. Example: >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n  Parameters \n \ntotal_count (int or Tensor) \u2013 number of Bernoulli trials \nprobs (Tensor) \u2013 Event probabilities \nlogits (Tensor) \u2013 Event log-odds     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} \n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} \n"}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.mean()", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.binomial.Binomial.param_shape()", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.support()", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.binomial.Binomial.variance()", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "torch.distributions", "text": " \nclass torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a categorical distribution parameterized by either probs or logits (but not both).  Note It is equivalent to the distribution that torch.multinomial() samples from.  Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}  where K is probs.size(-1). If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index. If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.multinomial() Example: >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.mean()", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.categorical.Categorical.param_shape()", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.support()", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.categorical.Categorical.variance()", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "torch.distributions", "text": " \nclass torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution. Example: >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n  Parameters \n \nloc (float or Tensor) \u2013 mode or median of the distribution. \nscale (float or Tensor) \u2013 half width at half maximum.     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.mean()", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.cauchy.Cauchy.variance()", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "torch.distributions", "text": " \nclass torch.distributions.chi2.Chi2(df, validate_args=None) [source]\n \nBases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n  Parameters \ndf (float or Tensor) \u2013 shape parameter of the distribution    \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n  \nproperty df \n  \nexpand(batch_shape, _instance=None) [source]\n\n \n"}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.chi2.Chi2.df()", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "torch.distributions", "text": " \nproperty df \n"}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.cat  \nalias of torch.distributions.constraints._Cat \n"}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "torch.distributions", "text": " \nclass torch.distributions.constraints.Constraint [source]\n \nAbstract base class for constraints. A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.  Variables \n \n~Constraint.is_discrete (bool) \u2013 Whether constrained space is discrete. Defaults to False. \n~Constraint.event_dim (int) \u2013 Number of rightmost dimensions that together define an event. The check() method will remove this many dimensions when computing validity.     \ncheck(value) [source]\n \nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. \n \n"}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "torch.distributions", "text": " \ncheck(value) [source]\n \nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. \n"}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.dependent_property  \nalias of torch.distributions.constraints._DependentProperty \n"}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.greater_than  \nalias of torch.distributions.constraints._GreaterThan \n"}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.greater_than_eq  \nalias of torch.distributions.constraints._GreaterThanEq \n"}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.half_open_interval  \nalias of torch.distributions.constraints._HalfOpenInterval \n"}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.independent  \nalias of torch.distributions.constraints._IndependentConstraint \n"}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.integer_interval  \nalias of torch.distributions.constraints._IntegerInterval \n"}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.interval  \nalias of torch.distributions.constraints._Interval \n"}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.less_than  \nalias of torch.distributions.constraints._LessThan \n"}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.multinomial  \nalias of torch.distributions.constraints._Multinomial \n"}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.stack  \nalias of torch.distributions.constraints._Stack \n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "torch.distributions", "text": " \nclass torch.distributions.constraint_registry.ConstraintRegistry [source]\n \nRegistry to link constraints to transforms.  \nregister(constraint, factory=None) [source]\n \nRegisters a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n  Parameters \n \nconstraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. \nfactory (callable) \u2013 A callable that inputs a constraint object and returns a Transform object.    \n \n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "torch.distributions", "text": " \nregister(constraint, factory=None) [source]\n \nRegisters a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n  Parameters \n \nconstraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. \nfactory (callable) \u2013 A callable that inputs a constraint object and returns a Transform object.    \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both). The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details. Example: >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n  Parameters \n \nprobs (Number, Tensor) \u2013 (0,1) valued parameters \nlogits (Number, Tensor) \u2013 real valued parameters whose sigmoid matches \u2018probs\u2019    [1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "torch.distributions", "text": " \nclass torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Dirichlet distribution parameterized by concentration concentration. Example: >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n  Parameters \nconcentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)    \narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Simplex() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)} \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.mean()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=()) [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "torch.distributions", "text": " \nsupport = Simplex() \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.variance()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "torch.distributions", "text": " \nclass torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: object Distribution is the abstract base class for probability distributions.  \nproperty arg_constraints  \nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. \n  \nproperty batch_shape  \nReturns the shape over which parameters are batched. \n  \ncdf(value) [source]\n \nReturns the cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nentropy() [source]\n \nReturns entropy of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nenumerate_support(expand=True) [source]\n \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters \nexpand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns \nTensor iterating over dimension 0.   \n  \nproperty event_shape  \nReturns the shape of a single sample (without batching). \n  \nexpand(batch_shape, _instance=None) [source]\n \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters \n \nbatch_shape (torch.Size) \u2013 the desired expanded size. \n_instance \u2013 new instance provided by subclasses that need to override .expand.   Returns \nNew distribution instance with batch dimensions expanded to batch_size.   \n  \nicdf(value) [source]\n \nReturns the inverse cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nlog_prob(value) [source]\n \nReturns the log of the probability density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nproperty mean  \nReturns the mean of the distribution. \n  \nperplexity() [source]\n \nReturns perplexity of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. \n  \nsample_n(n) [source]\n \nGenerates n samples or n batches of samples if the distribution parameters are batched. \n  \nstatic set_default_validate_args(value) [source]\n \nSets whether validation is enabled or disabled. The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.  Parameters \nvalue (bool) \u2013 Whether to enable validation.   \n  \nproperty stddev  \nReturns the standard deviation of the distribution. \n  \nproperty support  \nReturns a Constraint object representing this distribution\u2019s support. \n  \nproperty variance  \nReturns the variance of the distribution. \n \n"}, {"name": "torch.distributions.distribution.Distribution.arg_constraints()", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "torch.distributions", "text": " \nproperty arg_constraints  \nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. \n"}, {"name": "torch.distributions.distribution.Distribution.batch_shape()", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "torch.distributions", "text": " \nproperty batch_shape  \nReturns the shape over which parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n \nReturns the cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n"}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n \nReturns entropy of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n"}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters \nexpand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns \nTensor iterating over dimension 0.   \n"}, {"name": "torch.distributions.distribution.Distribution.event_shape()", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "torch.distributions", "text": " \nproperty event_shape  \nReturns the shape of a single sample (without batching). \n"}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters \n \nbatch_shape (torch.Size) \u2013 the desired expanded size. \n_instance \u2013 new instance provided by subclasses that need to override .expand.   Returns \nNew distribution instance with batch dimensions expanded to batch_size.   \n"}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n \nReturns the inverse cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n"}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n \nReturns the log of the probability density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n"}, {"name": "torch.distributions.distribution.Distribution.mean()", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "torch.distributions", "text": " \nproperty mean  \nReturns the mean of the distribution. \n"}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "torch.distributions", "text": " \nperplexity() [source]\n \nReturns perplexity of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n"}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "torch.distributions", "text": " \nsample_n(n) [source]\n \nGenerates n samples or n batches of samples if the distribution parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "torch.distributions", "text": " \nstatic set_default_validate_args(value) [source]\n \nSets whether validation is enabled or disabled. The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.  Parameters \nvalue (bool) \u2013 Whether to enable validation.   \n"}, {"name": "torch.distributions.distribution.Distribution.stddev()", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "torch.distributions", "text": " \nproperty stddev  \nReturns the standard deviation of the distribution. \n"}, {"name": "torch.distributions.distribution.Distribution.support()", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "torch.distributions", "text": " \nproperty support  \nReturns a Constraint object representing this distribution\u2019s support. \n"}, {"name": "torch.distributions.distribution.Distribution.variance()", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "torch.distributions", "text": " \nproperty variance  \nReturns the variance of the distribution. \n"}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "torch.distributions", "text": " \nclass torch.distributions.exponential.Exponential(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Exponential distribution parameterized by rate. Example: >>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n  Parameters \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.mean()", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.stddev()", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.exponential.Exponential.variance()", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "torch.distributions", "text": " \nclass torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below  pF(x;\u03b8)=exp\u2061(\u27e8t(x),\u03b8\u27e9\u2212F(\u03b8)+k(x))p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x)) \nwhere \u03b8\\theta  denotes the natural parameters, t(x)t(x)  denotes the sufficient statistic, F(\u03b8)F(\\theta)  is the log normalizer function for a given family and k(x)k(x)  is the carrier measure.  Note This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).   \nentropy() [source]\n \nMethod to compute the entropy using Bregman divergence of the log normalizer. \n \n"}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n \nMethod to compute the entropy using Bregman divergence of the log normalizer. \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "torch.distributions", "text": " \nclass torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Fisher-Snedecor distribution parameterized by df1 and df2. Example: >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n  Parameters \n \ndf1 (float or Tensor) \u2013 degrees of freedom parameter 1 \ndf2 (float or Tensor) \u2013 degrees of freedom parameter 2     \narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "torch.distributions", "text": " \nclass torch.distributions.gamma.Gamma(concentration, rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Gamma distribution parameterized by shape concentration and rate. Example: >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n  Parameters \n \nconcentration (float or Tensor) \u2013 shape parameter of the distribution (often referred to as alpha) \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution (often referred to as beta)     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.mean()", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.gamma.Gamma.variance()", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "torch.distributions", "text": " \nclass torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1  Bernoulli trials, the first kk  trials failed, before seeing a success. Samples are non-negative integers [0, inf\u2061\\inf ). Example: >>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1. Must be in range (0, 1] \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1.     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.mean()", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "torch.distributions", "text": " \nsupport = IntegerGreaterThan(lower_bound=0) \n"}, {"name": "torch.distributions.geometric.Geometric.variance()", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "torch.distributions", "text": " \nclass torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Gumbel Distribution. Examples: >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n  Parameters \n \nloc (float or Tensor) \u2013 Location parameter of the distribution \nscale (float or Tensor) \u2013 Scale parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.mean()", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.gumbel.Gumbel.stddev()", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.gumbel.Gumbel.variance()", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "torch.distributions", "text": " \nclass torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-Cauchy distribution parameterized by scale where: X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n Example: >>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Cauchy distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "torch.distributions", "text": " \nicdf(prob) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "torch.distributions", "text": " \nproperty scale \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.half_normal.HalfNormal(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n Example: >>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Normal distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "torch.distributions", "text": " \nicdf(prob) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.mean()", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.half_normal.HalfNormal.scale()", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "torch.distributions", "text": " \nproperty scale \n"}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.half_normal.HalfNormal.variance()", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "torch.distributions", "text": " \nclass torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Reinterprets some of the batch dims of a distribution as event dims. This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can: >>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n  Parameters \n \nbase_distribution (torch.distributions.distribution.Distribution) \u2013 a base distribution \nreinterpreted_batch_ndims (int) \u2013 the number of batch dims to reinterpret as event dims     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_enumerate_support \n  \nproperty has_rsample \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n"}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.has_enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "torch.distributions", "text": " \nproperty has_enumerate_support \n"}, {"name": "torch.distributions.independent.Independent.has_rsample()", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "torch.distributions", "text": " \nproperty has_rsample \n"}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.mean()", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.support()", "path": "distributions#torch.distributions.independent.Independent.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.independent.Independent.variance()", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "torch.distributions", "text": " \ntorch.distributions.kl.kl_divergence(p, q) [source]\n \nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)  between two distributions.  KL(p\u2225q)=\u222bp(x)log\u2061p(x)q(x)dxKL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx \n Parameters \n \np (Distribution) \u2013 A Distribution object. \nq (Distribution) \u2013 A Distribution object.   Returns \nA batch of KL divergences of shape batch_shape.  Return type \nTensor  Raises \nNotImplementedError \u2013 If the distribution types have not been registered via register_kl().   \n"}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "torch.distributions", "text": " \ntorch.distributions.kl.register_kl(type_p, type_q) [source]\n \nDecorator to register a pairwise function with kl_divergence(). Usage: @register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation: @register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n  Parameters \n \ntype_p (type) \u2013 A subclass of Distribution. \ntype_q (type) \u2013 A subclass of Distribution.    \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "torch.distributions", "text": " \nclass torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Kumaraswamy distribution. Example: >>> m = Kumaraswamy(torch.Tensor([1.0]), torch.Tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty mean \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "torch.distributions", "text": " \nclass torch.distributions.laplace.Laplace(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Laplace distribution parameterized by loc and scale. Example: >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.mean()", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.stddev()", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.laplace.Laplace.variance()", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "torch.distributions", "text": " \nclass torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta  to make the probability of the correlation matrix MM  generated from a Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3. L ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration) Example: >>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n  Parameters \n \ndimension (dim) \u2013 dimension of the matrices \nconcentration (float or Tensor) \u2013 concentration/shape parameter of the distribution (often referred to as eta)    References [1] Generating random correlation matrices based on vines and extended onion method, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.  \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = CorrCholesky() \n \n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "torch.distributions", "text": " \nsupport = CorrCholesky() \n"}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a log-normal distribution parameterized by loc and scale where: X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n Example: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of log of distribution \nscale (float or Tensor) \u2013 standard deviation of log of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty loc \n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.log_normal.LogNormal.loc()", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "torch.distributions", "text": " \nproperty loc \n"}, {"name": "torch.distributions.log_normal.LogNormal.mean()", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.log_normal.LogNormal.scale()", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "torch.distributions", "text": " \nproperty scale \n"}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.log_normal.LogNormal.variance()", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n Example >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution with shape batch_shape + event_shape\n \ncov_factor (Tensor) \u2013 factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,)\n \ncov_diag (Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape\n     Note The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n   \narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nvariance [source]\n\n \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)} \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": " \ncovariance_matrix [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "torch.distributions", "text": " \nprecision_matrix [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "torch.distributions", "text": " \nscale_tril [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "torch.distributions", "text": " \nsupport = IndependentConstraint(Real(), 1) \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "torch.distributions", "text": " \nvariance [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torch.distributions", "text": " \nclass torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component. Examples: # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n  Parameters \n \nmixture_distribution \u2013 torch.distributions.Categorical-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the component_distribution. Must have either scalar batch_shape or batch_shape matching component_distribution.batch_shape[:-1]\n \ncomponent_distribution \u2013 torch.distributions.Distribution-like instance. Right-most batch dimension indexes component.     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(x) [source]\n\n  \nproperty component_distribution \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(x) [source]\n\n  \nproperty mean \n  \nproperty mixture_distribution \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "torch.distributions", "text": " \ncdf(x) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "torch.distributions", "text": " \nproperty component_distribution \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = False \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "torch.distributions", "text": " \nlog_prob(x) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "torch.distributions", "text": " \nproperty mixture_distribution \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "torch.distributions", "text": " \nclass torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches. Note that total_count need not be specified if only log_prob() is called (see example below)  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.   \nsample() requires a single shared total_count for all parameters and samples. \nlog_prob() allows different total_count for each parameter and sample.  Example: >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n  Parameters \n \ntotal_count (int) \u2013 number of trials \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \ntotal_count: int = None \n  \nproperty variance \n \n"}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.logits()", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.mean()", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.multinomial.Multinomial.param_shape()", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.multinomial.Multinomial.probs()", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.support()", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "torch.distributions", "text": " \ntotal_count: int = None \n"}, {"name": "torch.distributions.multinomial.Multinomial.variance()", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix. The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}  or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}  or a lower-triangular matrix L\\mathbf{L}  with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance. Example >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution \ncovariance_matrix (Tensor) \u2013 positive-definite covariance matrix \nprecision_matrix (Tensor) \u2013 positive-definite precision matrix \nscale_tril (Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal     Note Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.   \narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": " \ncovariance_matrix [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "torch.distributions", "text": " \nprecision_matrix [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "torch.distributions", "text": " \nscale_tril [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "torch.distributions", "text": " \nsupport = IndependentConstraint(Real(), 1) \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "torch.distributions", "text": " \nclass torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of failure of each Bernoulli trial is probs.  Parameters \n \ntotal_count (float or Tensor) \u2013 non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count \nprobs (Tensor) \u2013 Event probabilities of failure in the half open interval [0, 1) \nlogits (Tensor) \u2013 Event log-odds for probabilities of failure     \narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "torch.distributions", "text": " \nsupport = IntegerGreaterThan(lower_bound=0) \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "torch.distributions", "text": " \nclass torch.distributions.normal.Normal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a normal (also called Gaussian) distribution parameterized by loc and scale. Example: >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution (often referred to as mu) \nscale (float or Tensor) \u2013 standard deviation of the distribution (often referred to as sigma)     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.mean()", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.stddev()", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.normal.Normal.variance()", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torch.distributions", "text": " \nclass torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a one-hot categorical distribution parameterized by probs or logits. Samples are one-hot coded vectors of size probs.size(-1).  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.distributions.Categorical() for specifications of probs and logits. Example: >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = OneHot() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "torch.distributions", "text": " \nsupport = OneHot() \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "torch.distributions", "text": " \nclass torch.distributions.pareto.Pareto(scale, alpha, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Pareto Type 1 distribution. Example: >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of the distribution \nalpha (float or Tensor) \u2013 Shape parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.pareto.Pareto.mean()", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.pareto.Pareto.support()", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.pareto.Pareto.variance()", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "torch.distributions", "text": " \nclass torch.distributions.poisson.Poisson(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Poisson distribution parameterized by rate, the rate parameter. Samples are nonnegative integers, with a pmf given by  rateke\u2212ratek!\\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}  \nExample: >>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n  Parameters \nrate (Number, Tensor) \u2013 the rate parameter    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.poisson.Poisson.mean()", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "torch.distributions", "text": " \nsupport = IntegerGreaterThan(lower_bound=0) \n"}, {"name": "torch.distributions.poisson.Poisson.variance()", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution. Samples are logits of values in (0, 1). See [1] for more details.  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017) [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples. Example: >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty temperature \n \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "torch.distributions", "text": " \nproperty temperature \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torch.distributions", "text": " \nclass torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable. Example: >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 unnormalized log probability for each event     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Simplex() \n  \nproperty temperature \n \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "torch.distributions", "text": " \nsupport = Simplex() \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "torch.distributions", "text": " \nproperty temperature \n"}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "torch.distributions", "text": " \nclass torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.mean()", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.studentT.StudentT.variance()", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "torch.distributions", "text": " \nclass torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied: X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events. An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical  \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(value) [source]\n \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_rsample \n  \nicdf(value) [source]\n \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. \n  \nlog_prob(value) [source]\n \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nproperty support \n \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "torch.distributions", "text": " \nproperty has_rsample \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.AbsTransform(cache_size=0) [source]\n \nTransform via the mapping y=\u2223x\u2223y = |x| . \n"}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0) [source]\n \nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x .  Parameters \n \nloc (Tensor or float) \u2013 Location parameter. \nscale (Tensor or float) \u2013 Scale parameter. \nevent_dim (int) \u2013 Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.    \n"}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.ComposeTransform(parts, cache_size=0) [source]\n \nComposes multiple transforms in a chain. The transforms being composed are responsible for caching.  Parameters \n \nparts (list of Transform) \u2013 A list of transforms to compose. \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.    \n"}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.CorrCholeskyTransform(cache_size=0) [source]\n \nTransforms an uncontrained real vector xx  with length D\u2217(D\u22121)/2D*(D-1)/2  into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:  First we convert x into a lower triangular matrix in row order. For each row XiX_i  of the lower triangular part, we apply a signed version of class StickBreakingTransform to transform XiX_i  into a unit Euclidean length vector using the following steps: - Scales into the interval (\u22121,1)(-1, 1)  domain: ri=tanh\u2061(Xi)r_i = \\tanh(X_i) . - Transforms into an unsigned domain: zi=ri2z_i = r_i^2 . - Applies si=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i) . - Transforms back into signed domain: yi=sign(ri)\u2217siy_i = sign(r_i) * \\sqrt{s_i} .  \n"}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.ExpTransform(cache_size=0) [source]\n \nTransform via the mapping y=exp\u2061(x)y = \\exp(x) . \n"}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0) [source]\n \nWrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian().  Parameters \n \nbase_transform (Transform) \u2013 A base transform. \nreinterpreted_batch_ndims (int) \u2013 The number of extra rightmost dimensions to treat as dependent.    \n"}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.LowerCholeskyTransform(cache_size=0) [source]\n \nTransform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries. This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. \n"}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.PowerTransform(exponent, cache_size=0) [source]\n \nTransform via the mapping y=xexponenty = x^{\\text{exponent}} . \n"}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0) [source]\n \nUnit Jacobian transform to reshape the rightmost part of a tensor. Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape().  Parameters \n \nin_shape (torch.Size) \u2013 The input event shape. \nout_shape (torch.Size) \u2013 The output event shape.    \n"}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.SigmoidTransform(cache_size=0) [source]\n \nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}  and x=logit(y)x = \\text{logit}(y) . \n"}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.SoftmaxTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)  then normalizing. This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. \n"}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0) [source]\n \nTransform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().  Example::\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)   \n"}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.StickBreakingTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex of one additional dimension via a stick-breaking process. This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses. This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. \n"}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.TanhTransform(cache_size=0) [source]\n \nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) . It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead. Note that one should use cache_size=1 when it comes to NaN/Inf values. \n"}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.Transform(cache_size=0) [source]\n \nAbstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution. Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching: y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n However the following will error when caching due to dependency reversal: y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().  Parameters \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.  Variables \n \n~Transform.domain (Constraint) \u2013 The constraint representing valid inputs to this transform. \n~Transform.codomain (Constraint) \u2013 The constraint representing valid outputs to this transform which are inputs to the inverse transform. \n~Transform.bijective (bool) \u2013 Whether this transform is bijective. A transform t is bijective iff t.inv(t(x)) == x and t(t.inv(y)) == y for every x in the domain and y in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). \n~Transform.sign (int or Tensor) \u2013 For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.     \nproperty inv  \nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t. \n  \nproperty sign  \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. \n  \nlog_abs_det_jacobian(x, y) [source]\n \nComputes the log det jacobian log |dy/dx| given input and output. \n  \nforward_shape(shape) [source]\n \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape. \n  \ninverse_shape(shape) [source]\n \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape. \n \n"}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "torch.distributions", "text": " \nforward_shape(shape) [source]\n \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape. \n"}, {"name": "torch.distributions.transforms.Transform.inv()", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "torch.distributions", "text": " \nproperty inv  \nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t. \n"}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "torch.distributions", "text": " \ninverse_shape(shape) [source]\n \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape. \n"}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "torch.distributions", "text": " \nlog_abs_det_jacobian(x, y) [source]\n \nComputes the log det jacobian log |dy/dx| given input and output. \n"}, {"name": "torch.distributions.transforms.Transform.sign()", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "torch.distributions", "text": " \nproperty sign  \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. \n"}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "torch.distributions", "text": " \nclass torch.distributions.uniform.Uniform(low, high, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Generates uniformly distributed random samples from the half-open interval [low, high). Example: >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n  Parameters \n \nlow (float or Tensor) \u2013 lower range (inclusive). \nhigh (float or Tensor) \u2013 upper range (exclusive).     \narg_constraints = {'high': Dependent(), 'low': Dependent()} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'high': Dependent(), 'low': Dependent()} \n"}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.mean()", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.stddev()", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.uniform.Uniform.support()", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.uniform.Uniform.variance()", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "torch.distributions", "text": " \nclass torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution A circular von Mises distribution. This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.  Example::\n\n>>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n    Parameters \n \nloc (torch.Tensor) \u2013 an angle in radians. \nconcentration (torch.Tensor) \u2013 concentration parameter     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()} \n  \nexpand(batch_shape) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(value) [source]\n\n  \nproperty mean  \nThe provided mean is the circular one. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157. \n  \nsupport = Real() \n  \nvariance [source]\n \nThe provided variance is the circular one. \n \n"}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()} \n"}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape) [source]\n\n"}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = False \n"}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.von_mises.VonMises.mean()", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "torch.distributions", "text": " \nproperty mean  \nThe provided mean is the circular one. \n"}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157. \n"}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "torch.distributions", "text": " \nvariance [source]\n \nThe provided variance is the circular one. \n"}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "torch.distributions", "text": " \nclass torch.distributions.weibull.Weibull(scale, concentration, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a two-parameter Weibull distribution. Example >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of distribution (lambda). \nconcentration (float or Tensor) \u2013 Concentration parameter of distribution (k/shape).     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.weibull.Weibull.mean()", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.weibull.Weibull.variance()", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.div()", "path": "generated/torch.div#torch.div", "type": "torch", "text": " \ntorch.div(input, other, *, rounding_mode=None, out=None) \u2192 Tensor  \nDivides each element of the input input by the corresponding element of other.  outi=inputiotheri\\text{out}_i = \\frac{\\text{input}_i}{\\text{other}_i}  \n Note By default, this performs a \u201ctrue\u201d division like Python 3. See the rounding_mode argument for floor division.  Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.  Parameters \n \ninput (Tensor) \u2013 the dividend \nother (Tensor or Number) \u2013 the divisor   Keyword Arguments \n \nrounding_mode (str, optional) \u2013 \nType of rounding applied to the result:  None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type. Equivalent to true division in Python (the / operator) and NumPy\u2019s np.true_divide. \n\"trunc\" - rounds the results of the division towards zero. Equivalent to C-style integer division. \n\"floor\" - rounds the results of the division down. Equivalent to floor division in Python (the // operator) and NumPy\u2019s np.floor_divide.   \nout (Tensor, optional) \u2013 the output tensor.    Examples: >>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])\n \n"}, {"name": "torch.divide()", "path": "generated/torch.divide#torch.divide", "type": "torch", "text": " \ntorch.divide(input, other, *, rounding_mode=None, out=None) \u2192 Tensor  \nAlias for torch.div(). \n"}, {"name": "torch.dot()", "path": "generated/torch.dot#torch.dot", "type": "torch", "text": " \ntorch.dot(input, other, *, out=None) \u2192 Tensor  \nComputes the dot product of two 1D tensors.  Note Unlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.   Parameters \n \ninput (Tensor) \u2013 first tensor in the dot product, must be 1D. \nother (Tensor) \u2013 second tensor in the dot product, must be 1D.   Keyword Arguments \n{out} \u2013    Example: >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n \n"}, {"name": "torch.dstack()", "path": "generated/torch.dstack#torch.dstack", "type": "torch", "text": " \ntorch.dstack(tensors, *, out=None) \u2192 Tensor  \nStack tensors in sequence depthwise (along third axis). This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d().  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.dstack((a,b))\ntensor([[[1, 4],\n         [2, 5],\n         [3, 6]]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.dstack((a,b))\ntensor([[[1, 4]],\n        [[2, 5]],\n        [[3, 6]]])\n   \n"}, {"name": "torch.eig()", "path": "generated/torch.eig#torch.eig", "type": "torch", "text": " \ntorch.eig(input, eigenvectors=False, *, out=None) -> (Tensor, Tensor)  \nComputes the eigenvalues and eigenvectors of a real square matrix.  Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only if eigenvalues and eigenvectors are all real valued. When input is on CUDA, torch.eig() causes host-device synchronization.   Parameters \n \ninput (Tensor) \u2013 the square matrix of shape (n\u00d7n)(n \\times n)  for which the eigenvalues and eigenvectors will be computed \neigenvectors (bool) \u2013 True to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed   Keyword Arguments \nout (tuple, optional) \u2013 the output tensors  Returns \nA namedtuple (eigenvalues, eigenvectors) containing  \neigenvalues (Tensor): Shape (n\u00d72)(n \\times 2) . Each row is an eigenvalue of input, where the first element is the real part and the second element is the imaginary part. The eigenvalues are not necessarily ordered. \neigenvectors (Tensor): If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor of shape (n\u00d7n)(n \\times n)  can be used to compute normalized (unit length) eigenvectors of corresponding eigenvalues as follows. If the corresponding eigenvalues[j] is a real number, column eigenvectors[:, j] is the eigenvector corresponding to eigenvalues[j]. If the corresponding eigenvalues[j] and eigenvalues[j + 1] form a complex conjugate pair, then the true eigenvectors can be computed as true eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1] , true eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1] .   Return type \n(Tensor, Tensor)   Example: Trivial example with a diagonal matrix. By default, only eigenvalues are computed:\n\n>>> a = torch.diag(torch.tensor([1, 2, 3], dtype=torch.double))\n>>> e, v = torch.eig(a)\n>>> e\ntensor([[1., 0.],\n        [2., 0.],\n        [3., 0.]], dtype=torch.float64)\n>>> v\ntensor([], dtype=torch.float64)\n\nCompute also the eigenvectors:\n\n>>> e, v = torch.eig(a, eigenvectors=True)\n>>> e\ntensor([[1., 0.],\n        [2., 0.],\n        [3., 0.]], dtype=torch.float64)\n>>> v\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]], dtype=torch.float64)\n \n"}, {"name": "torch.einsum()", "path": "generated/torch.einsum#torch.einsum", "type": "torch", "text": " \ntorch.einsum(equation, *operands) \u2192 Tensor [source]\n \nSums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention. Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention, given by equation. The details of this format are described below, but the general idea is to label every dimension of the input operands with some subscript and define which subscripts are part of the output. The output is then computed by summing the product of the elements of the operands along the dimensions whose subscripts are not part of the output. For example, matrix multiplication can be computed using einsum as torch.einsum(\u201cij,jk->ik\u201d, A, B). Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: The equation string specifies the subscripts (lower case letters [\u2018a\u2019, \u2018z\u2019]) for each dimension of the input operands in the same order as the dimensions, separating subcripts for each operand by a comma (\u2018,\u2019), e.g. \u2018ij,jk\u2019 specify subscripts for two 2D operands. The dimensions labeled with the same subscript must be broadcastable, that is, their size must either match or be 1. The exception is if a subscript is repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order. The output is computed by multiplying the input operands element-wise, with their dimensions aligned based on the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation followed by the subscripts for the output. For instance, the following equation computes the transpose of a matrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and at most once for the output. Ellipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis. Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts, e.g. for an input operand with 5 dimensions, the ellipsis in the equation \u2018ab\u2026c\u2019 cover the third and fourth dimensions. The ellipsis does not need to cover the same number of dimensions across the operands but the \u2018shape\u2019 of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis will come first in the output (left-most dimensions), before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements batch matrix multiplication \u2018\u2026ij,\u2026jk\u2019. A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis, arrow and comma) but something like \u2018\u2026\u2019 is not valid. An empty string \u2018\u2019 is valid for scalar operands.  Note torch.einsum handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.   Note This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) can optimize the formula for you.   Parameters \n \nequation (string) \u2013 The subscripts for the Einstein summation. \noperands (Tensor) \u2013 The operands to compute the Einstein sum of.    Examples: # trace\n>>> torch.einsum('ii', torch.randn(4, 4))\ntensor(-1.2104)\n\n# diagonal\n>>> torch.einsum('ii->i', torch.randn(4, 4))\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n# outer product\n>>> x = torch.randn(5)\n>>> y = torch.randn(4)\n>>> torch.einsum('i,j->ij', x, y)\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n        [-0.3744,  0.9381,  1.2685, -1.6070],\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n# batch matrix multiplication\n>>> As = torch.randn(3,2,5)\n>>> Bs = torch.randn(3,5,4)\n>>> torch.einsum('bij,bjk->bik', As, Bs)\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# batch permute\n>>> A = torch.randn(2, 3, 4, 5)\n>>> torch.einsum('...ij->...ji', A).shape\ntorch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\n>>> A = torch.randn(3,5,4)\n>>> l = torch.randn(2,5)\n>>> r = torch.randn(2,4)\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])\n \n"}, {"name": "torch.empty()", "path": "generated/torch.empty#torch.empty", "type": "torch", "text": " \ntorch.empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) \u2192 Tensor  \nReturns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \npin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format.    Example: >>> torch.empty(2, 3)\ntensor(1.00000e-08 *\n       [[ 6.3984,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n \n"}, {"name": "torch.empty_like()", "path": "generated/torch.empty_like#torch.empty_like", "type": "torch", "text": " \ntorch.empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters \ninput (Tensor) \u2013 the size of input will determine size of the output tensor.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    Example: >>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\n \n"}, {"name": "torch.empty_strided()", "path": "generated/torch.empty_strided#torch.empty_strided", "type": "torch", "text": " \ntorch.empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) \u2192 Tensor  \nReturns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument size and stride respectively. torch.empty_strided(size, stride) is equivalent to torch.empty(size).as_strided(size, stride).  Warning More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.   Parameters \n \nsize (tuple of python:ints) \u2013 the shape of the output tensor \nstride (tuple of python:ints) \u2013 the strides of the output tensor   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \npin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: >>> a = torch.empty_strided((2, 3), (1, 2))\n>>> a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n>>> a.stride()\n(1, 2)\n>>> a.size()\ntorch.Size([2, 3])\n \n"}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "torch", "text": " \nclass torch.enable_grad [source]\n \nContext-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n \n"}, {"name": "torch.eq()", "path": "generated/torch.eq#torch.eq", "type": "torch", "text": " \ntorch.eq(input, other, *, out=None) \u2192 Tensor  \nComputes element-wise equality The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or float) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is equal to other and False elsewhere   Example: >>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ True, False],\n        [False, True]])\n \n"}, {"name": "torch.equal()", "path": "generated/torch.equal#torch.equal", "type": "torch", "text": " \ntorch.equal(input, other) \u2192 bool  \nTrue if two tensors have the same size and elements, False otherwise. Example: >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue\n \n"}, {"name": "torch.erf()", "path": "generated/torch.erf#torch.erf", "type": "torch", "text": " \ntorch.erf(input, *, out=None) \u2192 Tensor  \nComputes the error function of each element. The error function is defined as follows:  erf(x)=2\u03c0\u222b0xe\u2212t2dt\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n \n"}, {"name": "torch.erfc()", "path": "generated/torch.erfc#torch.erfc", "type": "torch", "text": " \ntorch.erfc(input, *, out=None) \u2192 Tensor  \nComputes the complementary error function of each element of input. The complementary error function is defined as follows:  erfc(x)=1\u22122\u03c0\u222b0xe\u2212t2dt\\mathrm{erfc}(x) = 1 - \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n \n"}, {"name": "torch.erfinv()", "path": "generated/torch.erfinv#torch.erfinv", "type": "torch", "text": " \ntorch.erfinv(input, *, out=None) \u2192 Tensor  \nComputes the inverse error function of each element of input. The inverse error function is defined in the range (\u22121,1)(-1, 1)  as:  erfinv(erf(x))=x\\mathrm{erfinv}(\\mathrm{erf}(x)) = x  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n \n"}, {"name": "torch.exp()", "path": "generated/torch.exp#torch.exp", "type": "torch", "text": " \ntorch.exp(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the exponential of the elements of the input tensor input.  yi=exiy_{i} = e^{x_{i}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])\n \n"}, {"name": "torch.exp2()", "path": "generated/torch.exp2#torch.exp2", "type": "torch", "text": " \ntorch.exp2(input, *, out=None) \u2192 Tensor  \nComputes the base two exponential function of input.  yi=2xiy_{i} = 2^{x_{i}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])\n \n"}, {"name": "torch.expm1()", "path": "generated/torch.expm1#torch.expm1", "type": "torch", "text": " \ntorch.expm1(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the exponential of the elements minus 1 of input.  yi=exi\u22121y_{i} = e^{x_{i}} - 1  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n \n"}, {"name": "torch.eye()", "path": "generated/torch.eye#torch.eye", "type": "torch", "text": " \ntorch.eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.  Parameters \n \nn (int) \u2013 the number of rows \nm (int, optional) \u2013 the number of columns with default being n\n   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 2-D tensor with ones on the diagonal and zeros elsewhere  Return type \nTensor   Example: >>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])\n \n"}, {"name": "torch.fake_quantize_per_channel_affine()", "path": "generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine", "type": "torch", "text": " \ntorch.fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max) \u2192 Tensor  \nReturns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.  output=min(quant_max,max(quant_min,std::nearby_int(input/scale)+zero_point))\\text{output} = min( \\text{quant\\_max}, max( \\text{quant\\_min}, \\text{std::nearby\\_int}(\\text{input} / \\text{scale}) + \\text{zero\\_point} ) )  \n Parameters \n \ninput (Tensor) \u2013 the input value(s), in torch.float32. \nscale (Tensor) \u2013 quantization scale, per channel \nzero_point (Tensor) \u2013 quantization zero_point, per channel \naxis (int32) \u2013 channel axis \nquant_min (int64) \u2013 lower bound of the quantized domain \nquant_max (int64) \u2013 upper bound of the quantized domain   Returns \nA newly fake_quantized per channel tensor  Return type \nTensor   Example: >>> x = torch.randn(2, 2, 2)\n>>> x\ntensor([[[-0.2525, -0.0466],\n         [ 0.3491, -0.2168]],\n\n        [[-0.5906,  1.6258],\n         [ 0.6444, -0.0542]]])\n>>> scales = (torch.randn(2) + 1) * 0.05\n>>> scales\ntensor([0.0475, 0.0486])\n>>> zero_points = torch.zeros(2).to(torch.long)\n>>> zero_points\ntensor([0, 0])\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\ntensor([[[0.0000, 0.0000],\n         [0.3405, 0.0000]],\n\n        [[0.0000, 1.6134],\n        [0.6323, 0.0000]]])\n \n"}, {"name": "torch.fake_quantize_per_tensor_affine()", "path": "generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine", "type": "torch", "text": " \ntorch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max) \u2192 Tensor  \nReturns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.  output=min(quant_max,max(quant_min,std::nearby_int(input/scale)+zero_point))\\text{output} = min( \\text{quant\\_max}, max( \\text{quant\\_min}, \\text{std::nearby\\_int}(\\text{input} / \\text{scale}) + \\text{zero\\_point} ) )  \n Parameters \n \ninput (Tensor) \u2013 the input value(s), in torch.float32. \nscale (double) \u2013 quantization scale \nzero_point (int64) \u2013 quantization zero_point \nquant_min (int64) \u2013 lower bound of the quantized domain \nquant_max (int64) \u2013 upper bound of the quantized domain   Returns \nA newly fake_quantized tensor  Return type \nTensor   Example: >>> x = torch.randn(4)\n>>> x\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\ntensor([0.1000, 1.0000, 0.4000, 0.0000])\n \n"}, {"name": "torch.fft", "path": "fft", "type": "torch.fft", "text": "torch.fft Discrete Fourier transforms and related functions. Fast Fourier Transforms  \ntorch.fft.fft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n >>> t = tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\n>>> torch.fft.fft(t)\ntensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\n \n  \ntorch.fft.ifft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n>>> torch.fft.ifft(t)\ntensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])\n \n  \ntorch.fft.fft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fft2 = torch.fft.fft2(t)\n The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fft2, two_ffts)\n \n  \ntorch.fft.ifft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifft2 = torch.fft.ifft2(t)\n The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifft2, two_iffts)\n \n  \ntorch.fft.fftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fftn = torch.fft.fftn(t)\n The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fftn, two_ffts)\n \n  \ntorch.fft.ifftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifftn = torch.fft.ifftn(t)\n The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifftn, two_iffts)\n \n  \ntorch.fft.rfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional Fourier transform of real-valued input. The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the real FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.rfft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j])\n Compare against the full output from fft(): >>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued. \n  \ntorch.fft.irfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of rfft(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the output signal. If given, the input will either be zero-padded or trimmed to this length before computing the real IFFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.rfft(t)\n>>> T\ntensor([10.0000+0.0000j, -2.5000+3.4410j, -2.5000+0.8123j])\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length: >>> torch.fft.irfft(T)\ntensor([0.6250, 1.4045, 3.1250, 4.8455])\n So, it is recommended to always pass the signal length n: >>> torch.fft.irfft(T, t.numel())\ntensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])\n \n  \ntorch.fft.rfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default. The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfft2 = torch.fft.rfft2(t)\n>>> rfft2.size()\ntorch.Size([10, 6])\n Compared against the full output from fft2(), we have all elements up to the Nyquist frequency. >>> fft2 = torch.fft.fft2(t)\n>>> torch.allclose(fft2[..., :6], rfft2)\nTrue\n The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfft2, two_ffts)\n \n  \ntorch.fft.irfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default. input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfft2(t)\n Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfft2(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfft2(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n  \ntorch.fft.rfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N-dimensional discrete Fourier transform of real input. The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfftn = torch.fft.rfftn(t)\n>>> rfftn.size()\ntorch.Size([10, 6])\n Compared against the full output from fftn(), we have all elements up to the Nyquist frequency. >>> fftn = torch.fft.fftn(t)\n>>> torch.allclose(fftn[..., :6], rfftn)\nTrue\n The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfftn, two_ffts)\n \n  \ntorch.fft.irfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the inverse of rfftn(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfftn(t)\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfftn(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfftn(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n  \ntorch.fft.hfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.  Note hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().   Note Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the real output. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian FFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (hfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)  Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (no normalization).     Example Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output: >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.ifft(t)\n>>> T\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n        -0.5000+0.6882j])\n Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies: >>> torch.fft.hfft(T[:3], n=5)\ntensor([0., 1., 2., 3., 4.])\n Like with irfft(), the output length must be given in order to recover an even length output: >>> torch.fft.hfft(T[:3])\ntensor([0.5000, 1.1236, 2.5000, 3.8764])\n \n  \ntorch.fft.ihfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of hfft(). input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ihfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> torch.fft.ihfft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j])\n Compare against the full output from ifft(): >>> torch.fft.ifft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n    -0.5000+0.6882j])\n \n Helper Functions  \ntorch.fft.fftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the discrete Fourier Transform sample frequencies for a signal of size n.  Note By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are: f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.   Parameters \n \nn (int) \u2013 the FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.fftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n For even input, we can see the Nyquist frequency at f[2] is given as negative: >>> torch.fft.fftfreq(4)\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n \n  \ntorch.fft.rfftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the sample frequencies for rfft() with a signal of size n.  Note rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are: f = torch.arange((n + 1) // 2) / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.   Parameters \n \nn (int) \u2013 the real FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.rfftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000])\n >>> torch.fft.rfftfreq(4)\ntensor([ 0.0000,  0.2500, 0.5000])\n Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500]) \n  \ntorch.fft.fftshift(input, dim=None) \u2192 Tensor  \nReorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first. This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.  Note By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().   Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(4)\n>>> f\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n >>> torch.fft.fftshift(f)\ntensor([-0.5000, -0.2500,  0.0000,  0.2500])\n Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor. This also works for multi-dimensional transforms: >>> x = torch.fft.fftfreq(5, d=1/5) + 0.1 * torch.fft.fftfreq(5, d=1/5).unsqueeze(1)\n>>> x\ntensor([[ 0.0000,  1.0000,  2.0000, -2.0000, -1.0000],\n        [ 0.1000,  1.1000,  2.1000, -1.9000, -0.9000],\n        [ 0.2000,  1.2000,  2.2000, -1.8000, -0.8000],\n        [-0.2000,  0.8000,  1.8000, -2.2000, -1.2000],\n        [-0.1000,  0.9000,  1.9000, -2.1000, -1.1000]])\n >>> torch.fft.fftshift(x)\ntensor([[-2.2000, -1.2000, -0.2000,  0.8000,  1.8000],\n        [-2.1000, -1.1000, -0.1000,  0.9000,  1.9000],\n        [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n        [-1.9000, -0.9000,  0.1000,  1.1000,  2.1000],\n        [-1.8000, -0.8000,  0.2000,  1.2000,  2.2000]])\n fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift(). >>> x_centered = torch.arange(-5, 5)\n>>> x_uncentered = torch.fft.ifftshift(x_centered)\n>>> fft_uncentered = torch.fft.fft(x_uncentered)\n Similarly, we can convert the frequency domain components to centered convention by applying fftshift(). >>> fft_centered = torch.fft.fftshift(fft_uncentered)\n The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order: >>> x_centered_2 = torch.fft.fftshift(torch.fft.ifft(torch.fft.ifftshift(fft_centered)))\n>>> torch.allclose(x_centered.to(torch.complex64), x_centered_2)\nTrue\n \n  \ntorch.fft.ifftshift(input, dim=None) \u2192 Tensor  \nInverse of fftshift().  Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(5)\n>>> f\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n A round-trip through fftshift() and ifftshift() gives the same result: >>> shifted = torch.fftshift(f)\n>>> torch.ifftshift(shifted)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n \n\n"}, {"name": "torch.fft.fft()", "path": "fft#torch.fft.fft", "type": "torch.fft", "text": " \ntorch.fft.fft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n >>> t = tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\n>>> torch.fft.fft(t)\ntensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\n \n"}, {"name": "torch.fft.fft2()", "path": "fft#torch.fft.fft2", "type": "torch.fft", "text": " \ntorch.fft.fft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fft2 = torch.fft.fft2(t)\n The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fft2, two_ffts)\n \n"}, {"name": "torch.fft.fftfreq()", "path": "fft#torch.fft.fftfreq", "type": "torch.fft", "text": " \ntorch.fft.fftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the discrete Fourier Transform sample frequencies for a signal of size n.  Note By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are: f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.   Parameters \n \nn (int) \u2013 the FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.fftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n For even input, we can see the Nyquist frequency at f[2] is given as negative: >>> torch.fft.fftfreq(4)\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n \n"}, {"name": "torch.fft.fftn()", "path": "fft#torch.fft.fftn", "type": "torch.fft", "text": " \ntorch.fft.fftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fftn = torch.fft.fftn(t)\n The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fftn, two_ffts)\n \n"}, {"name": "torch.fft.fftshift()", "path": "fft#torch.fft.fftshift", "type": "torch.fft", "text": " \ntorch.fft.fftshift(input, dim=None) \u2192 Tensor  \nReorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first. This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.  Note By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().   Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(4)\n>>> f\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n >>> torch.fft.fftshift(f)\ntensor([-0.5000, -0.2500,  0.0000,  0.2500])\n Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor. This also works for multi-dimensional transforms: >>> x = torch.fft.fftfreq(5, d=1/5) + 0.1 * torch.fft.fftfreq(5, d=1/5).unsqueeze(1)\n>>> x\ntensor([[ 0.0000,  1.0000,  2.0000, -2.0000, -1.0000],\n        [ 0.1000,  1.1000,  2.1000, -1.9000, -0.9000],\n        [ 0.2000,  1.2000,  2.2000, -1.8000, -0.8000],\n        [-0.2000,  0.8000,  1.8000, -2.2000, -1.2000],\n        [-0.1000,  0.9000,  1.9000, -2.1000, -1.1000]])\n >>> torch.fft.fftshift(x)\ntensor([[-2.2000, -1.2000, -0.2000,  0.8000,  1.8000],\n        [-2.1000, -1.1000, -0.1000,  0.9000,  1.9000],\n        [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n        [-1.9000, -0.9000,  0.1000,  1.1000,  2.1000],\n        [-1.8000, -0.8000,  0.2000,  1.2000,  2.2000]])\n fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift(). >>> x_centered = torch.arange(-5, 5)\n>>> x_uncentered = torch.fft.ifftshift(x_centered)\n>>> fft_uncentered = torch.fft.fft(x_uncentered)\n Similarly, we can convert the frequency domain components to centered convention by applying fftshift(). >>> fft_centered = torch.fft.fftshift(fft_uncentered)\n The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order: >>> x_centered_2 = torch.fft.fftshift(torch.fft.ifft(torch.fft.ifftshift(fft_centered)))\n>>> torch.allclose(x_centered.to(torch.complex64), x_centered_2)\nTrue\n \n"}, {"name": "torch.fft.hfft()", "path": "fft#torch.fft.hfft", "type": "torch.fft", "text": " \ntorch.fft.hfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.  Note hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().   Note Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the real output. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian FFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (hfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)  Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (no normalization).     Example Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output: >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.ifft(t)\n>>> T\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n        -0.5000+0.6882j])\n Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies: >>> torch.fft.hfft(T[:3], n=5)\ntensor([0., 1., 2., 3., 4.])\n Like with irfft(), the output length must be given in order to recover an even length output: >>> torch.fft.hfft(T[:3])\ntensor([0.5000, 1.1236, 2.5000, 3.8764])\n \n"}, {"name": "torch.fft.ifft()", "path": "fft#torch.fft.ifft", "type": "torch.fft", "text": " \ntorch.fft.ifft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n>>> torch.fft.ifft(t)\ntensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])\n \n"}, {"name": "torch.fft.ifft2()", "path": "fft#torch.fft.ifft2", "type": "torch.fft", "text": " \ntorch.fft.ifft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifft2 = torch.fft.ifft2(t)\n The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifft2, two_iffts)\n \n"}, {"name": "torch.fft.ifftn()", "path": "fft#torch.fft.ifftn", "type": "torch.fft", "text": " \ntorch.fft.ifftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifftn = torch.fft.ifftn(t)\n The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifftn, two_iffts)\n \n"}, {"name": "torch.fft.ifftshift()", "path": "fft#torch.fft.ifftshift", "type": "torch.fft", "text": " \ntorch.fft.ifftshift(input, dim=None) \u2192 Tensor  \nInverse of fftshift().  Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(5)\n>>> f\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n A round-trip through fftshift() and ifftshift() gives the same result: >>> shifted = torch.fftshift(f)\n>>> torch.ifftshift(shifted)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n \n"}, {"name": "torch.fft.ihfft()", "path": "fft#torch.fft.ihfft", "type": "torch.fft", "text": " \ntorch.fft.ihfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of hfft(). input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ihfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> torch.fft.ihfft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j])\n Compare against the full output from ifft(): >>> torch.fft.ifft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n    -0.5000+0.6882j])\n \n"}, {"name": "torch.fft.irfft()", "path": "fft#torch.fft.irfft", "type": "torch.fft", "text": " \ntorch.fft.irfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of rfft(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the output signal. If given, the input will either be zero-padded or trimmed to this length before computing the real IFFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.rfft(t)\n>>> T\ntensor([10.0000+0.0000j, -2.5000+3.4410j, -2.5000+0.8123j])\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length: >>> torch.fft.irfft(T)\ntensor([0.6250, 1.4045, 3.1250, 4.8455])\n So, it is recommended to always pass the signal length n: >>> torch.fft.irfft(T, t.numel())\ntensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])\n \n"}, {"name": "torch.fft.irfft2()", "path": "fft#torch.fft.irfft2", "type": "torch.fft", "text": " \ntorch.fft.irfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default. input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfft2(t)\n Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfft2(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfft2(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n"}, {"name": "torch.fft.irfftn()", "path": "fft#torch.fft.irfftn", "type": "torch.fft", "text": " \ntorch.fft.irfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the inverse of rfftn(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfftn(t)\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfftn(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfftn(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n"}, {"name": "torch.fft.rfft()", "path": "fft#torch.fft.rfft", "type": "torch.fft", "text": " \ntorch.fft.rfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional Fourier transform of real-valued input. The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the real FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.rfft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j])\n Compare against the full output from fft(): >>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued. \n"}, {"name": "torch.fft.rfft2()", "path": "fft#torch.fft.rfft2", "type": "torch.fft", "text": " \ntorch.fft.rfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default. The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfft2 = torch.fft.rfft2(t)\n>>> rfft2.size()\ntorch.Size([10, 6])\n Compared against the full output from fft2(), we have all elements up to the Nyquist frequency. >>> fft2 = torch.fft.fft2(t)\n>>> torch.allclose(fft2[..., :6], rfft2)\nTrue\n The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfft2, two_ffts)\n \n"}, {"name": "torch.fft.rfftfreq()", "path": "fft#torch.fft.rfftfreq", "type": "torch.fft", "text": " \ntorch.fft.rfftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the sample frequencies for rfft() with a signal of size n.  Note rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are: f = torch.arange((n + 1) // 2) / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.   Parameters \n \nn (int) \u2013 the real FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.rfftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000])\n >>> torch.fft.rfftfreq(4)\ntensor([ 0.0000,  0.2500, 0.5000])\n Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500]) \n"}, {"name": "torch.fft.rfftn()", "path": "fft#torch.fft.rfftn", "type": "torch.fft", "text": " \ntorch.fft.rfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N-dimensional discrete Fourier transform of real input. The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfftn = torch.fft.rfftn(t)\n>>> rfftn.size()\ntorch.Size([10, 6])\n Compared against the full output from fftn(), we have all elements up to the Nyquist frequency. >>> fftn = torch.fft.fftn(t)\n>>> torch.allclose(fftn[..., :6], rfftn)\nTrue\n The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfftn, two_ffts)\n \n"}, {"name": "torch.fix()", "path": "generated/torch.fix#torch.fix", "type": "torch", "text": " \ntorch.fix(input, *, out=None) \u2192 Tensor  \nAlias for torch.trunc() \n"}, {"name": "torch.flatten()", "path": "generated/torch.flatten#torch.flatten", "type": "torch", "text": " \ntorch.flatten(input, start_dim=0, end_dim=-1) \u2192 Tensor  \nFlattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim are passed, only dimensions starting with start_dim and ending with end_dim are flattened. The order of elements in input is unchanged. Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view, or copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned.  Note Flattening a zero-dimensional tensor will return a one-dimensional view.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nstart_dim (int) \u2013 the first dim to flatten \nend_dim (int) \u2013 the last dim to flatten    Example: >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n>>> torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n \n"}, {"name": "torch.flip()", "path": "generated/torch.flip#torch.flip", "type": "torch", "text": " \ntorch.flip(input, dims) \u2192 Tensor  \nReverse the order of a n-D tensor along given axis in dims.  Note torch.flip makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flip, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flip is expected to be slower than np.flip.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndims (a list or tuple) \u2013 axis to flip on    Example: >>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])\n \n"}, {"name": "torch.fliplr()", "path": "generated/torch.fliplr#torch.fliplr", "type": "torch", "text": " \ntorch.fliplr(input) \u2192 Tensor  \nFlip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.  Note Requires the tensor to be at least 2-D.   Note torch.fliplr makes a copy of input\u2019s data. This is different from NumPy\u2019s np.fliplr, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.fliplr is expected to be slower than np.fliplr.   Parameters \ninput (Tensor) \u2013 Must be at least 2-dimensional.   Example: >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.fliplr(x)\ntensor([[1, 0],\n        [3, 2]])\n \n"}, {"name": "torch.flipud()", "path": "generated/torch.flipud#torch.flipud", "type": "torch", "text": " \ntorch.flipud(input) \u2192 Tensor  \nFlip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.  Note Requires the tensor to be at least 1-D.   Note torch.flipud makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flipud, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.   Parameters \ninput (Tensor) \u2013 Must be at least 1-dimensional.   Example: >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])\n \n"}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "torch.Storage", "text": " \nclass torch.FloatStorage(*args, **kwargs) [source]\n \n \nbfloat16()  \nCasts this storage to bfloat16 type \n  \nbool()  \nCasts this storage to bool type \n  \nbyte()  \nCasts this storage to byte type \n  \nchar()  \nCasts this storage to char type \n  \nclone()  \nReturns a copy of this storage \n  \ncomplex_double()  \nCasts this storage to complex double type \n  \ncomplex_float()  \nCasts this storage to complex float type \n  \ncopy_() \n  \ncpu()  \nReturns a CPU copy of this storage if it\u2019s not already on the CPU \n  \ncuda(device=None, non_blocking=False, **kwargs)  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (int) \u2013 The destination GPU id. Defaults to the current device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.    \n  \ndata_ptr() \n  \ndevice \n  \ndouble()  \nCasts this storage to double type \n  \ndtype \n  \nelement_size() \n  \nfill_() \n  \nfloat()  \nCasts this storage to float type \n  \nstatic from_buffer() \n  \nstatic from_file(filename, shared=False, size=0) \u2192 Storage  \nIf shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters \n \nfilename (str) \u2013 file name to map \nshared (bool) \u2013 whether to share memory \nsize (int) \u2013 number of elements in the storage    \n  \nget_device() \n  \nhalf()  \nCasts this storage to half type \n  \nint()  \nCasts this storage to int type \n  \nis_cuda: bool = False \n  \nis_pinned() \n  \nis_shared() \n  \nis_sparse: bool = False \n  \nlong()  \nCasts this storage to long type \n  \nnew() \n  \npin_memory()  \nCopies the storage to pinned memory, if it\u2019s not already pinned. \n  \nresize_() \n  \nshare_memory_()  \nMoves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self \n  \nshort()  \nCasts this storage to short type \n  \nsize() \n  \ntolist()  \nReturns a list containing the elements of this storage \n  \ntype(dtype=None, non_blocking=False, **kwargs)  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n \n"}, {"name": "torch.FloatStorage.bfloat16()", "path": "storage#torch.FloatStorage.bfloat16", "type": "torch.Storage", "text": " \nbfloat16()  \nCasts this storage to bfloat16 type \n"}, {"name": "torch.FloatStorage.bool()", "path": "storage#torch.FloatStorage.bool", "type": "torch.Storage", "text": " \nbool()  \nCasts this storage to bool type \n"}, {"name": "torch.FloatStorage.byte()", "path": "storage#torch.FloatStorage.byte", "type": "torch.Storage", "text": " \nbyte()  \nCasts this storage to byte type \n"}, {"name": "torch.FloatStorage.char()", "path": "storage#torch.FloatStorage.char", "type": "torch.Storage", "text": " \nchar()  \nCasts this storage to char type \n"}, {"name": "torch.FloatStorage.clone()", "path": "storage#torch.FloatStorage.clone", "type": "torch.Storage", "text": " \nclone()  \nReturns a copy of this storage \n"}, {"name": "torch.FloatStorage.complex_double()", "path": "storage#torch.FloatStorage.complex_double", "type": "torch.Storage", "text": " \ncomplex_double()  \nCasts this storage to complex double type \n"}, {"name": "torch.FloatStorage.complex_float()", "path": "storage#torch.FloatStorage.complex_float", "type": "torch.Storage", "text": " \ncomplex_float()  \nCasts this storage to complex float type \n"}, {"name": "torch.FloatStorage.copy_()", "path": "storage#torch.FloatStorage.copy_", "type": "torch.Storage", "text": " \ncopy_() \n"}, {"name": "torch.FloatStorage.cpu()", "path": "storage#torch.FloatStorage.cpu", "type": "torch.Storage", "text": " \ncpu()  \nReturns a CPU copy of this storage if it\u2019s not already on the CPU \n"}, {"name": "torch.FloatStorage.cuda()", "path": "storage#torch.FloatStorage.cuda", "type": "torch.Storage", "text": " \ncuda(device=None, non_blocking=False, **kwargs)  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (int) \u2013 The destination GPU id. Defaults to the current device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.    \n"}, {"name": "torch.FloatStorage.data_ptr()", "path": "storage#torch.FloatStorage.data_ptr", "type": "torch.Storage", "text": " \ndata_ptr() \n"}, {"name": "torch.FloatStorage.device", "path": "storage#torch.FloatStorage.device", "type": "torch.Storage", "text": " \ndevice \n"}, {"name": "torch.FloatStorage.double()", "path": "storage#torch.FloatStorage.double", "type": "torch.Storage", "text": " \ndouble()  \nCasts this storage to double type \n"}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "torch.Storage", "text": " \ndtype \n"}, {"name": "torch.FloatStorage.element_size()", "path": "storage#torch.FloatStorage.element_size", "type": "torch.Storage", "text": " \nelement_size() \n"}, {"name": "torch.FloatStorage.fill_()", "path": "storage#torch.FloatStorage.fill_", "type": "torch.Storage", "text": " \nfill_() \n"}, {"name": "torch.FloatStorage.float()", "path": "storage#torch.FloatStorage.float", "type": "torch.Storage", "text": " \nfloat()  \nCasts this storage to float type \n"}, {"name": "torch.FloatStorage.from_buffer()", "path": "storage#torch.FloatStorage.from_buffer", "type": "torch.Storage", "text": " \nstatic from_buffer() \n"}, {"name": "torch.FloatStorage.from_file()", "path": "storage#torch.FloatStorage.from_file", "type": "torch.Storage", "text": " \nstatic from_file(filename, shared=False, size=0) \u2192 Storage  \nIf shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters \n \nfilename (str) \u2013 file name to map \nshared (bool) \u2013 whether to share memory \nsize (int) \u2013 number of elements in the storage    \n"}, {"name": "torch.FloatStorage.get_device()", "path": "storage#torch.FloatStorage.get_device", "type": "torch.Storage", "text": " \nget_device() \n"}, {"name": "torch.FloatStorage.half()", "path": "storage#torch.FloatStorage.half", "type": "torch.Storage", "text": " \nhalf()  \nCasts this storage to half type \n"}, {"name": "torch.FloatStorage.int()", "path": "storage#torch.FloatStorage.int", "type": "torch.Storage", "text": " \nint()  \nCasts this storage to int type \n"}, {"name": "torch.FloatStorage.is_cuda", "path": "storage#torch.FloatStorage.is_cuda", "type": "torch.Storage", "text": " \nis_cuda: bool = False \n"}, {"name": "torch.FloatStorage.is_pinned()", "path": "storage#torch.FloatStorage.is_pinned", "type": "torch.Storage", "text": " \nis_pinned() \n"}, {"name": "torch.FloatStorage.is_shared()", "path": "storage#torch.FloatStorage.is_shared", "type": "torch.Storage", "text": " \nis_shared() \n"}, {"name": "torch.FloatStorage.is_sparse", "path": "storage#torch.FloatStorage.is_sparse", "type": "torch.Storage", "text": " \nis_sparse: bool = False \n"}, {"name": "torch.FloatStorage.long()", "path": "storage#torch.FloatStorage.long", "type": "torch.Storage", "text": " \nlong()  \nCasts this storage to long type \n"}, {"name": "torch.FloatStorage.new()", "path": "storage#torch.FloatStorage.new", "type": "torch.Storage", "text": " \nnew() \n"}, {"name": "torch.FloatStorage.pin_memory()", "path": "storage#torch.FloatStorage.pin_memory", "type": "torch.Storage", "text": " \npin_memory()  \nCopies the storage to pinned memory, if it\u2019s not already pinned. \n"}, {"name": "torch.FloatStorage.resize_()", "path": "storage#torch.FloatStorage.resize_", "type": "torch.Storage", "text": " \nresize_() \n"}, {"name": "torch.FloatStorage.share_memory_()", "path": "storage#torch.FloatStorage.share_memory_", "type": "torch.Storage", "text": " \nshare_memory_()  \nMoves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self \n"}, {"name": "torch.FloatStorage.short()", "path": "storage#torch.FloatStorage.short", "type": "torch.Storage", "text": " \nshort()  \nCasts this storage to short type \n"}, {"name": "torch.FloatStorage.size()", "path": "storage#torch.FloatStorage.size", "type": "torch.Storage", "text": " \nsize() \n"}, {"name": "torch.FloatStorage.tolist()", "path": "storage#torch.FloatStorage.tolist", "type": "torch.Storage", "text": " \ntolist()  \nReturns a list containing the elements of this storage \n"}, {"name": "torch.FloatStorage.type()", "path": "storage#torch.FloatStorage.type", "type": "torch.Storage", "text": " \ntype(dtype=None, non_blocking=False, **kwargs)  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n"}, {"name": "torch.float_power()", "path": "generated/torch.float_power#torch.float_power", "type": "torch", "text": " \ntorch.float_power(input, exponent, *, out=None) \u2192 Tensor  \nRaises input to the power of exponent, elementwise, in double precision. If neither input is complex returns a torch.float64 tensor, and if one or more inputs is complex returns a torch.complex128 tensor.  Note This function always computes in double precision, unlike torch.pow(), which implements more typical type promotion. This is useful when the computation needs to be performed in a wider or more precise dtype, or the results of the computation may contain fractional values not representable in the input dtypes, like when an integer base is raised to a negative integer exponent.   Parameters \n \ninput (Tensor or Number) \u2013 the base value(s) \nexponent (Tensor or Number) \u2013 the exponent value(s)   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randint(10, (4,))\n>>> a\ntensor([6, 4, 7, 1])\n>>> torch.float_power(a, 2)\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\n\n>>> a = torch.arange(1, 5)\n>>> a\ntensor([ 1,  2,  3,  4])\n>>> exp = torch.tensor([2, -3, 4, -5])\n>>> exp\ntensor([ 2, -3,  4, -5])\n>>> torch.float_power(a, exp)\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)\n \n"}, {"name": "torch.floor()", "path": "generated/torch.floor#torch.floor", "type": "torch", "text": " \ntorch.floor(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.  outi=\u230ainputi\u230b\\text{out}_{i} = \\left\\lfloor \\text{input}_{i} \\right\\rfloor  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])\n \n"}, {"name": "torch.floor_divide()", "path": "generated/torch.floor_divide#torch.floor_divide", "type": "torch", "text": " \ntorch.floor_divide(input, other, *, out=None) \u2192 Tensor  \n Warning This function\u2019s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.  Computes input divided by other, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):  outi=trunc(inputiotheri)\\text{{out}}_i = \\text{trunc} \\left( \\frac{{\\text{{input}}_i}}{{\\text{{other}}_i}} \\right)  \nSupports broadcasting to a common shape, type promotion, and integer and float inputs.  Parameters \n \ninput (Tensor or Number) \u2013 the dividend \nother (Tensor or Number) \u2013 the divisor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([4.0, 3.0])\n>>> b = torch.tensor([2.0, 2.0])\n>>> torch.floor_divide(a, b)\ntensor([2.0, 1.0])\n>>> torch.floor_divide(a, 1.4)\ntensor([2.0, 2.0])\n \n"}, {"name": "torch.fmax()", "path": "generated/torch.fmax#torch.fmax", "type": "torch", "text": " \ntorch.fmax(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])\n \n"}, {"name": "torch.fmin()", "path": "generated/torch.fmin#torch.fmin", "type": "torch", "text": " \ntorch.fmin(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise minimum of input and other. This is like torch.minimum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmin and is similar to NumPy\u2019s fmin function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])\n \n"}, {"name": "torch.fmod()", "path": "generated/torch.fmod#torch.fmod", "type": "torch", "text": " \ntorch.fmod(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape, type promotion, and integer and float inputs.  Note When the divisor is zero, returns NaN for floating point dtypes on both CPU and GPU; raises RuntimeError for integer division by zero on CPU; Integer division by zero on GPU may return any value.   Parameters \n \ninput (Tensor) \u2013 the dividend \nother (Tensor or Scalar) \u2013 the divisor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])\n \n"}, {"name": "torch.frac()", "path": "generated/torch.frac#torch.frac", "type": "torch", "text": " \ntorch.frac(input, *, out=None) \u2192 Tensor  \nComputes the fractional portion of each element in input.  outi=inputi\u2212\u230a\u2223inputi\u2223\u230b\u2217sgn\u2061(inputi)\\text{out}_{i} = \\text{input}_{i} - \\left\\lfloor |\\text{input}_{i}| \\right\\rfloor * \\operatorname{sgn}(\\text{input}_{i})  \nExample: >>> torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])\n \n"}, {"name": "torch.from_numpy()", "path": "generated/torch.from_numpy#torch.from_numpy", "type": "torch", "text": " \ntorch.from_numpy(ndarray) \u2192 Tensor  \nCreates a Tensor from a numpy.ndarray. The returned tensor and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable. It currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16, numpy.complex64, numpy.complex128, numpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8, and numpy.bool. Example: >>> a = numpy.array([1, 2, 3])\n>>> t = torch.from_numpy(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n \n"}, {"name": "torch.full()", "path": "generated/torch.full#torch.full", "type": "torch", "text": " \ntorch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nCreates a tensor of size size filled with fill_value. The tensor\u2019s dtype is inferred from fill_value.  Parameters \n \nsize (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. \nfill_value (Scalar) \u2013 the value to fill the output tensor with.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])\n \n"}, {"name": "torch.full_like()", "path": "generated/torch.full_like#torch.full_like", "type": "torch", "text": " \ntorch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device).  Parameters \n \ninput (Tensor) \u2013 the size of input will determine size of the output tensor. \nfill_value \u2013 the number to fill the output tensor with.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n"}, {"name": "torch.futures", "path": "futures", "type": "torch.futures", "text": "torch.futures  Warning The torch.futures package is experimental and subject to change.  This package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.  \nclass torch.futures.Future  \nWrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.  \nadd_done_callback(self: torch._C.Future, arg0: function) \u2192 None \n  \ndone() [source]\n \nReturn True if this Future is done. A Future is done if it has a result or an exception. \n  \nset_exception(result) [source]\n \nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.  Parameters \nresult (BaseException) \u2013 the exception for this Future.    Example::\n\n>>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n   \n  \nset_result(result) [source]\n \nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.  Parameters \nresult (object) \u2013 the result object of this Future.    Example::\n\n>>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n   \n  \nthen(callback) [source]\n \nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.  Parameters \ncallback (Callable) \u2013 a Callable that takes this Future as the only argument.  Returns \nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.    Example::\n\n>>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n   \n  \nvalue(self: torch._C.Future) \u2192 object \n  \nwait() [source]\n \nBlock until the value of this Future is ready.  Returns \nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.   \n \n  \ntorch.futures.collect_all(futures) [source]\n \nCollects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.  Parameters \nfutures (list) \u2013 a list of Future objects.  Returns \nReturns a Future object to a list of the passed in Futures.    Example::\n\n>>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1\n   \n  \ntorch.futures.wait_all(futures) [source]\n \nWaits for all provided futures to be complete, and returns the list of completed values.  Parameters \nfutures (list) \u2013 a list of Future object.  Returns \nA list of the completed Future results. This method will throw an error if wait on any Future throws.   \n\n"}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "torch.futures", "text": " \ntorch.futures.collect_all(futures) [source]\n \nCollects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.  Parameters \nfutures (list) \u2013 a list of Future objects.  Returns \nReturns a Future object to a list of the passed in Futures.    Example::\n\n>>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1\n   \n"}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "torch.futures", "text": " \nclass torch.futures.Future  \nWrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.  \nadd_done_callback(self: torch._C.Future, arg0: function) \u2192 None \n  \ndone() [source]\n \nReturn True if this Future is done. A Future is done if it has a result or an exception. \n  \nset_exception(result) [source]\n \nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.  Parameters \nresult (BaseException) \u2013 the exception for this Future.    Example::\n\n>>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n   \n  \nset_result(result) [source]\n \nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.  Parameters \nresult (object) \u2013 the result object of this Future.    Example::\n\n>>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n   \n  \nthen(callback) [source]\n \nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.  Parameters \ncallback (Callable) \u2013 a Callable that takes this Future as the only argument.  Returns \nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.    Example::\n\n>>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n   \n  \nvalue(self: torch._C.Future) \u2192 object \n  \nwait() [source]\n \nBlock until the value of this Future is ready.  Returns \nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.   \n \n"}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "torch.futures", "text": " \nadd_done_callback(self: torch._C.Future, arg0: function) \u2192 None \n"}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "torch.futures", "text": " \ndone() [source]\n \nReturn True if this Future is done. A Future is done if it has a result or an exception. \n"}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "torch.futures", "text": " \nset_exception(result) [source]\n \nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.  Parameters \nresult (BaseException) \u2013 the exception for this Future.    Example::\n\n>>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n   \n"}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "torch.futures", "text": " \nset_result(result) [source]\n \nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.  Parameters \nresult (object) \u2013 the result object of this Future.    Example::\n\n>>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n   \n"}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "torch.futures", "text": " \nthen(callback) [source]\n \nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.  Parameters \ncallback (Callable) \u2013 a Callable that takes this Future as the only argument.  Returns \nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.    Example::\n\n>>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n   \n"}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "torch.futures", "text": " \nvalue(self: torch._C.Future) \u2192 object \n"}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "torch.futures", "text": " \nwait() [source]\n \nBlock until the value of this Future is ready.  Returns \nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.   \n"}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "torch.futures", "text": " \ntorch.futures.wait_all(futures) [source]\n \nWaits for all provided futures to be complete, and returns the list of completed values.  Parameters \nfutures (list) \u2013 a list of Future object.  Returns \nA list of the completed Future results. This method will throw an error if wait on any Future throws.   \n"}, {"name": "torch.fx", "path": "fx", "type": "torch.fx", "text": "torch.fx Overview This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action: import torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph(x):\n    %param : [#users=1] = self.param\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\n    return clamp_1\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n\"\"\"\n The symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation. The intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph\u2019s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing \u2192 intermediate representation \u2192 transforms \u2192 Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX! Several example transformations can be found at the examples repository. Writing Transformations What is an FX transform? Essentially, it\u2019s a function that looks like this. import torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n Your transform will take in an torch.nn.Module, acquire a Graph from it, do some modifications, and return a new torch.nn.Module. You should think of the torch.nn.Module that your FX transform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for composability.  Note It is also possible to modify an existing GraphModule instead of creating a new one, like so: import torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module):\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n Note that you MUST call GraphModule.recompile() to bring the generated forward() method on the GraphModule in sync with the modified Graph.  Given that you\u2019ve passed in a torch.nn.Module that has been traced into a Graph, there are now two primary approaches you can take to building a new Graph. A Quick Primer on Graphs Full treatment of the semantics of graphs can be found in the Graph documentation, but we are going to cover the basics here. A Graph is a data structure that represents a method on a GraphModule. The information that this requires is:  What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method?  All three of these concepts are represented with Node instances. Let\u2019s see what we mean by that with a short example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n Here we define a module MyModule for demonstration purposes, instantiate it, symbolically trace it, then call the Graph.print_tabular() method to print out a table showing the nodes of this Graph:   \nopcode name target args kwargs   \nplaceholder x x () {}  \nget_attr linear_weight linear.weight () {}  \ncall_function add_1 <built-in function add> (x, linear_weight) {}  \ncall_module linear_1 linear (add_1,) {}  \ncall_method relu_1 relu (linear_1,) {}  \ncall_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1}  \ncall_function topk_1 <built-in method topk \u2026> (sum_1, 3) {}  \noutput output output (topk_1,) {}   We can use this information to answer the questions we posed above.  What are the inputs to the method? In FX, method inputs are specified via special placeholder nodes. In this case, we have a single placeholder node with a target of x, meaning we have a single (non-self) argument named x. What are the operations within the method? The get_attr, call_function, call_module, and call_method nodes represent the operations in the method. A full treatment of the semantics of all of these can be found in the Node documentation. What is the return value of the method? The return value in a Graph is specified by a special output node.  Given that we now know the basics of how code is represented in FX, we can now explore how we would edit a Graph. Graph Manipulation Direct Graph Manipulation One approach to building this new Graph is to directly manipulate your old one. To aid in this, we can simply take the Graph we obtain from symbolic tracing and modify it. For example, let\u2019s say we desire to replace torch.add() calls with torch.mul() calls. import torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n We can also do more involved Graph rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the Graph documentation. An example of using these APIs to append a torch.relu() call can be found below. # Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n For simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter. Subgraph Rewriting With replace_pattern() FX also provides another level of automation on top of direct graph manipulation. The replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing Graphs. It allows you to specify a pattern and replacement function and it will trace through those functions, find instances of the group of operations in the pattern graph, and replace those instances with copies of the replacement graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex. Graph Manipulation Examples  Replace one op Conv/Batch Norm fusion replace_pattern: Basic usage Quantization Invert Transformation  Proxy/Retracing Another way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. # Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here. The Interpreter Pattern A useful code organizational pattern in FX is to loop over all the Nodes in a Graph and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys. For example, suppose we want to run a GraphModule and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime. That might look like: import torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n As you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the Interpreter class, which encompasses the above logic in a way that certain aspects of the interpreter\u2019s execution can be overridden via method overrides. In addition to executing operations, we can also generate a new Graph by feeding Proxy values through an interpreter. Similarly, we provide the Transformer class to encompass this pattern. Transformer behaves similarly to Interpreter, but instead of calling the run method to get a concrete output value from the Module, you would call the Transformer.transform() method to return a new GraphModule which was subject to any transformation rules you installed as overridden methods. Examples of the Interpreter Pattern  Shape Propagation Performance Profiler  Debugging Introduction Often in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section Available Debuggers. Checking Correctness of Modules Because the output of most deep learning modules consists of floating point torch.Tensor instances, checking for equivalence between the results of two torch.nn.Module is not as straightforward as doing a simple equality check. To motivate this, let\u2019s use an example: import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n Here, we\u2019ve tried to check equality of the values of two deep learning models with the == equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use torch.allclose() instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold: assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n This is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation. Debugging the Generated Code Because FX generates the forward() function on GraphModules, using traditional debugging techniques like print statements or pdb is not as straightfoward. Luckily, we have several techniques we can use for debugging the generated code. Use pdb\n Invoke pdb to step into the running program. Although the code that represents the Graph is not in any source file, we can still step into it manually using pdb when the forward pass is invoked. import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n Print the Generated Code If you\u2019d like to run the same code multiple times, then it can be a bit tedious to step to the right code with pdb. In that case, one approach is to simply copy-paste the generated forward pass into your code and examine it from there. # Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n Use the to_folder Function From GraphModule\n GraphModule.to_folder() is a method in GraphModule that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using to_folder. m = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n After running the above example, we can then look at the code within foo/module.py and modify it as desired (e.g. adding print statements or using pdb) to debug the generated code. Debugging the Transformation Now that we\u2019ve identified that a transformation is creating incorrect code, it\u2019s time to debug the transformation itself. First, we\u2019ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our GraphModule transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module: # Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph(x, y):\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\n    return add_1\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args      kwargs\n-------------  ------  -----------------------  --------  --------\nplaceholder    x       x                        ()        {}\nplaceholder    y       y                        ()        {}\ncall_function  add_1   <built-in function add>  (x, y)    {}\n\"\"\"\n Using the utility functions above, we can compare our traced Module before and after we\u2019ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it\u2019s still not clear what\u2019s going wrong, a debugger like pdb can be a good next step. Going off of the example above, consider the following code: # Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n Using the above example, let\u2019s say that the call to print(traced) showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a pdb session. We can see what\u2019s happening during the transform by breaking on transform_graph(traced), then pressing s to \u201cstep into\u201d the call to transform_graph(traced). We may also have good luck by editing the print_tabular method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node\u2019s input_nodes and users.) Available Debuggers The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb). Limitations of Symbolic Tracing FX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is tracing in that it executes the program (really a torch.nn.Module or function) to record operations. It is symbolic in that the data flowing through the program during this execution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some limitations. Dynamic Control Flow The main limitation of symbolic tracing is it does not currently support dynamic control flow. That is, loops or if statements where the condition may depend on the input values of the program. For example, let\u2019s examine the following program: def func_to_trace(x):\n    dim0 = x.size[0]\n    if dim0 == 3:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if dim0 == 3:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n The condition to the if statement relies on the value of dim0, which eventually relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens. Static Control Flow On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model\u2019s architecture based on hyper-parameters. As a concrete example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by passing constant values during symbolic tracing: def f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\ndef wrapper(flag):\n    return lambda x: f(x, flag)\n\nnew_f = wrapper(flag=True)\nfx.symbolic_trace(new_f)\n In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather than tracing through them. Non-torch Functions FX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are things that are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example: import torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n The error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API: torch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n Customizing Tracing with the Tracer class The Tracer class is the class that underlies the implementation of symbolic_trace. The behavior of tracing can be customized by subclassing Tracer, like so: class MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n Leaf Modules Leaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard torch.nn module instances. For example: class MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n The set of leaf modules can be customized by overriding Tracer.is_leaf_module(). Miscellanea  \nTensor constructors (e.g. torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_coo_tensor) are currently not traceable.  The deterministic constructors (zeros, ones) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case, ones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a single random value embedded in the trace. This is likely not the intended behavior. This behavior may be fixed in a future release.   \nType annotations  Python 3-style type annotations (e.g. func(x : torch.Tensor, y : int) -> torch.Tensor) are supported and will be preserved by symbolic tracing. Python 2-style comment type annotations # type: (torch.Tensor, int) -> torch.Tensor are not currently supported. Annotations on local names within a function are not currently supported.    API Reference  \ntorch.fx.symbolic_trace(root, concrete_args=None) [source]\n \nSymbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.  Parameters \n \nroot (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted into a Graph representation. \nconcrete_args (Optional[Dict[str, any]]) \u2013 Concrete arguments that should not be treated as Proxies.   Returns \na Module created from the recorded operations from root.  Return type \nGraphModule   \n  \ntorch.fx.wrap(fn_or_name) [source]\n \nThis function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through: # foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n This function can also equivalently be used as a decorator: # foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.  Parameters \nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called   \n  \nclass torch.fx.GraphModule(root, graph, class_name='GraphModule') [source]\n \nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.  Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.   \n__init__(root, graph, class_name='GraphModule') [source]\n \nConstruct a GraphModule.  Parameters \n \nroot (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. \ngraph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation \nname (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform.    \n  \nproperty code  \nReturn the Python code generated from the Graph underlying this GraphModule. \n  \nproperty graph  \nReturn the Graph underlying this GraphModule \n  \nrecompile() [source]\n \nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. \n  \nto_folder(folder, module_name='FxModule') [source]\n \nDumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>  Parameters \n \nfolder (Union[str, os.PathLike]) \u2013 The folder to write the code out to \nmodule_name (str) \u2013 Top-level name to use for the Module while writing out the code    \n \n  \nclass torch.fx.Graph [source]\n \nGraph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n Will produce the following Graph: print(gm.graph)\n graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n For the semantics of operations represented in the Graph, please see Node.  \n__init__() [source]\n \nConstruct an empty Graph. \n  \ncall_function(the_function, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be  Parameters \n \nthe_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.    Returns The newly created and inserted call_function node.  Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_method(method_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.  Parameters \n \nmethod_name (str) \u2013 The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly created and inserted call_method node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_module(module_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.  Parameters \n \nmodule_name (str) \u2013 The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should not include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted call_module node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None) [source]\n \nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().  Parameters \n \nop (str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019, \u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are described in the Graph docstring. \nargs (Optional[Tuple[Argument, ..]]) \u2013 is a tuple of arguments to this node. \nkwargs (Optional[Dict[str, Argument]]) \u2013 the kwargs of this Node \nname (Optional[str]) \u2013 an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted node.   \n  \nerase_node(to_erase) [source]\n \nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.  Parameters \nto_erase (Node) \u2013 The Node to erase from the Graph.   \n  \nget_attr(qualified_name, type_expr=None) [source]\n \nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.  Parameters \n \nqualified_name (str) \u2013 the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted get_attr node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \ngraph_copy(g, val_map) [source]\n \nCopy all nodes from a given graph into self.  Parameters \n \ng (Graph) \u2013 The source graph from which to copy Nodes. \nval_map (Dict[Node, Node]) \u2013 a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.   Returns \nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.   \n  \ninserting_after(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \ninserting_before(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \nlint(root=None) [source]\n \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root  Parameters \nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.   \n  \nnode_copy(node, arg_transform=<function Graph.<lambda>>) [source]\n \nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example: # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n  Parameters \n \nnode (Node) \u2013 The node to copy into self. \narg_transform (Callable[[Node], Argument]) \u2013 A function that transforms Node arguments in node\u2019s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.    \n  \nproperty nodes  \nGet the list of Nodes that constitute this Graph. Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.  Returns \nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.   \n  \noutput(result, type_expr=None) [source]\n \nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.  Parameters \n \nresult (Argument) \u2013 The value to be returned. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nplaceholder(name, type_expr=None) [source]\n \nInsert a placeholder node into the Graph. A placeholder represents a function input.  Parameters \n \nname (str) \u2013 A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nprint_tabular() [source]\n \nPrints the intermediate representation of the graph in tabular format. \n  \npython_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.   \n \n  \nclass torch.fx.Node(graph, name, op, target, args, kwargs, type=None) [source]\n \nNode is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:  \nplaceholder represents a function input. The name attribute specifies the name this value will take on. target is similarly the name of the argument. args holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. kwargs is don\u2019t-care. Placeholders correspond to the function parameters (e.g. x) in the graph printout. \nget_attr retrieves a parameter from the module hierarchy. name is similarly the name the result of the fetch is assigned to. target is the fully-qualified name of the parameter\u2019s position in the module hierarchy. args and kwargs are don\u2019t-care \ncall_function applies a free function to some values. name is similarly the name of the value to assign to. target is the function to be applied. args and kwargs represent the arguments to the function, following the Python calling convention \ncall_module applies a module in the module hierarchy\u2019s forward() method to given arguments. name is as previous. target is the fully-qualified name of the module in the module hierarchy to call. args and kwargs represent the arguments to invoke the module on, including the self argument. \ncall_method calls a method on a value. name is as similar. target is the string name of the method to apply to the self argument. args and kwargs represent the arguments to invoke the module on, including the self argument\n \noutput contains the output of the traced function in its args[0] attribute. This corresponds to the \u201creturn\u201d statement in the Graph printout.   \nproperty all_input_nodes  \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.  Returns \nList of Nodes that appear in the args and kwargs of this Node, in that order.   \n  \nappend(x) [source]\n \nInsert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)  Parameters \nx (Node) \u2013 The node to put after this node. Must be a member of the same graph.   \n  \nproperty args  \nThe tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty kwargs  \nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty next  \nReturns the next Node in the linked list of Nodes.  Returns \nThe next Node in the linked list of Nodes.   \n  \nprepend(x) [source]\n \nInsert x before this node in the list of nodes in the graph. Example: Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n  Parameters \nx (Node) \u2013 The node to put before this node. Must be a member of the same graph.   \n  \nproperty prev  \nReturns the previous Node in the linked list of Nodes.  Returns \nThe previous Node in the linked list of Nodes.   \n  \nreplace_all_uses_with(replace_with) [source]\n \nReplace all uses of self in the Graph with the Node replace_with.  Parameters \nreplace_with (Node) \u2013 The node to replace all uses of self with.  Returns \nThe list of Nodes on which this change was made.   \n \n  \nclass torch.fx.Tracer(autowrap_modules=(<module 'math' from '/home/matti/miniconda3/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so'>, )) [source]\n \nTracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m). Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.  \ncall_module(m, forward, args, kwargs) [source]\n \nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance. By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function. This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.  Parameters \n \nm (Module) \u2013 The module for which a call is being emitted \nforward (Callable) \u2013 The forward() method of the Module to be invoked \nargs (Tuple) \u2013 args of the module callsite \nkwargs (Dict) \u2013 kwargs of the module callsite   Returns \nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.   \n  \ncreate_arg(a) [source]\n \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph. By default, the behavior includes:  Iterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements. Given a Proxy object, return a reference to the underlying IR Node\n \nGiven a non-Proxy Tensor object, emit IR for various cases:  For a Parameter, emit a get_attr node referring to that Parameter For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.    This method can be overridden to support more types.  Parameters \na (Any) \u2013 The value to be emitted as an Argument in the Graph.  Returns \nThe value a converted into the appropriate Argument   \n  \ncreate_args_for_root(root_fn, is_module, concrete_args=None) [source]\n \nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs. \n  \nis_leaf_module(m, module_qualified_name) [source]\n \nA method to specify whether a given nn.Module is a \u201cleaf\u201d module. Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.  Parameters \n \nm (Module) \u2013 The module being queried about \nmodule_qualified_name (str) \u2013 The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.    \n  \npath_of_module(mod) [source]\n \nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.  Parameters \nmod (str) \u2013 The Module to retrieve the qualified name for.   \n  \ntrace(root, concrete_args=None) [source]\n \nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable. Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.  Parameters \nroot (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.  Returns \nA Graph representing the semantics of the passed-in root.   \n \n  \nclass torch.fx.Proxy(node, tracer=None) [source]\n \nProxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph. If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph. \n  \nclass torch.fx.Interpreter(module) [source]\n \nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes. Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy: run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so: class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The module to be executed    \ncall_function(target, args, kwargs) [source]\n \nExecute a call_function node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the function invocation   \n  \ncall_method(target, args, kwargs) [source]\n \nExecute a call_method node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the method invocation   \n  \ncall_module(target, args, kwargs) [source]\n \nExecute a call_module node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the module invocation   \n  \nfetch_args_kwargs_from_env(n) [source]\n \nFetch the concrete values of args and kwargs of node n from the current execution environment.  Parameters \nn (Node) \u2013 The node for which args and kwargs should be fetched.  Returns \nargs and kwargs with concrete values for n.  Return type \nTuple[Tuple, Dict]   \n  \nfetch_attr(target) [source]\n \nFetch an attribute from the Module hierarchy of self.module.  Parameters \ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch  Returns \nThe value of the attribute.  Return type \nAny   \n  \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe value of the attribute that was retrieved  Return type \nAny   \n  \nmap_nodes_to_values(args, n) [source]\n \nRecursively descend through args and look up the concrete value for each Node in the current execution environment.  Parameters \n \nargs (Argument) \u2013 Data structure within which to look up concrete values \nn (Node) \u2013 Node to which args belongs. This is only used for error reporting.    \n  \noutput(target, args, kwargs) [source]\n \nExecute an output node. This really just retrieves the value referenced by the output node and returns it.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe return value referenced by the output node  Return type \nAny   \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe argument value that was retrieved.  Return type \nAny   \n  \nrun(*args, initial_env=None) [source]\n \nRun module via interpretation and return the result.  Parameters \n \n*args \u2013 The arguments to the Module to run, in positional order \ninitial_env (Optional[Dict[Node, Any]]) \u2013 An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.   Returns \nThe value returned from executing the Module  Return type \nAny   \n  \nrun_node(n) [source]\n \nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op  Parameters \nn (Node) \u2013 The Node to execute  Returns \nThe result of executing n  Return type \nAny   \n \n  \nclass torch.fx.Transformer(module) [source]\n \nTransformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically. Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so: class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The Module to be transformed.    \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \ntransform() [source]\n \nTransform self.module and return the transformed GraphModule. \n \n  \ntorch.fx.replace_pattern(gm, pattern, replacement) [source]\n \nMatches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).  Parameters \n \ngm \u2013 The GraphModule that wraps the Graph to operate on \npattern \u2013 The subgraph to match in gm for replacement \nreplacement \u2013 The subgraph to replace pattern with   Returns \nA list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as: class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n  Return type \nList[Match]   Examples: import torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2]).sum()\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m). The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph. When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.) One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n with def replacement(x, y):\n    return torch.relu(x)\n In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement. After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this: def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n \n\n"}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "torch.fx", "text": " \nclass torch.fx.Graph [source]\n \nGraph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n Will produce the following Graph: print(gm.graph)\n graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n For the semantics of operations represented in the Graph, please see Node.  \n__init__() [source]\n \nConstruct an empty Graph. \n  \ncall_function(the_function, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be  Parameters \n \nthe_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.    Returns The newly created and inserted call_function node.  Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_method(method_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.  Parameters \n \nmethod_name (str) \u2013 The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly created and inserted call_method node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_module(module_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.  Parameters \n \nmodule_name (str) \u2013 The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should not include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted call_module node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None) [source]\n \nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().  Parameters \n \nop (str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019, \u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are described in the Graph docstring. \nargs (Optional[Tuple[Argument, ..]]) \u2013 is a tuple of arguments to this node. \nkwargs (Optional[Dict[str, Argument]]) \u2013 the kwargs of this Node \nname (Optional[str]) \u2013 an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted node.   \n  \nerase_node(to_erase) [source]\n \nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.  Parameters \nto_erase (Node) \u2013 The Node to erase from the Graph.   \n  \nget_attr(qualified_name, type_expr=None) [source]\n \nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.  Parameters \n \nqualified_name (str) \u2013 the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted get_attr node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \ngraph_copy(g, val_map) [source]\n \nCopy all nodes from a given graph into self.  Parameters \n \ng (Graph) \u2013 The source graph from which to copy Nodes. \nval_map (Dict[Node, Node]) \u2013 a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.   Returns \nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.   \n  \ninserting_after(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \ninserting_before(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \nlint(root=None) [source]\n \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root  Parameters \nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.   \n  \nnode_copy(node, arg_transform=<function Graph.<lambda>>) [source]\n \nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example: # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n  Parameters \n \nnode (Node) \u2013 The node to copy into self. \narg_transform (Callable[[Node], Argument]) \u2013 A function that transforms Node arguments in node\u2019s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.    \n  \nproperty nodes  \nGet the list of Nodes that constitute this Graph. Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.  Returns \nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.   \n  \noutput(result, type_expr=None) [source]\n \nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.  Parameters \n \nresult (Argument) \u2013 The value to be returned. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nplaceholder(name, type_expr=None) [source]\n \nInsert a placeholder node into the Graph. A placeholder represents a function input.  Parameters \n \nname (str) \u2013 A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nprint_tabular() [source]\n \nPrints the intermediate representation of the graph in tabular format. \n  \npython_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.   \n \n"}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "torch.fx", "text": " \ncall_function(the_function, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be  Parameters \n \nthe_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.    Returns The newly created and inserted call_function node.  Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n"}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "torch.fx", "text": " \ncall_method(method_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.  Parameters \n \nmethod_name (str) \u2013 The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly created and inserted call_method node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n"}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "torch.fx", "text": " \ncall_module(module_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.  Parameters \n \nmodule_name (str) \u2013 The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should not include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted call_module node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n"}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "torch.fx", "text": " \ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None) [source]\n \nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().  Parameters \n \nop (str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019, \u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are described in the Graph docstring. \nargs (Optional[Tuple[Argument, ..]]) \u2013 is a tuple of arguments to this node. \nkwargs (Optional[Dict[str, Argument]]) \u2013 the kwargs of this Node \nname (Optional[str]) \u2013 an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted node.   \n"}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "torch.fx", "text": " \nerase_node(to_erase) [source]\n \nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.  Parameters \nto_erase (Node) \u2013 The Node to erase from the Graph.   \n"}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "torch.fx", "text": " \nget_attr(qualified_name, type_expr=None) [source]\n \nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.  Parameters \n \nqualified_name (str) \u2013 the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted get_attr node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n"}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "torch.fx", "text": " \ngraph_copy(g, val_map) [source]\n \nCopy all nodes from a given graph into self.  Parameters \n \ng (Graph) \u2013 The source graph from which to copy Nodes. \nval_map (Dict[Node, Node]) \u2013 a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.   Returns \nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.   \n"}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "torch.fx", "text": " \ninserting_after(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n"}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "torch.fx", "text": " \ninserting_before(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n"}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "torch.fx", "text": " \nlint(root=None) [source]\n \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root  Parameters \nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.   \n"}, {"name": "torch.fx.Graph.nodes()", "path": "fx#torch.fx.Graph.nodes", "type": "torch.fx", "text": " \nproperty nodes  \nGet the list of Nodes that constitute this Graph. Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.  Returns \nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.   \n"}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "torch.fx", "text": " \nnode_copy(node, arg_transform=<function Graph.<lambda>>) [source]\n \nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example: # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n  Parameters \n \nnode (Node) \u2013 The node to copy into self. \narg_transform (Callable[[Node], Argument]) \u2013 A function that transforms Node arguments in node\u2019s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.    \n"}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "torch.fx", "text": " \noutput(result, type_expr=None) [source]\n \nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.  Parameters \n \nresult (Argument) \u2013 The value to be returned. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n"}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "torch.fx", "text": " \nplaceholder(name, type_expr=None) [source]\n \nInsert a placeholder node into the Graph. A placeholder represents a function input.  Parameters \n \nname (str) \u2013 A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n"}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "torch.fx", "text": " \nprint_tabular() [source]\n \nPrints the intermediate representation of the graph in tabular format. \n"}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "torch.fx", "text": " \npython_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.   \n"}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "torch.fx", "text": " \n__init__() [source]\n \nConstruct an empty Graph. \n"}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "torch.fx", "text": " \nclass torch.fx.GraphModule(root, graph, class_name='GraphModule') [source]\n \nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.  Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.   \n__init__(root, graph, class_name='GraphModule') [source]\n \nConstruct a GraphModule.  Parameters \n \nroot (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. \ngraph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation \nname (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform.    \n  \nproperty code  \nReturn the Python code generated from the Graph underlying this GraphModule. \n  \nproperty graph  \nReturn the Graph underlying this GraphModule \n  \nrecompile() [source]\n \nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. \n  \nto_folder(folder, module_name='FxModule') [source]\n \nDumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>  Parameters \n \nfolder (Union[str, os.PathLike]) \u2013 The folder to write the code out to \nmodule_name (str) \u2013 Top-level name to use for the Module while writing out the code    \n \n"}, {"name": "torch.fx.GraphModule.code()", "path": "fx#torch.fx.GraphModule.code", "type": "torch.fx", "text": " \nproperty code  \nReturn the Python code generated from the Graph underlying this GraphModule. \n"}, {"name": "torch.fx.GraphModule.graph()", "path": "fx#torch.fx.GraphModule.graph", "type": "torch.fx", "text": " \nproperty graph  \nReturn the Graph underlying this GraphModule \n"}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "torch.fx", "text": " \nrecompile() [source]\n \nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. \n"}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "torch.fx", "text": " \nto_folder(folder, module_name='FxModule') [source]\n \nDumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>  Parameters \n \nfolder (Union[str, os.PathLike]) \u2013 The folder to write the code out to \nmodule_name (str) \u2013 Top-level name to use for the Module while writing out the code    \n"}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "torch.fx", "text": " \n__init__(root, graph, class_name='GraphModule') [source]\n \nConstruct a GraphModule.  Parameters \n \nroot (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. \ngraph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation \nname (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform.    \n"}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "torch.fx", "text": " \nclass torch.fx.Interpreter(module) [source]\n \nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes. Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy: run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so: class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The module to be executed    \ncall_function(target, args, kwargs) [source]\n \nExecute a call_function node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the function invocation   \n  \ncall_method(target, args, kwargs) [source]\n \nExecute a call_method node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the method invocation   \n  \ncall_module(target, args, kwargs) [source]\n \nExecute a call_module node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the module invocation   \n  \nfetch_args_kwargs_from_env(n) [source]\n \nFetch the concrete values of args and kwargs of node n from the current execution environment.  Parameters \nn (Node) \u2013 The node for which args and kwargs should be fetched.  Returns \nargs and kwargs with concrete values for n.  Return type \nTuple[Tuple, Dict]   \n  \nfetch_attr(target) [source]\n \nFetch an attribute from the Module hierarchy of self.module.  Parameters \ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch  Returns \nThe value of the attribute.  Return type \nAny   \n  \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe value of the attribute that was retrieved  Return type \nAny   \n  \nmap_nodes_to_values(args, n) [source]\n \nRecursively descend through args and look up the concrete value for each Node in the current execution environment.  Parameters \n \nargs (Argument) \u2013 Data structure within which to look up concrete values \nn (Node) \u2013 Node to which args belongs. This is only used for error reporting.    \n  \noutput(target, args, kwargs) [source]\n \nExecute an output node. This really just retrieves the value referenced by the output node and returns it.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe return value referenced by the output node  Return type \nAny   \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe argument value that was retrieved.  Return type \nAny   \n  \nrun(*args, initial_env=None) [source]\n \nRun module via interpretation and return the result.  Parameters \n \n*args \u2013 The arguments to the Module to run, in positional order \ninitial_env (Optional[Dict[Node, Any]]) \u2013 An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.   Returns \nThe value returned from executing the Module  Return type \nAny   \n  \nrun_node(n) [source]\n \nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op  Parameters \nn (Node) \u2013 The Node to execute  Returns \nThe result of executing n  Return type \nAny   \n \n"}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "torch.fx", "text": " \ncall_function(target, args, kwargs) [source]\n \nExecute a call_function node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the function invocation   \n"}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "torch.fx", "text": " \ncall_method(target, args, kwargs) [source]\n \nExecute a call_method node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the method invocation   \n"}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "torch.fx", "text": " \ncall_module(target, args, kwargs) [source]\n \nExecute a call_module node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the module invocation   \n"}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "torch.fx", "text": " \nfetch_args_kwargs_from_env(n) [source]\n \nFetch the concrete values of args and kwargs of node n from the current execution environment.  Parameters \nn (Node) \u2013 The node for which args and kwargs should be fetched.  Returns \nargs and kwargs with concrete values for n.  Return type \nTuple[Tuple, Dict]   \n"}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "torch.fx", "text": " \nfetch_attr(target) [source]\n \nFetch an attribute from the Module hierarchy of self.module.  Parameters \ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch  Returns \nThe value of the attribute.  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "torch.fx", "text": " \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe value of the attribute that was retrieved  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "torch.fx", "text": " \nmap_nodes_to_values(args, n) [source]\n \nRecursively descend through args and look up the concrete value for each Node in the current execution environment.  Parameters \n \nargs (Argument) \u2013 Data structure within which to look up concrete values \nn (Node) \u2013 Node to which args belongs. This is only used for error reporting.    \n"}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "torch.fx", "text": " \noutput(target, args, kwargs) [source]\n \nExecute an output node. This really just retrieves the value referenced by the output node and returns it.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe return value referenced by the output node  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "torch.fx", "text": " \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe argument value that was retrieved.  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "torch.fx", "text": " \nrun(*args, initial_env=None) [source]\n \nRun module via interpretation and return the result.  Parameters \n \n*args \u2013 The arguments to the Module to run, in positional order \ninitial_env (Optional[Dict[Node, Any]]) \u2013 An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.   Returns \nThe value returned from executing the Module  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "torch.fx", "text": " \nrun_node(n) [source]\n \nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op  Parameters \nn (Node) \u2013 The Node to execute  Returns \nThe result of executing n  Return type \nAny   \n"}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "torch.fx", "text": " \nclass torch.fx.Node(graph, name, op, target, args, kwargs, type=None) [source]\n \nNode is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:  \nplaceholder represents a function input. The name attribute specifies the name this value will take on. target is similarly the name of the argument. args holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. kwargs is don\u2019t-care. Placeholders correspond to the function parameters (e.g. x) in the graph printout. \nget_attr retrieves a parameter from the module hierarchy. name is similarly the name the result of the fetch is assigned to. target is the fully-qualified name of the parameter\u2019s position in the module hierarchy. args and kwargs are don\u2019t-care \ncall_function applies a free function to some values. name is similarly the name of the value to assign to. target is the function to be applied. args and kwargs represent the arguments to the function, following the Python calling convention \ncall_module applies a module in the module hierarchy\u2019s forward() method to given arguments. name is as previous. target is the fully-qualified name of the module in the module hierarchy to call. args and kwargs represent the arguments to invoke the module on, including the self argument. \ncall_method calls a method on a value. name is as similar. target is the string name of the method to apply to the self argument. args and kwargs represent the arguments to invoke the module on, including the self argument\n \noutput contains the output of the traced function in its args[0] attribute. This corresponds to the \u201creturn\u201d statement in the Graph printout.   \nproperty all_input_nodes  \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.  Returns \nList of Nodes that appear in the args and kwargs of this Node, in that order.   \n  \nappend(x) [source]\n \nInsert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)  Parameters \nx (Node) \u2013 The node to put after this node. Must be a member of the same graph.   \n  \nproperty args  \nThe tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty kwargs  \nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty next  \nReturns the next Node in the linked list of Nodes.  Returns \nThe next Node in the linked list of Nodes.   \n  \nprepend(x) [source]\n \nInsert x before this node in the list of nodes in the graph. Example: Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n  Parameters \nx (Node) \u2013 The node to put before this node. Must be a member of the same graph.   \n  \nproperty prev  \nReturns the previous Node in the linked list of Nodes.  Returns \nThe previous Node in the linked list of Nodes.   \n  \nreplace_all_uses_with(replace_with) [source]\n \nReplace all uses of self in the Graph with the Node replace_with.  Parameters \nreplace_with (Node) \u2013 The node to replace all uses of self with.  Returns \nThe list of Nodes on which this change was made.   \n \n"}, {"name": "torch.fx.Node.all_input_nodes()", "path": "fx#torch.fx.Node.all_input_nodes", "type": "torch.fx", "text": " \nproperty all_input_nodes  \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.  Returns \nList of Nodes that appear in the args and kwargs of this Node, in that order.   \n"}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "torch.fx", "text": " \nappend(x) [source]\n \nInsert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)  Parameters \nx (Node) \u2013 The node to put after this node. Must be a member of the same graph.   \n"}, {"name": "torch.fx.Node.args()", "path": "fx#torch.fx.Node.args", "type": "torch.fx", "text": " \nproperty args  \nThe tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n"}, {"name": "torch.fx.Node.kwargs()", "path": "fx#torch.fx.Node.kwargs", "type": "torch.fx", "text": " \nproperty kwargs  \nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n"}, {"name": "torch.fx.Node.next()", "path": "fx#torch.fx.Node.next", "type": "torch.fx", "text": " \nproperty next  \nReturns the next Node in the linked list of Nodes.  Returns \nThe next Node in the linked list of Nodes.   \n"}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "torch.fx", "text": " \nprepend(x) [source]\n \nInsert x before this node in the list of nodes in the graph. Example: Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n  Parameters \nx (Node) \u2013 The node to put before this node. Must be a member of the same graph.   \n"}, {"name": "torch.fx.Node.prev()", "path": "fx#torch.fx.Node.prev", "type": "torch.fx", "text": " \nproperty prev  \nReturns the previous Node in the linked list of Nodes.  Returns \nThe previous Node in the linked list of Nodes.   \n"}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "torch.fx", "text": " \nreplace_all_uses_with(replace_with) [source]\n \nReplace all uses of self in the Graph with the Node replace_with.  Parameters \nreplace_with (Node) \u2013 The node to replace all uses of self with.  Returns \nThe list of Nodes on which this change was made.   \n"}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "torch.fx", "text": " \nclass torch.fx.Proxy(node, tracer=None) [source]\n \nProxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph. If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph. \n"}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "torch.fx", "text": " \ntorch.fx.replace_pattern(gm, pattern, replacement) [source]\n \nMatches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).  Parameters \n \ngm \u2013 The GraphModule that wraps the Graph to operate on \npattern \u2013 The subgraph to match in gm for replacement \nreplacement \u2013 The subgraph to replace pattern with   Returns \nA list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as: class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n  Return type \nList[Match]   Examples: import torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2]).sum()\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m). The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph. When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.) One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n with def replacement(x, y):\n    return torch.relu(x)\n In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement. After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this: def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n \n"}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "torch.fx", "text": " \ntorch.fx.symbolic_trace(root, concrete_args=None) [source]\n \nSymbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.  Parameters \n \nroot (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted into a Graph representation. \nconcrete_args (Optional[Dict[str, any]]) \u2013 Concrete arguments that should not be treated as Proxies.   Returns \na Module created from the recorded operations from root.  Return type \nGraphModule   \n"}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "torch.fx", "text": " \nclass torch.fx.Tracer(autowrap_modules=(<module 'math' from '/home/matti/miniconda3/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so'>, )) [source]\n \nTracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m). Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.  \ncall_module(m, forward, args, kwargs) [source]\n \nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance. By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function. This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.  Parameters \n \nm (Module) \u2013 The module for which a call is being emitted \nforward (Callable) \u2013 The forward() method of the Module to be invoked \nargs (Tuple) \u2013 args of the module callsite \nkwargs (Dict) \u2013 kwargs of the module callsite   Returns \nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.   \n  \ncreate_arg(a) [source]\n \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph. By default, the behavior includes:  Iterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements. Given a Proxy object, return a reference to the underlying IR Node\n \nGiven a non-Proxy Tensor object, emit IR for various cases:  For a Parameter, emit a get_attr node referring to that Parameter For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.    This method can be overridden to support more types.  Parameters \na (Any) \u2013 The value to be emitted as an Argument in the Graph.  Returns \nThe value a converted into the appropriate Argument   \n  \ncreate_args_for_root(root_fn, is_module, concrete_args=None) [source]\n \nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs. \n  \nis_leaf_module(m, module_qualified_name) [source]\n \nA method to specify whether a given nn.Module is a \u201cleaf\u201d module. Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.  Parameters \n \nm (Module) \u2013 The module being queried about \nmodule_qualified_name (str) \u2013 The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.    \n  \npath_of_module(mod) [source]\n \nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.  Parameters \nmod (str) \u2013 The Module to retrieve the qualified name for.   \n  \ntrace(root, concrete_args=None) [source]\n \nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable. Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.  Parameters \nroot (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.  Returns \nA Graph representing the semantics of the passed-in root.   \n \n"}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "torch.fx", "text": " \ncall_module(m, forward, args, kwargs) [source]\n \nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance. By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function. This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.  Parameters \n \nm (Module) \u2013 The module for which a call is being emitted \nforward (Callable) \u2013 The forward() method of the Module to be invoked \nargs (Tuple) \u2013 args of the module callsite \nkwargs (Dict) \u2013 kwargs of the module callsite   Returns \nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.   \n"}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "torch.fx", "text": " \ncreate_arg(a) [source]\n \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph. By default, the behavior includes:  Iterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements. Given a Proxy object, return a reference to the underlying IR Node\n \nGiven a non-Proxy Tensor object, emit IR for various cases:  For a Parameter, emit a get_attr node referring to that Parameter For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.    This method can be overridden to support more types.  Parameters \na (Any) \u2013 The value to be emitted as an Argument in the Graph.  Returns \nThe value a converted into the appropriate Argument   \n"}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "torch.fx", "text": " \ncreate_args_for_root(root_fn, is_module, concrete_args=None) [source]\n \nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs. \n"}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "torch.fx", "text": " \nis_leaf_module(m, module_qualified_name) [source]\n \nA method to specify whether a given nn.Module is a \u201cleaf\u201d module. Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.  Parameters \n \nm (Module) \u2013 The module being queried about \nmodule_qualified_name (str) \u2013 The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.    \n"}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "torch.fx", "text": " \npath_of_module(mod) [source]\n \nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.  Parameters \nmod (str) \u2013 The Module to retrieve the qualified name for.   \n"}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "torch.fx", "text": " \ntrace(root, concrete_args=None) [source]\n \nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable. Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.  Parameters \nroot (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.  Returns \nA Graph representing the semantics of the passed-in root.   \n"}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "torch.fx", "text": " \nclass torch.fx.Transformer(module) [source]\n \nTransformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically. Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so: class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The Module to be transformed.    \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \ntransform() [source]\n \nTransform self.module and return the transformed GraphModule. \n \n"}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "torch.fx", "text": " \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n"}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "torch.fx", "text": " \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n"}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "torch.fx", "text": " \ntransform() [source]\n \nTransform self.module and return the transformed GraphModule. \n"}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "torch.fx", "text": " \ntorch.fx.wrap(fn_or_name) [source]\n \nThis function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through: # foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n This function can also equivalently be used as a decorator: # foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.  Parameters \nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called   \n"}, {"name": "torch.gather()", "path": "generated/torch.gather#torch.gather", "type": "torch", "text": " \ntorch.gather(input, dim, index, *, sparse_grad=False, out=None) \u2192 Tensor  \nGathers values along an axis specified by dim. For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n input and index must have the same number of dimensions. It is also required that index.size(d) <= input.size(d) for all dimensions d != dim. out will have the same shape as index. Note that input and index do not broadcast against each other.  Parameters \n \ninput (Tensor) \u2013 the source tensor \ndim (int) \u2013 the axis along which to index \nindex (LongTensor) \u2013 the indices of elements to gather   Keyword Arguments \n \nsparse_grad (bool, optional) \u2013 If True, gradient w.r.t. input will be a sparse tensor. \nout (Tensor, optional) \u2013 the destination tensor    Example: >>> t = torch.tensor([[1, 2], [3, 4]])\n>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\ntensor([[ 1,  1],\n        [ 4,  3]])\n \n"}, {"name": "torch.gcd()", "path": "generated/torch.gcd#torch.gcd", "type": "torch", "text": " \ntorch.gcd(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise greatest common divisor (GCD) of input and other. Both input and other must have integer types.  Note This defines gcd(0,0)=0gcd(0, 0) = 0 .   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.gcd(a, b)\ntensor([1, 2, 5])\n>>> c = torch.tensor([3])\n>>> torch.gcd(a, c)\ntensor([1, 1, 3])\n \n"}, {"name": "torch.ge()", "path": "generated/torch.ge#torch.ge", "type": "torch", "text": " \ntorch.ge(input, other, *, out=None) \u2192 Tensor  \nComputes input\u2265other\\text{input} \\geq \\text{other}  element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or float) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is greater than or equal to other and False elsewhere   Example: >>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, True], [False, True]])\n \n"}, {"name": "torch.Generator", "path": "generated/torch.generator#torch.Generator", "type": "torch", "text": " \nclass torch.Generator(device='cpu') \u2192 Generator  \nCreates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many In-place random sampling functions.  Parameters \ndevice (torch.device, optional) \u2013 the desired device for the generator.  Returns \nAn torch.Generator object.  Return type \nGenerator   Example: >>> g_cpu = torch.Generator()\n>>> g_cuda = torch.Generator(device='cuda')\n  \ndevice  \nGenerator.device -> device Gets the current device of the generator. Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.device\ndevice(type='cpu')\n \n  \nget_state() \u2192 Tensor  \nReturns the Generator state as a torch.ByteTensor.  Returns \nA torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.  Return type \nTensor   Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()\n \n  \ninitial_seed() \u2192 int  \nReturns the initial seed for generating random numbers. Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647\n \n  \nmanual_seed(seed) \u2192 Generator  \nSets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.  Parameters \nseed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.  Returns \nAn torch.Generator object.  Return type \nGenerator   Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.manual_seed(2147483647)\n \n  \nseed() \u2192 int  \nGets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator. Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.seed()\n1516516984916\n \n  \nset_state(new_state) \u2192 void  \nSets the Generator state.  Parameters \nnew_state (torch.ByteTensor) \u2013 The desired state.   Example: >>> g_cpu = torch.Generator()\n>>> g_cpu_other = torch.Generator()\n>>> g_cpu.set_state(g_cpu_other.get_state())\n \n \n"}, {"name": "torch.Generator.device", "path": "generated/torch.generator#torch.Generator.device", "type": "torch", "text": " \ndevice  \nGenerator.device -> device Gets the current device of the generator. Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.device\ndevice(type='cpu')\n \n"}, {"name": "torch.Generator.get_state()", "path": "generated/torch.generator#torch.Generator.get_state", "type": "torch", "text": " \nget_state() \u2192 Tensor  \nReturns the Generator state as a torch.ByteTensor.  Returns \nA torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.  Return type \nTensor   Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()\n \n"}, {"name": "torch.Generator.initial_seed()", "path": "generated/torch.generator#torch.Generator.initial_seed", "type": "torch", "text": " \ninitial_seed() \u2192 int  \nReturns the initial seed for generating random numbers. Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647\n \n"}, {"name": "torch.Generator.manual_seed()", "path": "generated/torch.generator#torch.Generator.manual_seed", "type": "torch", "text": " \nmanual_seed(seed) \u2192 Generator  \nSets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.  Parameters \nseed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.  Returns \nAn torch.Generator object.  Return type \nGenerator   Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.manual_seed(2147483647)\n \n"}, {"name": "torch.Generator.seed()", "path": "generated/torch.generator#torch.Generator.seed", "type": "torch", "text": " \nseed() \u2192 int  \nGets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator. Example: >>> g_cpu = torch.Generator()\n>>> g_cpu.seed()\n1516516984916\n \n"}, {"name": "torch.Generator.set_state()", "path": "generated/torch.generator#torch.Generator.set_state", "type": "torch", "text": " \nset_state(new_state) \u2192 void  \nSets the Generator state.  Parameters \nnew_state (torch.ByteTensor) \u2013 The desired state.   Example: >>> g_cpu = torch.Generator()\n>>> g_cpu_other = torch.Generator()\n>>> g_cpu.set_state(g_cpu_other.get_state())\n \n"}, {"name": "torch.geqrf()", "path": "generated/torch.geqrf#torch.geqrf", "type": "torch", "text": " \ntorch.geqrf(input, *, out=None) -> (Tensor, Tensor)  \nThis is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf . You\u2019ll generally want to use torch.qr() instead. Computes a QR decomposition of input, but without constructing QQ  and RR  as explicit separate matrices. Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of \u2018elementary reflectors\u2019. See LAPACK documentation for geqrf for further details.  Parameters \ninput (Tensor) \u2013 the input matrix  Keyword Arguments \nout (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)   \n"}, {"name": "torch.ger()", "path": "generated/torch.ger#torch.ger", "type": "torch", "text": " \ntorch.ger(input, vec2, *, out=None) \u2192 Tensor  \nAlias of torch.outer().  Warning This function is deprecated and will be removed in a future PyTorch release. Use torch.outer() instead.  \n"}, {"name": "torch.get_default_dtype()", "path": "generated/torch.get_default_dtype#torch.get_default_dtype", "type": "torch", "text": " \ntorch.get_default_dtype() \u2192 torch.dtype  \nGet the current default floating point torch.dtype. Example: >>> torch.get_default_dtype()  # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.get_default_dtype()  # default is now changed to torch.float64\ntorch.float64\n>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\n>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\ntorch.float32\n \n"}, {"name": "torch.get_num_interop_threads()", "path": "generated/torch.get_num_interop_threads#torch.get_num_interop_threads", "type": "torch", "text": " \ntorch.get_num_interop_threads() \u2192 int  \nReturns the number of threads used for inter-op parallelism on CPU (e.g. in JIT interpreter) \n"}, {"name": "torch.get_num_threads()", "path": "generated/torch.get_num_threads#torch.get_num_threads", "type": "torch", "text": " \ntorch.get_num_threads() \u2192 int  \nReturns the number of threads used for parallelizing CPU operations \n"}, {"name": "torch.get_rng_state()", "path": "generated/torch.get_rng_state#torch.get_rng_state", "type": "torch", "text": " \ntorch.get_rng_state() [source]\n \nReturns the random number generator state as a torch.ByteTensor. \n"}, {"name": "torch.greater()", "path": "generated/torch.greater#torch.greater", "type": "torch", "text": " \ntorch.greater(input, other, *, out=None) \u2192 Tensor  \nAlias for torch.gt(). \n"}, {"name": "torch.greater_equal()", "path": "generated/torch.greater_equal#torch.greater_equal", "type": "torch", "text": " \ntorch.greater_equal(input, other, *, out=None) \u2192 Tensor  \nAlias for torch.ge(). \n"}, {"name": "torch.gt()", "path": "generated/torch.gt#torch.gt", "type": "torch", "text": " \ntorch.gt(input, other, *, out=None) \u2192 Tensor  \nComputes input>other\\text{input} > \\text{other}  element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or float) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is greater than other and False elsewhere   Example: >>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [False, False]])\n \n"}, {"name": "torch.hamming_window()", "path": "generated/torch.hamming_window#torch.hamming_window", "type": "torch", "text": " \ntorch.hamming_window(window_length, periodic=True, alpha=0.54, beta=0.46, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nHamming window function.  w[n]=\u03b1\u2212\u03b2cos\u2061(2\u03c0nN\u22121),w[n] = \\alpha - \\beta\\ \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right),  \nwhere NN  is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.hamming_window(L, periodic=True) equal to torch.hamming_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1 , the returned window contains a single value 1.   Note This is a generalized version of torch.hann_window().   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window. \nalpha (float, optional) \u2013 The coefficient \u03b1\\alpha  in the equation above \nbeta (float, optional) \u2013 The coefficient \u03b2\\beta  in the equation above   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window  Return type \nTensor   \n"}, {"name": "torch.hann_window()", "path": "generated/torch.hann_window#torch.hann_window", "type": "torch", "text": " \ntorch.hann_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nHann window function.  w[n]=12[1\u2212cos\u2061(2\u03c0nN\u22121)]=sin\u20612(\u03c0nN\u22121),w[n] = \\frac{1}{2}\\ \\left[1 - \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right)\\right] = \\sin^2 \\left( \\frac{\\pi n}{N - 1} \\right),  \nwhere NN  is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.hann_window(L, periodic=True) equal to torch.hann_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1 , the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window  Return type \nTensor   \n"}, {"name": "torch.heaviside()", "path": "generated/torch.heaviside#torch.heaviside", "type": "torch", "text": " \ntorch.heaviside(input, values, *, out=None) \u2192 Tensor  \nComputes the Heaviside step function for each element in input. The Heaviside step function is defined as:  heaviside(input,values)={0,if input < 0values,if input == 01,if input > 0\\text{{heaviside}}(input, values) = \\begin{cases} 0, & \\text{if input < 0}\\\\ values, & \\text{if input == 0}\\\\ 1, & \\text{if input > 0} \\end{cases}  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \nvalues (Tensor) \u2013 The values to use where input is zero.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> input = torch.tensor([-1.5, 0, 2.0])\n>>> values = torch.tensor([0.5])\n>>> torch.heaviside(input, values)\ntensor([0.0000, 0.5000, 1.0000])\n>>> values = torch.tensor([1.2, -2.0, 3.5])\n>>> torch.heaviside(input, values)\ntensor([0., -2., 1.])\n \n"}, {"name": "torch.histc()", "path": "generated/torch.histc#torch.histc", "type": "torch", "text": " \ntorch.histc(input, bins=100, min=0, max=0, *, out=None) \u2192 Tensor  \nComputes the histogram of a tensor. The elements are sorted into equal width bins between min and max. If min and max are both zero, the minimum and maximum values of the data are used. Elements lower than min and higher than max are ignored.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nbins (int) \u2013 number of histogram bins \nmin (int) \u2013 lower end of the range (inclusive) \nmax (int) \u2013 upper end of the range (inclusive)   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nHistogram represented as a tensor  Return type \nTensor   Example: >>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])\n \n"}, {"name": "torch.hspmm()", "path": "sparse#torch.hspmm", "type": "torch.sparse", "text": " \ntorch.hspmm(mat1, mat2, *, out=None) \u2192 Tensor  \nPerforms a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The result is a (1 + 1)-dimensional hybrid COO matrix.  Parameters \n \nmat1 (Tensor) \u2013 the first sparse matrix to be matrix multiplied \nmat2 (Tensor) \u2013 the second strided matrix to be matrix multiplied   Keyword Arguments \n{out} \u2013    \n"}, {"name": "torch.hstack()", "path": "generated/torch.hstack#torch.hstack", "type": "torch", "text": " \ntorch.hstack(tensors, *, out=None) \u2192 Tensor  \nStack tensors in sequence horizontally (column wise). This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.hstack((a,b))\ntensor([1, 2, 3, 4, 5, 6])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.hstack((a,b))\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n \n"}, {"name": "torch.hub", "path": "hub", "type": "torch.hub", "text": "torch.hub Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility. Publishing models Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple hubconf.py file; hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish). def entrypoint_name(*args, **kwargs):\n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    ...\n How to implement an entrypoint? Here is a code snippet specifies an entrypoint for resnet18 model if we expand the implementation in pytorch/vision/hubconf.py. In most case importing the right function in hubconf.py is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in pytorch/vision repo dependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model\n  \ndependencies variable is a list of package names required to load the model. Note this might be slightly different from dependencies required for training a model. \nargs and kwargs are passed along to the real callable function. Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here. Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers. Callables prefixed with underscore are considered as helper functions which won\u2019t show up in torch.hub.list(). Pretrained weights can either be stored locally in the github repo, or loadable by torch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to a project release and use the url from the release. In the example above torchvision.models.resnet.resnet18 handles pretrained, alternatively you can put the following logic in the entrypoint definition.  if pretrained:\n    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n Important Notice  The published models should be at least in a branch/tag. It can\u2019t be a random commit.  Loading models from Hub Pytorch Hub provides convenient APIs to explore all available models in hub through torch.hub.list(), show docstring and examples through torch.hub.help() and load the pre-trained models using torch.hub.load().  \ntorch.hub.list(github, force_reload=False) [source]\n \nList all entrypoints available in github hubconf.  Parameters \n \ngithub (string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 \nforce_reload (bool, optional) \u2013 whether to discard the existing cache and force a fresh download. Default is False.   Returns \na list of available entrypoint names  Return type \nentrypoints   Example >>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n \n  \ntorch.hub.help(github, model, force_reload=False) [source]\n \nShow the docstring of entrypoint model.  Parameters \n \ngithub (string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 \nmodel (string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py \nforce_reload (bool, optional) \u2013 whether to discard the existing cache and force a fresh download. Default is False.    Example >>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n \n  \ntorch.hub.load(repo_or_dir, model, *args, **kwargs) [source]\n \nLoad a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc. If source is 'github', repo_or_dir is expected to be of the form repo_owner/repo_name[:tag_name] with an optional tag/branch. If source is 'local', repo_or_dir is expected to be a path to a local directory.  Parameters \n \nrepo_or_dir (string) \u2013 repo name (repo_owner/repo_name[:tag_name]), if source = 'github'; or a path to a local directory, if source = 'local'. \nmodel (string) \u2013 the name of a callable (entrypoint) defined in the repo/dir\u2019s hubconf.py. \n*args (optional) \u2013 the corresponding args for callable model. \nsource (string, optional) \u2013 'github' | 'local'. Specifies how repo_or_dir is to be interpreted. Default is 'github'. \nforce_reload (bool, optional) \u2013 whether to force a fresh download of the github repo unconditionally. Does not have any effect if source = 'local'. Default is False. \nverbose (bool, optional) \u2013 If False, mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if source = 'local'. Default is True. \n**kwargs (optional) \u2013 the corresponding kwargs for callable model.   Returns \nThe output of the model callable when called with the given *args and **kwargs.   Example >>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)\n \n  \ntorch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True) [source]\n \nDownload object at the given URL to a local path.  Parameters \n \nurl (string) \u2013 URL of the object to download \ndst (string) \u2013 Full path where object will be saved, e.g. /tmp/temporary_file\n \nhash_prefix (string, optional) \u2013 If not None, the SHA256 downloaded file should start with hash_prefix. Default: None \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr Default: True    Example >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n \n  \ntorch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None) [source]\n \nLoads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().  Parameters \n \nurl (string) \u2013 URL of the object to download \nmodel_dir (string, optional) \u2013 directory in which to save the object \nmap_location (optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr. Default: True \ncheck_hash (bool, optional) \u2013 If True, the filename part of the URL should follow the naming convention filename-<sha256>.ext where <sha256> is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False \nfile_name (string, optional) \u2013 name for the downloaded file. Filename from url will be used if not set.    Example >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n \n Running a loaded model: Note that *args and **kwargs in torch.hub.load() are used to instantiate a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is  \ndir(model) to see all available methods of the model. \nhelp(model.foo) to check what arguments model.foo takes to run  To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It\u2019s also helpful to include a minimal working example. Where are my downloaded models saved? The locations are used in the order of  Calling hub.set_dir(<PATH_TO_HUB_DIR>)\n \n$TORCH_HOME/hub, if environment variable TORCH_HOME is set. \n$XDG_CACHE_HOME/torch/hub, if environment variable XDG_CACHE_HOME is set. ~/.cache/torch/hub   \ntorch.hub.get_dir() [source]\n \nGet the Torch Hub cache directory used for storing downloaded models & weights. If set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set. \n  \ntorch.hub.set_dir(d) [source]\n \nOptionally set the Torch Hub directory used to save downloaded models & weights.  Parameters \nd (string) \u2013 path to a local folder to save downloaded models & weights.   \n Caching logic By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by get_dir(). Users can force a reload by calling hub.load(..., force_reload=True). This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release. Known limitations: Torch hub works by importing the package as if it was installed. There\u2019re some side effects introduced by importing in Python. For example, you can see new items in Python caches sys.modules and sys.path_importer_cache which is normal Python behavior. A known limitation that worth mentioning here is user CANNOT load two different branches of the same repo in the same python process. It\u2019s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it\u2019s totally fine to load them in separate processes.\n"}, {"name": "torch.hub.download_url_to_file()", "path": "hub#torch.hub.download_url_to_file", "type": "torch.hub", "text": " \ntorch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True) [source]\n \nDownload object at the given URL to a local path.  Parameters \n \nurl (string) \u2013 URL of the object to download \ndst (string) \u2013 Full path where object will be saved, e.g. /tmp/temporary_file\n \nhash_prefix (string, optional) \u2013 If not None, the SHA256 downloaded file should start with hash_prefix. Default: None \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr Default: True    Example >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n \n"}, {"name": "torch.hub.get_dir()", "path": "hub#torch.hub.get_dir", "type": "torch.hub", "text": " \ntorch.hub.get_dir() [source]\n \nGet the Torch Hub cache directory used for storing downloaded models & weights. If set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set. \n"}, {"name": "torch.hub.help()", "path": "hub#torch.hub.help", "type": "torch.hub", "text": " \ntorch.hub.help(github, model, force_reload=False) [source]\n \nShow the docstring of entrypoint model.  Parameters \n \ngithub (string) \u2013 a string with format <repo_owner/repo_name[:tag_name]> with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 \nmodel (string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py \nforce_reload (bool, optional) \u2013 whether to discard the existing cache and force a fresh download. Default is False.    Example >>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n \n"}, {"name": "torch.hub.list()", "path": "hub#torch.hub.list", "type": "torch.hub", "text": " \ntorch.hub.list(github, force_reload=False) [source]\n \nList all entrypoints available in github hubconf.  Parameters \n \ngithub (string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 \nforce_reload (bool, optional) \u2013 whether to discard the existing cache and force a fresh download. Default is False.   Returns \na list of available entrypoint names  Return type \nentrypoints   Example >>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n \n"}, {"name": "torch.hub.load()", "path": "hub#torch.hub.load", "type": "torch.hub", "text": " \ntorch.hub.load(repo_or_dir, model, *args, **kwargs) [source]\n \nLoad a model from a github repo or a local directory. Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc. If source is 'github', repo_or_dir is expected to be of the form repo_owner/repo_name[:tag_name] with an optional tag/branch. If source is 'local', repo_or_dir is expected to be a path to a local directory.  Parameters \n \nrepo_or_dir (string) \u2013 repo name (repo_owner/repo_name[:tag_name]), if source = 'github'; or a path to a local directory, if source = 'local'. \nmodel (string) \u2013 the name of a callable (entrypoint) defined in the repo/dir\u2019s hubconf.py. \n*args (optional) \u2013 the corresponding args for callable model. \nsource (string, optional) \u2013 'github' | 'local'. Specifies how repo_or_dir is to be interpreted. Default is 'github'. \nforce_reload (bool, optional) \u2013 whether to force a fresh download of the github repo unconditionally. Does not have any effect if source = 'local'. Default is False. \nverbose (bool, optional) \u2013 If False, mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if source = 'local'. Default is True. \n**kwargs (optional) \u2013 the corresponding kwargs for callable model.   Returns \nThe output of the model callable when called with the given *args and **kwargs.   Example >>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)\n \n"}, {"name": "torch.hub.load_state_dict_from_url()", "path": "hub#torch.hub.load_state_dict_from_url", "type": "torch.hub", "text": " \ntorch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None) [source]\n \nLoads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().  Parameters \n \nurl (string) \u2013 URL of the object to download \nmodel_dir (string, optional) \u2013 directory in which to save the object \nmap_location (optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr. Default: True \ncheck_hash (bool, optional) \u2013 If True, the filename part of the URL should follow the naming convention filename-<sha256>.ext where <sha256> is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False \nfile_name (string, optional) \u2013 name for the downloaded file. Filename from url will be used if not set.    Example >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n \n"}, {"name": "torch.hub.set_dir()", "path": "hub#torch.hub.set_dir", "type": "torch.hub", "text": " \ntorch.hub.set_dir(d) [source]\n \nOptionally set the Torch Hub directory used to save downloaded models & weights.  Parameters \nd (string) \u2013 path to a local folder to save downloaded models & weights.   \n"}, {"name": "torch.hypot()", "path": "generated/torch.hypot#torch.hypot", "type": "torch", "text": " \ntorch.hypot(input, other, *, out=None) \u2192 Tensor  \nGiven the legs of a right triangle, return its hypotenuse.  outi=inputi2+otheri2\\text{out}_{i} = \\sqrt{\\text{input}_{i}^{2} + \\text{other}_{i}^{2}}  \nThe shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))\ntensor([5.0000, 5.6569, 6.4031])\n \n"}, {"name": "torch.i0()", "path": "generated/torch.i0#torch.i0", "type": "torch", "text": " \ntorch.i0(input, *, out=None) \u2192 Tensor  \nComputes the zeroth order modified Bessel function of the first kind for each element of input.  outi=I0(inputi)=\u2211k=0\u221e(inputi2/4)k(k!)2\\text{out}_{i} = I_0(\\text{input}_{i}) = \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2}  \n Parameters \ninput (Tensor) \u2013 the input tensor  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\n \n"}, {"name": "torch.igamma()", "path": "generated/torch.igamma#torch.igamma", "type": "torch", "text": " \ntorch.igamma(input, other, *, out=None) \u2192 Tensor  \nComputes the regularized lower incomplete gamma function:  outi=1\u0393(inputi)\u222b0otheritinputi\u22121e\u2212tdt\\text{out}_{i} = \\frac{1}{\\Gamma(\\text{input}_i)} \\int_0^{\\text{other}_i} t^{\\text{input}_i-1} e^{-t} dt  \nwhere both inputi\\text{input}_i  and otheri\\text{other}_i  are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot)  in the equation above is the gamma function,  \u0393(inputi)=\u222b0\u221et(inputi\u22121)e\u2212tdt.\\Gamma(\\text{input}_i) = \\int_0^\\infty t^{(\\text{input}_i-1)} e^{-t} dt.  \nSee torch.igammac() and torch.lgamma() for related functions. Supports broadcasting to a common shape and float inputs.  Note The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.   Parameters \n \ninput (Tensor) \u2013 the first non-negative input tensor \nother (Tensor) \u2013 the second non-negative input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])\n \n"}, {"name": "torch.igammac()", "path": "generated/torch.igammac#torch.igammac", "type": "torch", "text": " \ntorch.igammac(input, other, *, out=None) \u2192 Tensor  \nComputes the regularized upper incomplete gamma function:  outi=1\u0393(inputi)\u222botheri\u221etinputi\u22121e\u2212tdt\\text{out}_{i} = \\frac{1}{\\Gamma(\\text{input}_i)} \\int_{\\text{other}_i}^{\\infty} t^{\\text{input}_i-1} e^{-t} dt  \nwhere both inputi\\text{input}_i  and otheri\\text{other}_i  are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot)  in the equation above is the gamma function,  \u0393(inputi)=\u222b0\u221et(inputi\u22121)e\u2212tdt.\\Gamma(\\text{input}_i) = \\int_0^\\infty t^{(\\text{input}_i-1)} e^{-t} dt.  \nSee torch.igamma() and torch.lgamma() for related functions. Supports broadcasting to a common shape and float inputs.  Note The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.   Parameters \n \ninput (Tensor) \u2013 the first non-negative input tensor \nother (Tensor) \u2013 the second non-negative input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])\n \n"}, {"name": "torch.imag()", "path": "generated/torch.imag#torch.imag", "type": "torch", "text": " \ntorch.imag(input) \u2192 Tensor  \nReturns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.  Warning imag() is only supported for tensors with complex dtypes.   Parameters \ninput (Tensor) \u2013 the input tensor.    Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n   \n"}, {"name": "torch.index_select()", "path": "generated/torch.index_select#torch.index_select", "type": "torch", "text": " \ntorch.index_select(input, dim, index, *, out=None) \u2192 Tensor  \nReturns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor. The returned tensor has the same number of dimensions as the original tensor (input). The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor.  Note The returned tensor does not use the same storage as the original tensor. If out has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension in which we index \nindex (IntTensor or LongTensor) \u2013 the 1-D tensor containing the indices to index   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-0.4664,  0.2647, -0.1228, -1.1068],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> indices = torch.tensor([0, 2])\n>>> torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n        [-0.4664, -0.1228],\n        [-1.1734,  0.7230]])\n \n"}, {"name": "torch.initial_seed()", "path": "generated/torch.initial_seed#torch.initial_seed", "type": "torch", "text": " \ntorch.initial_seed() [source]\n \nReturns the initial seed for generating random numbers as a Python long. \n"}, {"name": "torch.inner()", "path": "generated/torch.inner#torch.inner", "type": "torch", "text": " \ntorch.inner(input, other, *, out=None) \u2192 Tensor  \nComputes the dot product for 1D tensors. For higher dimensions, sums the product of elements from input and other along their last dimension.  Note If either input or other is a scalar, the result is equivalent to torch.mul(input, other). If both input and other are non-scalars, the size of their last dimension must match and the result is equivalent to torch.tensordot(input, other, dims=([-1], [-1]))   Parameters \n \ninput (Tensor) \u2013 First input tensor \nother (Tensor) \u2013 Second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 Optional output tensor to write result into. The output shape is input.shape[:-1] + other.shape[:-1].   Example: # Dot product\n>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))\ntensor(7)\n\n# Multidimensional input tensors\n>>> a = torch.randn(2, 3)\n>>> a\ntensor([[0.8173, 1.0874, 1.1784],\n        [0.3279, 0.1234, 2.7894]])\n>>> b = torch.randn(2, 4, 3)\n>>> b\ntensor([[[-0.4682, -0.7159,  0.1506],\n        [ 0.4034, -0.3657,  1.0387],\n        [ 0.9892, -0.6684,  0.1774],\n        [ 0.9482,  1.3261,  0.3917]],\n\n        [[ 0.4537,  0.7493,  1.1724],\n        [ 0.2291,  0.5749, -0.2267],\n        [-0.7920,  0.3607, -0.3701],\n        [ 1.3666, -0.5850, -1.7242]]])\n>>> torch.inner(a, b)\ntensor([[[-0.9837,  1.1560,  0.2907,  2.6785],\n        [ 2.5671,  0.5452, -0.6912, -1.5509]],\n\n        [[ 0.1782,  2.9843,  0.7366,  1.5672],\n        [ 3.5115, -0.4864, -1.2476, -4.4337]]])\n\n# Scalar input\n>>> torch.inner(a, torch.tensor(2))\ntensor([[1.6347, 2.1748, 2.3567],\n        [0.6558, 0.2469, 5.5787]])\n \n"}, {"name": "torch.inverse()", "path": "generated/torch.inverse#torch.inverse", "type": "torch", "text": " \ntorch.inverse(input, *, out=None) \u2192 Tensor  \nTakes the inverse of the square matrix input. input can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses. Supports real and complex input.  Note torch.inverse() is deprecated. Please use torch.linalg.inv() instead.   Note Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like input.contiguous().transpose(-2, -1).stride()   Parameters \ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  where * is zero or more batch dimensions  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Examples: >>> x = torch.rand(4, 4)\n>>> y = torch.inverse(x)\n>>> z = torch.mm(x, y)\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.max(torch.abs(z - torch.eye(4))) # Max non-zero\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.inverse(x)\n>>> z = torch.matmul(x, y)\n>>> torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.inverse(x)\n>>> z = torch.mm(x, y)\n>>> z\ntensor([[ 1.0000e+00+0.0000e+00j, -1.3878e-16+3.4694e-16j,\n        5.5511e-17-1.1102e-16j,  0.0000e+00-1.6653e-16j],\n        [ 5.5511e-16-1.6653e-16j,  1.0000e+00+6.9389e-17j,\n        2.2204e-16-1.1102e-16j, -2.2204e-16+1.1102e-16j],\n        [ 3.8858e-16-1.2490e-16j,  2.7756e-17+3.4694e-17j,\n        1.0000e+00+0.0000e+00j, -4.4409e-16+5.5511e-17j],\n        [ 4.4409e-16+5.5511e-16j, -3.8858e-16+1.8041e-16j,\n        2.2204e-16+0.0000e+00j,  1.0000e+00-3.4694e-16j]],\n    dtype=torch.complex128)\n>>> torch.max(torch.abs(z - torch.eye(4, dtype=torch.cdouble))) # Max non-zero\ntensor(7.5107e-16, dtype=torch.float64)\n \n"}, {"name": "torch.isclose()", "path": "generated/torch.isclose#torch.isclose", "type": "torch", "text": " \ntorch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor  \nReturns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other. Closeness is defined as:  \u2223input\u2212other\u2223\u2264atol+rtol\u00d7\u2223other\u2223\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert  \nwhere input and other are finite. Where input and/or other are nonfinite they are close if and only if they are equal, with NaNs being considered equal to each other when equal_nan is True.  Parameters \n \ninput (Tensor) \u2013 first tensor to compare \nother (Tensor) \u2013 second tensor to compare \natol (float, optional) \u2013 absolute tolerance. Default: 1e-08 \nrtol (float, optional) \u2013 relative tolerance. Default: 1e-05 \nequal_nan (bool, optional) \u2013 if True, then two NaN s will be considered equal. Default: False\n    Examples: >>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))\ntensor([ True, False, False])\n>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)\ntensor([True, True])\n \n"}, {"name": "torch.isfinite()", "path": "generated/torch.isfinite#torch.isfinite", "type": "torch", "text": " \ntorch.isfinite(input) \u2192 Tensor  \nReturns a new tensor with boolean elements representing if each element is finite or not. Real values are finite when they are not NaN, negative infinity, or infinity. Complex values are finite when both their real and imaginary parts are finite.  Args:\n\ninput (Tensor): the input tensor.  Returns:\n\nA boolean tensor that is True where input is finite and False elsewhere   Example: >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([True,  False,  True,  False,  False])\n \n"}, {"name": "torch.isinf()", "path": "generated/torch.isinf#torch.isinf", "type": "torch", "text": " \ntorch.isinf(input) \u2192 Tensor  \nTests if each element of input is infinite (positive or negative infinity) or not.  Note Complex values are infinite when their real or imaginary part is infinite.  Args:\n\n{input}  Returns:\n\nA boolean tensor that is True where input is infinite and False elsewhere   Example: >>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])\n  \n"}, {"name": "torch.isnan()", "path": "generated/torch.isnan#torch.isnan", "type": "torch", "text": " \ntorch.isnan(input) \u2192 Tensor  \nReturns a new tensor with boolean elements representing if each element of input is NaN or not. Complex values are considered NaN when either their real and/or imaginary part is NaN.  Parameters \ninput (Tensor) \u2013 the input tensor.  Returns \nA boolean tensor that is True where input is NaN and False elsewhere   Example: >>> torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([False, True, False])\n \n"}, {"name": "torch.isneginf()", "path": "generated/torch.isneginf#torch.isneginf", "type": "torch", "text": " \ntorch.isneginf(input, *, out=None) \u2192 Tensor  \nTests if each element of input is negative infinity or not.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isneginf(a)\ntensor([ True, False, False])\n   \n"}, {"name": "torch.isposinf()", "path": "generated/torch.isposinf#torch.isposinf", "type": "torch", "text": " \ntorch.isposinf(input, *, out=None) \u2192 Tensor  \nTests if each element of input is positive infinity or not.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isposinf(a)\ntensor([False,  True, False])\n   \n"}, {"name": "torch.isreal()", "path": "generated/torch.isreal#torch.isreal", "type": "torch", "text": " \ntorch.isreal(input) \u2192 Tensor  \nReturns a new tensor with boolean elements representing if each element of input is real-valued or not. All real-valued types are considered real. Complex values are considered real when their imaginary part is 0.  Parameters \ninput (Tensor) \u2013 the input tensor.  Returns \nA boolean tensor that is True where input is real and False elsewhere   Example: >>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))\ntensor([True, False, True])\n \n"}, {"name": "torch.istft()", "path": "generated/torch.istft#torch.istft", "type": "torch", "text": " \ntorch.istft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False) [source]\n \nInverse short time Fourier Transform. This is expected to be the inverse of stft(). It has the same parameters (+ additional optional parameter of length) and it should return the least squares estimation of the original signal. The algorithm will check using the NOLA condition ( nonzero overlap). Important consideration in the parameters window and center so that the envelop created by the summation of all the windows is never zero at certain point in time. Specifically, \u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0 . Since stft() discards elements at the end of the signal if they do not fit in a frame, istft may return a shorter signal than the original signal (can occur if center is False since the signal isn\u2019t padded). If center is True, then there will be padding e.g. 'constant', 'reflect', etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information. Example: Suppose the last window is: [17, 18, 0, 0, 0] vs [18, 0, 0, 0, 0] The n_fft, hop_length, win_length are all the same which prevents the calculation of right padding. These additional values could be zeros or a reflection of the signal so providing length could be useful. If length is None then padding will be aggressively removed (some loss of signal). [1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.  Parameters \n \ninput (Tensor) \u2013 \nThe input tensor. Expected to be output of stft(), can either be complex (channel, fft_size, n_frame), or real (channel, fft_size, n_frame, 2) where the channel dimension is optional.  Deprecated since version 1.8.0: Real input is deprecated, use complex inputs as returned by stft(..., return_complex=True) instead.   \nn_fft (int) \u2013 Size of Fourier transform \nhop_length (Optional[int]) \u2013 The distance between neighboring sliding window frames. (Default: n_fft // 4) \nwin_length (Optional[int]) \u2013 The size of window frame and STFT filter. (Default: n_fft) \nwindow (Optional[torch.Tensor]) \u2013 The optional window function. (Default: torch.ones(win_length)) \ncenter (bool) \u2013 Whether input was padded on both sides so that the tt -th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length} . (Default: True) \nnormalized (bool) \u2013 Whether the STFT was normalized. (Default: False) \nonesided (Optional[bool]) \u2013 Whether the STFT was onesided. (Default: True if n_fft != fft_size in the input size) \nlength (Optional[int]) \u2013 The amount to trim the signal by (i.e. the original signal length). (Default: whole signal) \nreturn_complex (Optional[bool]) \u2013 Whether the output should be complex, or if the input should be assumed to derive from a real signal and window. Note that this is incompatible with onesided=True. (Default: False)   Returns \nLeast squares estimation of the original signal of size (\u2026, signal_length)  Return type \nTensor   \n"}, {"name": "torch.is_complex()", "path": "generated/torch.is_complex#torch.is_complex", "type": "torch", "text": " \ntorch.is_complex(input) -> (bool)  \nReturns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.  Parameters \ninput (Tensor) \u2013 the input tensor.   \n"}, {"name": "torch.is_floating_point()", "path": "generated/torch.is_floating_point#torch.is_floating_point", "type": "torch", "text": " \ntorch.is_floating_point(input) -> (bool)  \nReturns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.  Parameters \ninput (Tensor) \u2013 the input tensor.   \n"}, {"name": "torch.is_nonzero()", "path": "generated/torch.is_nonzero#torch.is_nonzero", "type": "torch", "text": " \ntorch.is_nonzero(input) -> (bool)  \nReturns True if the input is a single element tensor which is not equal to zero after type conversions. i.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or torch.tensor([False]). Throws a RuntimeError if torch.numel() != 1 (even in case of sparse tensors).  Parameters \ninput (Tensor) \u2013 the input tensor.   Examples: >>> torch.is_nonzero(torch.tensor([0.]))\nFalse\n>>> torch.is_nonzero(torch.tensor([1.5]))\nTrue\n>>> torch.is_nonzero(torch.tensor([False]))\nFalse\n>>> torch.is_nonzero(torch.tensor([3]))\nTrue\n>>> torch.is_nonzero(torch.tensor([1, 3, 5]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with more than one value is ambiguous\n>>> torch.is_nonzero(torch.tensor([]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with no values is ambiguous\n \n"}, {"name": "torch.is_storage()", "path": "generated/torch.is_storage#torch.is_storage", "type": "torch", "text": " \ntorch.is_storage(obj) [source]\n \nReturns True if obj is a PyTorch storage object.  Parameters \nobj (Object) \u2013 Object to test   \n"}, {"name": "torch.is_tensor()", "path": "generated/torch.is_tensor#torch.is_tensor", "type": "torch", "text": " \ntorch.is_tensor(obj) [source]\n \nReturns True if obj is a PyTorch tensor. Note that this function is simply doing isinstance(obj, Tensor). Using that isinstance check is better for typechecking with mypy, and more explicit - so it\u2019s recommended to use that instead of is_tensor.  Parameters \nobj (Object) \u2013 Object to test   \n"}, {"name": "torch.jit.export()", "path": "jit#torch.jit.export", "type": "TorchScript", "text": " \ntorch.jit.export(fn) [source]\n \nThis decorator indicates that a method on an nn.Module is used as an entry point into a ScriptModule and should be compiled. forward implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from forward are compiled as they are seen by the compiler, so they do not need this decorator either. Example (using @torch.jit.export on a method): import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def implicitly_compiled_method(self, x):\n        return x + 99\n\n    # `forward` is implicitly decorated with `@torch.jit.export`,\n    # so adding it here would have no effect\n    def forward(self, x):\n        return x + 10\n\n    @torch.jit.export\n    def another_forward(self, x):\n        # When the compiler sees this call, it will compile\n        # `implicitly_compiled_method`\n        return self.implicitly_compiled_method(x)\n\n    def unused_method(self, x):\n        return x - 20\n\n# `m` will contain compiled methods:\n#     `forward`\n#     `another_forward`\n#     `implicitly_compiled_method`\n# `unused_method` will not be compiled since it was not called from\n# any compiled methods and wasn't decorated with `@torch.jit.export`\nm = torch.jit.script(MyModule())\n \n"}, {"name": "torch.jit.fork()", "path": "generated/torch.jit.fork#torch.jit.fork", "type": "TorchScript", "text": " \ntorch.jit.fork(func, *args, **kwargs) [source]\n \nCreates an asynchronous task executing func and a reference to the value of the result of this execution. fork will return immediately, so the return value of func may not have been computed yet. To force completion of the task and access the return value invoke torch.jit.wait on the Future. fork invoked with a func which returns T is typed as torch.jit.Future[T]. fork calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, fork will not execute in parallel. fork will also not execute in parallel when invoked while tracing, however the fork and wait calls will be captured in the exported IR Graph. .. warning: `fork` tasks will execute non-deterministicly. We recommend only spawning\nparallel fork tasks for pure functions that do not modify their inputs,\nmodule attributes, or global state.\n  Parameters \n \nfunc (callable or torch.nn.Module) \u2013 A Python function or torch.nn.Module that will be invoked. If executed in TorchScript, it will execute asynchronously, otherwise it will not. Traced invocations of fork will be captured in the IR. \n**kwargs (*args,) \u2013 arguments to invoke func with.   Returns \na reference to the execution of func. The value T can only be accessed by forcing completion of func through torch.jit.wait.  Return type \ntorch.jit.Future[T]   Example (fork a free function): import torch\nfrom torch import Tensor\ndef foo(a : Tensor, b : int) -> Tensor:\n    return a + b\ndef bar(a):\n    fut : torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)\n    return torch.jit.wait(fut)\nscript_bar = torch.jit.script(bar)\ninput = torch.tensor(2)\n# only the scripted version executes asynchronously\nassert script_bar(input) == bar(input)\n# trace is not run asynchronously, but fork is captured in IR\ngraph = torch.jit.trace(bar, (input,)).graph\nassert \"fork\" in str(graph)\n Example (fork a module method): import torch\nfrom torch import Tensor\nclass AddMod(torch.nn.Module):\n    def forward(self, a: Tensor, b : int):\n        return a + b\nclass Mod(torch.nn.Module):\n    def __init__(self):\n        super(self).__init__()\n        self.mod = AddMod()\n    def forward(self, input):\n        fut = torch.jit.fork(self.mod, a, b=2)\n        return torch.jit.wait(fut)\ninput = torch.tensor(2)\nmod = Mod()\nassert mod(input) == torch.jit.script(mod).forward(input)\n \n"}, {"name": "torch.jit.freeze()", "path": "generated/torch.jit.freeze#torch.jit.freeze", "type": "TorchScript", "text": " \ntorch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True) [source]\n \nFreezing a ScriptModule will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, forward will be preserved, as well as attributes & methods specified in preserved_attrs. Additionally, any attribute that is modified within a preserved method will be preserved. Freezing currently only accepts ScriptModules that are in eval mode.  Parameters \n \nmod (ScriptModule) \u2013 a module to be frozen \npreserved_attrs (Optional[List[str]]) \u2013 a list of attributes to preserve in addition to the forward method. \nmodified in preserved methods will also be preserved. (Attributes) \u2013  \noptimize_numerics (bool) \u2013 If True, a set of optimization passes will be run that does not strictly \nnumerics. Full details of optimization can be found at torch.jit.optimize_frozen_module. (preserve) \u2013    Returns \nFrozen ScriptModule.   Example (Freezing a simple module with a Parameter):     def forward(self, input):\n        output = self.weight.mm(input)\n        output = self.linear(output)\n        return output\n\nscripted_module = torch.jit.script(MyModule(2, 3).eval())\nfrozen_module = torch.jit.freeze(scripted_module)\n# parameters have been removed and inlined into the Graph as constants\nassert len(list(frozen_module.named_parameters())) == 0\n# See the compiled graph as Python code\nprint(frozen_module.code)\n Example (Freezing a module with preserved attributes)     def forward(self, input):\n        self.modified_tensor += 1\n        return input + self.modified_tensor\n\nscripted_module = torch.jit.script(MyModule2().eval())\nfrozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[\"version\"])\n# we've manually preserved `version`, so it still exists on the frozen module and can be modified\nassert frozen_module.version == 1\nfrozen_module.version = 2\n# `modified_tensor` is detected as being mutated in the forward, so freezing preserves\n# it to retain model semantics\nassert frozen_module(torch.tensor(1)) == torch.tensor(12)\n# now that we've run it once, the next result will be incremented by one\nassert frozen_module(torch.tensor(1)) == torch.tensor(13)\n  Note If you\u2019re not sure why an attribute is not being inlined as a constant, you can run dump_alias_db on frozen_module.forward.graph to see if freezing has detected the attribute is being modified.  \n"}, {"name": "torch.jit.ignore()", "path": "generated/torch.jit.ignore#torch.jit.ignore", "type": "TorchScript", "text": " \ntorch.jit.ignore(drop=False, **kwargs) [source]\n \nThis decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use @torch.jit.unused instead. Example (using @torch.jit.ignore on a method): import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    @torch.jit.ignore\n    def debugger(self, x):\n        import pdb\n        pdb.set_trace()\n\n    def forward(self, x):\n        x += 10\n        # The compiler would normally try to compile `debugger`,\n        # but since it is `@ignore`d, it will be left as a call\n        # to Python\n        self.debugger(x)\n        return x\n\nm = torch.jit.script(MyModule())\n\n# Error! The call `debugger` cannot be saved since it calls into Python\nm.save(\"m.pt\")\n Example (using @torch.jit.ignore(drop=True) on a method): import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    @torch.jit.ignore(drop=True)\n    def training_method(self, x):\n        import pdb\n        pdb.set_trace()\n\n    def forward(self, x):\n        if self.training:\n            self.training_method(x)\n        return x\n\nm = torch.jit.script(MyModule())\n\n# This is OK since `training_method` is not saved, the call is replaced\n# with a `raise`.\nm.save(\"m.pt\")\n \n"}, {"name": "torch.jit.isinstance()", "path": "generated/torch.jit.isinstance#torch.jit.isinstance", "type": "TorchScript", "text": " \ntorch.jit.isinstance(obj, target_type) [source]\n \nThis function provides for conatiner type refinement in TorchScript. It can refine parameterized containers of the List, Dict, Tuple, and Optional types. E.g. List[str], Dict[str, List[torch.Tensor]], Optional[Tuple[int,str,int]]. It can also refine basic types such as bools and ints that are available in TorchScript.  Parameters \n \nobj \u2013 object to refine the type of \ntarget_type \u2013 type to try to refine obj to   Returns \n True if obj was successfully refined to the type of target_type,\n\nFalse otherwise with no new type refinement    Return type \nbool   Example (using torch.jit.isinstance for type refinement): .. testcode: import torch\nfrom typing import Any, Dict, List\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n\n    def forward(self, input: Any): # note the Any type\n        if torch.jit.isinstance(input, List[torch.Tensor]):\n            for t in input:\n                y = t.clamp(0, 0.5)\n        elif torch.jit.isinstance(input, Dict[str, str]):\n            for val in input.values():\n                print(val)\n\nm = torch.jit.script(MyModule())\nx = [torch.rand(3,3), torch.rand(4,3)]\nm(x)\ny = {\"key1\":\"val1\",\"key2\":\"val2\"}\nm(y)\n \n"}, {"name": "torch.jit.is_scripting()", "path": "jit_language_reference#torch.jit.is_scripting", "type": "TorchScript", "text": " \ntorch.jit.is_scripting() [source]\n \nFunction that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode: import torch\n\n@torch.jit.unused\ndef unsupported_linear_op(x):\n    return x\n\ndef linear(x):\n   if not torch.jit.is_scripting():\n      return torch.linear(x)\n   else:\n      return unsupported_linear_op(x)\n \n"}, {"name": "torch.jit.load()", "path": "generated/torch.jit.load#torch.jit.load", "type": "TorchScript", "text": " \ntorch.jit.load(f, map_location=None, _extra_files=None) [source]\n \nLoad a ScriptModule or ScriptFunction previously saved with torch.jit.save All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised.  Parameters \n \nf \u2013 a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name \nmap_location (string or torch.device) \u2013 A simplified version of map_location in torch.jit.save used to dynamically remap storages to an alternative set of devices. \n_extra_files (dictionary of filename to content) \u2013 The extra filenames given in the map would be loaded and their content would be stored in the provided map.   Returns \nA ScriptModule object.   Example: import torch\nimport io\n\ntorch.jit.load('scriptmodule.pt')\n\n# Load ScriptModule from io.BytesIO object\nwith open('scriptmodule.pt', 'rb') as f:\n    buffer = io.BytesIO(f.read())\n\n# Load all tensors to the original device\ntorch.jit.load(buffer)\n\n# Load all tensors onto CPU, using a device\nbuffer.seek(0)\ntorch.jit.load(buffer, map_location=torch.device('cpu'))\n\n# Load all tensors onto CPU, using a string\nbuffer.seek(0)\ntorch.jit.load(buffer, map_location='cpu')\n\n# Load with extra files.\nextra_files = {'foo.txt': ''}  # values will be replaced with data\ntorch.jit.load('scriptmodule.pt', _extra_files=extra_files)\nprint(extra_files['foo.txt'])\n \n"}, {"name": "torch.jit.save()", "path": "generated/torch.jit.save#torch.jit.save", "type": "TorchScript", "text": " \ntorch.jit.save(m, f, _extra_files=None) [source]\n \nSave an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using torch::jit::load(filename) or into the Python API with torch.jit.load. To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of ScriptModule as well.  Danger All modules, no matter their device, are always loaded onto the CPU during loading. This is different from torch.load()\u2019s semantics and may change in the future.   Parameters \n \nm \u2013 A ScriptModule to save. \nf \u2013 A file-like object (has to implement write and flush) or a string containing a file name. \n_extra_files \u2013 Map from filename to contents which will be stored as part of f.     Note torch.jit.save attempts to preserve the behavior of some operators across versions. For example, dividing two integer tensors in PyTorch 1.5 performed floor division, and if the module containing that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6 its division behavior will be preserved. The same module saved in PyTorch 1.6 will fail to load in PyTorch 1.5, however, since the behavior of division changed in 1.6, and 1.5 does not know how to replicate the 1.6 behavior.  Example: import torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nm = torch.jit.script(MyModule())\n\n# Save to file\ntorch.jit.save(m, 'scriptmodule.pt')\n# This line is equivalent to the previous\nm.save(\"scriptmodule.pt\")\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.jit.save(m, buffer)\n\n# Save with extra files\nextra_files = {'foo.txt': b'bar'}\ntorch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)\n \n"}, {"name": "torch.jit.script()", "path": "generated/torch.jit.script#torch.jit.script", "type": "TorchScript", "text": " \ntorch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None) [source]\n \nScripting a function or nn.Module will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a ScriptModule or ScriptFunction. TorchScript itself is a subset of the Python language, so not all features in Python work, but we provide enough functionality to compute on tensors and do control-dependent operations. For a complete guide, see the TorchScript Language Reference. torch.jit.script can be used as a function for modules and functions, and as a decorator @torch.jit.script for TorchScript Classes and functions.  Parameters \nobj (callable, class, or nn.Module) \u2013 The nn.Module, function, or class type to compile.  Returns \nIf obj is nn.Module, script returns a ScriptModule object. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If obj is a standalone function, a ScriptFunction will be returned.    Scripting a function\n\nThe @torch.jit.script decorator will construct a ScriptFunction by compiling the body of the function. Example (scripting a function): import torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max() > y.max():\n        r = x\n    else:\n        r = y\n    return r\n\nprint(type(foo))  # torch.jit.ScriptFuncion\n\n# See the compiled graph as Python code\nprint(foo.code)\n\n# Call the function using the TorchScript interpreter\nfoo(torch.ones(2, 2), torch.ones(2, 2))\n  Scripting an nn.Module\n\nScripting an nn.Module by default will compile the forward method and recursively compile any methods, submodules, and functions called by forward. If a nn.Module only uses features supported in TorchScript, no changes to the original module code should be necessary. script will construct ScriptModule that has copies of the attributes, parameters, and methods of the original module. Example (scripting a simple module with a Parameter): import torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        # This parameter will be copied to the new ScriptModule\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n        # When this submodule is used, it will be compiled\n        self.linear = torch.nn.Linear(N, M)\n\n    def forward(self, input):\n        output = self.weight.mv(input)\n\n        # This calls the `forward` method of the `nn.Linear` module, which will\n        # cause the `self.linear` submodule to be compiled to a `ScriptModule` here\n        output = self.linear(output)\n        return output\n\nscripted_module = torch.jit.script(MyModule(2, 3))\n Example (scripting a module with traced submodules): import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        # torch.jit.trace produces a ScriptModule's conv1 and conv2\n        self.conv1 = torch.jit.trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))\n        self.conv2 = torch.jit.trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))\n\n    def forward(self, input):\n        input = F.relu(self.conv1(input))\n        input = F.relu(self.conv2(input))\n        return input\n\nscripted_module = torch.jit.script(MyModule())\n To compile a method other than forward (and recursively compile anything it calls), add the @torch.jit.export decorator to the method. To opt out of compilation use @torch.jit.ignore or @torch.jit.unused. Example (an exported and ignored method in a module): import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n\n    @torch.jit.export\n    def some_entry_point(self, input):\n        return input + 10\n\n    @torch.jit.ignore\n    def python_only_fn(self, input):\n        # This function won't be compiled, so any\n        # Python APIs can be used\n        import pdb\n        pdb.set_trace()\n\n    def forward(self, input):\n        if self.training:\n            self.python_only_fn(input)\n        return input * 99\n\nscripted_module = torch.jit.script(MyModule())\nprint(scripted_module.some_entry_point(torch.randn(2, 2)))\nprint(scripted_module(torch.randn(2, 2)))\n   \n"}, {"name": "torch.jit.ScriptFunction", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction", "type": "TorchScript", "text": " \nclass torch.jit.ScriptFunction  \nFunctionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters.  \nget_debug_state(self: torch._C.ScriptFunction) \u2192 torch._C.GraphExecutorState \n  \nsave(self: torch._C.ScriptFunction, filename: str, _extra_files: Dict[str, str] = {}) \u2192 None \n  \nsave_to_buffer(self: torch._C.ScriptFunction, _extra_files: Dict[str, str] = {}) \u2192 bytes \n \n"}, {"name": "torch.jit.ScriptFunction.get_debug_state()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.get_debug_state", "type": "TorchScript", "text": " \nget_debug_state(self: torch._C.ScriptFunction) \u2192 torch._C.GraphExecutorState \n"}, {"name": "torch.jit.ScriptFunction.save()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save", "type": "TorchScript", "text": " \nsave(self: torch._C.ScriptFunction, filename: str, _extra_files: Dict[str, str] = {}) \u2192 None \n"}, {"name": "torch.jit.ScriptFunction.save_to_buffer()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer", "type": "TorchScript", "text": " \nsave_to_buffer(self: torch._C.ScriptFunction, _extra_files: Dict[str, str] = {}) \u2192 bytes \n"}, {"name": "torch.jit.ScriptModule", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule", "type": "TorchScript", "text": " \nclass torch.jit.ScriptModule [source]\n \nA wrapper around C++ torch::jit::Module. ScriptModules contain methods, attributes, parameters, and constants. These can be accessed the same as on a normal nn.Module.  \nadd_module(name, module)  \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n  \napply(fn)  \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n  \nbfloat16()  \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n  \nbuffers(recurse=True)  \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nchildren()  \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n  \nproperty code  \nReturns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details. \n  \nproperty code_with_constants  \nReturns a tuple of: [0] a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See code. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant\u2019s values. See Inspecting Code for details. \n  \ncpu()  \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n  \ncuda(device=None)  \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \ndouble()  \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n  \neval()  \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n  \nextra_repr()  \nSet the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \n  \nfloat()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n  \nproperty graph  \nReturns a string representation of the internal graph for the forward method. See Interpreting Graphs for details. \n  \nhalf()  \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n  \nproperty inlined_graph  \nReturns a string representation of the internal graph for the forward method. This graph will be preprocessed to inline all function and method calls. See Interpreting Graphs for details. \n  \nload_state_dict(state_dict, strict=True)  \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n  \nmodules()  \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n  \nnamed_buffers(prefix='', recurse=True)  \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n  \nnamed_children()  \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n  \nnamed_modules(memo=None, prefix='')  \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n  \nnamed_parameters(prefix='', recurse=True)  \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n  \nparameters(recurse=True)  \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nregister_backward_hook(hook)  \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_buffer(name, tensor, persistent=True)  \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n  \nregister_forward_hook(hook)  \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_forward_pre_hook(hook)  \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_full_backward_hook(hook)  \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_parameter(name, param)  \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n  \nrequires_grad_(requires_grad=True)  \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n  \nsave(f, _extra_files={})  \nSee torch.jit.save for details. \n  \nstate_dict(destination=None, prefix='', keep_vars=False)  \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n  \nto(*args, **kwargs)  \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) \n  \nto(dtype, non_blocking=False) \n  \nto(tensor, non_blocking=False) \n  \nto(memory_format=torch.channels_last) \n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n  \ntrain(mode=True)  \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n  \ntype(dst_type)  \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n  \nxpu(device=None)  \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \nzero_grad(set_to_none=False)  \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n \n"}, {"name": "torch.jit.ScriptModule.add_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.add_module", "type": "TorchScript", "text": " \nadd_module(name, module)  \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n"}, {"name": "torch.jit.ScriptModule.apply()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.apply", "type": "TorchScript", "text": " \napply(fn)  \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n"}, {"name": "torch.jit.ScriptModule.bfloat16()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.bfloat16", "type": "TorchScript", "text": " \nbfloat16()  \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.buffers", "type": "TorchScript", "text": " \nbuffers(recurse=True)  \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.jit.ScriptModule.children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.children", "type": "TorchScript", "text": " \nchildren()  \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n"}, {"name": "torch.jit.ScriptModule.code()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code", "type": "TorchScript", "text": " \nproperty code  \nReturns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details. \n"}, {"name": "torch.jit.ScriptModule.code_with_constants()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code_with_constants", "type": "TorchScript", "text": " \nproperty code_with_constants  \nReturns a tuple of: [0] a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See code. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant\u2019s values. See Inspecting Code for details. \n"}, {"name": "torch.jit.ScriptModule.cpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cpu", "type": "TorchScript", "text": " \ncpu()  \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.cuda()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cuda", "type": "TorchScript", "text": " \ncuda(device=None)  \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.double()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.double", "type": "TorchScript", "text": " \ndouble()  \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.eval()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.eval", "type": "TorchScript", "text": " \neval()  \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.extra_repr()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.extra_repr", "type": "TorchScript", "text": " \nextra_repr()  \nSet the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \n"}, {"name": "torch.jit.ScriptModule.float()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.float", "type": "TorchScript", "text": " \nfloat()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.graph", "type": "TorchScript", "text": " \nproperty graph  \nReturns a string representation of the internal graph for the forward method. See Interpreting Graphs for details. \n"}, {"name": "torch.jit.ScriptModule.half()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.half", "type": "TorchScript", "text": " \nhalf()  \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.inlined_graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.inlined_graph", "type": "TorchScript", "text": " \nproperty inlined_graph  \nReturns a string representation of the internal graph for the forward method. This graph will be preprocessed to inline all function and method calls. See Interpreting Graphs for details. \n"}, {"name": "torch.jit.ScriptModule.load_state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.load_state_dict", "type": "TorchScript", "text": " \nload_state_dict(state_dict, strict=True)  \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n"}, {"name": "torch.jit.ScriptModule.modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.modules", "type": "TorchScript", "text": " \nmodules()  \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n"}, {"name": "torch.jit.ScriptModule.named_buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_buffers", "type": "TorchScript", "text": " \nnamed_buffers(prefix='', recurse=True)  \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n"}, {"name": "torch.jit.ScriptModule.named_children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_children", "type": "TorchScript", "text": " \nnamed_children()  \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n"}, {"name": "torch.jit.ScriptModule.named_modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_modules", "type": "TorchScript", "text": " \nnamed_modules(memo=None, prefix='')  \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n"}, {"name": "torch.jit.ScriptModule.named_parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_parameters", "type": "TorchScript", "text": " \nnamed_parameters(prefix='', recurse=True)  \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n"}, {"name": "torch.jit.ScriptModule.parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.parameters", "type": "TorchScript", "text": " \nparameters(recurse=True)  \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.jit.ScriptModule.register_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_backward_hook", "type": "TorchScript", "text": " \nregister_backward_hook(hook)  \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.jit.ScriptModule.register_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_buffer", "type": "TorchScript", "text": " \nregister_buffer(name, tensor, persistent=True)  \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n"}, {"name": "torch.jit.ScriptModule.register_forward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_hook", "type": "TorchScript", "text": " \nregister_forward_hook(hook)  \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.jit.ScriptModule.register_forward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_pre_hook", "type": "TorchScript", "text": " \nregister_forward_pre_hook(hook)  \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.jit.ScriptModule.register_full_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_hook", "type": "TorchScript", "text": " \nregister_full_backward_hook(hook)  \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.jit.ScriptModule.register_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_parameter", "type": "TorchScript", "text": " \nregister_parameter(name, param)  \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n"}, {"name": "torch.jit.ScriptModule.requires_grad_()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.requires_grad_", "type": "TorchScript", "text": " \nrequires_grad_(requires_grad=True)  \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.save()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.save", "type": "TorchScript", "text": " \nsave(f, _extra_files={})  \nSee torch.jit.save for details. \n"}, {"name": "torch.jit.ScriptModule.state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.state_dict", "type": "TorchScript", "text": " \nstate_dict(destination=None, prefix='', keep_vars=False)  \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n"}, {"name": "torch.jit.ScriptModule.to()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to", "type": "TorchScript", "text": " \nto(*args, **kwargs)  \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) \n  \nto(dtype, non_blocking=False) \n  \nto(tensor, non_blocking=False) \n  \nto(memory_format=torch.channels_last) \n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n"}, {"name": "torch.jit.ScriptModule.train()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.train", "type": "TorchScript", "text": " \ntrain(mode=True)  \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.type()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.type", "type": "TorchScript", "text": " \ntype(dst_type)  \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.xpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.xpu", "type": "TorchScript", "text": " \nxpu(device=None)  \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.jit.ScriptModule.zero_grad()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.zero_grad", "type": "TorchScript", "text": " \nzero_grad(set_to_none=False)  \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n"}, {"name": "torch.jit.script_if_tracing()", "path": "generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing", "type": "TorchScript", "text": " \ntorch.jit.script_if_tracing(fn) [source]\n \nCompiles fn when it is first called during tracing. torch.jit.script has a non-negligible start up time when it is first called due to lazy-initializations of many compiler builtins. Therefore you should not use it in library code. However, you may want to have parts of your library work in tracing even if they use control flow. In these cases, you should use @torch.jit.script_if_tracing to substitute for torch.jit.script.  Parameters \nfn \u2013 A function to compile.  Returns \nIf called during tracing, a ScriptFunction created by torch.jit.script is returned. Otherwise, the original function fn is returned.   \n"}, {"name": "torch.jit.trace()", "path": "generated/torch.jit.trace#torch.jit.trace", "type": "TorchScript", "text": " \ntorch.jit.trace(func, example_inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object>) [source]\n \nTrace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on Tensors and lists, dictionaries, and tuples of Tensors. Using torch.jit.trace and torch.jit.trace_module, you can turn an existing module or Python function into a TorchScript ScriptFunction or ScriptModule. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.  The resulting recording of a standalone function produces ScriptFunction. The resulting recording of nn.Module.forward or nn.Module produces ScriptModule.  This module also contains any parameters that the original module had as well.  Warning Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned ScriptModule will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,  Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence. In the returned ScriptModule, operations that have different behaviors in training and eval modes will always behave as if it is in the mode it was in during tracing, no matter which mode the ScriptModule is in.  In cases like these, tracing would not be appropriate and scripting is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.   Parameters \n \nfunc (callable or torch.nn.Module) \u2013 A Python function or torch.nn.Module that will be run with example_inputs. func arguments and return values must be tensors or (possibly nested) tuples that contain tensors. When a module is passed torch.jit.trace, only the forward method is run and traced (see torch.jit.trace for details). \nexample_inputs (tuple or torch.Tensor) \u2013 A tuple of example inputs that will be passed to the function while tracing. The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes. example_inputs may also be a single Tensor in which case it is automatically wrapped in a tuple.   Keyword Arguments \n \ncheck_trace (bool, optional) \u2013 Check if the same inputs run through traced code produce the same outputs. Default: True. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure. \ncheck_inputs (list of tuples, optional) \u2013 A list of tuples of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in example_inputs. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original example_inputs are used for checking \ncheck_tolerance (float, optional) \u2013 Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion. \nstrict (bool, optional) \u2013 run the tracer in a strict mode or not (default: True). Only turn this off when you want the tracer to record your mutable container types (currently list/dict) and you are sure that the container you are using in your problem is a constant structure and does not get used as control flow (if, for) conditions.   Returns \nIf func is nn.Module or forward of nn.Module, trace returns a ScriptModule object with a single forward method containing the traced code. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If func is a standalone function, trace returns ScriptFunction.   Example (tracing a function): import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\n# Run `foo` with the provided inputs and record the tensor operations\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n# `traced_foo` can now be run with the TorchScript interpreter or saved\n# and loaded in a Python-free environment\n Example (tracing an existing module): import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Conv2d(1, 1, 3)\n\n    def forward(self, x):\n        return self.conv(x)\n\nn = Net()\nexample_weight = torch.rand(1, 1, 3, 3)\nexample_forward_input = torch.rand(1, 1, 3, 3)\n\n# Trace a specific method and construct `ScriptModule` with\n# a single `forward` method\nmodule = torch.jit.trace(n.forward, example_forward_input)\n\n# Trace a module (implicitly traces `forward`) and construct a\n# `ScriptModule` with a single `forward` method\nmodule = torch.jit.trace(n, example_forward_input)\n \n"}, {"name": "torch.jit.trace_module()", "path": "generated/torch.jit.trace_module#torch.jit.trace_module", "type": "TorchScript", "text": " \ntorch.jit.trace_module(mod, inputs, optimize=None, check_trace=True, check_inputs=None, check_tolerance=1e-05, strict=True, _force_outplace=False, _module_class=None, _compilation_unit=<torch.jit.CompilationUnit object>) [source]\n \nTrace a module and return an executable ScriptModule that will be optimized using just-in-time compilation. When a module is passed to torch.jit.trace, only the forward method is run and traced. With trace_module, you can specify a dictionary of method names to example inputs to trace (see the inputs) argument below. See torch.jit.trace for more information on tracing.  Parameters \n \nmod (torch.nn.Module) \u2013 A torch.nn.Module containing methods whose names are specified in inputs. The given methods will be compiled as a part of a single ScriptModule. \ninputs (dict) \u2013 A dict containing sample inputs indexed by method names in mod. The inputs will be passed to methods whose names correspond to inputs\u2019 keys while tracing. { 'forward' : example_forward_input, 'method2': example_method2_input}\n   Keyword Arguments \n \ncheck_trace (bool, optional) \u2013 Check if the same inputs run through traced code produce the same outputs. Default: True. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure. \ncheck_inputs (list of dicts, optional) \u2013 A list of dicts of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in inputs. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original inputs are used for checking \ncheck_tolerance (float, optional) \u2013 Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion.   Returns \nA ScriptModule object with a single forward method containing the traced code. When func is a torch.nn.Module, the returned ScriptModule will have the same set of sub-modules and parameters as func.   Example (tracing a module with multiple methods): import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Conv2d(1, 1, 3)\n\n    def forward(self, x):\n        return self.conv(x)\n\n    def weighted_kernel_sum(self, weight):\n        return weight * self.conv.weight\n\n\nn = Net()\nexample_weight = torch.rand(1, 1, 3, 3)\nexample_forward_input = torch.rand(1, 1, 3, 3)\n\n# Trace a specific method and construct `ScriptModule` with\n# a single `forward` method\nmodule = torch.jit.trace(n.forward, example_forward_input)\n\n# Trace a module (implicitly traces `forward`) and construct a\n# `ScriptModule` with a single `forward` method\nmodule = torch.jit.trace(n, example_forward_input)\n\n# Trace specific methods on a module (specified in `inputs`), constructs\n# a `ScriptModule` with `forward` and `weighted_kernel_sum` methods\ninputs = {'forward' : example_forward_input, 'weighted_kernel_sum' : example_weight}\nmodule = torch.jit.trace_module(n, inputs)\n \n"}, {"name": "torch.jit.unused()", "path": "generated/torch.jit.unused#torch.jit.unused", "type": "TorchScript", "text": " \ntorch.jit.unused(fn) [source]\n \nThis decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model. Example (using @torch.jit.unused on a method): import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def __init__(self, use_memory_efficient):\n        super(MyModule, self).__init__()\n        self.use_memory_efficient = use_memory_efficient\n\n    @torch.jit.unused\n    def memory_efficient(self, x):\n        import pdb\n        pdb.set_trace()\n        return x + 10\n\n    def forward(self, x):\n        # Use not-yet-scriptable memory efficient mode\n        if self.use_memory_efficient:\n            return self.memory_efficient(x)\n        else:\n            return x + 10\n\nm = torch.jit.script(MyModule(use_memory_efficient=False))\nm.save(\"m.pt\")\n\nm = torch.jit.script(MyModule(use_memory_efficient=True))\n# exception raised\nm(torch.rand(100))\n \n"}, {"name": "torch.jit.wait()", "path": "generated/torch.jit.wait#torch.jit.wait", "type": "TorchScript", "text": " \ntorch.jit.wait(future) [source]\n \nForces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task. See fork() for docs and examples. :param func: an asynchronous task reference, created through torch.jit.fork :type func: torch.jit.Future[T]  Returns \nthe return value of the the completed task  Return type \nT   \n"}, {"name": "torch.kaiser_window()", "path": "generated/torch.kaiser_window#torch.kaiser_window", "type": "torch", "text": " \ntorch.kaiser_window(window_length, periodic=True, beta=12.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and N = L - 1 if periodic is False and L if periodic is True, where L is the window_length. This function computes:  outi=I0(\u03b21\u2212(i\u2212N/2N/2)2)/I0(\u03b2)out_i = I_0 \\left( \\beta \\sqrt{1 - \\left( {\\frac{i - N/2}{N/2}} \\right) ^2 } \\right) / I_0( \\beta )  \nCalling torch.kaiser_window(L, B, periodic=True) is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]). The periodic argument is intended as a helpful shorthand to produce a periodic window as input to functions like torch.stft().  Note If window_length is one, then the returned window is a single element tensor containing a one.   Parameters \n \nwindow_length (int) \u2013 length of the window. \nperiodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis. If False, returns a symmetric window suitable for use in filter design. \nbeta (float, optional) \u2013 shape parameter for the window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    \n"}, {"name": "torch.kron()", "path": "generated/torch.kron#torch.kron", "type": "torch", "text": " \ntorch.kron(input, other, *, out=None) \u2192 Tensor  \nComputes the Kronecker product, denoted by \u2297\\otimes , of input and other. If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n)  tensor and other is a (b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n)  tensor, the result will be a (a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n)  tensor with the following entries:  (input\u2297other)k0,k1,\u2026,kn=inputi0,i1,\u2026,in\u2217otherj0,j1,\u2026,jn,(\\text{input} \\otimes \\text{other})_{k_0, k_1, \\dots, k_n} = \\text{input}_{i_0, i_1, \\dots, i_n} * \\text{other}_{j_0, j_1, \\dots, j_n},  \nwhere kt=it\u2217bt+jtk_t = i_t * b_t + j_t  for 0\u2264t\u2264n0 \\leq t \\leq n . If one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions. Supports real-valued and complex-valued inputs.  Note This function generalizes the typical definition of the Kronecker product for two matrices to two tensors, as described above. When input is a (m\u00d7n)(m \\times n)  matrix and other is a (p\u00d7q)(p \\times q)  matrix, the result will be a (p\u2217m\u00d7q\u2217n)(p*m \\times q*n)  block matrix:  A\u2297B=[a11B\u22efa1nB\u22ee\u22f1\u22eeam1B\u22efamnB]\\mathbf{A} \\otimes \\mathbf{B}=\\begin{bmatrix} a_{11} \\mathbf{B} & \\cdots & a_{1 n} \\mathbf{B} \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m 1} \\mathbf{B} & \\cdots & a_{m n} \\mathbf{B} \\end{bmatrix}  \nwhere input is A\\mathbf{A}  and other is B\\mathbf{B} .   Parameters \n \ninput (Tensor) \u2013  \nother (Tensor) \u2013    Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> mat1 = torch.eye(2)\n>>> mat2 = torch.ones(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 1., 0., 0.],\n        [1., 1., 0., 0.],\n        [0., 0., 1., 1.],\n        [0., 0., 1., 1.]])\n\n>>> mat1 = torch.eye(2)\n>>> mat2 = torch.arange(1, 5).reshape(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]])\n \n"}, {"name": "torch.kthvalue()", "path": "generated/torch.kthvalue#torch.kthvalue", "type": "torch", "text": " \ntorch.kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)  \nReturns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim. And indices is the index location of each element found. If dim is not given, the last dimension of the input is chosen. If keepdim is True, both the values and indices tensors are the same size as input, except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in both the values and indices tensors having 1 fewer dimension than the input tensor.  Note When input is a CUDA tensor and there are multiple valid k th values, this function may nondeterministically return indices for any of them.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nk (int) \u2013 k for the k-th smallest element \ndim (int, optional) \u2013 the dimension to find the kth value along \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers   Example: >>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.kthvalue(x, 4)\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n\n>>> x=torch.arange(1.,7.).resize_(2,3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.]])\n>>> torch.kthvalue(x, 2, 0, True)\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))\n \n"}, {"name": "torch.lcm()", "path": "generated/torch.lcm#torch.lcm", "type": "torch", "text": " \ntorch.lcm(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise least common multiple (LCM) of input and other. Both input and other must have integer types.  Note This defines lcm(0,0)=0lcm(0, 0) = 0  and lcm(0,a)=0lcm(0, a) = 0 .   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.lcm(a, b)\ntensor([15, 20, 15])\n>>> c = torch.tensor([3])\n>>> torch.lcm(a, c)\ntensor([15, 30, 15])\n \n"}, {"name": "torch.ldexp()", "path": "generated/torch.ldexp#torch.ldexp", "type": "torch", "text": " \ntorch.ldexp(input, other, *, out=None) \u2192 Tensor  \nMultiplies input by 2**:attr:other.  outi=inputi\u22172iother\\text{{out}}_i = \\text{{input}}_i * 2^\\text{{other}}_i  \nTypically this function is used to construct floating point numbers by multiplying mantissas in input with integral powers of two created from the exponents in :attr:\u2019other\u2019.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 a tensor of exponents, typically integers.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))\ntensor([2.])\n>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))\ntensor([ 2.,  4.,  8., 16.])\n   \n"}, {"name": "torch.le()", "path": "generated/torch.le#torch.le", "type": "torch", "text": " \ntorch.le(input, other, *, out=None) \u2192 Tensor  \nComputes input\u2264other\\text{input} \\leq \\text{other}  element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or Scalar) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is less than or equal to other and False elsewhere   Example: >>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, False], [True, True]])\n \n"}, {"name": "torch.lerp()", "path": "generated/torch.lerp#torch.lerp", "type": "torch", "text": " \ntorch.lerp(input, end, weight, *, out=None)  \nDoes a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.  outi=starti+weighti\u00d7(endi\u2212starti)\\text{out}_i = \\text{start}_i + \\text{weight}_i \\times (\\text{end}_i - \\text{start}_i)  \nThe shapes of start and end must be broadcastable. If weight is a tensor, then the shapes of weight, start, and end must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the tensor with the starting points \nend (Tensor) \u2013 the tensor with the ending points \nweight (float or tensor) \u2013 the weight for the interpolation formula   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> start = torch.arange(1., 5.)\n>>> end = torch.empty(4).fill_(10)\n>>> start\ntensor([ 1.,  2.,  3.,  4.])\n>>> end\ntensor([ 10.,  10.,  10.,  10.])\n>>> torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n \n"}, {"name": "torch.less()", "path": "generated/torch.less#torch.less", "type": "torch", "text": " \ntorch.less(input, other, *, out=None) \u2192 Tensor  \nAlias for torch.lt(). \n"}, {"name": "torch.less_equal()", "path": "generated/torch.less_equal#torch.less_equal", "type": "torch", "text": " \ntorch.less_equal(input, other, *, out=None) \u2192 Tensor  \nAlias for torch.le(). \n"}, {"name": "torch.lgamma()", "path": "generated/torch.lgamma#torch.lgamma", "type": "torch", "text": " \ntorch.lgamma(input, *, out=None) \u2192 Tensor  \nComputes the logarithm of the gamma function on input.  outi=log\u2061\u0393(inputi)\\text{out}_{i} = \\log \\Gamma(\\text{input}_{i})  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.lgamma(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n \n"}, {"name": "torch.linalg", "path": "linalg", "type": "torch.linalg", "text": "torch.linalg Common linear algebra operations. This module is in BETA. New functions are still being added, and some functions may change in future PyTorch releases. See the documentation of each function for details. Functions  \ntorch.linalg.cholesky(input, *, out=None) \u2192 Tensor  \nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-valued matrices) positive-definite matrix or the Cholesky decompositions for a batch of such matrices. Each decomposition has the form:  input=LLH\\text{input} = LL^H \nwhere LL  is a lower-triangular matrix and LHL^H  is the conjugate transpose of LL , which is just a transpose for the case of real-valued input matrices. In code it translates to input = L @ L.t() if input is real-valued and input = L @ L.conj().t() if input is complex-valued. The batch of LL  matrices is returned. Supports real-valued and complex-valued inputs.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note LAPACK\u2019s potrf is used for CPU inputs, and MAGMA\u2019s potrf is used for CUDA inputs.   Note If input is not a Hermitian positive-definite matrix, or if it\u2019s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown. If input is a batch of matrices, then the error message will include the batch index of the first matrix that is not Hermitian positive-definite.   Parameters \ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  consisting of Hermitian positive-definite n\u00d7nn \\times n  matrices, where \u2217*  is zero or more batch dimensions.  Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> a = torch.randn(2, 2, dtype=torch.complex128)\n>>> a = torch.mm(a, a.t().conj())  # creates a Hermitian positive-definite matrix\n>>> l = torch.linalg.cholesky(a)\n>>> a\ntensor([[2.5266+0.0000j, 1.9586-2.0626j],\n        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)\n>>> l\ntensor([[1.5895+0.0000j, 0.0000+0.0000j],\n        [1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)\n>>> torch.mm(l, l.t().conj())\ntensor([[2.5266+0.0000j, 1.9586-2.0626j],\n        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)\n\n>>> a = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> a = torch.matmul(a, a.transpose(-2, -1))  # creates a symmetric positive-definite matrix\n>>> l = torch.linalg.cholesky(a)\n>>> a\ntensor([[[ 1.1629,  2.0237],\n        [ 2.0237,  6.6593]],\n\n        [[ 0.4187,  0.1830],\n        [ 0.1830,  0.1018]],\n\n        [[ 1.9348, -2.5744],\n        [-2.5744,  4.6386]]], dtype=torch.float64)\n>>> l\ntensor([[[ 1.0784,  0.0000],\n        [ 1.8766,  1.7713]],\n\n        [[ 0.6471,  0.0000],\n        [ 0.2829,  0.1477]],\n\n        [[ 1.3910,  0.0000],\n        [-1.8509,  1.1014]]], dtype=torch.float64)\n>>> torch.allclose(torch.matmul(l, l.transpose(-2, -1)), a)\nTrue\n \n  \ntorch.linalg.cond(input, p=None, *, out=None) \u2192 Tensor  \nComputes the condition number of a matrix input, or of each matrix in a batched input, using the matrix norm defined by p. For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} this is defined as the matrix norm of input times the matrix norm of the inverse of input computed using torch.linalg.norm(). While for norms {None, 2, -2} this is defined as the ratio between the largest and smallest singular values computed using torch.linalg.svd(). This function supports float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function may synchronize that device with the CPU depending on which norm p is used.   Note For norms {None, 2, -2}, input may be a non-square matrix or batch of non-square matrices. For other norms, however, input must be a square matrix or a batch of square matrices, and if this requirement is not satisfied a RuntimeError will be thrown.   Note For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} if input is a non-invertible matrix then a tensor containing infinity will be returned. If input is a batch of matrices and one or more of them is not invertible then a RuntimeError will be thrown.   Parameters \n \ninput (Tensor) \u2013 the input matrix of size (m, n) or the batch of matrices of size (*, m, n) where * is one or more batch dimensions. \np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 \nthe type of the matrix norm to use in the computations. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be used:   \np norm for matrices   \nNone ratio of the largest singular value to the smallest singular value  \n\u2019fro\u2019 Frobenius norm  \n\u2019nuc\u2019 nuclear norm  \ninf max(sum(abs(x), dim=1))  \n-inf min(sum(abs(x), dim=1))  \n1 max(sum(abs(x), dim=0))  \n-1 min(sum(abs(x), dim=0))  \n2 ratio of the largest singular value to the smallest singular value  \n-2 ratio of the smallest singular value to the largest singular value   Default: None    Keyword Arguments \nout (Tensor, optional) \u2013 tensor to write the output to. Default is None.  Returns \nThe condition number of input. The output dtype is always real valued even for complex inputs (e.g. float if input is cfloat).   Examples: >>> a = torch.randn(3, 4, 4, dtype=torch.complex64)\n>>> torch.linalg.cond(a)\n>>> a = torch.tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])\n>>> torch.linalg.cond(a)\ntensor([1.4142])\n>>> torch.linalg.cond(a, 'fro')\ntensor(3.1623)\n>>> torch.linalg.cond(a, 'nuc')\ntensor(9.2426)\n>>> torch.linalg.cond(a, float('inf'))\ntensor(2.)\n>>> torch.linalg.cond(a, float('-inf'))\ntensor(1.)\n>>> torch.linalg.cond(a, 1)\ntensor(2.)\n>>> torch.linalg.cond(a, -1)\ntensor(1.)\n>>> torch.linalg.cond(a, 2)\ntensor([1.4142])\n>>> torch.linalg.cond(a, -2)\ntensor([0.7071])\n\n>>> a = torch.randn(2, 3, 3)\n>>> a\ntensor([[[-0.9204,  1.1140,  1.2055],\n        [ 0.3988, -0.2395, -0.7441],\n        [-0.5160,  0.3115,  0.2619]],\n\n        [[-2.2128,  0.9241,  2.1492],\n        [-1.1277,  2.7604, -0.8760],\n        [ 1.2159,  0.5960,  0.0498]]])\n>>> torch.linalg.cond(a)\ntensor([[9.5917],\n        [3.2538]])\n\n>>> a = torch.randn(2, 3, 3, dtype=torch.complex64)\n>>> a\ntensor([[[-0.4671-0.2137j, -0.1334-0.9508j,  0.6252+0.1759j],\n        [-0.3486-0.2991j, -0.1317+0.1252j,  0.3025-0.1604j],\n        [-0.5634+0.8582j,  0.1118-0.4677j, -0.1121+0.7574j]],\n\n        [[ 0.3964+0.2533j,  0.9385-0.6417j, -0.0283-0.8673j],\n        [ 0.2635+0.2323j, -0.8929-1.1269j,  0.3332+0.0733j],\n        [ 0.1151+0.1644j, -1.1163+0.3471j, -0.5870+0.1629j]]])\n>>> torch.linalg.cond(a)\ntensor([[4.6245],\n        [4.5671]])\n>>> torch.linalg.cond(a, 1)\ntensor([9.2589, 9.3486])\n \n  \ntorch.linalg.det(input) \u2192 Tensor  \nComputes the determinant of a square matrix input, or of each square matrix in a batched input. This function supports float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.   Note Backward through det internally uses torch.linalg.svd() when input is not invertible. In this case, double backward through det will be unstable when input doesn\u2019t have distinct singular values. See torch.linalg.svd() for more details.   Parameters \ninput (Tensor) \u2013 the input matrix of size (n, n) or the batch of matrices of size (*, n, n) where * is one or more batch dimensions.   Example: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.9478,  0.9158, -1.1295],\n        [ 0.9701,  0.7346, -1.8044],\n        [-0.2337,  0.0557,  0.6929]])\n>>> torch.linalg.det(a)\ntensor(0.0934)\n\n>>> a = torch.randn(3, 2, 2)\n>>> a\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> torch.linalg.det(a)\ntensor([1.1990, 0.4099, 0.7386])\n \n  \ntorch.linalg.slogdet(input, *, out=None) -> (Tensor, Tensor)  \nCalculates the sign and natural logarithm of the absolute value of a square matrix\u2019s determinant, or of the absolute values of the determinants of a batch of square matrices input. The determinant can be computed with sign * exp(logabsdet). Supports input of float, double, cfloat and cdouble datatypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.   Note For matrices that have zero determinant, this returns (0, -inf). If input is batched then the entries in the result tensors corresponding to matrices with the zero determinant have sign 0 and the natural logarithm of the absolute value of the determinant -inf.   Parameters \ninput (Tensor) \u2013 the input matrix of size (n,n)(n, n)  or the batch of matrices of size (\u2217,n,n)(*, n, n)  where \u2217*  is one or more batch dimensions.  Keyword Arguments \nout (tuple, optional) \u2013 tuple of two tensors to write the output to.  Returns \nA namedtuple (sign, logabsdet) containing the sign of the determinant and the natural logarithm of the absolute value of determinant, respectively.   Example: >>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.linalg.det(A)\ntensor(-0.7576)\n>>> torch.linalg.logdet(A)\ntensor(nan)\n>>> torch.linalg.slogdet(A)\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n \n  \ntorch.linalg.eigh(input, UPLO='L', *, out=None) -> (Tensor, Tensor)  \nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input. For a single matrix input, the tensor of eigenvalues w and the tensor of eigenvectors V decompose the input such that input = V diag(w) V\u1d34, where V\u1d34 is the transpose of V for real-valued input, or the conjugate transpose of V for complex-valued input. Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used. Supports input of float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The eigenvalues/eigenvectors are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.   Note The eigenvalues of real symmetric or complex Hermitian matrices are always real.   Note The eigenvectors of matrices are not unique, so any eigenvector multiplied by a constant remains a valid eigenvector. This function may compute different eigenvector representations on different device types. Usually the difference is only in the sign of the eigenvector.   Note See torch.linalg.eigvalsh() for a related function that computes only eigenvalues. However, that function is not differentiable.   Parameters \n \ninput (Tensor) \u2013 the Hermitian n times n matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions. \nUPLO ('L', 'U', optional) \u2013 controls whether to use the upper-triangular or the lower-triangular part of input in the computations. Default is 'L'.   Keyword Arguments \nout (tuple, optional) \u2013 tuple of two tensors to write the output to. Default is None.  Returns \nA namedtuple (eigenvalues, eigenvectors) containing  \n \neigenvalues (Tensor): Shape (*, m). \n\nThe eigenvalues in ascending order.    \n \neigenvectors (Tensor): Shape (*, m, m). \n\nThe orthonormal eigenvectors of the input.      Return type \n(Tensor, Tensor)   Examples: >>> a = torch.randn(2, 2, dtype=torch.complex128)\n>>> a = a + a.t().conj()  # creates a Hermitian matrix\n>>> a\ntensor([[2.9228+0.0000j, 0.2029-0.0862j],\n        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n>>> w, v = torch.linalg.eigh(a)\n>>> w\ntensor([0.3277, 2.9415], dtype=torch.float64)\n>>> v\ntensor([[-0.0846+-0.0000j, -0.9964+0.0000j],\n        [ 0.9170+0.3898j, -0.0779-0.0331j]], dtype=torch.complex128)\n>>> torch.allclose(torch.matmul(v, torch.matmul(w.to(v.dtype).diag_embed(), v.t().conj())), a)\nTrue\n\n>>> a = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> a = a + a.transpose(-2, -1)  # creates a symmetric matrix\n>>> w, v = torch.linalg.eigh(a)\n>>> torch.allclose(torch.matmul(v, torch.matmul(w.diag_embed(), v.transpose(-2, -1))), a)\nTrue\n \n  \ntorch.linalg.eigvalsh(input, UPLO='L', *, out=None) \u2192 Tensor  \nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input. The eigenvalues are returned in ascending order. Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used. Supports input of float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The eigenvalues are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.   Note The eigenvalues of real symmetric or complex Hermitian matrices are always real.   Note This function doesn\u2019t support backpropagation, please use torch.linalg.eigh() instead, which also computes the eigenvectors.   Note See torch.linalg.eigh() for a related function that computes both eigenvalues and eigenvectors.   Parameters \n \ninput (Tensor) \u2013 the Hermitian n times n matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions. \nUPLO ('L', 'U', optional) \u2013 controls whether to use the upper-triangular or the lower-triangular part of input in the computations. Default is 'L'.   Keyword Arguments \nout (Tensor, optional) \u2013 tensor to write the output to. Default is None.   Examples: >>> a = torch.randn(2, 2, dtype=torch.complex128)\n>>> a = a + a.t().conj()  # creates a Hermitian matrix\n>>> a\ntensor([[2.9228+0.0000j, 0.2029-0.0862j],\n        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n>>> w = torch.linalg.eigvalsh(a)\n>>> w\ntensor([0.3277, 2.9415], dtype=torch.float64)\n\n>>> a = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> a = a + a.transpose(-2, -1)  # creates a symmetric matrix\n>>> a\ntensor([[[ 2.8050, -0.3850],\n        [-0.3850,  3.2376]],\n\n        [[-1.0307, -2.7457],\n        [-2.7457, -1.7517]],\n\n        [[ 1.7166,  2.2207],\n        [ 2.2207, -2.0898]]], dtype=torch.float64)\n>>> w = torch.linalg.eigvalsh(a)\n>>> w\ntensor([[ 2.5797,  3.4629],\n        [-4.1605,  1.3780],\n        [-3.1113,  2.7381]], dtype=torch.float64)\n \n  \ntorch.linalg.matrix_rank(input, tol=None, hermitian=False, *, out=None) \u2192 Tensor  \nComputes the numerical rank of a matrix input, or of each matrix in a batched input. The matrix rank is computed as the number of singular values (or absolute eigenvalues when hermitian is True) that are greater than the specified tol threshold. If tol is not specified, tol is set to S.max(dim=-1)*max(input.shape[-2:])*eps, where S is the singular values (or absolute eigenvalues when hermitian is True), and eps is the epsilon value for the datatype of input. The epsilon value can be obtained using the eps attribute of torch.finfo. Supports input of float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The matrix rank is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation is done by obtaining the eigenvalues (see torch.linalg.eigvalsh()).   Parameters \n \ninput (Tensor) \u2013 the input matrix of size (m, n) or the batch of matrices of size (*, m, n) where * is one or more batch dimensions. \ntol (float, optional) \u2013 the tolerance value. Default is None\n \nhermitian (bool, optional) \u2013 indicates whether input is Hermitian. Default is False.   Keyword Arguments \nout (Tensor, optional) \u2013 tensor to write the output to. Default is None.   Examples: >>> a = torch.eye(10)\n>>> torch.linalg.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.linalg.matrix_rank(b)\ntensor(9)\n\n>>> a = torch.randn(4, 3, 2)\n>>> torch.linalg.matrix_rank(a)\ntensor([2, 2, 2, 2])\n\n>>> a = torch.randn(2, 4, 2, 3)\n>>> torch.linalg.matrix_rank(a)\ntensor([[2, 2, 2, 2],\n        [2, 2, 2, 2]])\n\n>>> a = torch.randn(2, 4, 3, 3, dtype=torch.complex64)\n>>> torch.linalg.matrix_rank(a)\ntensor([[3, 3, 3, 3],\n        [3, 3, 3, 3]])\n>>> torch.linalg.matrix_rank(a, hermitian=True)\ntensor([[3, 3, 3, 3],\n        [3, 3, 3, 3]])\n>>> torch.linalg.matrix_rank(a, tol=1.0)\ntensor([[3, 2, 2, 2],\n        [1, 2, 1, 2]])\n>>> torch.linalg.matrix_rank(a, tol=1.0, hermitian=True)\ntensor([[2, 2, 2, 1],\n        [1, 2, 2, 2]])\n \n  \ntorch.linalg.norm(input, ord=None, dim=None, keepdim=False, *, out=None, dtype=None) \u2192 Tensor  \nReturns the matrix norm or vector norm of a given tensor. This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the ord parameter.  Parameters \n \ninput (Tensor) \u2013 The input tensor. If dim is None, x must be 1-D or 2-D, unless ord is None. If both dim and ord are None, the 2-norm of the input flattened to 1-D will be returned. Its data type must be either a floating point or complex type. For complex inputs, the norm is calculated on of the absolute values of each element. If the input is complex and neither dtype nor out is specified, the result\u2019s data type will be the corresponding floating point type (e.g. float if input is complexfloat). \nord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 \nThe order of norm. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be calculated:   \nord norm for matrices norm for vectors   \nNone Frobenius norm 2-norm  \n\u2019fro\u2019 Frobenius norm \u2013 not supported \u2013  \n\u2018nuc\u2019 nuclear norm \u2013 not supported \u2013  \ninf max(sum(abs(x), dim=1)) max(abs(x))  \n-inf min(sum(abs(x), dim=1)) min(abs(x))  \n0 \u2013 not supported \u2013 sum(x != 0)  \n1 max(sum(abs(x), dim=0)) as below  \n-1 min(sum(abs(x), dim=0)) as below  \n2 2-norm (largest sing. value) as below  \n-2 smallest singular value as below  \nother \u2013 not supported \u2013 sum(abs(x)**ord)**(1./ord)   Default: None  \ndim (int, 2-tuple of python:ints, 2-list of python:ints, optional) \u2013 If dim is an int, vector norm will be calculated over the specified dimension. If dim is a 2-tuple of ints, matrix norm will be calculated over the specified dimensions. If dim is None, matrix norm will be calculated when the input tensor has two dimensions, and vector norm will be calculated when the input tensor has one dimension. Default: None\n \nkeepdim (bool, optional) \u2013 If set to True, the reduced dimensions are retained in the result as dimensions with size one. Default: False\n   Keyword Arguments \n \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None\n \ndtype (torch.dtype, optional) \u2013 If specified, the input tensor is cast to dtype before performing the operation, and the returned tensor\u2019s type will be dtype. If this argument is used in conjunction with the out argument, the output tensor\u2019s type must match this argument or a RuntimeError will be raised. Default: None\n    Examples: >>> import torch\n>>> from torch import linalg as LA\n>>> a = torch.arange(9, dtype=torch.float) - 4\n>>> a\ntensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n>>> b = a.reshape((3, 3))\n>>> b\ntensor([[-4., -3., -2.],\n        [-1.,  0.,  1.],\n        [ 2.,  3.,  4.]])\n\n>>> LA.norm(a)\ntensor(7.7460)\n>>> LA.norm(b)\ntensor(7.7460)\n>>> LA.norm(b, 'fro')\ntensor(7.7460)\n>>> LA.norm(a, float('inf'))\ntensor(4.)\n>>> LA.norm(b, float('inf'))\ntensor(9.)\n>>> LA.norm(a, -float('inf'))\ntensor(0.)\n>>> LA.norm(b, -float('inf'))\ntensor(2.)\n\n>>> LA.norm(a, 1)\ntensor(20.)\n>>> LA.norm(b, 1)\ntensor(7.)\n>>> LA.norm(a, -1)\ntensor(0.)\n>>> LA.norm(b, -1)\ntensor(6.)\n>>> LA.norm(a, 2)\ntensor(7.7460)\n>>> LA.norm(b, 2)\ntensor(7.3485)\n\n>>> LA.norm(a, -2)\ntensor(0.)\n>>> LA.norm(b.double(), -2)\ntensor(1.8570e-16, dtype=torch.float64)\n>>> LA.norm(a, 3)\ntensor(5.8480)\n>>> LA.norm(a, -3)\ntensor(0.)\n Using the dim argument to compute vector norms: >>> c = torch.tensor([[1., 2., 3.],\n...                   [-1, 1, 4]])\n>>> LA.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> LA.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> LA.norm(c, ord=1, dim=1)\ntensor([6., 6.])\n Using the dim argument to compute matrix norms: >>> m = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)\n>>> LA.norm(m, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n \n  \ntorch.linalg.pinv(input, rcond=1e-15, hermitian=False, *, out=None) \u2192 Tensor  \nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a matrix input, or of each matrix in a batched input. The singular values (or the absolute values of the eigenvalues when hermitian is True) that are below the specified rcond threshold are treated as zero and discarded in the computation. Supports input of float, double, cfloat and cdouble datatypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The pseudo-inverse is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation of the pseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see torch.linalg.eigh()).   Note If singular value decomposition or eigenvalue decomposition algorithms do not converge then a RuntimeError will be thrown.   Parameters \n \ninput (Tensor) \u2013 the input matrix of size (m, n) or the batch of matrices of size (*, m, n) where * is one or more batch dimensions. \nrcond (float, Tensor, optional) \u2013 the tolerance value to determine the cutoff for small singular values. Must be broadcastable to the singular values of input as returned by torch.svd(). Default is 1e-15. \nhermitian (bool, optional) \u2013 indicates whether input is Hermitian. Default is False.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.   Examples: >>> input = torch.randn(3, 5)\n>>> input\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.linalg.pinv(input)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n\nBatched linalg.pinv example\n>>> a = torch.randn(2, 6, 3)\n>>> b = torch.linalg.pinv(a)\n>>> torch.matmul(b, a)\ntensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],\n        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],\n        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],\n\n        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],\n        [-2.2352e-07,  1.0000e+00,  1.1921e-07],\n        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])\n\nHermitian input example\n>>> a = torch.randn(3, 3, dtype=torch.complex64)\n>>> a = a + a.t().conj()  # creates a Hermitian matrix\n>>> b = torch.linalg.pinv(a, hermitian=True)\n>>> torch.matmul(b, a)\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\n        5.9605e-08-2.3842e-07j],\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\n        -4.7684e-07+1.1921e-07j],\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\n        1.0000e+00-1.7897e-07j]])\n\nNon-default rcond example\n>>> rcond = 0.5\n>>> a = torch.randn(3, 3)\n>>> torch.linalg.pinv(a)\ntensor([[ 0.2971, -0.4280, -2.0111],\n        [-0.0090,  0.6426, -0.1116],\n        [-0.7832, -0.2465,  1.0994]])\n>>> torch.linalg.pinv(a, rcond)\ntensor([[-0.2672, -0.2351, -0.0539],\n        [-0.0211,  0.6467, -0.0698],\n        [-0.4400, -0.3638, -0.0910]])\n\nMatrix-wise rcond example\n>>> a = torch.randn(5, 6, 2, 3, 3)\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\n>>> torch.linalg.pinv(a, rcond)\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\n>>> torch.linalg.pinv(a, rcond)\n \n  \ntorch.linalg.svd(input, full_matrices=True, compute_uv=True, *, out=None) -> (Tensor, Tensor, Tensor)  \nComputes the singular value decomposition of either a matrix or batch of matrices input.\u201d The singular value decomposition is represented as a namedtuple (U, S, Vh), such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@} diag(S) \\times Vh . If input is a batch of tensors, then U, S, and Vh are also batched with the same batch dimensions as input. If full_matrices is False (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n,m)min(n, m)  orthonormal columns. If compute_uv is False, the returned U and Vh will be empy tensors with no elements and the same device as input. The full_matrices argument has no effect when compute_uv is False. The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.  Note Unlike NumPy\u2019s linalg.svd, this always returns a namedtuple of three tensors, even when compute_uv=False. This behavior may change in a future PyTorch release.   Note The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.   Note The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the cuSOLVER routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and uses the MAGMA routine gesdd on earlier versions of CUDA.   Note The returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride().   Note Gradients computed using U and Vh may be unstable if input is not full rank or has non-unique singular values.   Note When full_matrices = True, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.   Note The S tensor can only be used to compute gradients if compute_uv is True.   Note Since U and V of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi}  while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different U and V tensors.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)  where * is zero or more batch dimensions consisting of m\u00d7nm \\times n  matrices. \nfull_matrices (bool, optional) \u2013 controls whether to compute the full or reduced decomposition, and consequently the shape of returned U and V. Defaults to True. \ncompute_uv (bool, optional) \u2013 whether to compute U and V or not. Defaults to True. \nout (tuple, optional) \u2013 a tuple of three tensors to use for the outputs. If compute_uv=False, the 1st and 3rd arguments must be tensors, but they are ignored. E.g. you can pass (torch.Tensor(), out_S, torch.Tensor())\n    Example: >>> import torch\n>>> a = torch.randn(5, 3)\n>>> a\ntensor([[-0.3357, -0.2987, -1.1096],\n        [ 1.4894,  1.0016, -0.4572],\n        [-1.9401,  0.7437,  2.0968],\n        [ 0.1515,  1.3812,  1.5491],\n        [-1.8489, -0.5907, -2.5673]])\n>>>\n>>> # reconstruction in the full_matrices=False case\n>>> u, s, vh = torch.linalg.svd(a, full_matrices=False)\n>>> u.shape, s.shape, vh.shape\n(torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(a, u @ torch.diag(s) @ vh)\ntensor(1.0486e-06)\n>>>\n>>> # reconstruction in the full_matrices=True case\n>>> u, s, vh = torch.linalg.svd(a)\n>>> u.shape, s.shape, vh.shape\n(torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)\n>>> torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)\ntensor(1.0486e-06)\n>>>\n>>> # extra dimensions\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, vh = torch.linalg.svd(a_big, full_matrices=False)\n>>> torch.dist(a_big, u @ torch.diag_embed(s) @ vh)\ntensor(3.0957e-06)\n \n  \ntorch.linalg.solve(input, other, *, out=None) \u2192 Tensor  \nComputes the solution x to the matrix equation matmul(input, x) = other with a square matrix, or batches of such matrices, input and one or more right-hand side vectors other. If input is batched and other is not, then other is broadcast to have the same batch dimensions as input. The resulting tensor has the same shape as the (possibly broadcast) other. Supports input of float, double, cfloat and cdouble dtypes.  Note If input is a non-square or non-invertible matrix, or a batch containing non-square matrices or one or more non-invertible matrices, then a RuntimeError will be thrown.   Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Parameters \n \ninput (Tensor) \u2013 the square n\u00d7nn \\times n  matrix or the batch of such matrices of size (\u2217,n,n)(*, n, n)  where * is one or more batch dimensions. \nother (Tensor) \u2013 right-hand side tensor of shape (\u2217,n)(*, n)  or (\u2217,n,k)(*, n, k) , where kk  is the number of right-hand side vectors.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> A = torch.eye(3)\n>>> b = torch.randn(3)\n>>> x = torch.linalg.solve(A, b)\n>>> torch.allclose(A @ x, b)\nTrue\n Batched input: >>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(3, 1)\n>>> x = torch.linalg.solve(A, b)\n>>> torch.allclose(A @ x, b)\nTrue\n>>> b = torch.rand(3) # b is broadcast internally to (*A.shape[:-2], 3)\n>>> x = torch.linalg.solve(A, b)\n>>> x.shape\ntorch.Size([2, 3])\n>>> Ax = A @ x.unsqueeze(-1)\n>>> torch.allclose(Ax, b.unsqueeze(-1).expand_as(Ax))\nTrue\n \n  \ntorch.linalg.tensorinv(input, ind=2, *, out=None) \u2192 Tensor  \nComputes a tensor input_inv such that tensordot(input_inv, input, ind) == I_n (inverse tensor equation), where I_n is the n-dimensional identity tensor and n is equal to input.ndim. The resulting tensor input_inv has shape equal to input.shape[ind:] + input.shape[:ind]. Supports input of float, double, cfloat and cdouble data types.  Note If input is not invertible or does not satisfy the requirement prod(input.shape[ind:]) == prod(input.shape[:ind]), then a RuntimeError will be thrown.   Note When input is a 2-dimensional tensor and ind=1, this function computes the (multiplicative) inverse of input, equivalent to calling torch.inverse().   Parameters \n \ninput (Tensor) \u2013 A tensor to invert. Its shape must satisfy prod(input.shape[:ind]) == prod(input.shape[ind:]). \nind (int) \u2013 A positive integer that describes the inverse tensor equation. See torch.tensordot() for details. Default: 2.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> a = torch.eye(4 * 6).reshape((4, 6, 8, 3))\n>>> ainv = torch.linalg.tensorinv(a, ind=2)\n>>> ainv.shape\ntorch.Size([8, 3, 4, 6])\n>>> b = torch.randn(4, 6)\n>>> torch.allclose(torch.tensordot(ainv, b), torch.linalg.tensorsolve(a, b))\nTrue\n\n>>> a = torch.randn(4, 4)\n>>> a_tensorinv = torch.linalg.tensorinv(a, ind=1)\n>>> a_inv = torch.inverse(a)\n>>> torch.allclose(a_tensorinv, a_inv)\nTrue\n \n  \ntorch.linalg.tensorsolve(input, other, dims=None, *, out=None) \u2192 Tensor  \nComputes a tensor x such that tensordot(input, x, dims=x.ndim) = other. The resulting tensor x has the same shape as input[other.ndim:]. Supports real-valued and complex-valued inputs.  Note If input does not satisfy the requirement prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim]) after (optionally) moving the dimensions using dims, then a RuntimeError will be thrown.   Parameters \n \ninput (Tensor) \u2013 \u201cleft-hand-side\u201d tensor, it must satisfy the requirement prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim]). \nother (Tensor) \u2013 \u201cright-hand-side\u201d tensor of shape input.shape[other.ndim]. \ndims (Tuple[int]) \u2013 dimensions of input to be moved before the computation. Equivalent to calling input = movedim(input, dims, range(len(dims) - input.ndim, 0)). If None (default), no dimensions are moved.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> a = torch.eye(2 * 3 * 4).reshape((2 * 3, 4, 2, 3, 4))\n>>> b = torch.randn(2 * 3, 4)\n>>> x = torch.linalg.tensorsolve(a, b)\n>>> x.shape\ntorch.Size([2, 3, 4])\n>>> torch.allclose(torch.tensordot(a, x, dims=x.ndim), b)\nTrue\n\n>>> a = torch.randn(6, 4, 4, 3, 2)\n>>> b = torch.randn(4, 3, 2)\n>>> x = torch.linalg.tensorsolve(a, b, dims=(0, 2))\n>>> x.shape\ntorch.Size([6, 4])\n>>> a = a.permute(1, 3, 4, 0, 2)\n>>> a.shape[b.ndim:]\ntorch.Size([6, 4])\n>>> torch.allclose(torch.tensordot(a, x, dims=x.ndim), b, atol=1e-6)\nTrue\n \n  \ntorch.linalg.inv(input, *, out=None) \u2192 Tensor  \nComputes the multiplicative inverse matrix of a square matrix input, or of each square matrix in a batched input. The result satisfies the relation: matmul(inv(input),input) = matmul(input,inv(input)) = eye(input.shape[0]).expand_as(input). Supports input of float, double, cfloat and cdouble data types.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The inverse matrix is computed using LAPACK\u2019s getrf and getri routines for CPU inputs. For CUDA inputs, cuSOLVER\u2019s getrf and getrs routines as well as cuBLAS\u2019 getrf and getri routines are used if CUDA version >= 10.1.243, otherwise MAGMA\u2019s getrf and getri routines are used instead.   Note If input is a non-invertible matrix or non-square matrix, or batch with at least one such matrix, then a RuntimeError will be thrown.   Parameters \ninput (Tensor) \u2013 the square (n, n) matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions.  Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.   Examples: >>> x = torch.rand(4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = torch.mm(x, y)\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.max(torch.abs(z - torch.eye(4))) # Max non-zero\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = torch.matmul(x, y)\n>>> torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.linalg.inv(x)\n>>> z = torch.mm(x, y)\n>>> z\ntensor([[ 1.0000e+00+0.0000e+00j, -1.3878e-16+3.4694e-16j,\n        5.5511e-17-1.1102e-16j,  0.0000e+00-1.6653e-16j],\n        [ 5.5511e-16-1.6653e-16j,  1.0000e+00+6.9389e-17j,\n        2.2204e-16-1.1102e-16j, -2.2204e-16+1.1102e-16j],\n        [ 3.8858e-16-1.2490e-16j,  2.7756e-17+3.4694e-17j,\n        1.0000e+00+0.0000e+00j, -4.4409e-16+5.5511e-17j],\n        [ 4.4409e-16+5.5511e-16j, -3.8858e-16+1.8041e-16j,\n        2.2204e-16+0.0000e+00j,  1.0000e+00-3.4694e-16j]],\n    dtype=torch.complex128)\n>>> torch.max(torch.abs(z - torch.eye(4, dtype=torch.cdouble))) # Max non-zero\ntensor(7.5107e-16, dtype=torch.float64)\n \n  \ntorch.linalg.qr(input, mode='reduced', *, out=None) -> (Tensor, Tensor)  \nComputes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices. Depending on the value of mode this function returns the reduced or complete QR factorization. See below for a list of valid modes.  Note Differences with numpy.linalg.qr:  \nmode='raw' is not implemented unlike numpy.linalg.qr, this function always returns a tuple of two tensors. When mode='r', the Q tensor is an empty tensor. This behavior may change in a future PyTorch release.    Note Backpropagation is not supported for mode='r'. Use mode='reduced' instead. Backpropagation is also not supported if the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))  columns of any matrix in input are not linearly independent. While no error will be thrown when this occurs the values of the \u201cgradient\u201d produced may be anything. This behavior may change in the future.   Note This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m\u00d7nm \\times n . \nmode (str, optional) \u2013 \nif k = min(m, n) then:  \n'reduced' : returns (Q, R) with dimensions (m, k), (k, n) (default) \n'complete': returns (Q, R) with dimensions (m, m), (m, n) \n'r': computes only R; returns (Q, R) where Q is empty and R has dimensions (k, n)     Keyword Arguments \nout (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of mode above.   Example: >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.linalg.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> q2, r2 = torch.linalg.qr(a, mode='r')\n>>> q2\ntensor([])\n>>> torch.equal(r, r2)\nTrue\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.linalg.qr(a, mode='complete')\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue\n \n\n"}, {"name": "torch.linalg.cholesky()", "path": "linalg#torch.linalg.cholesky", "type": "torch.linalg", "text": " \ntorch.linalg.cholesky(input, *, out=None) \u2192 Tensor  \nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-valued matrices) positive-definite matrix or the Cholesky decompositions for a batch of such matrices. Each decomposition has the form:  input=LLH\\text{input} = LL^H \nwhere LL  is a lower-triangular matrix and LHL^H  is the conjugate transpose of LL , which is just a transpose for the case of real-valued input matrices. In code it translates to input = L @ L.t() if input is real-valued and input = L @ L.conj().t() if input is complex-valued. The batch of LL  matrices is returned. Supports real-valued and complex-valued inputs.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note LAPACK\u2019s potrf is used for CPU inputs, and MAGMA\u2019s potrf is used for CUDA inputs.   Note If input is not a Hermitian positive-definite matrix, or if it\u2019s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown. If input is a batch of matrices, then the error message will include the batch index of the first matrix that is not Hermitian positive-definite.   Parameters \ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  consisting of Hermitian positive-definite n\u00d7nn \\times n  matrices, where \u2217*  is zero or more batch dimensions.  Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> a = torch.randn(2, 2, dtype=torch.complex128)\n>>> a = torch.mm(a, a.t().conj())  # creates a Hermitian positive-definite matrix\n>>> l = torch.linalg.cholesky(a)\n>>> a\ntensor([[2.5266+0.0000j, 1.9586-2.0626j],\n        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)\n>>> l\ntensor([[1.5895+0.0000j, 0.0000+0.0000j],\n        [1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)\n>>> torch.mm(l, l.t().conj())\ntensor([[2.5266+0.0000j, 1.9586-2.0626j],\n        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)\n\n>>> a = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> a = torch.matmul(a, a.transpose(-2, -1))  # creates a symmetric positive-definite matrix\n>>> l = torch.linalg.cholesky(a)\n>>> a\ntensor([[[ 1.1629,  2.0237],\n        [ 2.0237,  6.6593]],\n\n        [[ 0.4187,  0.1830],\n        [ 0.1830,  0.1018]],\n\n        [[ 1.9348, -2.5744],\n        [-2.5744,  4.6386]]], dtype=torch.float64)\n>>> l\ntensor([[[ 1.0784,  0.0000],\n        [ 1.8766,  1.7713]],\n\n        [[ 0.6471,  0.0000],\n        [ 0.2829,  0.1477]],\n\n        [[ 1.3910,  0.0000],\n        [-1.8509,  1.1014]]], dtype=torch.float64)\n>>> torch.allclose(torch.matmul(l, l.transpose(-2, -1)), a)\nTrue\n \n"}, {"name": "torch.linalg.cond()", "path": "linalg#torch.linalg.cond", "type": "torch.linalg", "text": " \ntorch.linalg.cond(input, p=None, *, out=None) \u2192 Tensor  \nComputes the condition number of a matrix input, or of each matrix in a batched input, using the matrix norm defined by p. For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} this is defined as the matrix norm of input times the matrix norm of the inverse of input computed using torch.linalg.norm(). While for norms {None, 2, -2} this is defined as the ratio between the largest and smallest singular values computed using torch.linalg.svd(). This function supports float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function may synchronize that device with the CPU depending on which norm p is used.   Note For norms {None, 2, -2}, input may be a non-square matrix or batch of non-square matrices. For other norms, however, input must be a square matrix or a batch of square matrices, and if this requirement is not satisfied a RuntimeError will be thrown.   Note For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} if input is a non-invertible matrix then a tensor containing infinity will be returned. If input is a batch of matrices and one or more of them is not invertible then a RuntimeError will be thrown.   Parameters \n \ninput (Tensor) \u2013 the input matrix of size (m, n) or the batch of matrices of size (*, m, n) where * is one or more batch dimensions. \np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 \nthe type of the matrix norm to use in the computations. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be used:   \np norm for matrices   \nNone ratio of the largest singular value to the smallest singular value  \n\u2019fro\u2019 Frobenius norm  \n\u2019nuc\u2019 nuclear norm  \ninf max(sum(abs(x), dim=1))  \n-inf min(sum(abs(x), dim=1))  \n1 max(sum(abs(x), dim=0))  \n-1 min(sum(abs(x), dim=0))  \n2 ratio of the largest singular value to the smallest singular value  \n-2 ratio of the smallest singular value to the largest singular value   Default: None    Keyword Arguments \nout (Tensor, optional) \u2013 tensor to write the output to. Default is None.  Returns \nThe condition number of input. The output dtype is always real valued even for complex inputs (e.g. float if input is cfloat).   Examples: >>> a = torch.randn(3, 4, 4, dtype=torch.complex64)\n>>> torch.linalg.cond(a)\n>>> a = torch.tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])\n>>> torch.linalg.cond(a)\ntensor([1.4142])\n>>> torch.linalg.cond(a, 'fro')\ntensor(3.1623)\n>>> torch.linalg.cond(a, 'nuc')\ntensor(9.2426)\n>>> torch.linalg.cond(a, float('inf'))\ntensor(2.)\n>>> torch.linalg.cond(a, float('-inf'))\ntensor(1.)\n>>> torch.linalg.cond(a, 1)\ntensor(2.)\n>>> torch.linalg.cond(a, -1)\ntensor(1.)\n>>> torch.linalg.cond(a, 2)\ntensor([1.4142])\n>>> torch.linalg.cond(a, -2)\ntensor([0.7071])\n\n>>> a = torch.randn(2, 3, 3)\n>>> a\ntensor([[[-0.9204,  1.1140,  1.2055],\n        [ 0.3988, -0.2395, -0.7441],\n        [-0.5160,  0.3115,  0.2619]],\n\n        [[-2.2128,  0.9241,  2.1492],\n        [-1.1277,  2.7604, -0.8760],\n        [ 1.2159,  0.5960,  0.0498]]])\n>>> torch.linalg.cond(a)\ntensor([[9.5917],\n        [3.2538]])\n\n>>> a = torch.randn(2, 3, 3, dtype=torch.complex64)\n>>> a\ntensor([[[-0.4671-0.2137j, -0.1334-0.9508j,  0.6252+0.1759j],\n        [-0.3486-0.2991j, -0.1317+0.1252j,  0.3025-0.1604j],\n        [-0.5634+0.8582j,  0.1118-0.4677j, -0.1121+0.7574j]],\n\n        [[ 0.3964+0.2533j,  0.9385-0.6417j, -0.0283-0.8673j],\n        [ 0.2635+0.2323j, -0.8929-1.1269j,  0.3332+0.0733j],\n        [ 0.1151+0.1644j, -1.1163+0.3471j, -0.5870+0.1629j]]])\n>>> torch.linalg.cond(a)\ntensor([[4.6245],\n        [4.5671]])\n>>> torch.linalg.cond(a, 1)\ntensor([9.2589, 9.3486])\n \n"}, {"name": "torch.linalg.det()", "path": "linalg#torch.linalg.det", "type": "torch.linalg", "text": " \ntorch.linalg.det(input) \u2192 Tensor  \nComputes the determinant of a square matrix input, or of each square matrix in a batched input. This function supports float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.   Note Backward through det internally uses torch.linalg.svd() when input is not invertible. In this case, double backward through det will be unstable when input doesn\u2019t have distinct singular values. See torch.linalg.svd() for more details.   Parameters \ninput (Tensor) \u2013 the input matrix of size (n, n) or the batch of matrices of size (*, n, n) where * is one or more batch dimensions.   Example: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.9478,  0.9158, -1.1295],\n        [ 0.9701,  0.7346, -1.8044],\n        [-0.2337,  0.0557,  0.6929]])\n>>> torch.linalg.det(a)\ntensor(0.0934)\n\n>>> a = torch.randn(3, 2, 2)\n>>> a\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> torch.linalg.det(a)\ntensor([1.1990, 0.4099, 0.7386])\n \n"}, {"name": "torch.linalg.eigh()", "path": "linalg#torch.linalg.eigh", "type": "torch.linalg", "text": " \ntorch.linalg.eigh(input, UPLO='L', *, out=None) -> (Tensor, Tensor)  \nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input. For a single matrix input, the tensor of eigenvalues w and the tensor of eigenvectors V decompose the input such that input = V diag(w) V\u1d34, where V\u1d34 is the transpose of V for real-valued input, or the conjugate transpose of V for complex-valued input. Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used. Supports input of float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The eigenvalues/eigenvectors are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.   Note The eigenvalues of real symmetric or complex Hermitian matrices are always real.   Note The eigenvectors of matrices are not unique, so any eigenvector multiplied by a constant remains a valid eigenvector. This function may compute different eigenvector representations on different device types. Usually the difference is only in the sign of the eigenvector.   Note See torch.linalg.eigvalsh() for a related function that computes only eigenvalues. However, that function is not differentiable.   Parameters \n \ninput (Tensor) \u2013 the Hermitian n times n matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions. \nUPLO ('L', 'U', optional) \u2013 controls whether to use the upper-triangular or the lower-triangular part of input in the computations. Default is 'L'.   Keyword Arguments \nout (tuple, optional) \u2013 tuple of two tensors to write the output to. Default is None.  Returns \nA namedtuple (eigenvalues, eigenvectors) containing  \n \neigenvalues (Tensor): Shape (*, m). \n\nThe eigenvalues in ascending order.    \n \neigenvectors (Tensor): Shape (*, m, m). \n\nThe orthonormal eigenvectors of the input.      Return type \n(Tensor, Tensor)   Examples: >>> a = torch.randn(2, 2, dtype=torch.complex128)\n>>> a = a + a.t().conj()  # creates a Hermitian matrix\n>>> a\ntensor([[2.9228+0.0000j, 0.2029-0.0862j],\n        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n>>> w, v = torch.linalg.eigh(a)\n>>> w\ntensor([0.3277, 2.9415], dtype=torch.float64)\n>>> v\ntensor([[-0.0846+-0.0000j, -0.9964+0.0000j],\n        [ 0.9170+0.3898j, -0.0779-0.0331j]], dtype=torch.complex128)\n>>> torch.allclose(torch.matmul(v, torch.matmul(w.to(v.dtype).diag_embed(), v.t().conj())), a)\nTrue\n\n>>> a = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> a = a + a.transpose(-2, -1)  # creates a symmetric matrix\n>>> w, v = torch.linalg.eigh(a)\n>>> torch.allclose(torch.matmul(v, torch.matmul(w.diag_embed(), v.transpose(-2, -1))), a)\nTrue\n \n"}, {"name": "torch.linalg.eigvalsh()", "path": "linalg#torch.linalg.eigvalsh", "type": "torch.linalg", "text": " \ntorch.linalg.eigvalsh(input, UPLO='L', *, out=None) \u2192 Tensor  \nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input. The eigenvalues are returned in ascending order. Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used. Supports input of float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The eigenvalues are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.   Note The eigenvalues of real symmetric or complex Hermitian matrices are always real.   Note This function doesn\u2019t support backpropagation, please use torch.linalg.eigh() instead, which also computes the eigenvectors.   Note See torch.linalg.eigh() for a related function that computes both eigenvalues and eigenvectors.   Parameters \n \ninput (Tensor) \u2013 the Hermitian n times n matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions. \nUPLO ('L', 'U', optional) \u2013 controls whether to use the upper-triangular or the lower-triangular part of input in the computations. Default is 'L'.   Keyword Arguments \nout (Tensor, optional) \u2013 tensor to write the output to. Default is None.   Examples: >>> a = torch.randn(2, 2, dtype=torch.complex128)\n>>> a = a + a.t().conj()  # creates a Hermitian matrix\n>>> a\ntensor([[2.9228+0.0000j, 0.2029-0.0862j],\n        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n>>> w = torch.linalg.eigvalsh(a)\n>>> w\ntensor([0.3277, 2.9415], dtype=torch.float64)\n\n>>> a = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> a = a + a.transpose(-2, -1)  # creates a symmetric matrix\n>>> a\ntensor([[[ 2.8050, -0.3850],\n        [-0.3850,  3.2376]],\n\n        [[-1.0307, -2.7457],\n        [-2.7457, -1.7517]],\n\n        [[ 1.7166,  2.2207],\n        [ 2.2207, -2.0898]]], dtype=torch.float64)\n>>> w = torch.linalg.eigvalsh(a)\n>>> w\ntensor([[ 2.5797,  3.4629],\n        [-4.1605,  1.3780],\n        [-3.1113,  2.7381]], dtype=torch.float64)\n \n"}, {"name": "torch.linalg.inv()", "path": "linalg#torch.linalg.inv", "type": "torch.linalg", "text": " \ntorch.linalg.inv(input, *, out=None) \u2192 Tensor  \nComputes the multiplicative inverse matrix of a square matrix input, or of each square matrix in a batched input. The result satisfies the relation: matmul(inv(input),input) = matmul(input,inv(input)) = eye(input.shape[0]).expand_as(input). Supports input of float, double, cfloat and cdouble data types.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The inverse matrix is computed using LAPACK\u2019s getrf and getri routines for CPU inputs. For CUDA inputs, cuSOLVER\u2019s getrf and getrs routines as well as cuBLAS\u2019 getrf and getri routines are used if CUDA version >= 10.1.243, otherwise MAGMA\u2019s getrf and getri routines are used instead.   Note If input is a non-invertible matrix or non-square matrix, or batch with at least one such matrix, then a RuntimeError will be thrown.   Parameters \ninput (Tensor) \u2013 the square (n, n) matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions.  Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.   Examples: >>> x = torch.rand(4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = torch.mm(x, y)\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.max(torch.abs(z - torch.eye(4))) # Max non-zero\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = torch.matmul(x, y)\n>>> torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.linalg.inv(x)\n>>> z = torch.mm(x, y)\n>>> z\ntensor([[ 1.0000e+00+0.0000e+00j, -1.3878e-16+3.4694e-16j,\n        5.5511e-17-1.1102e-16j,  0.0000e+00-1.6653e-16j],\n        [ 5.5511e-16-1.6653e-16j,  1.0000e+00+6.9389e-17j,\n        2.2204e-16-1.1102e-16j, -2.2204e-16+1.1102e-16j],\n        [ 3.8858e-16-1.2490e-16j,  2.7756e-17+3.4694e-17j,\n        1.0000e+00+0.0000e+00j, -4.4409e-16+5.5511e-17j],\n        [ 4.4409e-16+5.5511e-16j, -3.8858e-16+1.8041e-16j,\n        2.2204e-16+0.0000e+00j,  1.0000e+00-3.4694e-16j]],\n    dtype=torch.complex128)\n>>> torch.max(torch.abs(z - torch.eye(4, dtype=torch.cdouble))) # Max non-zero\ntensor(7.5107e-16, dtype=torch.float64)\n \n"}, {"name": "torch.linalg.matrix_rank()", "path": "linalg#torch.linalg.matrix_rank", "type": "torch.linalg", "text": " \ntorch.linalg.matrix_rank(input, tol=None, hermitian=False, *, out=None) \u2192 Tensor  \nComputes the numerical rank of a matrix input, or of each matrix in a batched input. The matrix rank is computed as the number of singular values (or absolute eigenvalues when hermitian is True) that are greater than the specified tol threshold. If tol is not specified, tol is set to S.max(dim=-1)*max(input.shape[-2:])*eps, where S is the singular values (or absolute eigenvalues when hermitian is True), and eps is the epsilon value for the datatype of input. The epsilon value can be obtained using the eps attribute of torch.finfo. Supports input of float, double, cfloat and cdouble dtypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The matrix rank is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation is done by obtaining the eigenvalues (see torch.linalg.eigvalsh()).   Parameters \n \ninput (Tensor) \u2013 the input matrix of size (m, n) or the batch of matrices of size (*, m, n) where * is one or more batch dimensions. \ntol (float, optional) \u2013 the tolerance value. Default is None\n \nhermitian (bool, optional) \u2013 indicates whether input is Hermitian. Default is False.   Keyword Arguments \nout (Tensor, optional) \u2013 tensor to write the output to. Default is None.   Examples: >>> a = torch.eye(10)\n>>> torch.linalg.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.linalg.matrix_rank(b)\ntensor(9)\n\n>>> a = torch.randn(4, 3, 2)\n>>> torch.linalg.matrix_rank(a)\ntensor([2, 2, 2, 2])\n\n>>> a = torch.randn(2, 4, 2, 3)\n>>> torch.linalg.matrix_rank(a)\ntensor([[2, 2, 2, 2],\n        [2, 2, 2, 2]])\n\n>>> a = torch.randn(2, 4, 3, 3, dtype=torch.complex64)\n>>> torch.linalg.matrix_rank(a)\ntensor([[3, 3, 3, 3],\n        [3, 3, 3, 3]])\n>>> torch.linalg.matrix_rank(a, hermitian=True)\ntensor([[3, 3, 3, 3],\n        [3, 3, 3, 3]])\n>>> torch.linalg.matrix_rank(a, tol=1.0)\ntensor([[3, 2, 2, 2],\n        [1, 2, 1, 2]])\n>>> torch.linalg.matrix_rank(a, tol=1.0, hermitian=True)\ntensor([[2, 2, 2, 1],\n        [1, 2, 2, 2]])\n \n"}, {"name": "torch.linalg.norm()", "path": "linalg#torch.linalg.norm", "type": "torch.linalg", "text": " \ntorch.linalg.norm(input, ord=None, dim=None, keepdim=False, *, out=None, dtype=None) \u2192 Tensor  \nReturns the matrix norm or vector norm of a given tensor. This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the ord parameter.  Parameters \n \ninput (Tensor) \u2013 The input tensor. If dim is None, x must be 1-D or 2-D, unless ord is None. If both dim and ord are None, the 2-norm of the input flattened to 1-D will be returned. Its data type must be either a floating point or complex type. For complex inputs, the norm is calculated on of the absolute values of each element. If the input is complex and neither dtype nor out is specified, the result\u2019s data type will be the corresponding floating point type (e.g. float if input is complexfloat). \nord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 \nThe order of norm. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be calculated:   \nord norm for matrices norm for vectors   \nNone Frobenius norm 2-norm  \n\u2019fro\u2019 Frobenius norm \u2013 not supported \u2013  \n\u2018nuc\u2019 nuclear norm \u2013 not supported \u2013  \ninf max(sum(abs(x), dim=1)) max(abs(x))  \n-inf min(sum(abs(x), dim=1)) min(abs(x))  \n0 \u2013 not supported \u2013 sum(x != 0)  \n1 max(sum(abs(x), dim=0)) as below  \n-1 min(sum(abs(x), dim=0)) as below  \n2 2-norm (largest sing. value) as below  \n-2 smallest singular value as below  \nother \u2013 not supported \u2013 sum(abs(x)**ord)**(1./ord)   Default: None  \ndim (int, 2-tuple of python:ints, 2-list of python:ints, optional) \u2013 If dim is an int, vector norm will be calculated over the specified dimension. If dim is a 2-tuple of ints, matrix norm will be calculated over the specified dimensions. If dim is None, matrix norm will be calculated when the input tensor has two dimensions, and vector norm will be calculated when the input tensor has one dimension. Default: None\n \nkeepdim (bool, optional) \u2013 If set to True, the reduced dimensions are retained in the result as dimensions with size one. Default: False\n   Keyword Arguments \n \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None\n \ndtype (torch.dtype, optional) \u2013 If specified, the input tensor is cast to dtype before performing the operation, and the returned tensor\u2019s type will be dtype. If this argument is used in conjunction with the out argument, the output tensor\u2019s type must match this argument or a RuntimeError will be raised. Default: None\n    Examples: >>> import torch\n>>> from torch import linalg as LA\n>>> a = torch.arange(9, dtype=torch.float) - 4\n>>> a\ntensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n>>> b = a.reshape((3, 3))\n>>> b\ntensor([[-4., -3., -2.],\n        [-1.,  0.,  1.],\n        [ 2.,  3.,  4.]])\n\n>>> LA.norm(a)\ntensor(7.7460)\n>>> LA.norm(b)\ntensor(7.7460)\n>>> LA.norm(b, 'fro')\ntensor(7.7460)\n>>> LA.norm(a, float('inf'))\ntensor(4.)\n>>> LA.norm(b, float('inf'))\ntensor(9.)\n>>> LA.norm(a, -float('inf'))\ntensor(0.)\n>>> LA.norm(b, -float('inf'))\ntensor(2.)\n\n>>> LA.norm(a, 1)\ntensor(20.)\n>>> LA.norm(b, 1)\ntensor(7.)\n>>> LA.norm(a, -1)\ntensor(0.)\n>>> LA.norm(b, -1)\ntensor(6.)\n>>> LA.norm(a, 2)\ntensor(7.7460)\n>>> LA.norm(b, 2)\ntensor(7.3485)\n\n>>> LA.norm(a, -2)\ntensor(0.)\n>>> LA.norm(b.double(), -2)\ntensor(1.8570e-16, dtype=torch.float64)\n>>> LA.norm(a, 3)\ntensor(5.8480)\n>>> LA.norm(a, -3)\ntensor(0.)\n Using the dim argument to compute vector norms: >>> c = torch.tensor([[1., 2., 3.],\n...                   [-1, 1, 4]])\n>>> LA.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> LA.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> LA.norm(c, ord=1, dim=1)\ntensor([6., 6.])\n Using the dim argument to compute matrix norms: >>> m = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)\n>>> LA.norm(m, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n \n"}, {"name": "torch.linalg.pinv()", "path": "linalg#torch.linalg.pinv", "type": "torch.linalg", "text": " \ntorch.linalg.pinv(input, rcond=1e-15, hermitian=False, *, out=None) \u2192 Tensor  \nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a matrix input, or of each matrix in a batched input. The singular values (or the absolute values of the eigenvalues when hermitian is True) that are below the specified rcond threshold are treated as zero and discarded in the computation. Supports input of float, double, cfloat and cdouble datatypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The pseudo-inverse is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation of the pseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see torch.linalg.eigh()).   Note If singular value decomposition or eigenvalue decomposition algorithms do not converge then a RuntimeError will be thrown.   Parameters \n \ninput (Tensor) \u2013 the input matrix of size (m, n) or the batch of matrices of size (*, m, n) where * is one or more batch dimensions. \nrcond (float, Tensor, optional) \u2013 the tolerance value to determine the cutoff for small singular values. Must be broadcastable to the singular values of input as returned by torch.svd(). Default is 1e-15. \nhermitian (bool, optional) \u2013 indicates whether input is Hermitian. Default is False.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.   Examples: >>> input = torch.randn(3, 5)\n>>> input\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.linalg.pinv(input)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n\nBatched linalg.pinv example\n>>> a = torch.randn(2, 6, 3)\n>>> b = torch.linalg.pinv(a)\n>>> torch.matmul(b, a)\ntensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],\n        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],\n        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],\n\n        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],\n        [-2.2352e-07,  1.0000e+00,  1.1921e-07],\n        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])\n\nHermitian input example\n>>> a = torch.randn(3, 3, dtype=torch.complex64)\n>>> a = a + a.t().conj()  # creates a Hermitian matrix\n>>> b = torch.linalg.pinv(a, hermitian=True)\n>>> torch.matmul(b, a)\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\n        5.9605e-08-2.3842e-07j],\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\n        -4.7684e-07+1.1921e-07j],\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\n        1.0000e+00-1.7897e-07j]])\n\nNon-default rcond example\n>>> rcond = 0.5\n>>> a = torch.randn(3, 3)\n>>> torch.linalg.pinv(a)\ntensor([[ 0.2971, -0.4280, -2.0111],\n        [-0.0090,  0.6426, -0.1116],\n        [-0.7832, -0.2465,  1.0994]])\n>>> torch.linalg.pinv(a, rcond)\ntensor([[-0.2672, -0.2351, -0.0539],\n        [-0.0211,  0.6467, -0.0698],\n        [-0.4400, -0.3638, -0.0910]])\n\nMatrix-wise rcond example\n>>> a = torch.randn(5, 6, 2, 3, 3)\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\n>>> torch.linalg.pinv(a, rcond)\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\n>>> torch.linalg.pinv(a, rcond)\n \n"}, {"name": "torch.linalg.qr()", "path": "linalg#torch.linalg.qr", "type": "torch.linalg", "text": " \ntorch.linalg.qr(input, mode='reduced', *, out=None) -> (Tensor, Tensor)  \nComputes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices. Depending on the value of mode this function returns the reduced or complete QR factorization. See below for a list of valid modes.  Note Differences with numpy.linalg.qr:  \nmode='raw' is not implemented unlike numpy.linalg.qr, this function always returns a tuple of two tensors. When mode='r', the Q tensor is an empty tensor. This behavior may change in a future PyTorch release.    Note Backpropagation is not supported for mode='r'. Use mode='reduced' instead. Backpropagation is also not supported if the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))  columns of any matrix in input are not linearly independent. While no error will be thrown when this occurs the values of the \u201cgradient\u201d produced may be anything. This behavior may change in the future.   Note This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m\u00d7nm \\times n . \nmode (str, optional) \u2013 \nif k = min(m, n) then:  \n'reduced' : returns (Q, R) with dimensions (m, k), (k, n) (default) \n'complete': returns (Q, R) with dimensions (m, m), (m, n) \n'r': computes only R; returns (Q, R) where Q is empty and R has dimensions (k, n)     Keyword Arguments \nout (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of mode above.   Example: >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.linalg.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> q2, r2 = torch.linalg.qr(a, mode='r')\n>>> q2\ntensor([])\n>>> torch.equal(r, r2)\nTrue\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.linalg.qr(a, mode='complete')\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue\n \n"}, {"name": "torch.linalg.slogdet()", "path": "linalg#torch.linalg.slogdet", "type": "torch.linalg", "text": " \ntorch.linalg.slogdet(input, *, out=None) -> (Tensor, Tensor)  \nCalculates the sign and natural logarithm of the absolute value of a square matrix\u2019s determinant, or of the absolute values of the determinants of a batch of square matrices input. The determinant can be computed with sign * exp(logabsdet). Supports input of float, double, cfloat and cdouble datatypes.  Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Note The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.   Note For matrices that have zero determinant, this returns (0, -inf). If input is batched then the entries in the result tensors corresponding to matrices with the zero determinant have sign 0 and the natural logarithm of the absolute value of the determinant -inf.   Parameters \ninput (Tensor) \u2013 the input matrix of size (n,n)(n, n)  or the batch of matrices of size (\u2217,n,n)(*, n, n)  where \u2217*  is one or more batch dimensions.  Keyword Arguments \nout (tuple, optional) \u2013 tuple of two tensors to write the output to.  Returns \nA namedtuple (sign, logabsdet) containing the sign of the determinant and the natural logarithm of the absolute value of determinant, respectively.   Example: >>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.linalg.det(A)\ntensor(-0.7576)\n>>> torch.linalg.logdet(A)\ntensor(nan)\n>>> torch.linalg.slogdet(A)\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n \n"}, {"name": "torch.linalg.solve()", "path": "linalg#torch.linalg.solve", "type": "torch.linalg", "text": " \ntorch.linalg.solve(input, other, *, out=None) \u2192 Tensor  \nComputes the solution x to the matrix equation matmul(input, x) = other with a square matrix, or batches of such matrices, input and one or more right-hand side vectors other. If input is batched and other is not, then other is broadcast to have the same batch dimensions as input. The resulting tensor has the same shape as the (possibly broadcast) other. Supports input of float, double, cfloat and cdouble dtypes.  Note If input is a non-square or non-invertible matrix, or a batch containing non-square matrices or one or more non-invertible matrices, then a RuntimeError will be thrown.   Note When given inputs on a CUDA device, this function synchronizes that device with the CPU.   Parameters \n \ninput (Tensor) \u2013 the square n\u00d7nn \\times n  matrix or the batch of such matrices of size (\u2217,n,n)(*, n, n)  where * is one or more batch dimensions. \nother (Tensor) \u2013 right-hand side tensor of shape (\u2217,n)(*, n)  or (\u2217,n,k)(*, n, k) , where kk  is the number of right-hand side vectors.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> A = torch.eye(3)\n>>> b = torch.randn(3)\n>>> x = torch.linalg.solve(A, b)\n>>> torch.allclose(A @ x, b)\nTrue\n Batched input: >>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(3, 1)\n>>> x = torch.linalg.solve(A, b)\n>>> torch.allclose(A @ x, b)\nTrue\n>>> b = torch.rand(3) # b is broadcast internally to (*A.shape[:-2], 3)\n>>> x = torch.linalg.solve(A, b)\n>>> x.shape\ntorch.Size([2, 3])\n>>> Ax = A @ x.unsqueeze(-1)\n>>> torch.allclose(Ax, b.unsqueeze(-1).expand_as(Ax))\nTrue\n \n"}, {"name": "torch.linalg.svd()", "path": "linalg#torch.linalg.svd", "type": "torch.linalg", "text": " \ntorch.linalg.svd(input, full_matrices=True, compute_uv=True, *, out=None) -> (Tensor, Tensor, Tensor)  \nComputes the singular value decomposition of either a matrix or batch of matrices input.\u201d The singular value decomposition is represented as a namedtuple (U, S, Vh), such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@} diag(S) \\times Vh . If input is a batch of tensors, then U, S, and Vh are also batched with the same batch dimensions as input. If full_matrices is False (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n,m)min(n, m)  orthonormal columns. If compute_uv is False, the returned U and Vh will be empy tensors with no elements and the same device as input. The full_matrices argument has no effect when compute_uv is False. The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.  Note Unlike NumPy\u2019s linalg.svd, this always returns a namedtuple of three tensors, even when compute_uv=False. This behavior may change in a future PyTorch release.   Note The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.   Note The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the cuSOLVER routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and uses the MAGMA routine gesdd on earlier versions of CUDA.   Note The returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride().   Note Gradients computed using U and Vh may be unstable if input is not full rank or has non-unique singular values.   Note When full_matrices = True, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.   Note The S tensor can only be used to compute gradients if compute_uv is True.   Note Since U and V of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi}  while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different U and V tensors.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)  where * is zero or more batch dimensions consisting of m\u00d7nm \\times n  matrices. \nfull_matrices (bool, optional) \u2013 controls whether to compute the full or reduced decomposition, and consequently the shape of returned U and V. Defaults to True. \ncompute_uv (bool, optional) \u2013 whether to compute U and V or not. Defaults to True. \nout (tuple, optional) \u2013 a tuple of three tensors to use for the outputs. If compute_uv=False, the 1st and 3rd arguments must be tensors, but they are ignored. E.g. you can pass (torch.Tensor(), out_S, torch.Tensor())\n    Example: >>> import torch\n>>> a = torch.randn(5, 3)\n>>> a\ntensor([[-0.3357, -0.2987, -1.1096],\n        [ 1.4894,  1.0016, -0.4572],\n        [-1.9401,  0.7437,  2.0968],\n        [ 0.1515,  1.3812,  1.5491],\n        [-1.8489, -0.5907, -2.5673]])\n>>>\n>>> # reconstruction in the full_matrices=False case\n>>> u, s, vh = torch.linalg.svd(a, full_matrices=False)\n>>> u.shape, s.shape, vh.shape\n(torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(a, u @ torch.diag(s) @ vh)\ntensor(1.0486e-06)\n>>>\n>>> # reconstruction in the full_matrices=True case\n>>> u, s, vh = torch.linalg.svd(a)\n>>> u.shape, s.shape, vh.shape\n(torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)\n>>> torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)\ntensor(1.0486e-06)\n>>>\n>>> # extra dimensions\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, vh = torch.linalg.svd(a_big, full_matrices=False)\n>>> torch.dist(a_big, u @ torch.diag_embed(s) @ vh)\ntensor(3.0957e-06)\n \n"}, {"name": "torch.linalg.tensorinv()", "path": "linalg#torch.linalg.tensorinv", "type": "torch.linalg", "text": " \ntorch.linalg.tensorinv(input, ind=2, *, out=None) \u2192 Tensor  \nComputes a tensor input_inv such that tensordot(input_inv, input, ind) == I_n (inverse tensor equation), where I_n is the n-dimensional identity tensor and n is equal to input.ndim. The resulting tensor input_inv has shape equal to input.shape[ind:] + input.shape[:ind]. Supports input of float, double, cfloat and cdouble data types.  Note If input is not invertible or does not satisfy the requirement prod(input.shape[ind:]) == prod(input.shape[:ind]), then a RuntimeError will be thrown.   Note When input is a 2-dimensional tensor and ind=1, this function computes the (multiplicative) inverse of input, equivalent to calling torch.inverse().   Parameters \n \ninput (Tensor) \u2013 A tensor to invert. Its shape must satisfy prod(input.shape[:ind]) == prod(input.shape[ind:]). \nind (int) \u2013 A positive integer that describes the inverse tensor equation. See torch.tensordot() for details. Default: 2.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> a = torch.eye(4 * 6).reshape((4, 6, 8, 3))\n>>> ainv = torch.linalg.tensorinv(a, ind=2)\n>>> ainv.shape\ntorch.Size([8, 3, 4, 6])\n>>> b = torch.randn(4, 6)\n>>> torch.allclose(torch.tensordot(ainv, b), torch.linalg.tensorsolve(a, b))\nTrue\n\n>>> a = torch.randn(4, 4)\n>>> a_tensorinv = torch.linalg.tensorinv(a, ind=1)\n>>> a_inv = torch.inverse(a)\n>>> torch.allclose(a_tensorinv, a_inv)\nTrue\n \n"}, {"name": "torch.linalg.tensorsolve()", "path": "linalg#torch.linalg.tensorsolve", "type": "torch.linalg", "text": " \ntorch.linalg.tensorsolve(input, other, dims=None, *, out=None) \u2192 Tensor  \nComputes a tensor x such that tensordot(input, x, dims=x.ndim) = other. The resulting tensor x has the same shape as input[other.ndim:]. Supports real-valued and complex-valued inputs.  Note If input does not satisfy the requirement prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim]) after (optionally) moving the dimensions using dims, then a RuntimeError will be thrown.   Parameters \n \ninput (Tensor) \u2013 \u201cleft-hand-side\u201d tensor, it must satisfy the requirement prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim]). \nother (Tensor) \u2013 \u201cright-hand-side\u201d tensor of shape input.shape[other.ndim]. \ndims (Tuple[int]) \u2013 dimensions of input to be moved before the computation. Equivalent to calling input = movedim(input, dims, range(len(dims) - input.ndim, 0)). If None (default), no dimensions are moved.   Keyword Arguments \nout (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None   Examples: >>> a = torch.eye(2 * 3 * 4).reshape((2 * 3, 4, 2, 3, 4))\n>>> b = torch.randn(2 * 3, 4)\n>>> x = torch.linalg.tensorsolve(a, b)\n>>> x.shape\ntorch.Size([2, 3, 4])\n>>> torch.allclose(torch.tensordot(a, x, dims=x.ndim), b)\nTrue\n\n>>> a = torch.randn(6, 4, 4, 3, 2)\n>>> b = torch.randn(4, 3, 2)\n>>> x = torch.linalg.tensorsolve(a, b, dims=(0, 2))\n>>> x.shape\ntorch.Size([6, 4])\n>>> a = a.permute(1, 3, 4, 0, 2)\n>>> a.shape[b.ndim:]\ntorch.Size([6, 4])\n>>> torch.allclose(torch.tensordot(a, x, dims=x.ndim), b, atol=1e-6)\nTrue\n \n"}, {"name": "torch.linspace()", "path": "generated/torch.linspace#torch.linspace", "type": "torch", "text": " \ntorch.linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nCreates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive. That is, the value are:  (start,start+end\u2212startsteps\u22121,\u2026,start+(steps\u22122)\u2217end\u2212startsteps\u22121,end)(\\text{start}, \\text{start} + \\frac{\\text{end} - \\text{start}}{\\text{steps} - 1}, \\ldots, \\text{start} + (\\text{steps} - 2) * \\frac{\\text{end} - \\text{start}}{\\text{steps} - 1}, \\text{end})  \n Warning Not providing a value for steps is deprecated. For backwards compatibility, not providing a value for steps will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for steps will throw a runtime error.   Parameters \n \nstart (float) \u2013 the starting value for the set of points \nend (float) \u2013 the ending value for the set of points \nsteps (int) \u2013 size of the constructed tensor   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n>>> torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=1)\ntensor([-10.])\n \n"}, {"name": "torch.load()", "path": "generated/torch.load#torch.load", "type": "torch", "text": " \ntorch.load(f, map_location=None, pickle_module=<module 'pickle' from '/home/matti/miniconda3/lib/python3.7/pickle.py'>, **pickle_load_args) [source]\n \nLoads an object saved with torch.save() from a file. torch.load() uses Python\u2019s unpickling facilities but treats storages, which underlie tensors, specially. They are first deserialized on the CPU and are then moved to the device they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the map_location argument. If map_location is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. The builtin location tags are 'cpu' for CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors. map_location should return either None or a storage. If map_location returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, torch.load() will fall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing a device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values). User extensions can register their own location tags and tagging and deserialization methods using torch.serialization.register_package().  Parameters \n \nf \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()), or a string or os.PathLike object containing a file name \nmap_location \u2013 a function, torch.device, string or a dict specifying how to remap storage locations \npickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) \npickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=....     Warning torch.load() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never load data that could have come from an untrusted source, or that could have been tampered with. Only load data you trust.   Note When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call torch.load(.., map_location='cpu') and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint.   Note By default, we decode byte strings as utf-8. This is to avoid a common error case UnicodeDecodeError: 'ascii' codec can't decode byte 0x... when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra encoding keyword argument to specify how these objects should be loaded, e.g., encoding='latin1' decodes them to strings using latin1 encoding, and encoding='bytes' keeps them as byte arrays which can be decoded later with byte_array.decode(...).  Example >>> torch.load('tensors.pt')\n# Load all tensors onto the CPU\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n>>> with open('tensor.pt', 'rb') as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n>>> torch.load('module.pt', encoding='ascii')\n \n"}, {"name": "torch.lobpcg()", "path": "generated/torch.lobpcg#torch.lobpcg", "type": "torch", "text": " \ntorch.lobpcg(A, k=None, B=None, X=None, n=None, iK=None, niter=None, tol=None, largest=None, method=None, tracker=None, ortho_iparams=None, ortho_fparams=None, ortho_bparams=None) [source]\n \nFind the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods. This function is a front-end to the following LOBPCG algorithms selectable via method argument: method=\u201dbasic\u201d - the LOBPCG method introduced by Andrew Knyazev, see [Knyazev2001]. A less robust method, may fail when Cholesky is applied to singular input. method=\u201dortho\u201d - the LOBPCG method with orthogonal basis selection [StathopoulosEtal2002]. A robust method. Supported inputs are dense, sparse, and batches of dense matrices.  Note In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.   Warning The backward method does not support sparse and complex inputs. It works only when B is not provided (i.e. B == None). We are actively working on extensions, and the details of the algorithms are going to be published promptly.   Warning While it is assumed that A is symmetric, A.grad is not. To make sure that A.grad is symmetric, so that A - t * A.grad is symmetric in first-order optimization routines, prior to running lobpcg we do the following symmetrization map: A -> (A + A.t()) / 2. The map is performed only when the A requires gradients.   Parameters \n \nA (Tensor) \u2013 the input tensor of size (\u2217,m,m)(*, m, m) \n \nB (Tensor, optional) \u2013 the input tensor of size (\u2217,m,m)(*, m, m) . When not specified, B is interpereted as identity matrix. \nX (tensor, optional) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)  where k <= n <= m. When specified, it is used as initial approximation of eigenvectors. X must be a dense tensor. \niK (tensor, optional) \u2013 the input tensor of size (\u2217,m,m)(*, m, m) . When specified, it will be used as preconditioner. \nk (integer, optional) \u2013 the number of requested eigenpairs. Default is the number of XX  columns (when specified) or 1. \nn (integer, optional) \u2013 if XX  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If XX  is specified, the value of n (when specified) must be the number of XX  columns. \ntol (float, optional) \u2013 residual tolerance for stopping criterion. Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type. \nlargest (bool, optional) \u2013 when True, solve the eigenproblem for the largest eigenvalues. Otherwise, solve the eigenproblem for smallest eigenvalues. Default is True. \nmethod (str, optional) \u2013 select LOBPCG method. See the description of the function above. Default is \u201cortho\u201d. \nniter (int, optional) \u2013 maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1. \ntracker (callable, optional) \u2013 \na function for tracing the iteration process. When specified, it is called at each iteration step with LOBPCG instance as an argument. The LOBPCG instance holds the full state of the iteration process in the following attributes: iparams, fparams, bparams - dictionaries of integer, float, and boolean valued input parameters, respectively ivars, fvars, bvars, tvars - dictionaries of integer, float, boolean, and Tensor valued iteration variables, respectively. A, B, iK - input Tensor arguments. E, X, S, R - iteration Tensor variables. For instance: ivars[\u201cistep\u201d] - the current iteration step X - the current approximation of eigenvectors E - the current approximation of eigenvalues R - the current residual ivars[\u201cconverged_count\u201d] - the current number of converged eigenpairs tvars[\u201crerr\u201d] - the current state of convergence criteria Note that when tracker stores Tensor objects from the LOBPCG instance, it must make copies of these. If tracker sets bvars[\u201cforce_stop\u201d] = True, the iteration process will be hard-stopped.  \northo_fparams, ortho_bparams (ortho_iparams,) \u2013 various parameters to LOBPCG algorithm when using method=\u201dortho\u201d.   Returns \ntensor of eigenvalues of size (\u2217,k)(*, k)  X (Tensor): tensor of eigenvectors of size (\u2217,m,k)(*, m, k)   Return type \nE (Tensor)   References [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) https://epubs.siam.org/doi/abs/10.1137/S1064827500366124 [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) https://epubs.siam.org/doi/10.1137/S1064827500370883 [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) https://epubs.siam.org/doi/abs/10.1137/17M1129830 \n"}, {"name": "torch.log()", "path": "generated/torch.log#torch.log", "type": "torch", "text": " \ntorch.log(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the natural logarithm of the elements of input.  yi=log\u2061e(xi)y_{i} = \\log_{e} (x_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(5)\n>>> a\ntensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])\n>>> torch.log(a)\ntensor([ nan,  nan,  nan,  nan,  nan])\n \n"}, {"name": "torch.log10()", "path": "generated/torch.log10#torch.log10", "type": "torch", "text": " \ntorch.log10(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the logarithm to the base 10 of the elements of input.  yi=log\u206110(xi)y_{i} = \\log_{10} (x_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.rand(5)\n>>> a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n>>> torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\n \n"}, {"name": "torch.log1p()", "path": "generated/torch.log1p#torch.log1p", "type": "torch", "text": " \ntorch.log1p(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the natural logarithm of (1 + input).  yi=log\u2061e(xi+1)y_i = \\log_{e} (x_i + 1)  \n Note This function is more accurate than torch.log() for small values of input   Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(5)\n>>> a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n>>> torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\n \n"}, {"name": "torch.log2()", "path": "generated/torch.log2#torch.log2", "type": "torch", "text": " \ntorch.log2(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the logarithm to the base 2 of the elements of input.  yi=log\u20612(xi)y_{i} = \\log_{2} (x_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.rand(5)\n>>> a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n\n>>> torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\n \n"}, {"name": "torch.logaddexp()", "path": "generated/torch.logaddexp#torch.logaddexp", "type": "torch", "text": " \ntorch.logaddexp(input, other, *, out=None) \u2192 Tensor  \nLogarithm of the sum of exponentiations of the inputs. Calculates pointwise log\u2061(ex+ey)\\log\\left(e^x + e^y\\right) . This function is useful in statistics where the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers. In such cases the logarithm of the calculated probability is stored. This function allows adding probabilities stored in such a fashion. This op should be disambiguated with torch.logsumexp() which performs a reduction on a single tensor.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))\ntensor([-0.3069, -0.6867, -0.8731])\n>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))\ntensor([-1., -2., -3.])\n>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))\ntensor([1.1269e+00, 2.0000e+03, 3.0000e+04])\n \n"}, {"name": "torch.logaddexp2()", "path": "generated/torch.logaddexp2#torch.logaddexp2", "type": "torch", "text": " \ntorch.logaddexp2(input, other, *, out=None) \u2192 Tensor  \nLogarithm of the sum of exponentiations of the inputs in base-2. Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right) . See torch.logaddexp() for more details.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   \n"}, {"name": "torch.logcumsumexp()", "path": "generated/torch.logcumsumexp#torch.logcumsumexp", "type": "torch", "text": " \ntorch.logcumsumexp(input, dim, *, out=None) \u2192 Tensor  \nReturns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim. For summation index jj  given by dim and other indices ii , the result is  logcumsumexp(x)ij=log\u2061\u2211j=0iexp\u2061(xij)\\text{logcumsumexp}(x)_{ij} = \\log \\sum\\limits_{j=0}^{i} \\exp(x_{ij})    Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.randn(10)\n>>> torch.logcumsumexp(a, dim=0)\ntensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,\n         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))\n   \n"}, {"name": "torch.logdet()", "path": "generated/torch.logdet#torch.logdet", "type": "torch", "text": " \ntorch.logdet(input) \u2192 Tensor  \nCalculates log determinant of a square matrix or batches of square matrices.  Note Result is -inf if input has zero log determinant, and is nan if input has negative determinant.   Note Backward through logdet() internally uses SVD results when input is not invertible. In this case, double backward through logdet() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.   Parameters \ninput (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.   Example: >>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(0.2611)\n>>> torch.logdet(A)\ntensor(-1.3430)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n>>> A.det().log()\ntensor([ 0.1815, -0.8917, -0.3031])\n \n"}, {"name": "torch.logical_and()", "path": "generated/torch.logical_and#torch.logical_and", "type": "torch", "text": " \ntorch.logical_and(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise logical AND of the given input tensors. Zeros are treated as False and nonzeros are treated as True.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the tensor to compute AND with   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False, False])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_and(a, b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b.double())\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([False, False,  True, False])\n \n"}, {"name": "torch.logical_not()", "path": "generated/torch.logical_not#torch.logical_not", "type": "torch", "text": " \ntorch.logical_not(input, *, out=None) \u2192 Tensor  \nComputes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as False and non-zeros are treated as True.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.logical_not(torch.tensor([True, False]))\ntensor([False,  True])\n>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\ntensor([1, 0, 0], dtype=torch.int16)\n \n"}, {"name": "torch.logical_or()", "path": "generated/torch.logical_or#torch.logical_or", "type": "torch", "text": " \ntorch.logical_or(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the tensor to compute OR with   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_or(a, b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b.double())\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True,  True, False])\n \n"}, {"name": "torch.logical_xor()", "path": "generated/torch.logical_xor#torch.logical_xor", "type": "torch", "text": " \ntorch.logical_xor(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise logical XOR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the tensor to compute XOR with   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([False, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_xor(a, b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b.double())\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True, False, False])\n \n"}, {"name": "torch.logit()", "path": "generated/torch.logit#torch.logit", "type": "torch", "text": " \ntorch.logit(input, eps=None, *, out=None) \u2192 Tensor  \nReturns a new tensor with the logit of the elements of input. input is clamped to [eps, 1 - eps] when eps is not None. When eps is None and input < 0 or input > 1, the function will yields NaN.  yi=ln\u2061(zi1\u2212zi)zi={xiif eps is Noneepsif xi<epsxiif eps\u2264xi\u22641\u2212eps1\u2212epsif xi>1\u2212epsy_{i} = \\ln(\\frac{z_{i}}{1 - z_{i}}) \\\\ z_{i} = \\begin{cases} x_{i} & \\text{if eps is None} \\\\ \\text{eps} & \\text{if } x_{i} < \\text{eps} \\\\ x_{i} & \\text{if } \\text{eps} \\leq x_{i} \\leq 1 - \\text{eps} \\\\ 1 - \\text{eps} & \\text{if } x_{i} > 1 - \\text{eps} \\end{cases}  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \neps (float, optional) \u2013 the epsilon for input clamp bound. Default: None\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\n \n"}, {"name": "torch.logspace()", "path": "generated/torch.logspace#torch.logspace", "type": "torch", "text": " \ntorch.logspace(start, end, steps, base=10.0, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nCreates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}  to baseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale with base base. That is, the values are:  (basestart,base(start+end\u2212startsteps\u22121),\u2026,base(start+(steps\u22122)\u2217end\u2212startsteps\u22121),baseend)(\\text{base}^{\\text{start}}, \\text{base}^{(\\text{start} + \\frac{\\text{end} - \\text{start}}{ \\text{steps} - 1})}, \\ldots, \\text{base}^{(\\text{start} + (\\text{steps} - 2) * \\frac{\\text{end} - \\text{start}}{ \\text{steps} - 1})}, \\text{base}^{\\text{end}})  \n Warning Not providing a value for steps is deprecated. For backwards compatibility, not providing a value for steps will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for steps will throw a runtime error.   Parameters \n \nstart (float) \u2013 the starting value for the set of points \nend (float) \u2013 the ending value for the set of points \nsteps (int) \u2013 size of the constructed tensor \nbase (float, optional) \u2013 base of the logarithm function. Default: 10.0.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])\n \n"}, {"name": "torch.logsumexp()", "path": "generated/torch.logsumexp#torch.logsumexp", "type": "torch", "text": " \ntorch.logsumexp(input, dim, keepdim=False, *, out=None)  \nReturns the log of summed exponentials of each row of the input tensor in the given dimension dim. The computation is numerically stabilized. For summation index jj  given by dim and other indices ii , the result is  logsumexp(x)i=log\u2061\u2211jexp\u2061(xij)\\text{logsumexp}(x)_{i} = \\log \\sum_j \\exp(x_{ij})   If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.randn(3, 3)\n>>> torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])\n   \n"}, {"name": "torch.lstsq()", "path": "generated/torch.lstsq#torch.lstsq", "type": "torch", "text": " \ntorch.lstsq(input, A, *, out=None) \u2192 Tensor  \nComputes the solution to the least squares and least norm problems for a full rank matrix AA  of size (m\u00d7n)(m \\times n)  and a matrix BB  of size (m\u00d7k)(m \\times k) . If m\u2265nm \\geq n , lstsq() solves the least-squares problem:  min\u2061X\u2225AX\u2212B\u22252.\\begin{array}{ll} \\min_X & \\|AX-B\\|_2. \\end{array} \nIf m<nm < n , lstsq() solves the least-norm problem:  min\u2061X\u2225X\u22252subject toAX=B.\\begin{array}{ll} \\min_X & \\|X\\|_2 & \\text{subject to} & AX = B. \\end{array} \nReturned tensor XX  has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k) . The first nn  rows of XX  contains the solution. If m\u2265nm \\geq n , the residual sum of squares for the solution in each column is given by the sum of squares of elements in the remaining m\u2212nm - n  rows of that column.  Note The case when m<nm < n  is not supported on the GPU.   Parameters \n \ninput (Tensor) \u2013 the matrix BB \n \nA (Tensor) \u2013 the mm  by nn  matrix AA \n   Keyword Arguments \nout (tuple, optional) \u2013 the optional destination tensor  Returns \nA namedtuple (solution, QR) containing:  \nsolution (Tensor): the least squares solution \nQR (Tensor): the details of the QR factorization   Return type \n(Tensor, Tensor)    Note The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride (1, m) instead of (m, 1).  Example: >>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])\n \n"}, {"name": "torch.lt()", "path": "generated/torch.lt#torch.lt", "type": "torch", "text": " \ntorch.lt(input, other, *, out=None) \u2192 Tensor  \nComputes input<other\\text{input} < \\text{other}  element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or float) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is less than other and False elsewhere   Example: >>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, False], [True, False]])\n \n"}, {"name": "torch.lu()", "path": "generated/torch.lu#torch.lu", "type": "torch", "text": " \ntorch.lu(*args, **kwargs)  \nComputes the LU factorization of a matrix or batches of matrices A. Returns a tuple containing the LU factorization and pivots of A. Pivoting is done if pivot is set to True.  Note The pivots returned by the function are 1-indexed. If pivot is False, then the returned pivots is a tensor filled with zeros of the appropriate size.   Note LU factorization with pivot = False is not available for CPU, and attempting to do so will throw an error. However, LU factorization with pivot = False is available for CUDA.   Note This function does not check if the factorization was successful or not if get_infos is True since the status of the factorization is present in the third element of the return tuple.   Note In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).   Note L, U, and P can be derived using torch.lu_unpack().   Warning The LU factorization does have backward support, but only for square inputs of full rank.   Parameters \n \nA (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n) \n \npivot (bool, optional) \u2013 controls whether pivoting is done. Default: True\n \nget_infos (bool, optional) \u2013 if set to True, returns an info IntTensor. Default: False\n \nout (tuple, optional) \u2013 optional output tuple. If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the elements in the tuple are Tensor, IntTensor. Default: None\n   Returns \nA tuple of tensors containing  \nfactorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n) \n \npivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n)) . pivots stores all the intermediate transpositions of rows. The final permutation perm could be reconstructed by applying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1, where perm is initially the identity permutation of mm  elements (essentially this is what torch.lu_unpack() is doing). \ninfos (IntTensor, optional): if get_infos is True, this is a tensor of size (\u2217)(*)  where non-zero values indicate whether factorization for the matrix or each minibatch has succeeded or failed   Return type \n(Tensor, IntTensor, IntTensor (optional))   Example: >>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = torch.lu(A)\n>>> A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n         [ 0.1684,  1.1551,  0.1940],\n         [ 0.1193,  0.6189, -0.5497]],\n\n        [[ 0.4526,  1.2526, -0.3285],\n         [-0.7988,  0.7175, -0.9701],\n         [ 0.2634, -0.9255, -0.3459]]])\n>>> pivots\ntensor([[ 3,  3,  3],\n        [ 3,  3,  3]], dtype=torch.int32)\n>>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n>>> if info.nonzero().size(0) == 0:\n...   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!\n \n"}, {"name": "torch.lu_solve()", "path": "generated/torch.lu_solve#torch.lu_solve", "type": "torch", "text": " \ntorch.lu_solve(b, LU_data, LU_pivots, *, out=None) \u2192 Tensor  \nReturns the LU solve of the linear system Ax=bAx = b  using the partially pivoted LU factorization of A from torch.lu(). This function supports float, double, cfloat and cdouble dtypes for input.  Parameters \n \nb (Tensor) \u2013 the RHS tensor of size (\u2217,m,k)(*, m, k) , where \u2217*  is zero or more batch dimensions. \nLU_data (Tensor) \u2013 the pivoted LU factorization of A from torch.lu() of size (\u2217,m,m)(*, m, m) , where \u2217*  is zero or more batch dimensions. \nLU_pivots (IntTensor) \u2013 the pivots of the LU factorization from torch.lu() of size (\u2217,m)(*, m) , where \u2217*  is zero or more batch dimensions. The batch dimensions of LU_pivots must be equal to the batch dimensions of LU_data.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(2, 3, 1)\n>>> A_LU = torch.lu(A)\n>>> x = torch.lu_solve(b, *A_LU)\n>>> torch.norm(torch.bmm(A, x) - b)\ntensor(1.00000e-07 *\n       2.8312)\n \n"}, {"name": "torch.lu_unpack()", "path": "generated/torch.lu_unpack#torch.lu_unpack", "type": "torch", "text": " \ntorch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True) [source]\n \nUnpacks the data and pivots from a LU factorization of a tensor. Returns a tuple of tensors as (the pivots, the L tensor, the U tensor).  Parameters \n \nLU_data (Tensor) \u2013 the packed LU factorization data \nLU_pivots (Tensor) \u2013 the packed LU factorization pivots \nunpack_data (bool) \u2013 flag indicating if the data should be unpacked \nunpack_pivots (bool) \u2013 flag indicating if the pivots should be unpacked    Examples: >>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>>\n>>> # can recover A from factorization\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n>>> # LU factorization of a rectangular matrix:\n>>> A = torch.randn(2, 3, 2)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>> P\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]])\n>>> A_L\ntensor([[[ 1.0000,  0.0000],\n         [ 0.4763,  1.0000],\n         [ 0.3683,  0.1135]],\n\n        [[ 1.0000,  0.0000],\n         [ 0.2957,  1.0000],\n         [-0.9668, -0.3335]]])\n>>> A_U\ntensor([[[ 2.1962,  1.0881],\n         [ 0.0000, -0.8681]],\n\n        [[-1.0947,  0.3736],\n         [ 0.0000,  0.5718]]])\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n>>> torch.norm(A_ - A)\ntensor(2.9802e-08)\n \n"}, {"name": "torch.manual_seed()", "path": "generated/torch.manual_seed#torch.manual_seed", "type": "torch", "text": " \ntorch.manual_seed(seed) [source]\n \nSets the seed for generating random numbers. Returns a torch.Generator object.  Parameters \nseed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.   \n"}, {"name": "torch.masked_select()", "path": "generated/torch.masked_select#torch.masked_select", "type": "torch", "text": " \ntorch.masked_select(input, mask, *, out=None) \u2192 Tensor  \nReturns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor. The shapes of the mask tensor and the input tensor don\u2019t need to match, but they must be broadcastable.  Note The returned tensor does not use the same storage as the original tensor   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nmask (BoolTensor) \u2013 the tensor containing the binary mask to index with   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n        [-1.2035,  1.2252,  0.5002,  0.6248],\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\n>>> mask = x.ge(0.5)\n>>> mask\ntensor([[False, False, False, False],\n        [False, True, True, True],\n        [False, False, False, True]])\n>>> torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n \n"}, {"name": "torch.matmul()", "path": "generated/torch.matmul#torch.matmul", "type": "torch", "text": " \ntorch.matmul(input, other, *, out=None) \u2192 Tensor  \nMatrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows:  If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned. \nIf both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if input is a (j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)  tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)  tensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)  tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions. For example, if input is a (j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)  tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)  tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the matrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)  tensor.   This operator supports TensorFloat32.  Note The 1-dimensional dot product version of this function does not support an out parameter.   Parameters \n \ninput (Tensor) \u2013 the first tensor to be multiplied \nother (Tensor) \u2013 the second tensor to be multiplied   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> # vector x vector\n>>> tensor1 = torch.randn(3)\n>>> tensor2 = torch.randn(3)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n>>> # matrix x vector\n>>> tensor1 = torch.randn(3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n>>> # batched matrix x broadcasted vector\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n>>> # batched matrix x batched matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(10, 4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n>>> # batched matrix x broadcasted matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n \n"}, {"name": "torch.matrix_exp()", "path": "generated/torch.matrix_exp#torch.matrix_exp", "type": "torch", "text": " \ntorch.matrix_exp()  \nReturns the matrix exponential. Supports batched input. For a matrix A, the matrix exponential is defined as  eA=\u2211k=0\u221eAk/k!\\mathrm{e}^A = \\sum_{k=0}^\\infty A^k / k!  \nThe implementation is based on: Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(2, 2, 2)\n>>> a[0, :, :] = torch.eye(2, 2)\n>>> a[1, :, :] = 2 * torch.eye(2, 2)\n>>> a\ntensor([[[1., 0.],\n         [0., 1.]],\n\n        [[2., 0.],\n         [0., 2.]]])\n>>> torch.matrix_exp(a)\ntensor([[[2.7183, 0.0000],\n         [0.0000, 2.7183]],\n\n         [[7.3891, 0.0000],\n          [0.0000, 7.3891]]])\n\n>>> import math\n>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])\n>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\ntensor([[ 0.5000,  0.8660],\n        [-0.8660,  0.5000]])\n \n"}, {"name": "torch.matrix_power()", "path": "generated/torch.matrix_power#torch.matrix_power", "type": "torch", "text": " \ntorch.matrix_power(input, n) \u2192 Tensor  \nReturns the matrix raised to the power n for square matrices. For batch of matrices, each individual matrix is raised to the power n. If n is negative, then the inverse of the matrix (if invertible) is raised to the power n. For a batch of matrices, the batched inverse (if invertible) is raised to the power n. If n is 0, then an identity matrix is returned.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nn (int) \u2013 the power to raise the matrix to    Example: >>> a = torch.randn(2, 2, 2)\n>>> a\ntensor([[[-1.9975, -1.9610],\n         [ 0.9592, -2.3364]],\n\n        [[-1.2534, -1.3429],\n         [ 0.4153, -1.4664]]])\n>>> torch.matrix_power(a, 3)\ntensor([[[  3.9392, -23.9916],\n         [ 11.7357,  -0.2070]],\n\n        [[  0.2468,  -6.7168],\n         [  2.0774,  -0.8187]]])\n \n"}, {"name": "torch.matrix_rank()", "path": "generated/torch.matrix_rank#torch.matrix_rank", "type": "torch", "text": " \ntorch.matrix_rank(input, tol=None, symmetric=False, *, out=None) \u2192 Tensor  \nReturns the numerical rank of a 2-D tensor. The method to compute the matrix rank is done using SVD by default. If symmetric is True, then input is assumed to be symmetric, and the computation of the rank is done by obtaining the eigenvalues. tol is the threshold below which the singular values (or the eigenvalues when symmetric is True) are considered to be 0. If tol is not specified, tol is set to S.max() * max(S.size()) * eps where S is the singular values (or the eigenvalues when symmetric is True), and eps is the epsilon value for the datatype of input.  Note torch.matrix_rank() is deprecated. Please use torch.linalg.matrix_rank() instead. The parameter symmetric was renamed in torch.linalg.matrix_rank() to hermitian.   Parameters \n \ninput (Tensor) \u2013 the input 2-D tensor \ntol (float, optional) \u2013 the tolerance value. Default: None\n \nsymmetric (bool, optional) \u2013 indicates whether input is symmetric. Default: False\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.eye(10)\n>>> torch.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.matrix_rank(b)\ntensor(9)\n \n"}, {"name": "torch.max()", "path": "generated/torch.max#torch.max", "type": "torch", "text": " \ntorch.max(input) \u2192 Tensor  \nReturns the maximum value of all elements in the input tensor.  Warning This function produces deterministic (sub)gradients unlike max(dim=0)   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n>>> torch.max(a)\ntensor(0.7445)\n  \ntorch.max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor) \n Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax). If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Note If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Default: False.   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (max, max_indices)   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n  \ntorch.max(input, other, *, out=None) \u2192 Tensor \n See torch.maximum(). \n"}, {"name": "torch.maximum()", "path": "generated/torch.maximum#torch.maximum", "type": "torch", "text": " \ntorch.maximum(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise maximum of input and other.  Note If one of the elements being compared is a NaN, then that element is returned. maximum() is not supported for tensors with complex dtypes.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.maximum(a, b)\ntensor([3, 2, 4])\n \n"}, {"name": "torch.mean()", "path": "generated/torch.mean#torch.mean", "type": "torch", "text": " \ntorch.mean(input) \u2192 Tensor  \nReturns the mean value of all elements in the input tensor.  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n>>> torch.mean(a)\ntensor(0.3367)\n  \ntorch.mean(input, dim, keepdim=False, *, out=None) \u2192 Tensor \n Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.3841,  0.6320,  0.4254, -0.7384],\n        [-0.9644,  1.0131, -0.6549, -1.4279],\n        [-0.2951, -1.3350, -0.7694,  0.5600],\n        [ 1.0842, -0.9580,  0.3623,  0.2343]])\n>>> torch.mean(a, 1)\ntensor([-0.0163, -0.5085, -0.4599,  0.1807])\n>>> torch.mean(a, 1, True)\ntensor([[-0.0163],\n        [-0.5085],\n        [-0.4599],\n        [ 0.1807]])\n \n"}, {"name": "torch.median()", "path": "generated/torch.median#torch.median", "type": "torch", "text": " \ntorch.median(input) \u2192 Tensor  \nReturns the median of the values in input.  Note The median is not unique for input tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians, use torch.quantile() with q=0.5 instead.   Warning This function produces deterministic (sub)gradients unlike median(dim=0)   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n>>> torch.median(a)\ntensor(0.2202)\n  \ntorch.median(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor) \n Returns a namedtuple (values, indices) where values contains the median of each row of input in the dimension dim, and indices contains the index of the median values found in the dimension dim. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the outputs tensor having 1 fewer dimension than input.  Note The median is not unique for input tensors with an even number of elements in the dimension dim. In this case the lower of the two medians is returned. To compute the mean of both medians in input, use torch.quantile() with q=0.5 instead.   Warning indices does not necessarily contain the first occurrence of each median value found, unless it is unique. The exact implementation details are device-specific. Do not expect the same result when run on CPU and GPU in general. For the same reason do not expect the gradients to be deterministic.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the dimension dim of input.   Example: >>> a = torch.randn(4, 5)\n>>> a\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n>>> torch.median(a, 1)\ntorch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))\n \n"}, {"name": "torch.meshgrid()", "path": "generated/torch.meshgrid#torch.meshgrid", "type": "torch", "text": " \ntorch.meshgrid(*tensors) [source]\n \nTake NN  tensors, each of which can be either scalar or 1-dimensional vector, and create NN  N-dimensional grids, where the ii  th grid is defined by expanding the ii  th input over dimensions defined by other inputs.  Parameters \ntensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be treated as tensors of size (1,)(1,)  automatically  Returns \nIf the input has kk  tensors of size (N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,) , then the output would also have kk  tensors, where all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k) .  Return type \nseq (sequence of Tensors)   Example: >>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([4, 5, 6])\n>>> grid_x, grid_y = torch.meshgrid(x, y)\n>>> grid_x\ntensor([[1, 1, 1],\n        [2, 2, 2],\n        [3, 3, 3]])\n>>> grid_y\ntensor([[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]])\n \n"}, {"name": "torch.min()", "path": "generated/torch.min#torch.min", "type": "torch", "text": " \ntorch.min(input) \u2192 Tensor  \nReturns the minimum value of all elements in the input tensor.  Warning This function produces deterministic (sub)gradients unlike min(dim=0)   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n>>> torch.min(a)\ntensor(0.6750)\n  \ntorch.min(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor) \n Returns a namedtuple (values, indices) where values is the minimum value of each row of the input tensor in the given dimension dim. And indices is the index location of each minimum value found (argmin). If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Note If there are multiple minimal values in a reduced row then the indices of the first minimal value are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (tuple, optional) \u2013 the tuple of two output tensors (min, min_indices)   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n        [-1.4644, -0.2635, -0.3651,  0.6134],\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\n>>> torch.min(a, 1)\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))\n  \ntorch.min(input, other, *, out=None) \u2192 Tensor \n See torch.minimum(). \n"}, {"name": "torch.minimum()", "path": "generated/torch.minimum#torch.minimum", "type": "torch", "text": " \ntorch.minimum(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise minimum of input and other.  Note If one of the elements being compared is a NaN, then that element is returned. minimum() is not supported for tensors with complex dtypes.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.minimum(a, b)\ntensor([1, 0, -1])\n \n"}, {"name": "torch.mm()", "path": "generated/torch.mm#torch.mm", "type": "torch", "text": " \ntorch.mm(input, mat2, *, out=None) \u2192 Tensor  \nPerforms a matrix multiplication of the matrices input and mat2. If input is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, out will be a (n\u00d7p)(n \\times p)  tensor.  Note This function does not broadcast. For broadcasting matrix products, see torch.matmul().  Supports strided and sparse 2-D tensors as inputs, autograd with respect to strided inputs. This operator supports TensorFloat32.  Parameters \n \ninput (Tensor) \u2013 the first matrix to be matrix multiplied \nmat2 (Tensor) \u2013 the second matrix to be matrix multiplied   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n        [-0.0760, -3.6705,  2.4784]])\n \n"}, {"name": "torch.mode()", "path": "generated/torch.mode#torch.mode", "type": "torch", "text": " \ntorch.mode(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor)  \nReturns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often in that row, and indices is the index location of each mode value found. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Note This function is not defined for torch.cuda.Tensor yet.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randint(10, (5,))\n>>> a\ntensor([6, 5, 1, 0, 2])\n>>> b = a + (torch.randn(50, 1) * 5).long()\n>>> torch.mode(b, 0)\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))\n \n"}, {"name": "torch.moveaxis()", "path": "generated/torch.moveaxis#torch.moveaxis", "type": "torch", "text": " \ntorch.moveaxis(input, source, destination) \u2192 Tensor  \nAlias for torch.movedim(). This function is equivalent to NumPy\u2019s moveaxis function. Examples: >>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.moveaxis(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.moveaxis(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.moveaxis(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.moveaxis(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])\n \n"}, {"name": "torch.movedim()", "path": "generated/torch.movedim#torch.movedim", "type": "torch", "text": " \ntorch.movedim(input, source, destination) \u2192 Tensor  \nMoves the dimension(s) of input at the position(s) in source to the position(s) in destination. Other dimensions of input that are not explicitly moved remain in their original order and appear at the positions not specified in destination.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nsource (int or tuple of python:ints) \u2013 Original positions of the dims to move. These must be unique. \ndestination (int or tuple of python:ints) \u2013 Destination positions for each of the original dims. These must also be unique.    Examples: >>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.movedim(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.movedim(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.movedim(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.movedim(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])\n \n"}, {"name": "torch.msort()", "path": "generated/torch.msort#torch.msort", "type": "torch", "text": " \ntorch.msort(input, *, out=None) \u2192 Tensor  \nSorts the elements of the input tensor along its first dimension in ascending order by value.  Note torch.msort(t) is equivalent to torch.sort(t, dim=0)[0]. See also torch.sort().   Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> t = torch.randn(3, 4)\n>>> t\ntensor([[-0.1321,  0.4370, -1.2631, -1.1289],\n        [-2.0527, -1.1250,  0.2275,  0.3077],\n        [-0.0881, -0.1259, -0.5495,  1.0284]])\n>>> torch.msort(t)\ntensor([[-2.0527, -1.1250, -1.2631, -1.1289],\n        [-0.1321, -0.1259, -0.5495,  0.3077],\n        [-0.0881,  0.4370,  0.2275,  1.0284]])\n \n"}, {"name": "torch.mul()", "path": "generated/torch.mul#torch.mul", "type": "torch", "text": " \ntorch.mul(input, other, *, out=None)  \nMultiplies each element of the input input with the scalar other and returns a new resulting tensor.  outi=other\u00d7inputi\\text{out}_i = \\text{other} \\times \\text{input}_i  \nIf input is of type FloatTensor or DoubleTensor, other should be a real number, otherwise it should be an integer  Parameters \n \n{input} \u2013  \nother (Number) \u2013 the number to be multiplied to each element of input\n   Keyword Arguments \n{out} \u2013    Example: >>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])\n  \ntorch.mul(input, other, *, out=None) \n Each element of the tensor input is multiplied by the corresponding element of the Tensor other. The resulting tensor is returned. The shapes of input and other must be broadcastable.  outi=inputi\u00d7otheri\\text{out}_i = \\text{input}_i \\times \\text{other}_i  \n Parameters \n \ninput (Tensor) \u2013 the first multiplicand tensor \nother (Tensor) \u2013 the second multiplicand tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 1)\n>>> a\ntensor([[ 1.1207],\n        [-0.3137],\n        [ 0.0700],\n        [ 0.8378]])\n>>> b = torch.randn(1, 4)\n>>> b\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n>>> torch.mul(a, b)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n        [-0.1614, -0.0382,  0.1645, -0.7021],\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])\n \n"}, {"name": "torch.multinomial()", "path": "generated/torch.multinomial#torch.multinomial", "type": "torch", "text": " \ntorch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) \u2192 LongTensor  \nReturns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.  Note The rows of input do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.  Indices are ordered from left to right according to when each was sampled (first samples are placed in first column). If input is a vector, out is a vector of size num_samples. If input is a matrix with m rows, out is an matrix of shape (m\u00d7num_samples)(m \\times \\text{num\\_samples}) . If replacement is True, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.  Note When drawn without replacement, num_samples must be lower than number of non-zero elements in input (or the min number of non-zero elements in each row of input if it is a matrix).   Parameters \n \ninput (Tensor) \u2013 the input tensor containing probabilities \nnum_samples (int) \u2013 number of samples to draw \nreplacement (bool, optional) \u2013 whether to draw with replacement or not   Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n>>> torch.multinomial(weights, 2)\ntensor([1, 2])\n>>> torch.multinomial(weights, 4) # ERROR!\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n>>> torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])\n \n"}, {"name": "torch.multiply()", "path": "generated/torch.multiply#torch.multiply", "type": "torch", "text": " \ntorch.multiply(input, other, *, out=None)  \nAlias for torch.mul(). \n"}, {"name": "torch.multiprocessing", "path": "multiprocessing", "type": "torch.multiprocessing", "text": "Multiprocessing package - torch.multiprocessing torch.multiprocessing is a wrapper around the native multiprocessing module. It registers custom reducers, that use shared memory to provide shared views on the same data in different processes. Once the tensor/storage is moved to shared_memory (see share_memory_()), it will be possible to send it to other processes without making any copies. The API is 100% compatible with the original module - it\u2019s enough to change import multiprocessing to import torch.multiprocessing to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory. Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.  Warning If the main process exits abruptly (e.g. because of an incoming signal), Python\u2019s multiprocessing sometimes fails to clean up its children. It\u2019s a known caveat, so if you\u2019re seeing any resource leaks after interrupting the interpreter, it probably means that this has just happened to you.  Strategy management  \ntorch.multiprocessing.get_all_sharing_strategies() [source]\n \nReturns a set of sharing strategies supported on a current system. \n  \ntorch.multiprocessing.get_sharing_strategy() [source]\n \nReturns the current strategy for sharing CPU tensors. \n  \ntorch.multiprocessing.set_sharing_strategy(new_strategy) [source]\n \nSets the strategy for sharing CPU tensors.  Parameters \nnew_strategy (str) \u2013 Name of the selected strategy. Should be one of the values returned by get_all_sharing_strategies().   \n Sharing CUDA tensors Sharing CUDA tensors between processes is supported only in Python 3, using a spawn or forkserver start methods. Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices.  Warning If the consumer process dies abnormally to a fatal signal, the shared tensor could be forever kept in memory as long as the sending process is running.   Release memory ASAP in the consumer.  ## Good\nx = queue.get()\n# do somethings with x\ndel x\n ## Bad\nx = queue.get()\n# do somethings with x\n# do everything else (producer have to keep x in memory)\n 2. Keep producer process running until all consumers exits. This will prevent the situation when the producer process releasing memory which is still in use by the consumer. ## producer\n# send tensors, do something\nevent.wait()\n ## consumer\n# receive tensors and use them\nevent.set()\n  Don\u2019t pass received tensors.  # not going to work\nx = queue.get()\nqueue_2.put(x)\n # you need to create a process-local copy\nx = queue.get()\nx_clone = x.clone()\nqueue_2.put(x_clone)\n # putting and getting from the same queue in the same process will likely end up with segfault\nqueue.put(tensor)\nx = queue.get()\n Sharing strategies This section provides a brief overview into how different sharing strategies work. Note that it applies only to CPU tensor - CUDA tensors will always use the CUDA API, as that\u2019s the only way they can be shared. File descriptor - file_descriptor\n  Note This is the default strategy (except for macOS and OS X where it\u2019s not supported).  This strategy will use file descriptors as shared memory handles. Whenever a storage is moved to shared memory, a file descriptor obtained from shm_open is cached with the object, and when it\u2019s going to be sent to other processes, the file descriptor will be transferred (e.g. via UNIX sockets) to it. The receiver will also cache the file descriptor and mmap it, to obtain a shared view onto the storage data. Note that if there will be a lot of tensors shared, this strategy will keep a large number of file descriptors open most of the time. If your system has low limits for the number of open file descriptors, and you can\u2019t raise them, you should use the file_system strategy. File system - file_system\n This strategy will use file names given to shm_open to identify the shared memory regions. This has a benefit of not requiring the implementation to cache the file descriptors obtained from it, but at the same time is prone to shared memory leaks. The file can\u2019t be deleted right after its creation, because other processes need to access it to open their views. If the processes fatally crash, or are killed, and don\u2019t call the storage destructors, the files will remain in the system. This is very serious, because they keep using up the memory until the system is restarted, or they\u2019re freed manually. To counter the problem of shared memory file leaks, torch.multiprocessing will spawn a daemon named torch_shm_manager that will isolate itself from the current process group, and will keep track of all shared memory allocations. Once all processes connected to it exit, it will wait a moment to ensure there will be no new connections, and will iterate over all shared memory files allocated by the group. If it finds that any of them still exist, they will be deallocated. We\u2019ve tested this method and it proved to be robust to various failures. Still, if your system has high enough limits, and file_descriptor is a supported strategy, we do not recommend switching to this one. Spawning subprocesses  Note Available for Python >= 3.4. This depends on the spawn start method in Python\u2019s multiprocessing package.  Spawning a number of subprocesses to perform some function can be done by creating Process instances and calling join to wait for their completion. This approach works fine when dealing with a single subprocess but presents potential issues when dealing with multiple processes. Namely, joining processes sequentially implies they will terminate sequentially. If they don\u2019t, and the first process does not terminate, the process termination will go unnoticed. Also, there are no native facilities for error propagation. The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.  \ntorch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn') [source]\n \nSpawns nprocs processes that run fn with args. If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.  Parameters \n \nfn (function) \u2013 \nFunction is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing. The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.  \nargs (tuple) \u2013 Arguments passed to fn. \nnprocs (int) \u2013 Number of processes to spawn. \njoin (bool) \u2013 Perform a blocking join on all processes. \ndaemon (bool) \u2013 The spawned processes\u2019 daemon flag. If set to True, daemonic processes will be created. \nstart_method (string) \u2013 (deprecated) this method will always use spawn as the start method. To use a different start method use start_processes().   Returns \nNone if join is True, ProcessContext if join is False   \n  \nclass torch.multiprocessing.SpawnContext [source]\n \nReturned by spawn() when called with join=False.  \njoin(timeout=None)  \nTries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting. Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.  Parameters \ntimeout (float) \u2013 Wait this long before giving up on waiting.   \n \n\n"}, {"name": "torch.multiprocessing.get_all_sharing_strategies()", "path": "multiprocessing#torch.multiprocessing.get_all_sharing_strategies", "type": "torch.multiprocessing", "text": " \ntorch.multiprocessing.get_all_sharing_strategies() [source]\n \nReturns a set of sharing strategies supported on a current system. \n"}, {"name": "torch.multiprocessing.get_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.get_sharing_strategy", "type": "torch.multiprocessing", "text": " \ntorch.multiprocessing.get_sharing_strategy() [source]\n \nReturns the current strategy for sharing CPU tensors. \n"}, {"name": "torch.multiprocessing.set_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.set_sharing_strategy", "type": "torch.multiprocessing", "text": " \ntorch.multiprocessing.set_sharing_strategy(new_strategy) [source]\n \nSets the strategy for sharing CPU tensors.  Parameters \nnew_strategy (str) \u2013 Name of the selected strategy. Should be one of the values returned by get_all_sharing_strategies().   \n"}, {"name": "torch.multiprocessing.spawn()", "path": "multiprocessing#torch.multiprocessing.spawn", "type": "torch.multiprocessing", "text": " \ntorch.multiprocessing.spawn(fn, args=(), nprocs=1, join=True, daemon=False, start_method='spawn') [source]\n \nSpawns nprocs processes that run fn with args. If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.  Parameters \n \nfn (function) \u2013 \nFunction is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing. The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.  \nargs (tuple) \u2013 Arguments passed to fn. \nnprocs (int) \u2013 Number of processes to spawn. \njoin (bool) \u2013 Perform a blocking join on all processes. \ndaemon (bool) \u2013 The spawned processes\u2019 daemon flag. If set to True, daemonic processes will be created. \nstart_method (string) \u2013 (deprecated) this method will always use spawn as the start method. To use a different start method use start_processes().   Returns \nNone if join is True, ProcessContext if join is False   \n"}, {"name": "torch.multiprocessing.SpawnContext", "path": "multiprocessing#torch.multiprocessing.SpawnContext", "type": "torch.multiprocessing", "text": " \nclass torch.multiprocessing.SpawnContext [source]\n \nReturned by spawn() when called with join=False.  \njoin(timeout=None)  \nTries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting. Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.  Parameters \ntimeout (float) \u2013 Wait this long before giving up on waiting.   \n \n"}, {"name": "torch.multiprocessing.SpawnContext.join()", "path": "multiprocessing#torch.multiprocessing.SpawnContext.join", "type": "torch.multiprocessing", "text": " \njoin(timeout=None)  \nTries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting. Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.  Parameters \ntimeout (float) \u2013 Wait this long before giving up on waiting.   \n"}, {"name": "torch.mv()", "path": "generated/torch.mv#torch.mv", "type": "torch", "text": " \ntorch.mv(input, vec, *, out=None) \u2192 Tensor  \nPerforms a matrix-vector product of the matrix input and the vector vec. If input is a (n\u00d7m)(n \\times m)  tensor, vec is a 1-D tensor of size mm , out will be 1-D of size nn .  Note This function does not broadcast.   Parameters \n \ninput (Tensor) \u2013 matrix to be multiplied \nvec (Tensor) \u2013 vector to be multiplied   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])\n \n"}, {"name": "torch.mvlgamma()", "path": "generated/torch.mvlgamma#torch.mvlgamma", "type": "torch", "text": " \ntorch.mvlgamma(input, p) \u2192 Tensor  \nComputes the multivariate log-gamma function) with dimension pp  element-wise, given by  log\u2061(\u0393p(a))=C+\u2211i=1plog\u2061(\u0393(a\u2212i\u221212))\\log(\\Gamma_{p}(a)) = C + \\displaystyle \\sum_{i=1}^{p} \\log\\left(\\Gamma\\left(a - \\frac{i - 1}{2}\\right)\\right)  \nwhere C=log\u2061(\u03c0)\u00d7p(p\u22121)4C = \\log(\\pi) \\times \\frac{p (p - 1)}{4}  and \u0393(\u22c5)\\Gamma(\\cdot)  is the Gamma function. All elements must be greater than p\u221212\\frac{p - 1}{2} , otherwise an error would be thrown.  Parameters \n \ninput (Tensor) \u2013 the tensor to compute the multivariate log-gamma function \np (int) \u2013 the number of dimensions    Example: >>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])\n \n"}, {"name": "torch.nanmedian()", "path": "generated/torch.nanmedian#torch.nanmedian", "type": "torch", "text": " \ntorch.nanmedian(input) \u2192 Tensor  \nReturns the median of the values in input, ignoring NaN values. This function is identical to torch.median() when there are no NaN values in input. When input has one or more NaN values, torch.median() will always return NaN, while this function will return the median of the non-NaN elements in input. If all the elements in input are NaN it will also return NaN.  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.tensor([1, float('nan'), 3, 2])\n>>> a.median()\ntensor(nan)\n>>> a.nanmedian()\ntensor(2.)\n  \ntorch.nanmedian(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor) \n Returns a namedtuple (values, indices) where values contains the median of each row of input in the dimension dim, ignoring NaN values, and indices contains the index of the median values found in the dimension dim. This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has one or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the median of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the dimension dim of input.   Example: >>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])\n>>> a\ntensor([[2., 3., 1.],\n        [nan, 1., nan]])\n>>> a.median(0)\ntorch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))\n>>> a.nanmedian(0)\ntorch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))\n \n"}, {"name": "torch.nanquantile()", "path": "generated/torch.nanquantile#torch.nanquantile", "type": "torch", "text": " \ntorch.nanquantile(input, q, dim=None, keepdim=False, *, out=None) \u2192 Tensor  \nThis is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist. If all values in a reduced row are NaN then the quantiles for that reduction will be NaN. See the documentation for torch.quantile().  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nq (float or Tensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> t = torch.tensor([float('nan'), 1, 2])\n>>> t.quantile(0.5)\ntensor(nan)\n>>> t.nanquantile(0.5)\ntensor(1.5000)\n\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\n>>> t\ntensor([[nan, nan],\n        [1., 2.]])\n>>> t.nanquantile(0.5, dim=0)\ntensor([1., 2.])\n>>> t.nanquantile(0.5, dim=1)\ntensor([   nan, 1.5000])\n \n"}, {"name": "torch.nansum()", "path": "generated/torch.nansum#torch.nansum", "type": "torch", "text": " \ntorch.nansum(input, *, dtype=None) \u2192 Tensor  \nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.   Example: >>> a = torch.tensor([1., 2., float('nan'), 4.])\n>>> torch.nansum(a)\ntensor(7.)\n  \ntorch.nansum(input, dim, keepdim=False, *, dtype=None) \u2192 Tensor \n Returns the sum of each row of the input tensor in the given dimension dim, treating Not a Numbers (NaNs) as zero. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.   Example: >>> torch.nansum(torch.tensor([1., float(\"nan\")]))\n1.0\n>>> a = torch.tensor([[1, 2], [3., float(\"nan\")]])\n>>> torch.nansum(a)\ntensor(6.)\n>>> torch.nansum(a, dim=0)\ntensor([4., 2.])\n>>> torch.nansum(a, dim=1)\ntensor([3., 3.])\n \n"}, {"name": "torch.nan_to_num()", "path": "generated/torch.nan_to_num#torch.nan_to_num", "type": "torch", "text": " \ntorch.nan_to_num(input, nan=0.0, posinf=None, neginf=None, *, out=None) \u2192 Tensor  \nReplaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively. By default, NaN`s are replaced with zero, positive infinity is replaced with the\ngreatest finite value representable by :attr:`input\u2019s dtype, and negative infinity is replaced with the least finite value representable by input\u2019s dtype.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nnan (Number, optional) \u2013 the value to replace NaNs with. Default is zero. \nposinf (Number, optional) \u2013 if a Number, the value to replace positive infinity values with. If None, positive infinity values are replaced with the greatest finite value representable by input\u2019s dtype. Default is None. \nneginf (Number, optional) \u2013 if a Number, the value to replace negative infinity values with. If None, negative infinity values are replaced with the lowest finite value representable by input\u2019s dtype. Default is None.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])\n>>> torch.nan_to_num(x)\ntensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0)\ntensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)\ntensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])\n \n"}, {"name": "torch.narrow()", "path": "generated/torch.narrow#torch.narrow", "type": "torch", "text": " \ntorch.narrow(input, dim, start, length) \u2192 Tensor  \nReturns a new tensor that is a narrowed version of input tensor. The dimension dim is input from start to start + length. The returned tensor and input tensor share the same underlying storage.  Parameters \n \ninput (Tensor) \u2013 the tensor to narrow \ndim (int) \u2013 the dimension along which to narrow \nstart (int) \u2013 the starting dimension \nlength (int) \u2013 the distance to the ending dimension    Example: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n \n"}, {"name": "torch.ne()", "path": "generated/torch.ne#torch.ne", "type": "torch", "text": " \ntorch.ne(input, other, *, out=None) \u2192 Tensor  \nComputes input\u2260other\\text{input} \\neq \\text{other}  element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or float) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is not equal to other and False elsewhere   Example: >>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [True, False]])\n \n"}, {"name": "torch.neg()", "path": "generated/torch.neg#torch.neg", "type": "torch", "text": " \ntorch.neg(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the negative of the elements of input.  out=\u22121\u00d7input\\text{out} = -1 \\times \\text{input}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(5)\n>>> a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])\n \n"}, {"name": "torch.negative()", "path": "generated/torch.negative#torch.negative", "type": "torch", "text": " \ntorch.negative(input, *, out=None) \u2192 Tensor  \nAlias for torch.neg() \n"}, {"name": "torch.nextafter()", "path": "generated/torch.nextafter#torch.nextafter", "type": "torch", "text": " \ntorch.nextafter(input, other, *, out=None) \u2192 Tensor  \nReturn the next floating-point value after input towards other, elementwise. The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> eps = torch.finfo(torch.float32).eps\n>>> torch.nextafter(torch.Tensor([1, 2]), torch.Tensor([2, 1])) == torch.Tensor([eps + 1, 2 - eps])\ntensor([True, True])\n   \n"}, {"name": "torch.nn", "path": "nn", "type": "torch.nn", "text": "torch.nn These are the basic building block for graphs torch.nn  Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   \n\nParameter\n A kind of Tensor that is to be considered a module parameter.  \n\nUninitializedParameter\n A parameter that is not initialized.   Containers  \n\nModule\n Base class for all neural network modules.  \n\nSequential\n A sequential container.  \n\nModuleList\n Holds submodules in a list.  \n\nModuleDict\n Holds submodules in a dictionary.  \n\nParameterList\n Holds parameters in a list.  \n\nParameterDict\n Holds parameters in a dictionary.   Global Hooks For Module  \n\nregister_module_forward_pre_hook\n Registers a forward pre-hook common to all modules.  \n\nregister_module_forward_hook\n Registers a global forward hook for all the modules  \n\nregister_module_backward_hook\n Registers a backward hook common to all the modules.   Convolution Layers  \nnn.Conv1d Applies a 1D convolution over an input signal composed of several input planes.  \nnn.Conv2d Applies a 2D convolution over an input signal composed of several input planes.  \nnn.Conv3d Applies a 3D convolution over an input signal composed of several input planes.  \nnn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes.  \nnn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes.  \nnn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes.  \nnn.LazyConv1d A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).  \nnn.LazyConv2d A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).  \nnn.LazyConv3d A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).  \nnn.LazyConvTranspose1d A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).  \nnn.LazyConvTranspose2d A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).  \nnn.LazyConvTranspose3d A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).  \nnn.Unfold Extracts sliding local blocks from a batched input tensor.  \nnn.Fold Combines an array of sliding local blocks into a large containing tensor.   Pooling layers  \nnn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes.  \nnn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes.  \nnn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes.  \nnn.MaxUnpool1d Computes a partial inverse of MaxPool1d.  \nnn.MaxUnpool2d Computes a partial inverse of MaxPool2d.  \nnn.MaxUnpool3d Computes a partial inverse of MaxPool3d.  \nnn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes.  \nnn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes.  \nnn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes.  \nnn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes.  \nnn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes.  \nnn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes.  \nnn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes.  \nnn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes.  \nnn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes.  \nnn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes.  \nnn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes.  \nnn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Padding Layers  \nnn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary.  \nnn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary.  \nnn.ReplicationPad1d Pads the input tensor using replication of the input boundary.  \nnn.ReplicationPad2d Pads the input tensor using replication of the input boundary.  \nnn.ReplicationPad3d Pads the input tensor using replication of the input boundary.  \nnn.ZeroPad2d Pads the input tensor boundaries with zero.  \nnn.ConstantPad1d Pads the input tensor boundaries with a constant value.  \nnn.ConstantPad2d Pads the input tensor boundaries with a constant value.  \nnn.ConstantPad3d Pads the input tensor boundaries with a constant value.   Non-linear Activations (weighted sum, nonlinearity)  \nnn.ELU Applies the element-wise function:  \nnn.Hardshrink Applies the hard shrinkage function element-wise:  \nnn.Hardsigmoid Applies the element-wise function:  \nnn.Hardtanh Applies the HardTanh function element-wise  \nnn.Hardswish Applies the hardswish function, element-wise, as described in the paper:  \nnn.LeakyReLU Applies the element-wise function:  \nnn.LogSigmoid Applies the element-wise function:  \nnn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces.  \nnn.PReLU Applies the element-wise function:  \nnn.ReLU Applies the rectified linear unit function element-wise:  \nnn.ReLU6 Applies the element-wise function:  \nnn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:  \nnn.SELU Applied element-wise, as:  \nnn.CELU Applies the element-wise function:  \nnn.GELU Applies the Gaussian Error Linear Units function:  \nnn.Sigmoid Applies the element-wise function:  \nnn.SiLU Applies the silu function, element-wise.  \nnn.Softplus Applies the element-wise function:  \nnn.Softshrink Applies the soft shrinkage function elementwise:  \nnn.Softsign Applies the element-wise function:  \nnn.Tanh Applies the element-wise function:  \nnn.Tanhshrink Applies the element-wise function:  \nnn.Threshold Thresholds each element of the input Tensor.   Non-linear Activations (other)  \nnn.Softmin Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.  \nnn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.  \nnn.Softmax2d Applies SoftMax over features to each spatial location.  \nnn.LogSoftmax Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))  function to an n-dimensional input Tensor.  \nnn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.   Normalization Layers  \nnn.BatchNorm1d Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  \nnn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  \nnn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  \nnn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization  \nnn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  \nnn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  \nnn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  \nnn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  \nnn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization  \nnn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.   Recurrent Layers  \nnn.RNNBase   \nnn.RNN Applies a multi-layer Elman RNN with tanh\u2061\\tanh  or ReLU\\text{ReLU}  non-linearity to an input sequence.  \nnn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.  \nnn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.  \nnn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity.  \nnn.LSTMCell A long short-term memory (LSTM) cell.  \nnn.GRUCell A gated recurrent unit (GRU) cell   Transformer Layers  \nnn.Transformer A transformer model.  \nnn.TransformerEncoder TransformerEncoder is a stack of N encoder layers  \nnn.TransformerDecoder TransformerDecoder is a stack of N decoder layers  \nnn.TransformerEncoderLayer TransformerEncoderLayer is made up of self-attn and feedforward network.  \nnn.TransformerDecoderLayer TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.   Linear Layers  \nnn.Identity A placeholder identity operator that is argument-insensitive.  \nnn.Linear Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b   \nnn.Bilinear Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b   \nnn.LazyLinear A torch.nn.Linear module with lazy initialization.   Dropout Layers  \nnn.Dropout During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.  \nnn.Dropout2d Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ).  \nnn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ).  \nnn.AlphaDropout Applies Alpha Dropout over the input.   Sparse Layers  \nnn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size.  \nnn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.   Distance Functions  \nnn.CosineSimilarity Returns cosine similarity between x1x_1  and x2x_2 , computed along dim.  \nnn.PairwiseDistance Computes the batchwise pairwise distance between vectors v1v_1 , v2v_2  using the p-norm:   Loss Functions  \nnn.L1Loss Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx  and target yy .  \nnn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx  and target yy .  \nnn.CrossEntropyLoss This criterion combines LogSoftmax and NLLLoss in one single class.  \nnn.CTCLoss The Connectionist Temporal Classification loss.  \nnn.NLLLoss The negative log likelihood loss.  \nnn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target.  \nnn.GaussianNLLLoss Gaussian negative log likelihood loss.  \nnn.KLDivLoss The Kullback-Leibler divergence loss measure  \nnn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output:  \nnn.BCEWithLogitsLoss This loss combines a Sigmoid layer and the BCELoss in one single class.  \nnn.MarginRankingLoss Creates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy  (containing 1 or -1).  \nnn.HingeEmbeddingLoss Measures the loss given an input tensor xx  and a labels tensor yy  (containing 1 or -1).  \nnn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 2D Tensor of target class indices).  \nnn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.  \nnn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx  and target tensor yy  (containing 1 or -1).  \nnn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx  and target yy  of size (N,C)(N, C) .  \nnn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensors x1x_1 , x2x_2  and a Tensor label yy  with values 1 or -1.  \nnn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-1 ):  \nnn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensors x1x1 , x2x2 , x3x3  and a margin with a value greater than 00 .  \nnn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensors aa , pp , and nn  (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).   Vision Layers  \nnn.PixelShuffle Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is an upscale factor.  \nnn.PixelUnshuffle Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.  \nnn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.  \nnn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.  \nnn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels.   Shuffle Layers  \nnn.ChannelShuffle Divide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W)  into g groups and rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the original tensor shape.   DataParallel Layers (multi-GPU, distributed)  \nnn.DataParallel Implements data parallelism at the module level.  \nnn.parallel.DistributedDataParallel Implements distributed data parallelism that is based on torch.distributed package at the module level.   Utilities From the torch.nn.utils module  \n\nclip_grad_norm_\n Clips gradient norm of an iterable of parameters.  \n\nclip_grad_value_\n Clips gradient of an iterable of parameters at specified value.  \n\nparameters_to_vector\n Convert parameters to one vector  \n\nvector_to_parameters\n Convert one vector to the parameters    \nprune.BasePruningMethod Abstract base class for creation of new pruning techniques.    \nprune.PruningContainer Container holding a sequence of pruning methods for iterative pruning.  \nprune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.  \nprune.RandomUnstructured Prune (currently unpruned) units in a tensor at random.  \nprune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  \nprune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random.  \nprune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.  \nprune.CustomFromMask   \nprune.identity Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units.  \nprune.random_unstructured Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random.  \nprune.l1_unstructured Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm.  \nprune.random_structured Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random.  \nprune.ln_structured Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm.  \nprune.global_unstructured Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.  \nprune.custom_from_mask Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask.  \nprune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook.  \nprune.is_pruned Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.  \n\nweight_norm\n Applies weight normalization to a parameter in the given module.  \n\nremove_weight_norm\n Removes the weight normalization reparameterization from a module.  \n\nspectral_norm\n Applies spectral normalization to a parameter in the given module.  \n\nremove_spectral_norm\n Removes the spectral normalization reparameterization from a module.   Utility functions in other modules  \nnn.utils.rnn.PackedSequence Holds the data and list of batch_sizes of a packed sequence.  \nnn.utils.rnn.pack_padded_sequence Packs a Tensor containing padded sequences of variable length.  \nnn.utils.rnn.pad_packed_sequence Pads a packed batch of variable length sequences.  \nnn.utils.rnn.pad_sequence Pad a list of variable length Tensors with padding_value  \nnn.utils.rnn.pack_sequence Packs a list of variable length Tensors  \nnn.Flatten Flattens a contiguous range of dims into a tensor.  \nnn.Unflatten Unflattens a tensor dim expanding it to a desired shape.   Quantized Functions Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation. Lazy Modules Initialization  \nnn.modules.lazy.LazyModuleMixin A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d  \n"}, {"name": "torch.nn.AdaptiveAvgPool1d", "path": "generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveAvgPool1d(output_size) [source]\n \nApplies a 1D adaptive average pooling over an input signal composed of several input planes. The output size is H, for any input size. The number of output features is equal to the number of input planes.  Parameters \noutput_size \u2013 the target output size H   Examples >>> # target output size of 5\n>>> m = nn.AdaptiveAvgPool1d(5)\n>>> input = torch.randn(1, 64, 8)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AdaptiveAvgPool2d", "path": "generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveAvgPool2d(output_size) [source]\n \nApplies a 2D adaptive average pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters \noutput_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input.   Examples >>> # target output size of 5x7\n>>> m = nn.AdaptiveAvgPool2d((5,7))\n>>> input = torch.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> # target output size of 7x7 (square)\n>>> m = nn.AdaptiveAvgPool2d(7)\n>>> input = torch.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> # target output size of 10x7\n>>> m = nn.AdaptiveAvgPool2d((None, 7))\n>>> input = torch.randn(1, 64, 10, 9)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AdaptiveAvgPool3d", "path": "generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveAvgPool3d(output_size) [source]\n \nApplies a 3D adaptive average pooling over an input signal composed of several input planes. The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters \noutput_size \u2013 the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input.   Examples >>> # target output size of 5x7x9\n>>> m = nn.AdaptiveAvgPool3d((5,7,9))\n>>> input = torch.randn(1, 64, 8, 9, 10)\n>>> output = m(input)\n>>> # target output size of 7x7x7 (cube)\n>>> m = nn.AdaptiveAvgPool3d(7)\n>>> input = torch.randn(1, 64, 10, 9, 8)\n>>> output = m(input)\n>>> # target output size of 7x9x8\n>>> m = nn.AdaptiveAvgPool3d((7, None, None))\n>>> input = torch.randn(1, 64, 10, 9, 8)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False) [source]\n \nEfficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou. Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf\u2019s law. Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated. The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute \u2013 that is, contain a small number of assigned labels. We highly recommend taking a look at the original paper for more details.  \ncutoffs should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting cutoffs = [10, 100, 1000] means that first 10 targets will be assigned to the \u2018head\u2019 of the adaptive softmax, targets 11, 12, \u2026, 100 will be assigned to the first cluster, and targets 101, 102, \u2026, 1000 will be assigned to the second cluster, while targets 1001, 1002, \u2026, n_classes - 1 will be assigned to the last, third cluster. \ndiv_value is used to compute the size of each additional cluster, which is given as \u230ain_featuresdiv_valueidx\u230b\\left\\lfloor\\frac{\\texttt{in\\_features}}{\\texttt{div\\_value}^{idx}}\\right\\rfloor , where idxidx  is the cluster index (with clusters for less frequent words having larger indices, and indices starting from 11 ). \nhead_bias if set to True, adds a bias term to the \u2018head\u2019 of the adaptive softmax. See paper for details. Set to False in the official implementation.   Warning Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1.   Note This module returns a NamedTuple with output and loss fields. See further documentation for details.   Note To compute log-probabilities for all classes, the log_prob method can be used.   Parameters \n \nin_features (int) \u2013 Number of features in the input tensor \nn_classes (int) \u2013 Number of classes in the dataset \ncutoffs (Sequence) \u2013 Cutoffs used to assign targets to their buckets \ndiv_value (float, optional) \u2013 value used as an exponent to compute sizes of the clusters. Default: 4.0 \nhead_bias (bool, optional) \u2013 If True, adds a bias term to the \u2018head\u2019 of the adaptive softmax. Default: False\n   Returns \n \noutput is a Tensor of size N containing computed target log probabilities for each example \nloss is a Scalar representing the computed negative log likelihood loss   Return type \nNamedTuple with output and loss fields    Shape:\n\n input: (N,in_features)(N, \\texttt{in\\_features}) \n target: (N)(N)  where each value satisfies 0<=target[i]<=n_classes0 <= \\texttt{target[i]} <= \\texttt{n\\_classes} \n output1: (N)(N) \n output2: Scalar\n     \nlog_prob(input) [source]\n \nComputes log probabilities for all n_classes\\texttt{n\\_classes}   Parameters \ninput (Tensor) \u2013 a minibatch of examples  Returns \nlog-probabilities of for each class cc  in range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes} , where n_classes\\texttt{n\\_classes}  is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.    Shape:\n\n Input: (N,in_features)(N, \\texttt{in\\_features}) \n Output: (N,n_classes)(N, \\texttt{n\\_classes}) \n    \n  \npredict(input) [source]\n \nThis is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.  Parameters \ninput (Tensor) \u2013 a minibatch of examples  Returns \na class with the highest probability for each example  Return type \noutput (Tensor)    Shape:\n\n Input: (N,in_features)(N, \\texttt{in\\_features}) \n Output: (N)(N) \n    \n \n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "torch.nn", "text": " \nlog_prob(input) [source]\n \nComputes log probabilities for all n_classes\\texttt{n\\_classes}   Parameters \ninput (Tensor) \u2013 a minibatch of examples  Returns \nlog-probabilities of for each class cc  in range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes} , where n_classes\\texttt{n\\_classes}  is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.    Shape:\n\n Input: (N,in_features)(N, \\texttt{in\\_features}) \n Output: (N,n_classes)(N, \\texttt{n\\_classes}) \n    \n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "torch.nn", "text": " \npredict(input) [source]\n \nThis is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.  Parameters \ninput (Tensor) \u2013 a minibatch of examples  Returns \na class with the highest probability for each example  Return type \noutput (Tensor)    Shape:\n\n Input: (N,in_features)(N, \\texttt{in\\_features}) \n Output: (N)(N) \n    \n"}, {"name": "torch.nn.AdaptiveMaxPool1d", "path": "generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False) [source]\n \nApplies a 1D adaptive max pooling over an input signal composed of several input planes. The output size is H, for any input size. The number of output features is equal to the number of input planes.  Parameters \n \noutput_size \u2013 the target output size H \nreturn_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool1d. Default: False\n    Examples >>> # target output size of 5\n>>> m = nn.AdaptiveMaxPool1d(5)\n>>> input = torch.randn(1, 64, 8)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AdaptiveMaxPool2d", "path": "generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False) [source]\n \nApplies a 2D adaptive max pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters \n \noutput_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input. \nreturn_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: False\n    Examples >>> # target output size of 5x7\n>>> m = nn.AdaptiveMaxPool2d((5,7))\n>>> input = torch.randn(1, 64, 8, 9)\n>>> output = m(input)\n>>> # target output size of 7x7 (square)\n>>> m = nn.AdaptiveMaxPool2d(7)\n>>> input = torch.randn(1, 64, 10, 9)\n>>> output = m(input)\n>>> # target output size of 10x7\n>>> m = nn.AdaptiveMaxPool2d((None, 7))\n>>> input = torch.randn(1, 64, 10, 9)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AdaptiveMaxPool3d", "path": "generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d", "type": "torch.nn", "text": " \nclass torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False) [source]\n \nApplies a 3D adaptive max pooling over an input signal composed of several input planes. The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters \n \noutput_size \u2013 the target output size of the image of the form D x H x W. Can be a tuple (D, H, W) or a single D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input. \nreturn_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: False\n    Examples >>> # target output size of 5x7x9\n>>> m = nn.AdaptiveMaxPool3d((5,7,9))\n>>> input = torch.randn(1, 64, 8, 9, 10)\n>>> output = m(input)\n>>> # target output size of 7x7x7 (cube)\n>>> m = nn.AdaptiveMaxPool3d(7)\n>>> input = torch.randn(1, 64, 10, 9, 8)\n>>> output = m(input)\n>>> # target output size of 7x9x8\n>>> m = nn.AdaptiveMaxPool3d((7, None, None))\n>>> input = torch.randn(1, 64, 10, 9, 8)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AlphaDropout", "path": "generated/torch.nn.alphadropout#torch.nn.AlphaDropout", "type": "torch.nn", "text": " \nclass torch.nn.AlphaDropout(p=0.5, inplace=False) [source]\n \nApplies Alpha Dropout over the input. Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation. During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation. During evaluation the module simply computes an identity function. More details can be found in the paper Self-Normalizing Neural Networks .  Parameters \n \np (float) \u2013 probability of an element to be dropped. Default: 0.5 \ninplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape:\n\n Input: (\u2217)(*) . Input can be of any shape Output: (\u2217)(*) . Output is of the same shape as input    Examples: >>> m = nn.AlphaDropout(p=0.2)\n>>> input = torch.randn(20, 16)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AvgPool1d", "path": "generated/torch.nn.avgpool1d#torch.nn.AvgPool1d", "type": "torch.nn", "text": " \nclass torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) [source]\n \nApplies a 1D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L) , output (N,C,Lout)(N, C, L_{out})  and kernel_size kk  can be precisely described as:  out(Ni,Cj,l)=1k\u2211m=0k\u22121input(Ni,Cj,stride\u00d7l+m)\\text{out}(N_i, C_j, l) = \\frac{1}{k} \\sum_{m=0}^{k-1} \\text{input}(N_i, C_j, \\text{stride} \\times l + m) \nIf padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.  Note When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size, stride, padding can each be an int or a one-element tuple.  Parameters \n \nkernel_size \u2013 the size of the window \nstride \u2013 the stride of the window. Default value is kernel_size\n \npadding \u2013 implicit zero padding to be added on both sides \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation     Shape:\n\n Input: (N,C,Lin)(N, C, L_{in}) \n \nOutput: (N,C,Lout)(N, C, L_{out}) , where  Lout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  \n    Examples: >>> # pool with window of size=3, stride=2\n>>> m = nn.AvgPool1d(3, stride=2)\n>>> m(torch.tensor([[[1.,2,3,4,5,6,7]]]))\ntensor([[[ 2.,  4.,  6.]]])\n \n"}, {"name": "torch.nn.AvgPool2d", "path": "generated/torch.nn.avgpool2d#torch.nn.AvgPool2d", "type": "torch.nn", "text": " \nclass torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) [source]\n \nApplies a 2D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  and kernel_size (kH,kW)(kH, kW)  can be precisely described as:  out(Ni,Cj,h,w)=1kH\u2217kW\u2211m=0kH\u22121\u2211n=0kW\u22121input(Ni,Cj,stride[0]\u00d7h+m,stride[1]\u00d7w+n)out(N_i, C_j, h, w) = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1} input(N_i, C_j, stride[0] \\times h + m, stride[1] \\times w + n) \nIf padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.  Note When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size, stride, padding can either be:  a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension   Parameters \n \nkernel_size \u2013 the size of the window \nstride \u2013 the stride of the window. Default value is kernel_size\n \npadding \u2013 implicit zero padding to be added on both sides \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise kernel_size will be used     Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where  Hout=\u230aHin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  \n Wout=\u230aWin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[1] - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  \n    Examples: >>> # pool of square window of size=3, stride=2\n>>> m = nn.AvgPool2d(3, stride=2)\n>>> # pool of non-square window\n>>> m = nn.AvgPool2d((3, 2), stride=(2, 1))\n>>> input = torch.randn(20, 16, 50, 32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.AvgPool3d", "path": "generated/torch.nn.avgpool3d#torch.nn.AvgPool3d", "type": "torch.nn", "text": " \nclass torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) [source]\n \nApplies a 3D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  and kernel_size (kD,kH,kW)(kD, kH, kW)  can be precisely described as:  out(Ni,Cj,d,h,w)=\u2211k=0kD\u22121\u2211m=0kH\u22121\u2211n=0kW\u22121input(Ni,Cj,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)kD\u00d7kH\u00d7kW\\begin{aligned} \\text{out}(N_i, C_j, d, h, w) ={} & \\sum_{k=0}^{kD-1} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1} \\\\ & \\frac{\\text{input}(N_i, C_j, \\text{stride}[0] \\times d + k, \\text{stride}[1] \\times h + m, \\text{stride}[2] \\times w + n)} {kD \\times kH \\times kW} \\end{aligned}  \nIf padding is non-zero, then the input is implicitly zero-padded on all three sides for padding number of points.  Note When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size, stride can either be:  a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension   Parameters \n \nkernel_size \u2013 the size of the window \nstride \u2013 the stride of the window. Default value is kernel_size\n \npadding \u2013 implicit zero padding to be added on all three sides \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise kernel_size will be used     Shape:\n\n Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in}) \n \nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where  Dout=\u230aDin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  \n Hout=\u230aHin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  \n Wout=\u230aWin+2\u00d7padding[2]\u2212kernel_size[2]stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor  \n    Examples: >>> # pool of square window of size=3, stride=2\n>>> m = nn.AvgPool3d(3, stride=2)\n>>> # pool of non-square window\n>>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))\n>>> input = torch.randn(20, 16, 50,44, 31)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.BatchNorm1d", "path": "generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d", "type": "torch.nn", "text": " \nclass torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) [source]\n \nApplies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are set to 1 and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it\u2019s common terminology to call this Temporal Batch Normalization.  Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,L)(N, C, L)  or LL  from input of size (N,L)(N, L) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nmomentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True\n \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True\n     Shape:\n\n Input: (N,C)(N, C)  or (N,C,L)(N, C, L) \n Output: (N,C)(N, C)  or (N,C,L)(N, C, L)  (same shape as input)    Examples: >>> # With Learnable Parameters\n>>> m = nn.BatchNorm1d(100)\n>>> # Without Learnable Parameters\n>>> m = nn.BatchNorm1d(100, affine=False)\n>>> input = torch.randn(20, 100)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.BatchNorm2d", "path": "generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d", "type": "torch.nn", "text": " \nclass torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) [source]\n \nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are set to 1 and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it\u2019s common terminology to call this Spatial Batch Normalization.  Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,H,W)(N, C, H, W) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nmomentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True\n \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True\n     Shape:\n\n Input: (N,C,H,W)(N, C, H, W) \n Output: (N,C,H,W)(N, C, H, W)  (same shape as input)    Examples: >>> # With Learnable Parameters\n>>> m = nn.BatchNorm2d(100)\n>>> # Without Learnable Parameters\n>>> m = nn.BatchNorm2d(100, affine=False)\n>>> input = torch.randn(20, 100, 35, 45)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.BatchNorm3d", "path": "generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d", "type": "torch.nn", "text": " \nclass torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) [source]\n \nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are set to 1 and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.  Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,D,H,W)(N, C, D, H, W) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nmomentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True\n \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True\n     Shape:\n\n Input: (N,C,D,H,W)(N, C, D, H, W) \n Output: (N,C,D,H,W)(N, C, D, H, W)  (same shape as input)    Examples: >>> # With Learnable Parameters\n>>> m = nn.BatchNorm3d(100)\n>>> # Without Learnable Parameters\n>>> m = nn.BatchNorm3d(100, affine=False)\n>>> input = torch.randn(20, 100, 35, 45, 10)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.BCELoss", "path": "generated/torch.nn.bceloss#torch.nn.BCELoss", "type": "torch.nn", "text": " \nclass torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that measures the Binary Cross Entropy between the target and the output: The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wn[yn\u22c5log\u2061xn+(1\u2212yn)\u22c5log\u2061(1\u2212xn)],\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],  \nwhere NN  is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nThis is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yy  should be numbers between 0 and 1. Notice that if xnx_n  is either 0 or 1, one of the log terms would be mathematically undefined in the above loss equation. PyTorch chooses to set log\u2061(0)=\u2212\u221e\\log (0) = -\\infty , since lim\u2061x\u21920log\u2061(x)=\u2212\u221e\\lim_{x\\to 0} \\log (x) = -\\infty . However, an infinite term in the loss equation is not desirable for several reasons. For one, if either yn=0y_n = 0  or (1\u2212yn)=0(1 - y_n) = 0 , then we would be multiplying 0 with infinity. Secondly, if we have an infinite loss value, then we would also have an infinite term in our gradient, since lim\u2061x\u21920ddxlog\u2061(x)=\u221e\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty . This would make BCELoss\u2019s backward method nonlinear with respect to xnx_n , and using it for things like linear regression would not be straight-forward. Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.  Parameters \n \nweight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *) , same shape as input.    Examples: >>> m = nn.Sigmoid()\n>>> loss = nn.BCELoss()\n>>> input = torch.randn(3, requires_grad=True)\n>>> target = torch.empty(3).random_(2)\n>>> output = loss(m(input), target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.BCEWithLogitsLoss", "path": "generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss", "type": "torch.nn", "text": " \nclass torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) [source]\n \nThis loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wn[yn\u22c5log\u2061\u03c3(xn)+(1\u2212yn)\u22c5log\u2061(1\u2212\u03c3(xn))],\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_n \\left[ y_n \\cdot \\log \\sigma(x_n) + (1 - y_n) \\cdot \\log (1 - \\sigma(x_n)) \\right],  \nwhere NN  is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nThis is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1. It\u2019s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:  \u2113c(x,y)=Lc={l1,c,\u2026,lN,c}\u22a4,ln,c=\u2212wn,c[pcyn,c\u22c5log\u2061\u03c3(xn,c)+(1\u2212yn,c)\u22c5log\u2061(1\u2212\u03c3(xn,c))],\\ell_c(x, y) = L_c = \\{l_{1,c},\\dots,l_{N,c}\\}^\\top, \\quad l_{n,c} = - w_{n,c} \\left[ p_c y_{n,c} \\cdot \\log \\sigma(x_{n,c}) + (1 - y_{n,c}) \\cdot \\log (1 - \\sigma(x_{n,c})) \\right],  \nwhere cc  is the class number (c>1c > 1  for multi-label binary classification, c=1c = 1  for single-label binary classification), nn  is the number of the sample in the batch and pcp_c  is the weight of the positive answer for the class cc . pc>1p_c > 1  increases the recall, pc<1p_c < 1  increases the precision. For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to 300100=3\\frac{300}{100}=3 . The loss would act as if the dataset contains 3\u00d7100=3003\\times 100=300  positive examples. Examples: >>> target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n>>> output = torch.full([10, 64], 1.5)  # A prediction (logit)\n>>> pos_weight = torch.ones([64])  # All weights are equal to 1\n>>> criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n>>> criterion(output, target)  # -log(sigmoid(1.5))\ntensor(0.2014)\n  Parameters \n \nweight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n \npos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.     Shape:\n  Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *) , same shape as input.  Examples: >>> loss = nn.BCEWithLogitsLoss()\n>>> input = torch.randn(3, requires_grad=True)\n>>> target = torch.empty(3).random_(2)\n>>> output = loss(input, target)\n>>> output.backward()\n   \n"}, {"name": "torch.nn.Bilinear", "path": "generated/torch.nn.bilinear#torch.nn.Bilinear", "type": "torch.nn", "text": " \nclass torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True) [source]\n \nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b   Parameters \n \nin1_features \u2013 size of each first input sample \nin2_features \u2013 size of each second input sample \nout_features \u2013 size of each output sample \nbias \u2013 If set to False, the layer will not learn an additive bias. Default: True\n     Shape:\n\n Input1: (N,\u2217,Hin1)(N, *, H_{in1})  where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}  and \u2217*  means any number of additional dimensions. All but the last dimension of the inputs should be the same. Input2: (N,\u2217,Hin2)(N, *, H_{in2})  where Hin2=in2_featuresH_{in2}=\\text{in2\\_features} . Output: (N,\u2217,Hout)(N, *, H_{out})  where Hout=out_featuresH_{out}=\\text{out\\_features}  and all but the last dimension are the same shape as the input.     Variables \n \n~Bilinear.weight \u2013 the learnable weights of the module of shape (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features}) . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) , where k=1in1_featuresk = \\frac{1}{\\text{in1\\_features}} \n \n~Bilinear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features}) . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) , where k=1in1_featuresk = \\frac{1}{\\text{in1\\_features}} \n    Examples: >>> m = nn.Bilinear(20, 30, 40)\n>>> input1 = torch.randn(128, 20)\n>>> input2 = torch.randn(128, 30)\n>>> output = m(input1, input2)\n>>> print(output.size())\ntorch.Size([128, 40])\n \n"}, {"name": "torch.nn.CELU", "path": "generated/torch.nn.celu#torch.nn.CELU", "type": "torch.nn", "text": " \nclass torch.nn.CELU(alpha=1.0, inplace=False) [source]\n \nApplies the element-wise function:  CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))  \nMore details can be found in the paper Continuously Differentiable Exponential Linear Units .  Parameters \n \nalpha \u2013 the \u03b1\\alpha  value for the CELU formulation. Default: 1.0 \ninplace \u2013 can optionally do the operation in-place. Default: False\n     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.CELU()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.ChannelShuffle", "path": "generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle", "type": "torch.nn", "text": " \nclass torch.nn.ChannelShuffle(groups) [source]\n \nDivide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W)  into g groups and rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the original tensor shape.  Parameters \ngroups (int) \u2013 number of groups to divide channels in.   Examples: >>> channel_shuffle = nn.ChannelShuffle(2)\n>>> input = torch.randn(1, 4, 2, 2)\n>>> print(input)\n[[[[1, 2],\n   [3, 4]],\n  [[5, 6],\n   [7, 8]],\n  [[9, 10],\n   [11, 12]],\n  [[13, 14],\n   [15, 16]],\n ]]\n>>> output = channel_shuffle(input)\n>>> print(output)\n[[[[1, 2],\n   [3, 4]],\n  [[9, 10],\n   [11, 12]],\n  [[5, 6],\n   [7, 8]],\n  [[13, 14],\n   [15, 16]],\n ]]\n \n"}, {"name": "torch.nn.ConstantPad1d", "path": "generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d", "type": "torch.nn", "text": " \nclass torch.nn.ConstantPad1d(padding, value) [source]\n \nPads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} )    Shape:\n\n Input: (N,C,Win)(N, C, W_{in}) \n \nOutput: (N,C,Wout)(N, C, W_{out})  where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ConstantPad1d(2, 3.5)\n>>> input = torch.randn(1, 2, 4)\n>>> input\ntensor([[[-1.0491, -0.7152, -0.0749,  0.8530],\n         [-1.3287,  1.8966,  0.1466, -0.2771]]])\n>>> m(input)\ntensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,\n           3.5000],\n         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,\n           3.5000]]])\n>>> m = nn.ConstantPad1d(2, 3.5)\n>>> input = torch.randn(1, 2, 3)\n>>> input\ntensor([[[ 1.6616,  1.4523, -1.1255],\n         [-3.6372,  0.1182, -1.8652]]])\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],\n         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])\n>>> # using different paddings for different sides\n>>> m = nn.ConstantPad1d((3, 1), 3.5)\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],\n         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])\n \n"}, {"name": "torch.nn.ConstantPad2d", "path": "generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d", "type": "torch.nn", "text": " \nclass torch.nn.ConstantPad2d(padding, value) [source]\n \nPads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )    Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}  Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ConstantPad2d(2, 3.5)\n>>> input = torch.randn(1, 2, 2)\n>>> input\ntensor([[[ 1.6585,  0.4320],\n         [-0.8701, -0.4649]]])\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],\n         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n>>> # using different paddings for different sides\n>>> m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)\n>>> m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],\n         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n \n"}, {"name": "torch.nn.ConstantPad3d", "path": "generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d", "type": "torch.nn", "text": " \nclass torch.nn.ConstantPad3d(padding, value) [source]\n \nPads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} , padding_front\\text{padding\\_front} , padding_back\\text{padding\\_back} )    Shape:\n\n Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in}) \n \nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}  Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}  Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ConstantPad3d(3, 3.5)\n>>> input = torch.randn(16, 3, 10, 20, 30)\n>>> output = m(input)\n>>> # using different paddings for different sides\n>>> m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Conv1d", "path": "generated/torch.nn.conv1d#torch.nn.Conv1d", "type": "torch.nn", "text": " \nclass torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nApplies a 1D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,L)(N, C_{\\text{in}}, L)  and output (N,Cout,Lout)(N, C_{\\text{out}}, L_{\\text{out}})  can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)  \nwhere \u22c6\\star  is the valid cross-correlation operator, NN  is a batch size, CC  denotes a number of channels, LL  is a length of signal sequence. This module supports TensorFloat32.  \nstride controls the stride for the cross-correlation, a single number or a one-element tuple. \npadding controls the amount of implicit padding on both sides for padding number of points. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. \ngroups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,  At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channelsin_channels\\frac{\\text{out\\_channels}}{\\text{in\\_channels}} ).     Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d. In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .   Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \nin_channels (int) \u2013 Number of channels in the input image \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 \npadding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n     Shape:\n\n Input: (N,Cin,Lin)(N, C_{in}, L_{in}) \n \nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})  where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  \n     Variables \n \n~Conv1d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,kernel_size)(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size}) . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCin\u2217kernel_sizek = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}} \n \n~Conv1d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCin\u2217kernel_sizek = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}} \n    Examples: >>> m = nn.Conv1d(16, 33, 3, stride=2)\n>>> input = torch.randn(20, 16, 50)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Conv2d", "path": "generated/torch.nn.conv2d#torch.nn.Conv2d", "type": "torch.nn", "text": " \nclass torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nApplies a 2D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,H,W)(N, C_{\\text{in}}, H, W)  and output (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})  can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)  \nwhere \u22c6\\star  is the valid 2D cross-correlation operator, NN  is a batch size, CC  denotes a number of channels, HH  is a height of input planes in pixels, and WW  is width in pixels. This module supports TensorFloat32.  \nstride controls the stride for the cross-correlation, a single number or a tuple. \npadding controls the amount of implicit padding on both sides for padding number of points for each dimension. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. \ngroups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,  At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channelsin_channels\\frac{\\text{out\\_channels}}{\\text{in\\_channels}} ).    The parameters kernel_size, stride, padding, dilation can either be:  a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d. In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .   Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \nin_channels (int) \u2013 Number of channels in the input image \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 \npadding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n     Shape:\n\n Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in}) \n \nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})  where  Hout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  \n Wout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  \n     Variables \n \n~Conv2d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},  kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}) . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCin\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]} \n \n~Conv2d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCin\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]} \n    Examples >>> # With square kernels and equal stride\n>>> m = nn.Conv2d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> # non-square kernels and unequal stride and with padding and dilation\n>>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n>>> input = torch.randn(20, 16, 50, 100)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Conv3d", "path": "generated/torch.nn.conv3d#torch.nn.Conv3d", "type": "torch.nn", "text": " \nclass torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nApplies a 3D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,D,H,W)(N, C_{in}, D, H, W)  and output (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})  can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)  \nwhere \u22c6\\star  is the valid 3D cross-correlation operator This module supports TensorFloat32.  \nstride controls the stride for the cross-correlation. \npadding controls the amount of implicit padding on both sides for padding number of points for each dimension. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. \ngroups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,  At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channelsin_channels\\frac{\\text{out\\_channels}}{\\text{in\\_channels}} ).    The parameters kernel_size, stride, padding, dilation can either be:  a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d. In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .   Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \nin_channels (int) \u2013 Number of channels in the input image \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 Zero-padding added to all three sides of the input. Default: 0 \npadding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n     Shape:\n\n Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in}) \n \nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})  where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  \n Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  \n Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  \n     Variables \n \n~Conv3d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},  kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]}) . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCin\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]} \n \n~Conv3d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCin\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]} \n    Examples: >>> # With square kernels and equal stride\n>>> m = nn.Conv3d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n>>> input = torch.randn(20, 16, 10, 50, 100)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.ConvTranspose1d", "path": "generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d", "type": "torch.nn", "text": " \nclass torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros') [source]\n \nApplies a 1D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation). This module supports TensorFloat32.  \nstride controls the stride for the cross-correlation. \npadding controls the amount of implicit zero padding on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. \noutput_padding controls the additional size added to one side of the output shape. See note below for details. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. \ngroups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,  At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channelsin_channels\\frac{\\text{out\\_channels}}{\\text{in\\_channels}} ).     Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv1d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic =\nTrue. Please see the notes on Reproducibility for background.   Parameters \n \nin_channels (int) \u2013 Number of channels in the input image \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of the input. Default: 0 \noutput_padding (int or tuple, optional) \u2013 Additional size added to one side of the output shape. Default: 0 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape:\n\n Input: (N,Cin,Lin)(N, C_{in}, L_{in}) \n \nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})  where  Lout=(Lin\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation} \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1  \n     Variables \n \n~ConvTranspose1d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},  kernel_size)\\text{kernel\\_size}) . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCout\u2217kernel_sizek = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}} \n \n~ConvTranspose1d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCout\u2217kernel_sizek = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}} \n    \n"}, {"name": "torch.nn.ConvTranspose2d", "path": "generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d", "type": "torch.nn", "text": " \nclass torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros') [source]\n \nApplies a 2D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation). This module supports TensorFloat32.  \nstride controls the stride for the cross-correlation. \npadding controls the amount of implicit zero padding on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. \noutput_padding controls the additional size added to one side of the output shape. See note below for details. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. \ngroups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,  At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channelsin_channels\\frac{\\text{out\\_channels}}{\\text{in\\_channels}} ).    The parameters kernel_size, stride, padding, output_padding can either be:  a single int \u2013 in which case the same value is used for the height and width dimensions a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv2d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \nin_channels (int) \u2013 Number of channels in the input image \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 \noutput_padding (int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape:\n\n Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in}) \n Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})  where   Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  \n Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  \n   Variables \n \n~ConvTranspose2d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},  kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}) . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCout\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]} \n \n~ConvTranspose2d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels) If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCout\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]} \n    Examples: >>> # With square kernels and equal stride\n>>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> input = torch.randn(20, 16, 50, 100)\n>>> output = m(input)\n>>> # exact output size can be also specified as an argument\n>>> input = torch.randn(1, 16, 12, 12)\n>>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n>>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n>>> h = downsample(input)\n>>> h.size()\ntorch.Size([1, 16, 6, 6])\n>>> output = upsample(h, output_size=input.size())\n>>> output.size()\ntorch.Size([1, 16, 12, 12])\n \n"}, {"name": "torch.nn.ConvTranspose3d", "path": "generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d", "type": "torch.nn", "text": " \nclass torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros') [source]\n \nApplies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes. This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation). This module supports TensorFloat32.  \nstride controls the stride for the cross-correlation. \npadding controls the amount of implicit zero padding on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. \noutput_padding controls the additional size added to one side of the output shape. See note below for details. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. \ngroups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,  At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size out_channelsin_channels\\frac{\\text{out\\_channels}}{\\text{in\\_channels}} ).    The parameters kernel_size, stride, padding, output_padding can either be:  a single int \u2013 in which case the same value is used for the depth, height and width dimensions a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv3d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \nin_channels (int) \u2013 Number of channels in the input image \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 \noutput_padding (int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape:\n\n Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in}) \n Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})  where   Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  \n Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  \n Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1  \n   Variables \n \n~ConvTranspose3d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},  kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]}) . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCout\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]} \n \n~ConvTranspose3d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels) If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=groupsCout\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]} \n    Examples: >>> # With square kernels and equal stride\n>>> m = nn.ConvTranspose3d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))\n>>> input = torch.randn(20, 16, 10, 50, 100)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.CosineEmbeddingLoss", "path": "generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss", "type": "torch.nn", "text": " \nclass torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that measures the loss given input tensors x1x_1 , x2x_2  and a Tensor label yy  with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for each sample is:  loss(x,y)={1\u2212cos\u2061(x1,x2),if y=1max\u2061(0,cos\u2061(x1,x2)\u2212margin),if y=\u22121\\text{loss}(x, y) = \\begin{cases} 1 - \\cos(x_1, x_2), & \\text{if } y = 1 \\\\ \\max(0, \\cos(x_1, x_2) - \\text{margin}), & \\text{if } y = -1 \\end{cases}  \n Parameters \n \nmargin (float, optional) \u2013 Should be a number from \u22121-1  to 11 , 00  to 0.50.5  is suggested. If margin is missing, the default value is 00 . \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    \n"}, {"name": "torch.nn.CosineSimilarity", "path": "generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity", "type": "torch.nn", "text": " \nclass torch.nn.CosineSimilarity(dim=1, eps=1e-08) [source]\n \nReturns cosine similarity between x1x_1  and x2x_2 , computed along dim.  similarity=x1\u22c5x2max\u2061(\u2225x1\u22252\u22c5\u2225x2\u22252,\u03f5).\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}.  \n Parameters \n \ndim (int, optional) \u2013 Dimension where cosine similarity is computed. Default: 1 \neps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8     Shape:\n\n Input1: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)  where D is at position dim\n Input2: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2) , same shape as the Input1 Output: (\u22171,\u22172)(\\ast_1, \\ast_2) \n   Examples::\n\n>>> input1 = torch.randn(100, 128)\n>>> input2 = torch.randn(100, 128)\n>>> cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n>>> output = cos(input1, input2)\n   \n"}, {"name": "torch.nn.CrossEntropyLoss", "path": "generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss", "type": "torch.nn", "text": " \nclass torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') [source]\n \nThis criterion combines LogSoftmax and NLLLoss in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input is expected to contain raw, unnormalized scores for each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)  or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  for the K-dimensional case (described later). This criterion expects a class index in the range [0,C\u22121][0, C-1]  as the target for each value of a 1D tensor of size minibatch; if ignore_index is specified, this criterion also accepts this class index (this index may not necessarily be in the class range). The loss can be described as:  loss(x,class)=\u2212log\u2061(exp\u2061(x[class])\u2211jexp\u2061(x[j]))=\u2212x[class]+log\u2061(\u2211jexp\u2061(x[j]))\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right) = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)  \nor in the case of the weight argument being specified:  loss(x,class)=weight[class](\u2212x[class]+log\u2061(\u2211jexp\u2061(x[j])))\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)  \nThe losses are averaged across observations for each minibatch. If the weight argument is specified then this is a weighted average:  loss=\u2211i=1Nloss(i,class[i])\u2211i=1Nweight[class[i]]\\text{loss} = \\frac{\\sum^{N}_{i=1} loss(i, class[i])}{\\sum^{N}_{i=1} weight[class[i]]}  \nCan also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1 , where KK  is the number of dimensions, and a target of appropriate shape (see below).  Parameters \n \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the weighted mean of the output is taken, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,C)(N, C)  where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  in the case of K-dimensional loss. Target: (N)(N)  where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  in the case of K-dimensional loss.    Examples: >>> loss = nn.CrossEntropyLoss()\n>>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.empty(3, dtype=torch.long).random_(5)\n>>> output = loss(input, target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.CTCLoss", "path": "generated/torch.nn.ctcloss#torch.nn.CTCLoss", "type": "torch.nn", "text": " \nclass torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False) [source]\n \nThe Connectionist Temporal Classification loss. Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d, which limits the length of the target sequence such that it must be \u2264\\leq  the input length.  Parameters \n \nblank (int, optional) \u2013 blank label. Default 00 . \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: 'mean'\n \nzero_infinity (bool, optional) \u2013 Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.     Shape:\n\n Log_probs: Tensor of size (T,N,C)(T, N, C) , where T=input lengthT = \\text{input length} , N=batch sizeN = \\text{batch size} , and C=number of classes (including blank)C = \\text{number of classes (including blank)} . The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). Targets: Tensor of size (N,S)(N, S)  or (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths})) , where N=batch sizeN = \\text{batch size}  and S=max target length, if shape is (N,S)S = \\text{max target length, if shape is } (N, S) . It represent the target sequences. Each element in the target sequence is a class index. And the target index cannot be blank (default=0). In the (N,S)(N, S)  form, targets are padded to the length of the longest sequence, and stacked. In the (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))  form, the targets are assumed to be un-padded and concatenated within 1 dimension. Input_lengths: Tuple or tensor of size (N)(N) , where N=batch sizeN = \\text{batch size} . It represent the lengths of the inputs (must each be \u2264T\\leq T ). And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. Target_lengths: Tuple or tensor of size (N)(N) , where N=batch sizeN = \\text{batch size} . It represent lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. If target shape is (N,S)(N,S) , target_lengths are effectively the stop index sns_n  for each target sequence, such that target_n = targets[n,0:s_n] for each target in a batch. Lengths must each be \u2264S\\leq S  If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor. Output: scalar. If reduction is 'none', then (N)(N) , where N=batch sizeN = \\text{batch size} .    Examples: >>> # Target are to be padded\n>>> T = 50      # Input sequence length\n>>> C = 20      # Number of classes (including blank)\n>>> N = 16      # Batch size\n>>> S = 30      # Target sequence length of longest target in batch (padding length)\n>>> S_min = 10  # Minimum target length, for demonstration purposes\n>>>\n>>> # Initialize random batch of input vectors, for *size = (T,N,C)\n>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n>>>\n>>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n>>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n>>>\n>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n>>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n>>> ctc_loss = nn.CTCLoss()\n>>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n>>> loss.backward()\n>>>\n>>>\n>>> # Target are to be un-padded\n>>> T = 50      # Input sequence length\n>>> C = 20      # Number of classes (including blank)\n>>> N = 16      # Batch size\n>>>\n>>> # Initialize random batch of input vectors, for *size = (T,N,C)\n>>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n>>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n>>>\n>>> # Initialize random batch of targets (0 = blank, 1:C = classes)\n>>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n>>> target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n>>> ctc_loss = nn.CTCLoss()\n>>> loss = ctc_loss(input, target, input_lengths, target_lengths)\n>>> loss.backward()\n  Reference:\n\nA. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf    Note In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T. blank=0blank=0 , target_lengths \u2264256\\leq 256 , the integer arguments must be of dtype torch.int32. The regular implementation uses the (more common in PyTorch) torch.long dtype.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic =\nTrue. Please see the notes on Reproducibility for background.  \n"}, {"name": "torch.nn.DataParallel", "path": "generated/torch.nn.dataparallel#torch.nn.DataParallel", "type": "torch.nn", "text": " \nclass torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) [source]\n \nImplements data parallelism at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module. The batch size should be larger than the number of GPUs used.  Warning It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node. See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel.  Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be scattered on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model\u2019s forward pass. The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module.  Warning In each forward, module is replicated on each device, so any updates to the running module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value because the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers.   Warning Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device.   Warning When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.   Note There is a subtlety in using the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn\u2019t work with data parallelism section in FAQ for details.   Parameters \n \nmodule (Module) \u2013 module to be parallelized \ndevice_ids (list of python:int or torch.device) \u2013 CUDA devices (default: all devices) \noutput_device (int or torch.device) \u2013 device location of output (default: device_ids[0])   Variables \n~DataParallel.module (Module) \u2013 the module to be parallelized   Example: >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n>>> output = net(input_var)  # input_var can be on any device, including CPU\n \n"}, {"name": "torch.nn.Dropout", "path": "generated/torch.nn.dropout#torch.nn.Dropout", "type": "torch.nn", "text": " \nclass torch.nn.Dropout(p=0.5, inplace=False) [source]\n \nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors . Furthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p}  during training. This means that during evaluation the module simply computes an identity function.  Parameters \n \np \u2013 probability of an element to be zeroed. Default: 0.5 \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n     Shape:\n\n Input: (\u2217)(*) . Input can be of any shape Output: (\u2217)(*) . Output is of the same shape as input    Examples: >>> m = nn.Dropout(p=0.2)\n>>> input = torch.randn(20, 16)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Dropout2d", "path": "generated/torch.nn.dropout2d#torch.nn.Dropout2d", "type": "torch.nn", "text": " \nclass torch.nn.Dropout2d(p=0.5, inplace=False) [source]\n \nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv2d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead.  Parameters \n \np (float, optional) \u2013 probability of an element to be zero-ed. \ninplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape:\n\n Input: (N,C,H,W)(N, C, H, W) \n Output: (N,C,H,W)(N, C, H, W)  (same shape as input)    Examples: >>> m = nn.Dropout2d(p=0.2)\n>>> input = torch.randn(20, 16, 32, 32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Dropout3d", "path": "generated/torch.nn.dropout3d#torch.nn.Dropout3d", "type": "torch.nn", "text": " \nclass torch.nn.Dropout3d(p=0.5, inplace=False) [source]\n \nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv3d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead.  Parameters \n \np (float, optional) \u2013 probability of an element to be zeroed. \ninplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape:\n\n Input: (N,C,D,H,W)(N, C, D, H, W) \n Output: (N,C,D,H,W)(N, C, D, H, W)  (same shape as input)    Examples: >>> m = nn.Dropout3d(p=0.2)\n>>> input = torch.randn(20, 16, 4, 32, 32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.ELU", "path": "generated/torch.nn.elu#torch.nn.ELU", "type": "torch.nn", "text": " \nclass torch.nn.ELU(alpha=1.0, inplace=False) [source]\n \nApplies the element-wise function:  ELU(x)={x, if x>0\u03b1\u2217(exp\u2061(x)\u22121), if x\u22640\\text{ELU}(x) = \\begin{cases} x, & \\text{ if } x > 0\\\\ \\alpha * (\\exp(x) - 1), & \\text{ if } x \\leq 0 \\end{cases}  \n Parameters \n \nalpha \u2013 the \u03b1\\alpha  value for the ELU formulation. Default: 1.0 \ninplace \u2013 can optionally do the operation in-place. Default: False\n     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.ELU()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Embedding", "path": "generated/torch.nn.embedding#torch.nn.Embedding", "type": "torch.nn", "text": " \nclass torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None) [source]\n \nA simple lookup table that stores embeddings of a fixed dictionary and size. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.  Parameters \n \nnum_embeddings (int) \u2013 size of the dictionary of embeddings \nembedding_dim (int) \u2013 the size of each embedding vector \npadding_idx (int, optional) \u2013 If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. \nmax_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. \nnorm_type (float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. \nsparse (bool, optional) \u2013 If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.   Variables \n~Embedding.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)     Shape:\n\n Input: (\u2217)(*) , IntTensor or LongTensor of arbitrary shape containing the indices to extract Output: (\u2217,H)(*, H) , where * is the input shape and H=embedding_dimH=\\text{embedding\\_dim} \n     Note Keep in mind that only a limited number of optimizers support sparse gradients: currently it\u2019s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)   Note With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero.   Note When max_norm is not None, Embedding\u2019s forward method will modify the weight tensor in-place. Since tensors needed for gradient computations cannot be modified in-place, performing a differentiable operation on Embedding.weight before calling Embedding\u2019s forward method requires cloning Embedding.weight when max_norm is not None. For example: n, d, m = 3, 5, 7\nembedding = nn.Embedding(n, d, max_norm=True)\nW = torch.randn((m, d), requires_grad=True)\nidx = torch.tensor([1, 2])\na = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\nb = embedding(idx) @ W.t()  # modifies weight in-place\nout = (a.unsqueeze(0) + b.unsqueeze(1))\nloss = out.sigmoid().prod()\nloss.backward()\n  Examples: >>> # an Embedding module containing 10 tensors of size 3\n>>> embedding = nn.Embedding(10, 3)\n>>> # a batch of 2 samples of 4 indices each\n>>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n>>> embedding(input)\ntensor([[[-0.0251, -1.6902,  0.7172],\n         [-0.6431,  0.0748,  0.6969],\n         [ 1.4970,  1.3448, -0.9685],\n         [-0.3677, -2.7265, -0.1685]],\n\n        [[ 1.4970,  1.3448, -0.9685],\n         [ 0.4362, -0.4004,  0.9400],\n         [-0.6431,  0.0748,  0.6969],\n         [ 0.9124, -2.3616,  1.1151]]])\n\n\n>>> # example with padding_idx\n>>> embedding = nn.Embedding(10, 3, padding_idx=0)\n>>> input = torch.LongTensor([[0,2,0,5]])\n>>> embedding(input)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.1535, -2.0309,  0.9315],\n         [ 0.0000,  0.0000,  0.0000],\n         [-0.1655,  0.9897,  0.0635]]])\n  \nclassmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False) [source]\n \nCreates Embedding instance from given 2-dimensional FloatTensor.  Parameters \n \nembeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as num_embeddings, second as embedding_dim. \nfreeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True\n \npadding_idx (int, optional) \u2013 See module initialization documentation. \nmax_norm (float, optional) \u2013 See module initialization documentation. \nnorm_type (float, optional) \u2013 See module initialization documentation. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. \nsparse (bool, optional) \u2013 See module initialization documentation.    Examples: >>> # FloatTensor containing pretrained weights\n>>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n>>> embedding = nn.Embedding.from_pretrained(weight)\n>>> # Get embeddings for index 1\n>>> input = torch.LongTensor([1])\n>>> embedding(input)\ntensor([[ 4.0000,  5.1000,  6.3000]])\n \n \n"}, {"name": "torch.nn.Embedding.from_pretrained()", "path": "generated/torch.nn.embedding#torch.nn.Embedding.from_pretrained", "type": "torch.nn", "text": " \nclassmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False) [source]\n \nCreates Embedding instance from given 2-dimensional FloatTensor.  Parameters \n \nembeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as num_embeddings, second as embedding_dim. \nfreeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True\n \npadding_idx (int, optional) \u2013 See module initialization documentation. \nmax_norm (float, optional) \u2013 See module initialization documentation. \nnorm_type (float, optional) \u2013 See module initialization documentation. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. \nsparse (bool, optional) \u2013 See module initialization documentation.    Examples: >>> # FloatTensor containing pretrained weights\n>>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n>>> embedding = nn.Embedding.from_pretrained(weight)\n>>> # Get embeddings for index 1\n>>> input = torch.LongTensor([1])\n>>> embedding(input)\ntensor([[ 4.0000,  5.1000,  6.3000]])\n \n"}, {"name": "torch.nn.EmbeddingBag", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag", "type": "torch.nn", "text": " \nclass torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False, _weight=None, include_last_offset=False) [source]\n \nComputes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings. For bags of constant length and no per_sample_weights and 2D inputs, this class  with mode=\"sum\" is equivalent to Embedding followed by torch.sum(dim=1), with mode=\"mean\" is equivalent to Embedding followed by torch.mean(dim=1), with mode=\"max\" is equivalent to Embedding followed by torch.max(dim=1).  However, EmbeddingBag is much more time and memory efficient than using a chain of these operations. EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by mode. If per_sample_weights` is passed, the only supported mode is \"sum\", which computes a weighted sum according to per_sample_weights.  Parameters \n \nnum_embeddings (int) \u2013 size of the dictionary of embeddings \nembedding_dim (int) \u2013 the size of each embedding vector \nmax_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. \nnorm_type (float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". \nmode (string, optional) \u2013 \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. \"sum\" computes the weighted sum, taking per_sample_weights into consideration. \"mean\" computes the average of the values in the bag, \"max\" computes the max value over each bag. Default: \"mean\"\n \nsparse (bool, optional) \u2013 if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". \ninclude_last_offset (bool, optional) \u2013 if True, offsets has one additional element, where the last element is equivalent to the size of indices. This matches the CSR format.   Variables \n~EmbeddingBag.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1) .    \nInputs: input (IntTensor or LongTensor), offsets (IntTensor or LongTensor, optional), and \n\nper_index_weights (Tensor, optional)  \ninput and offsets have to be of the same type, either int or long \nIf input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  \nIf input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    per_sample_weights (Tensor, optional): a tensor of float / double weights, or None\n\nto indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None. Only supported for mode='sum'.     Output shape: (B, embedding_dim) Examples: >>> # an Embedding module containing 10 tensors of size 3\n>>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\n>>> # a batch of 2 samples of 4 indices each\n>>> input = torch.LongTensor([1,2,4,5,4,3,2,9])\n>>> offsets = torch.LongTensor([0,4])\n>>> embedding_sum(input, offsets)\ntensor([[-0.8861, -5.4350, -0.0523],\n        [ 1.1306, -2.5798, -1.0044]])\n  \nclassmethod from_pretrained(embeddings, freeze=True, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False, include_last_offset=False) [source]\n \nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters \n \nembeddings (Tensor) \u2013 FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019. \nfreeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embeddingbag.weight.requires_grad = False. Default: True\n \nmax_norm (float, optional) \u2013 See module initialization documentation. Default: None\n \nnorm_type (float, optional) \u2013 See module initialization documentation. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. \nmode (string, optional) \u2013 See module initialization documentation. Default: \"mean\"\n \nsparse (bool, optional) \u2013 See module initialization documentation. Default: False. \ninclude_last_offset (bool, optional) \u2013 See module initialization documentation. Default: False.    Examples: >>> # FloatTensor containing pretrained weights\n>>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n>>> embeddingbag = nn.EmbeddingBag.from_pretrained(weight)\n>>> # Get embeddings for index 1\n>>> input = torch.LongTensor([[1, 0]])\n>>> embeddingbag(input)\ntensor([[ 2.5000,  3.7000,  4.6500]])\n \n \n"}, {"name": "torch.nn.EmbeddingBag.from_pretrained()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.from_pretrained", "type": "torch.nn", "text": " \nclassmethod from_pretrained(embeddings, freeze=True, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False, include_last_offset=False) [source]\n \nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters \n \nembeddings (Tensor) \u2013 FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019. \nfreeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embeddingbag.weight.requires_grad = False. Default: True\n \nmax_norm (float, optional) \u2013 See module initialization documentation. Default: None\n \nnorm_type (float, optional) \u2013 See module initialization documentation. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. \nmode (string, optional) \u2013 See module initialization documentation. Default: \"mean\"\n \nsparse (bool, optional) \u2013 See module initialization documentation. Default: False. \ninclude_last_offset (bool, optional) \u2013 See module initialization documentation. Default: False.    Examples: >>> # FloatTensor containing pretrained weights\n>>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n>>> embeddingbag = nn.EmbeddingBag.from_pretrained(weight)\n>>> # Get embeddings for index 1\n>>> input = torch.LongTensor([[1, 0]])\n>>> embeddingbag(input)\ntensor([[ 2.5000,  3.7000,  4.6500]])\n \n"}, {"name": "torch.nn.Flatten", "path": "generated/torch.nn.flatten#torch.nn.Flatten", "type": "torch.nn", "text": " \nclass torch.nn.Flatten(start_dim=1, end_dim=-1) [source]\n \nFlattens a contiguous range of dims into a tensor. For use with Sequential.  Shape:\n\n Input: (N,\u2217dims)(N, *dims) \n Output: (N,\u220f\u2217dims)(N, \\prod *dims)  (for the default case).     Parameters \n \nstart_dim \u2013 first dim to flatten (default = 1). \nend_dim \u2013 last dim to flatten (default = -1).     Examples::\n\n>>> input = torch.randn(32, 1, 5, 5)\n>>> m = nn.Sequential(\n>>>     nn.Conv2d(1, 32, 5, 1, 1),\n>>>     nn.Flatten()\n>>> )\n>>> output = m(input)\n>>> output.size()\ntorch.Size([32, 288])\n    \nadd_module(name, module)  \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n  \napply(fn)  \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n  \nbfloat16()  \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n  \nbuffers(recurse=True)  \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nchildren()  \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n  \ncpu()  \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n  \ncuda(device=None)  \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \ndouble()  \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n  \neval()  \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n  \nfloat()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n  \nhalf()  \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n  \nload_state_dict(state_dict, strict=True)  \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n  \nmodules()  \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n  \nnamed_buffers(prefix='', recurse=True)  \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n  \nnamed_children()  \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n  \nnamed_modules(memo=None, prefix='')  \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n  \nnamed_parameters(prefix='', recurse=True)  \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n  \nparameters(recurse=True)  \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nregister_backward_hook(hook)  \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_buffer(name, tensor, persistent=True)  \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n  \nregister_forward_hook(hook)  \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_forward_pre_hook(hook)  \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_full_backward_hook(hook)  \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_parameter(name, param)  \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n  \nrequires_grad_(requires_grad=True)  \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n  \nstate_dict(destination=None, prefix='', keep_vars=False)  \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n  \nto(*args, **kwargs)  \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) \n  \nto(dtype, non_blocking=False) \n  \nto(tensor, non_blocking=False) \n  \nto(memory_format=torch.channels_last) \n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n  \ntrain(mode=True)  \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n  \ntype(dst_type)  \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n  \nxpu(device=None)  \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \nzero_grad(set_to_none=False)  \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n \n"}, {"name": "torch.nn.Flatten.add_module()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.add_module", "type": "torch.nn", "text": " \nadd_module(name, module)  \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n"}, {"name": "torch.nn.Flatten.apply()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.apply", "type": "torch.nn", "text": " \napply(fn)  \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n"}, {"name": "torch.nn.Flatten.bfloat16()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.bfloat16", "type": "torch.nn", "text": " \nbfloat16()  \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.buffers", "type": "torch.nn", "text": " \nbuffers(recurse=True)  \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.nn.Flatten.children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.children", "type": "torch.nn", "text": " \nchildren()  \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n"}, {"name": "torch.nn.Flatten.cpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cpu", "type": "torch.nn", "text": " \ncpu()  \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.cuda()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cuda", "type": "torch.nn", "text": " \ncuda(device=None)  \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.double()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.double", "type": "torch.nn", "text": " \ndouble()  \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.eval()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.eval", "type": "torch.nn", "text": " \neval()  \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.float()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.float", "type": "torch.nn", "text": " \nfloat()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.half()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.half", "type": "torch.nn", "text": " \nhalf()  \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.load_state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.load_state_dict", "type": "torch.nn", "text": " \nload_state_dict(state_dict, strict=True)  \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n"}, {"name": "torch.nn.Flatten.modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.modules", "type": "torch.nn", "text": " \nmodules()  \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n"}, {"name": "torch.nn.Flatten.named_buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_buffers", "type": "torch.nn", "text": " \nnamed_buffers(prefix='', recurse=True)  \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n"}, {"name": "torch.nn.Flatten.named_children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_children", "type": "torch.nn", "text": " \nnamed_children()  \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n"}, {"name": "torch.nn.Flatten.named_modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_modules", "type": "torch.nn", "text": " \nnamed_modules(memo=None, prefix='')  \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n"}, {"name": "torch.nn.Flatten.named_parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_parameters", "type": "torch.nn", "text": " \nnamed_parameters(prefix='', recurse=True)  \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n"}, {"name": "torch.nn.Flatten.parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.parameters", "type": "torch.nn", "text": " \nparameters(recurse=True)  \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.nn.Flatten.register_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_backward_hook", "type": "torch.nn", "text": " \nregister_backward_hook(hook)  \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Flatten.register_buffer()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_buffer", "type": "torch.nn", "text": " \nregister_buffer(name, tensor, persistent=True)  \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n"}, {"name": "torch.nn.Flatten.register_forward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_hook", "type": "torch.nn", "text": " \nregister_forward_hook(hook)  \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Flatten.register_forward_pre_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_pre_hook", "type": "torch.nn", "text": " \nregister_forward_pre_hook(hook)  \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Flatten.register_full_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_full_backward_hook", "type": "torch.nn", "text": " \nregister_full_backward_hook(hook)  \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Flatten.register_parameter()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_parameter", "type": "torch.nn", "text": " \nregister_parameter(name, param)  \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n"}, {"name": "torch.nn.Flatten.requires_grad_()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.requires_grad_", "type": "torch.nn", "text": " \nrequires_grad_(requires_grad=True)  \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.state_dict", "type": "torch.nn", "text": " \nstate_dict(destination=None, prefix='', keep_vars=False)  \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n"}, {"name": "torch.nn.Flatten.to()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.to", "type": "torch.nn", "text": " \nto(*args, **kwargs)  \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) \n  \nto(dtype, non_blocking=False) \n  \nto(tensor, non_blocking=False) \n  \nto(memory_format=torch.channels_last) \n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n"}, {"name": "torch.nn.Flatten.train()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.train", "type": "torch.nn", "text": " \ntrain(mode=True)  \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.type()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.type", "type": "torch.nn", "text": " \ntype(dst_type)  \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.xpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.xpu", "type": "torch.nn", "text": " \nxpu(device=None)  \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Flatten.zero_grad()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.zero_grad", "type": "torch.nn", "text": " \nzero_grad(set_to_none=False)  \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n"}, {"name": "torch.nn.Fold", "path": "generated/torch.nn.fold#torch.nn.Fold", "type": "torch.nn", "text": " \nclass torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1) [source]\n \nCombines an array of sliding local blocks into a large containing tensor. Consider a batched input tensor containing sliding local blocks, e.g., patches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L) , where NN  is batch dimension, C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})  is the number of values within a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})  spatial locations each containing a CC -channeled vector), and LL  is the total number of blocks. (This is exactly the same specification as the output shape of Unfold.) This operation combines these local blocks into the large output tensor of shape (N,C,output_size[0],output_size[1],\u2026)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)  by summing the overlapping values. Similar to Unfold, the arguments must satisfy  L=\u220fd\u230aoutput_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121stride[d]+1\u230b,L = \\prod_d \\left\\lfloor\\frac{\\text{output\\_size}[d] + 2 \\times \\text{padding}[d] % - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,  \nwhere dd  is over all spatial dimensions.  \noutput_size describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with stride > 0.  The padding, stride and dilation arguments specify how the sliding blocks are retrieved.  \nstride controls the stride for the sliding blocks. \npadding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.   Parameters \n \noutput_size (int or tuple) \u2013 the shape of the spatial dimensions of the output (i.e., output.sizes()[2:]) \nkernel_size (int or tuple) \u2013 the size of the sliding blocks \nstride (int or tuple) \u2013 the stride of the sliding blocks in the input spatial dimensions. Default: 1 \npadding (int or tuple, optional) \u2013 implicit zero padding to be added on both sides of input. Default: 0 \ndilation (int or tuple, optional) \u2013 a parameter that controls the stride of elements within the neighborhood. Default: 1     If output_size, kernel_size, dilation, padding or stride is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions. For the case of two output spatial dimensions this operation is sometimes called col2im.   Note Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other. In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters: >>> fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)\n>>> fold = nn.Fold(output_size=..., **fold_params)\n>>> unfold = nn.Unfold(**fold_params)\n Then for any (supported) input tensor the following equality holds: fold(unfold(input)) == divisor * input\n where divisor is a tensor that depends only on the shape and dtype of the input: >>> input_ones = torch.ones(input.shape, dtype=input.dtype)\n>>> divisor = fold(unfold(input_ones))\n When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).   Warning Currently, only 4-D output tensors (batched image-like tensors) are supported.   Shape:\n\n Input: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L) \n Output: (N,C,output_size[0],output_size[1],\u2026)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)  as described above    Examples: >>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))\n>>> input = torch.randn(1, 3 * 2 * 2, 12)\n>>> output = fold(input)\n>>> output.size()\ntorch.Size([1, 3, 4, 5])\n \n"}, {"name": "torch.nn.FractionalMaxPool2d", "path": "generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d", "type": "torch.nn", "text": " \nclass torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None) [source]\n \nApplies a 2D fractional max pooling over an input signal composed of several input planes. Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham The max-pooling operation is applied in kH\u00d7kWkH \\times kW  regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes.  Parameters \n \nkernel_size \u2013 the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)\n \noutput_size \u2013 the target output size of the image of the form oH x oW. Can be a tuple (oH, oW) or a single number oH for a square image oH x oH\n \noutput_ratio \u2013 If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1) \nreturn_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d(). Default: False\n    Examples >>> # pool of square window of size=3, and target output size 13x12\n>>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12))\n>>> # pool of square window and target output size being half of input image size\n>>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))\n>>> input = torch.randn(20, 16, 50, 32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.functional", "path": "nn.functional", "type": "torch.nn.functional", "text": "torch.nn.functional Convolution functions conv1d  \ntorch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor  \nApplies a 1D convolution over an input signal composed of several input planes. This operator supports TensorFloat32. See Conv1d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nweight \u2013 filters of shape (out_channels,in_channelsgroups,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None\n \nstride \u2013 the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a one-element tuple (padW,). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a one-element tuple (dW,). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1    Examples: >>> filters = torch.randn(33, 16, 3)\n>>> inputs = torch.randn(20, 16, 50)\n>>> F.conv1d(inputs, filters)\n \n conv2d  \ntorch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor  \nApplies a 2D convolution over an input image composed of several input planes. This operator supports TensorFloat32. See Conv2d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nweight \u2013 filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW) \n \nbias \u2013 optional bias tensor of shape (out_channels)(\\text{out\\_channels}) . Default: None\n \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1    Examples: >>> # With square kernels and equal stride\n>>> filters = torch.randn(8,4,3,3)\n>>> inputs = torch.randn(1,4,5,5)\n>>> F.conv2d(inputs, filters, padding=1)\n \n conv3d  \ntorch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor  \nApplies a 3D convolution over an input image composed of several input planes. This operator supports TensorFloat32. See Conv3d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW) \n \nweight \u2013 filters of shape (out_channels,in_channelsgroups,kT,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW) \n \nbias \u2013 optional bias tensor of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dT, dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1    Examples: >>> filters = torch.randn(33, 16, 3, 3, 3)\n>>> inputs = torch.randn(20, 16, 50, 10, 20)\n>>> F.conv3d(inputs, filters)\n \n conv_transpose1d  \ntorch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor  \nApplies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d. This operator supports TensorFloat32. See ConvTranspose1d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nweight \u2013 filters of shape (in_channels,out_channelsgroups,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sW,). Default: 1 \npadding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padW,). Default: 0 \noutput_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padW). Default: 0 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dW,). Default: 1    Examples: >>> inputs = torch.randn(20, 16, 50)\n>>> weights = torch.randn(16, 33, 5)\n>>> F.conv_transpose1d(inputs, weights)\n \n conv_transpose2d  \ntorch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor  \nApplies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d. This operator supports TensorFloat32. See ConvTranspose2d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nweight \u2013 filters of shape (in_channels,out_channelsgroups,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 \npadding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padH, padW). Default: 0 \noutput_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padH, out_padW). Default: 0 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1    Examples: >>> # With square kernels and equal stride\n>>> inputs = torch.randn(1, 4, 5, 5)\n>>> weights = torch.randn(4, 8, 3, 3)\n>>> F.conv_transpose2d(inputs, weights, padding=1)\n \n conv_transpose3d  \ntorch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor  \nApplies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d This operator supports TensorFloat32. See ConvTranspose3d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW) \n \nweight \u2013 filters of shape (in_channels,out_channelsgroups,kT,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1 \npadding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padT, padH, padW). Default: 0 \noutput_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padT, out_padH, out_padW). Default: 0 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dT, dH, dW). Default: 1    Examples: >>> inputs = torch.randn(20, 16, 50, 10, 20)\n>>> weights = torch.randn(16, 33, 3, 3, 3)\n>>> F.conv_transpose3d(inputs, weights)\n \n unfold  \ntorch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1) [source]\n \nExtracts sliding local blocks from a batched input tensor.  Warning Currently, only 4-D input tensors (batched image-like tensors) are supported.   Warning More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.  See torch.nn.Unfold for details \n fold  \ntorch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1) [source]\n \nCombines an array of sliding local blocks into a large containing tensor.  Warning Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.  See torch.nn.Fold for details \n Pooling functions avg_pool1d  \ntorch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u2192 Tensor  \nApplies a 1D average pooling over an input signal composed of several input planes. See AvgPool1d for details and output shape.  Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nkernel_size \u2013 the size of the window. Can be a single number or a tuple (kW,)\n \nstride \u2013 the stride of the window. Can be a single number or a tuple (sW,). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n    Examples: >>> # pool of square window of size=3, stride=2\n>>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n>>> F.avg_pool1d(input, kernel_size=3, stride=2)\ntensor([[[ 2.,  4.,  6.]]])\n \n avg_pool2d  \ntorch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) \u2192 Tensor  \nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW  regions by step size sH\u00d7sWsH \\times sW  steps. The number of output features is equal to the number of input planes. See AvgPool2d for details and output shape.  Parameters \n \ninput \u2013 input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n avg_pool3d  \ntorch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) \u2192 Tensor  \nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW  regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW  steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor . See AvgPool3d for details and output shape.  Parameters \n \ninput \u2013 input tensor (minibatch,in_channels,iT\u00d7iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kT, kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW), Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n max_pool1d  \ntorch.nn.functional.max_pool1d(*args, **kwargs)  \nApplies a 1D max pooling over an input signal composed of several input planes. See MaxPool1d for details. \n max_pool2d  \ntorch.nn.functional.max_pool2d(*args, **kwargs)  \nApplies a 2D max pooling over an input signal composed of several input planes. See MaxPool2d for details. \n max_pool3d  \ntorch.nn.functional.max_pool3d(*args, **kwargs)  \nApplies a 3D max pooling over an input signal composed of several input planes. See MaxPool3d for details. \n max_unpool1d  \ntorch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None) [source]\n \nComputes a partial inverse of MaxPool1d. See MaxUnpool1d for details. \n max_unpool2d  \ntorch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None) [source]\n \nComputes a partial inverse of MaxPool2d. See MaxUnpool2d for details. \n max_unpool3d  \ntorch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None) [source]\n \nComputes a partial inverse of MaxPool3d. See MaxUnpool3d for details. \n lp_pool1d  \ntorch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False) [source]\n \nApplies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well. See LPPool1d for details. \n lp_pool2d  \ntorch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False) [source]\n \nApplies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well. See LPPool2d for details. \n adaptive_max_pool1d  \ntorch.nn.functional.adaptive_max_pool1d(*args, **kwargs)  \nApplies a 1D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool1d for details and output shape.  Parameters \n \noutput_size \u2013 the target output size (single integer) \nreturn_indices \u2013 whether to return pooling indices. Default: False\n    \n adaptive_max_pool2d  \ntorch.nn.functional.adaptive_max_pool2d(*args, **kwargs)  \nApplies a 2D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool2d for details and output shape.  Parameters \n \noutput_size \u2013 the target output size (single integer or double-integer tuple) \nreturn_indices \u2013 whether to return pooling indices. Default: False\n    \n adaptive_max_pool3d  \ntorch.nn.functional.adaptive_max_pool3d(*args, **kwargs)  \nApplies a 3D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool3d for details and output shape.  Parameters \n \noutput_size \u2013 the target output size (single integer or triple-integer tuple) \nreturn_indices \u2013 whether to return pooling indices. Default: False\n    \n adaptive_avg_pool1d  \ntorch.nn.functional.adaptive_avg_pool1d(input, output_size) \u2192 Tensor  \nApplies a 1D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool1d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer)   \n adaptive_avg_pool2d  \ntorch.nn.functional.adaptive_avg_pool2d(input, output_size) [source]\n \nApplies a 2D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool2d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer or double-integer tuple)   \n adaptive_avg_pool3d  \ntorch.nn.functional.adaptive_avg_pool3d(input, output_size) [source]\n \nApplies a 3D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool3d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer or triple-integer tuple)   \n Non-linear activation functions threshold  \ntorch.nn.functional.threshold(input, threshold, value, inplace=False)  \nThresholds each element of the input Tensor. See Threshold for more details. \n  \ntorch.nn.functional.threshold_(input, threshold, value) \u2192 Tensor  \nIn-place version of threshold(). \n relu  \ntorch.nn.functional.relu(input, inplace=False) \u2192 Tensor [source]\n \nApplies the rectified linear unit function element-wise. See ReLU for more details. \n  \ntorch.nn.functional.relu_(input) \u2192 Tensor  \nIn-place version of relu(). \n hardtanh  \ntorch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) \u2192 Tensor [source]\n \nApplies the HardTanh function element-wise. See Hardtanh for more details. \n  \ntorch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) \u2192 Tensor  \nIn-place version of hardtanh(). \n hardswish  \ntorch.nn.functional.hardswish(input, inplace=False) [source]\n \nApplies the hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.  Hardswish(x)={0if x\u2264\u22123,xif x\u2265+3,x\u22c5(x+3)/6otherwise\\text{Hardswish}(x) = \\begin{cases} 0 & \\text{if~} x \\le -3, \\\\ x & \\text{if~} x \\ge +3, \\\\ x \\cdot (x + 3) /6 & \\text{otherwise} \\end{cases}  \nSee Hardswish for more details. \n relu6  \ntorch.nn.functional.relu6(input, inplace=False) \u2192 Tensor [source]\n \nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6) . See ReLU6 for more details. \n elu  \ntorch.nn.functional.elu(input, alpha=1.0, inplace=False) [source]\n \nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) . See ELU for more details. \n  \ntorch.nn.functional.elu_(input, alpha=1.) \u2192 Tensor  \nIn-place version of elu(). \n selu  \ntorch.nn.functional.selu(input, inplace=False) \u2192 Tensor [source]\n \nApplies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717  and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 . See SELU for more details. \n celu  \ntorch.nn.functional.celu(input, alpha=1., inplace=False) \u2192 Tensor [source]\n \nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) . See CELU for more details. \n leaky_relu  \ntorch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) \u2192 Tensor [source]\n \nApplies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  See LeakyReLU for more details. \n  \ntorch.nn.functional.leaky_relu_(input, negative_slope=0.01) \u2192 Tensor  \nIn-place version of leaky_relu(). \n prelu  \ntorch.nn.functional.prelu(input, weight) \u2192 Tensor [source]\n \nApplies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)  where weight is a learnable parameter. See PReLU for more details. \n rrelu  \ntorch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) \u2192 Tensor [source]\n \nRandomized leaky ReLU. See RReLU for more details. \n  \ntorch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) \u2192 Tensor  \nIn-place version of rrelu(). \n glu  \ntorch.nn.functional.glu(input, dim=-1) \u2192 Tensor [source]\n \nThe gated linear unit. Computes:  GLU(a,b)=a\u2297\u03c3(b)\\text{GLU}(a, b) = a \\otimes \\sigma(b)  \nwhere input is split in half along dim to form a and b, \u03c3\\sigma  is the sigmoid function and \u2297\\otimes  is the element-wise product between matrices. See Language Modeling with Gated Convolutional Networks.  Parameters \n \ninput (Tensor) \u2013 input tensor \ndim (int) \u2013 dimension on which to split the input. Default: -1    \n gelu  \ntorch.nn.functional.gelu(input) \u2192 Tensor [source]\n \nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)  where \u03a6(x)\\Phi(x)  is the Cumulative Distribution Function for Gaussian Distribution. See Gaussian Error Linear Units (GELUs). \n logsigmoid  \ntorch.nn.functional.logsigmoid(input) \u2192 Tensor  \nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)  See LogSigmoid for more details. \n hardshrink  \ntorch.nn.functional.hardshrink(input, lambd=0.5) \u2192 Tensor [source]\n \nApplies the hard shrinkage function element-wise See Hardshrink for more details. \n tanhshrink  \ntorch.nn.functional.tanhshrink(input) \u2192 Tensor [source]\n \nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)  See Tanhshrink for more details. \n softsign  \ntorch.nn.functional.softsign(input) \u2192 Tensor [source]\n \nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}  See Softsign for more details. \n softplus  \ntorch.nn.functional.softplus(input, beta=1, threshold=20) \u2192 Tensor  \nApplies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) . For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold . See Softplus for more details. \n softmin  \ntorch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None) [source]\n \nApplies a softmin function. Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See softmax definition for mathematical formula. See Softmin for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmin will be computed (so every slice along dim will sum to 1). \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    \n softmax  \ntorch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None) [source]\n \nApplies a softmax function. Softmax is defined as: Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}  It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1. See Softmax for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.     Note This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties).  \n softshrink  \ntorch.nn.functional.softshrink(input, lambd=0.5) \u2192 Tensor  \nApplies the soft shrinkage function elementwise See Softshrink for more details. \n gumbel_softmax  \ntorch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1) [source]\n \nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.  Parameters \n \nlogits \u2013 [\u2026, num_features] unnormalized log probabilities \ntau \u2013 non-negative scalar temperature \nhard \u2013 if True, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd \ndim (int) \u2013 A dimension along which softmax will be computed. Default: -1.   Returns \nSampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.    Note This function is here for legacy reasons, may be removed from nn.Functional in the future.   Note The main trick for hard is to do y_hard - y_soft.detach() + y_soft It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)   Examples::\n\n>>> logits = torch.randn(20, 32)\n>>> # Sample soft categorical using reparametrization trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=False)\n>>> # Sample hard categorical using \"Straight-through\" trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=True)\n   \n log_softmax  \ntorch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None) [source]\n \nApplies a softmax followed by a logarithm. While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly. See LogSoftmax for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which log_softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    \n tanh  \ntorch.nn.functional.tanh(input) \u2192 Tensor [source]\n \nApplies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}  See Tanh for more details. \n sigmoid  \ntorch.nn.functional.sigmoid(input) \u2192 Tensor [source]\n \nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}  See Sigmoid for more details. \n hardsigmoid  \ntorch.nn.functional.hardsigmoid(input) \u2192 Tensor [source]\n \nApplies the element-wise function  Hardsigmoid(x)={0if x\u2264\u22123,1if x\u2265+3,x/6+1/2otherwise\\text{Hardsigmoid}(x) = \\begin{cases} 0 & \\text{if~} x \\le -3, \\\\ 1 & \\text{if~} x \\ge +3, \\\\ x / 6 + 1 / 2 & \\text{otherwise} \\end{cases}  \n Parameters \ninplace \u2013 If set to True, will do this operation in-place. Default: False   See Hardsigmoid for more details. \n silu  \ntorch.nn.functional.silu(input, inplace=False) [source]\n \nApplies the silu function, element-wise.  silu(x)=x\u2217\u03c3(x),where \u03c3(x) is the logistic sigmoid.\\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}  \n Note See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.  See SiLU for more details. \n Normalization functions batch_norm  \ntorch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05) [source]\n \nApplies Batch Normalization for each channel across a batch of data. See BatchNorm1d, BatchNorm2d, BatchNorm3d for details. \n instance_norm  \ntorch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05) [source]\n \nApplies Instance Normalization for each channel in each data sample in a batch. See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details. \n layer_norm  \ntorch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05) [source]\n \nApplies Layer Normalization for last certain number of dimensions. See LayerNorm for details. \n local_response_norm  \ntorch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0) [source]\n \nApplies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels. See LocalResponseNorm for details. \n normalize  \ntorch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None) [source]\n \nPerforms LpL_p  normalization of inputs over specified dimension. For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k) , each ndimn_{dim}  -element vector vv  along dimension dim is transformed as  v=vmax\u2061(\u2225v\u2225p,\u03f5).v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.  \nWith the default arguments it uses the Euclidean norm over vectors along dimension 11  for normalization.  Parameters \n \ninput \u2013 input tensor of any shape \np (float) \u2013 the exponent value in the norm formulation. Default: 2 \ndim (int) \u2013 the dimension to reduce. Default: 1 \neps (float) \u2013 small value to avoid division by zero. Default: 1e-12 \nout (Tensor, optional) \u2013 the output tensor. If out is used, this operation won\u2019t be differentiable.    \n Linear functions linear  \ntorch.nn.functional.linear(input, weight, bias=None) [source]\n \nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b . This operator supports TensorFloat32. Shape:  Input: (N,\u2217,in_features)(N, *, in\\_features)  N is the batch size, * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features) \n Bias: (out_features)(out\\_features) \n Output: (N,\u2217,out_features)(N, *, out\\_features) \n  \n bilinear  \ntorch.nn.functional.bilinear(input1, input2, weight, bias=None) [source]\n \nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b  Shape:  input1: (N,\u2217,Hin1)(N, *, H_{in1})  where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}  and \u2217*  means any number of additional dimensions. All but the last dimension of the inputs should be the same. input2: (N,\u2217,Hin2)(N, *, H_{in2})  where Hin2=in2_featuresH_{in2}=\\text{in2\\_features} \n weight: (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features}) \n bias: (out_features)(\\text{out\\_features}) \n output: (N,\u2217,Hout)(N, *, H_{out})  where Hout=out_featuresH_{out}=\\text{out\\_features}  and all but the last dimension are the same shape as the input.  \n Dropout functions dropout  \ntorch.nn.functional.dropout(input, p=0.5, training=True, inplace=False) [source]\n \nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. See Dropout for details.  Parameters \n \np \u2013 probability of an element to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n alpha_dropout  \ntorch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False) [source]\n \nApplies alpha dropout to the input. See AlphaDropout for details. \n feature_alpha_dropout  \ntorch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False) [source]\n \nRandomly masks out entire channels (a channel is a feature map, e.g. the jj -th channel of the ii -th sample in the batch input is a tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function. Each element will be masked independently on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance. See FeatureAlphaDropout for details.  Parameters \n \np \u2013 dropout probability of a channel to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n dropout2d  \ntorch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False) [source]\n \nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. See Dropout2d for details.  Parameters \n \np \u2013 probability of a channel to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n dropout3d  \ntorch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False) [source]\n \nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. See Dropout3d for details.  Parameters \n \np \u2013 probability of a channel to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n Sparse functions embedding  \ntorch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False) [source]\n \nA simple lookup table that looks up embeddings in a fixed dictionary and size. This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings. See torch.nn.Embedding for more details.  Parameters \n \ninput (LongTensor) \u2013 Tensor containing indices into the embedding matrix \nweight (Tensor) \u2013 The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size \npadding_idx (int, optional) \u2013 If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. \nmax_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. Note: this will modify weight in-place. \nnorm_type (float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. \nsparse (bool, optional) \u2013 If True, gradient w.r.t. weight will be a sparse tensor. See Notes under torch.nn.Embedding for more details regarding sparse gradients.     Shape:\n\n Input: LongTensor of arbitrary shape containing the indices to extract \n \nWeight: Embedding matrix of floating point type with shape (V, embedding_dim), \n\nwhere V = maximum index + 1 and embedding_dim = the embedding size    Output: (*, embedding_dim), where * is the input shape    Examples: >>> # a batch of 2 samples of 4 indices each\n>>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])\n>>> # an embedding matrix containing 10 tensors of size 3\n>>> embedding_matrix = torch.rand(10, 3)\n>>> F.embedding(input, embedding_matrix)\ntensor([[[ 0.8490,  0.9625,  0.6753],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.6246,  0.9751,  0.3618],\n         [ 0.4161,  0.2419,  0.7383]],\n\n        [[ 0.6246,  0.9751,  0.3618],\n         [ 0.0237,  0.7794,  0.0528],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.3385,  0.8612,  0.1867]]])\n\n>>> # example with padding_idx\n>>> weights = torch.rand(10, 3)\n>>> weights[0, :].zero_()\n>>> embedding_matrix = weights\n>>> input = torch.tensor([[0,2,0,5]])\n>>> F.embedding(input, embedding_matrix, padding_idx=0)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.5609,  0.5384,  0.8720],\n         [ 0.0000,  0.0000,  0.0000],\n         [ 0.6262,  0.2438,  0.7471]]])\n \n embedding_bag  \ntorch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False) [source]\n \nComputes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings. See torch.nn.EmbeddingBag for more details.  Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ninput (LongTensor) \u2013 Tensor containing bags of indices into the embedding matrix \nweight (Tensor) \u2013 The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size \noffsets (LongTensor, optional) \u2013 Only used when input is 1D. offsets determines the starting index position of each bag (sequence) in input. \nmax_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. Note: this will modify weight in-place. \nnorm_type (float, optional) \u2013 The p in the p-norm to compute for the max_norm option. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". \nmode (string, optional) \u2013 \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. Default: \"mean\"\n \nsparse (bool, optional) \u2013 if True, gradient w.r.t. weight will be a sparse tensor. See Notes under torch.nn.Embedding for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". \nper_sample_weights (Tensor, optional) \u2013 a tensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None. \ninclude_last_offset (bool, optional) \u2013 if True, the size of offsets is equal to the number of bags + 1. \nlast element is the size of the input, or the ending index position of the last bag (The) \u2013     Shape:  \ninput (LongTensor) and offsets (LongTensor, optional)  \nIf input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  \nIf input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    \nweight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n \nper_sample_weights (Tensor, optional). Has the same shape as input. \noutput: aggregated embedding values of shape (B, embedding_dim)\n  Examples: >>> # an Embedding module containing 10 tensors of size 3\n>>> embedding_matrix = torch.rand(10, 3)\n>>> # a batch of 2 samples of 4 indices each\n>>> input = torch.tensor([1,2,4,5,4,3,2,9])\n>>> offsets = torch.tensor([0,4])\n>>> F.embedding_bag(embedding_matrix, input, offsets)\ntensor([[ 0.3397,  0.3552,  0.5545],\n        [ 0.5893,  0.4386,  0.5882]])\n \n one_hot  \ntorch.nn.functional.one_hot(tensor, num_classes=-1) \u2192 LongTensor  \nTakes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. See also One-hot on Wikipedia .  Parameters \n \ntensor (LongTensor) \u2013 class values of any shape. \nnum_classes (int) \u2013 Total number of classes. If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.   Returns \nLongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.   Examples >>> F.one_hot(torch.arange(0, 5) % 3)\ntensor([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0],\n        [0, 1, 0]])\n>>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\ntensor([[1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0]])\n>>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\ntensor([[[1, 0, 0],\n         [0, 1, 0]],\n        [[0, 0, 1],\n         [1, 0, 0]],\n        [[0, 1, 0],\n         [0, 0, 1]]])\n \n Distance functions pairwise_distance  \ntorch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False) [source]\n \nSee torch.nn.PairwiseDistance for details \n cosine_similarity  \ntorch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) \u2192 Tensor  \nReturns cosine similarity between x1 and x2, computed along dim.  similarity=x1\u22c5x2max\u2061(\u2225x1\u22252\u22c5\u2225x2\u22252,\u03f5)\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}  \n Parameters \n \nx1 (Tensor) \u2013 First input. \nx2 (Tensor) \u2013 Second input (of size matching x1). \ndim (int, optional) \u2013 Dimension of vectors. Default: 1 \neps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8     Shape:\n\n Input: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)  where D is at position dim. Output: (\u22171,\u22172)(\\ast_1, \\ast_2)  where 1 is at position dim.    Example: >>> input1 = torch.randn(100, 128)\n>>> input2 = torch.randn(100, 128)\n>>> output = F.cosine_similarity(input1, input2)\n>>> print(output)\n \n pdist  \ntorch.nn.functional.pdist(input, p=2) \u2192 Tensor  \nComputes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous. If input has shape N\u00d7MN \\times M  then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1) . This function is equivalent to scipy.spatial.distance.pdist(input, \u2018minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()).  Parameters \n \ninput \u2013 input tensor of shape N\u00d7MN \\times M . \np \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty] .    \n Loss functions binary_cross_entropy  \ntorch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \nFunction that measures the Binary Cross Entropy between the target and the output. See BCELoss for details.  Parameters \n \ninput \u2013 Tensor of arbitrary shape \ntarget \u2013 Tensor of the same shape as input \nweight (Tensor, optional) \u2013 a manual rescaling weight if provided it\u2019s repeated to match input tensor shape \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Examples: >>> input = torch.randn((3, 2), requires_grad=True)\n>>> target = torch.rand((3, 2), requires_grad=False)\n>>> loss = F.binary_cross_entropy(F.sigmoid(input), target)\n>>> loss.backward()\n \n binary_cross_entropy_with_logits  \ntorch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) [source]\n \nFunction that measures Binary Cross Entropy between target and output logits. See BCEWithLogitsLoss for details.  Parameters \n \ninput \u2013 Tensor of arbitrary shape \ntarget \u2013 Tensor of the same shape as input \nweight (Tensor, optional) \u2013 a manual rescaling weight if provided it\u2019s repeated to match input tensor shape \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n \npos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.    Examples: >>> input = torch.randn(3, requires_grad=True)\n>>> target = torch.empty(3).random_(2)\n>>> loss = F.binary_cross_entropy_with_logits(input, target)\n>>> loss.backward()\n \n poisson_nll_loss  \ntorch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') [source]\n \nPoisson negative log likelihood loss. See PoissonNLLLoss for details.  Parameters \n \ninput \u2013 expectation of underlying Poisson distribution. \ntarget \u2013 random sample target\u223cPoisson(input)target \\sim \\text{Poisson}(input) . \nlog_input \u2013 if True the loss is computed as exp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target} * \\text{input} , if False then loss is input\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps}) . Default: True\n \nfull \u2013 whether to compute full loss, i. e. to add the Stirling approximation term. Default: False target\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u2217\u03c0\u2217target)\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target}) . \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \neps (float, optional) \u2013 Small value to avoid evaluation of log\u2061(0)\\log(0)  when log_input`=``False`. Default: 1e-8 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    \n cosine_embedding_loss  \ntorch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee CosineEmbeddingLoss for details. \n cross_entropy  \ntorch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') [source]\n \nThis criterion combines log_softmax and nll_loss in a single function. See CrossEntropyLoss for details.  Parameters \n \ninput (Tensor) \u2013 (N,C)(N, C)  where C = number of classes or (N,C,H,W)(N, C, H, W)  in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  in the case of K-dimensional loss. \ntarget (Tensor) \u2013 (N)(N)  where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  for K-dimensional loss. \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Default: -100 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Examples: >>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.randint(5, (3,), dtype=torch.int64)\n>>> loss = F.cross_entropy(input, target)\n>>> loss.backward()\n \n ctc_loss  \ntorch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False) [source]\n \nThe Connectionist Temporal Classification loss. See CTCLoss for details.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \nlog_probs \u2013 (T,N,C)(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size. The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). \ntargets \u2013 (N,S)(N, S)  or (sum(target_lengths)). Targets cannot be blank. In the second form, the targets are assumed to be concatenated. \ninput_lengths \u2013 (N)(N) . Lengths of the inputs (must each be \u2264T\\leq T ) \ntarget_lengths \u2013 (N)(N) . Lengths of the targets \nblank (int, optional) \u2013 Blank label. Default 00 . \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken, 'sum': the output will be summed. Default: 'mean'\n \nzero_infinity (bool, optional) \u2013 Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.    Example: >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n>>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n>>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n>>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n>>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n>>> loss.backward()\n \n hinge_embedding_loss  \ntorch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee HingeEmbeddingLoss for details. \n kl_div  \ntorch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False) [source]\n \nThe Kullback-Leibler divergence Loss See KLDivLoss for details.  Parameters \n \ninput \u2013 Tensor of arbitrary shape \ntarget \u2013 Tensor of the same shape as input \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied 'batchmean': the sum of the output will be divided by the batchsize 'sum': the output will be summed 'mean': the output will be divided by the number of elements in the output Default: 'mean'\n \nlog_target (bool) \u2013 A flag indicating whether target is passed in the log space. It is recommended to pass certain distributions (like softmax) in the log space to avoid numerical issues caused by explicit log. Default: False\n     Note size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.   Note :attr:reduction = 'mean' doesn\u2019t return the true kl divergence value, please use :attr:reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as \u2018batchmean\u2019.  \n l1_loss  \ntorch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nFunction that takes the mean element-wise absolute value difference. See L1Loss for details. \n mse_loss  \ntorch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nMeasures the element-wise mean squared error. See MSELoss for details. \n margin_ranking_loss  \ntorch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee MarginRankingLoss for details. \n multilabel_margin_loss  \ntorch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee MultiLabelMarginLoss for details. \n multilabel_soft_margin_loss  \ntorch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) \u2192 Tensor [source]\n \nSee MultiLabelSoftMarginLoss for details. \n multi_margin_loss  \ntorch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \n multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,\n\nreduce=None, reduction=\u2019mean\u2019) -> Tensor   See MultiMarginLoss for details. \n nll_loss  \ntorch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') [source]\n \nThe negative log likelihood loss. See NLLLoss for details.  Parameters \n \ninput \u2013 (N,C)(N, C)  where C = number of classes or (N,C,H,W)(N, C, H, W)  in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  in the case of K-dimensional loss. \ntarget \u2013 (N)(N)  where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  for K-dimensional loss. \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Default: -100 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Example: >>> # input is of size N x C = 3 x 5\n>>> input = torch.randn(3, 5, requires_grad=True)\n>>> # each element in target has to have 0 <= value < C\n>>> target = torch.tensor([1, 0, 4])\n>>> output = F.nll_loss(F.log_softmax(input), target)\n>>> output.backward()\n \n smooth_l1_loss  \ntorch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0) [source]\n \nFunction that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. See SmoothL1Loss for details. \n soft_margin_loss  \ntorch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee SoftMarginLoss for details. \n triplet_margin_loss  \ntorch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') [source]\n \nSee TripletMarginLoss for details \n triplet_margin_with_distance_loss  \ntorch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean') [source]\n \nSee TripletMarginWithDistanceLoss for details. \n Vision functions pixel_shuffle  \ntorch.nn.functional.pixel_shuffle(input, upscale_factor) \u2192 Tensor  \nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is the upscale_factor. See PixelShuffle for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor \nupscale_factor (int) \u2013 factor to increase spatial resolution by    Examples: >>> input = torch.randn(1, 9, 4, 4)\n>>> output = torch.nn.functional.pixel_shuffle(input, 3)\n>>> print(output.size())\ntorch.Size([1, 1, 12, 12])\n \n pixel_unshuffle  \ntorch.nn.functional.pixel_unshuffle(input, downscale_factor) \u2192 Tensor  \nReverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the downscale_factor. See PixelUnshuffle for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ndownscale_factor (int) \u2013 factor to increase spatial resolution by    Examples: >>> input = torch.randn(1, 1, 12, 12)\n>>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n>>> print(output.size())\ntorch.Size([1, 9, 4, 4])\n \n pad  \ntorch.nn.functional.pad(input, pad, mode='constant', value=0)  \nPads tensor.  Padding size:\n\nThe padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor  dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right}) ; to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom}) ; to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom}  padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back}) .  Padding mode:\n\nSee torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.    Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters \n \ninput (Tensor) \u2013 N-dimensional tensor \npad (tuple) \u2013 m-elements tuple, where m2\u2264\\frac{m}{2} \\leq  input dimensions and mm  is even. \nmode \u2013 'constant', 'reflect', 'replicate' or 'circular'. Default: 'constant'\n \nvalue \u2013 fill value for 'constant' padding. Default: 0\n    Examples: >>> t4d = torch.empty(3, 3, 4, 2)\n>>> p1d = (1, 1) # pad last dim by 1 on each side\n>>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n>>> print(out.size())\ntorch.Size([3, 3, 4, 4])\n>>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n>>> out = F.pad(t4d, p2d, \"constant\", 0)\n>>> print(out.size())\ntorch.Size([3, 3, 8, 4])\n>>> t4d = torch.empty(3, 3, 4, 2)\n>>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n>>> out = F.pad(t4d, p3d, \"constant\", 0)\n>>> print(out.size())\ntorch.Size([3, 9, 7, 3])\n \n interpolate  \ntorch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None) [source]\n \nDown/up samples the input to either the given size or the given scale_factor The algorithm used for interpolation is determined by mode. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width. The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area  Parameters \n \ninput (Tensor) \u2013 the input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (str) \u2013 algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'. Default: 'nearest'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False\n \nrecompute_scale_factor (bool, optional) \u2013 recompute the scale_factor for use in the interpolation calculation. When scale_factor is passed as a parameter, it is used to compute the output_size. If recompute_scale_factor is False or not specified, the passed-in scale_factor will be used in the interpolation computation. Otherwise, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly). Note that when scale_factor is floating-point, the recomputed scale_factor may differ from the one passed in due to rounding and precision issues.     Note With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.   Warning When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.   Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  \n upsample  \ntorch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nUpsamples the input to either the given size or the given scale_factor  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).   Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  The algorithm used for upsampling is determined by mode. Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width. The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)  Parameters \n \ninput (Tensor) \u2013 the input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (string) \u2013 algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear'. Default: 'nearest'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False\n     Note With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.  \n upsample_nearest  \ntorch.nn.functional.upsample_nearest(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using nearest neighbours\u2019 pixel values.  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').  Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).  Parameters \n \ninput (Tensor) \u2013 input \nsize (int or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatia size. \nscale_factor (int) \u2013 multiplier for spatial size. Has to be an integer.     Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  \n upsample_bilinear  \ntorch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using bilinear upsampling.  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).  Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.  Parameters \n \ninput (Tensor) \u2013 input \nsize (int or Tuple[int, int]) \u2013 output spatial size. \nscale_factor (int or Tuple[int, int]) \u2013 multiplier for spatial size     Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  \n grid_sample  \ntorch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None) [source]\n \nGiven an input and a flow-field grid, computes the output using input values and pixel locations from grid. Currently, only spatial (4-D) and volumetric (5-D) input are supported. In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})  and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out}) . For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels. grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input. If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are  \npadding_mode=\"zeros\": use 0 for out-of-bound grid locations, \npadding_mode=\"border\": use border values for out-of-bound grid locations, \npadding_mode=\"reflection\": use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location x = -3.5 reflects by border -1 and becomes x' = 1.5, then reflects by border 1 and becomes x'' = -0.5.   Note This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .   Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Note NaN values in grid would be interpreted as -1.   Parameters \n \ninput (Tensor) \u2013 input of shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})  (4-D case) or (N,C,Din,Hin,Win)(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})  (5-D case) \ngrid (Tensor) \u2013 flow-field of shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2)  (4-D case) or (N,Dout,Hout,Wout,3)(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)  (5-D case) \nmode (str) \u2013 interpolation mode to calculate output values 'bilinear' | 'nearest' | 'bicubic'. Default: 'bilinear' Note: mode='bicubic' supports only 4-D input. When mode='bilinear' and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear. \npadding_mode (str) \u2013 padding mode for outside grid values 'zeros' | 'border' | 'reflection'. Default: 'zeros'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input as squares rather than points. If set to True, the extrema (-1 and 1) are considered as referring to the center points of the input\u2019s corner pixels. If set to False, they are instead considered as referring to the corner points of the input\u2019s corner pixels, making the sampling more resolution agnostic. This option parallels the align_corners option in interpolate(), and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: False\n   Returns \noutput Tensor  Return type \noutput (Tensor)    Warning When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().   Note mode='bicubic' is implemented using the cubic convolution algorithm with \u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha  might be different from packages to packages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This algorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func: torch.clamp to ensure they are within the valid range.  \n affine_grid  \ntorch.nn.functional.affine_grid(theta, size, align_corners=None) [source]\n \nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.  Note This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .   Parameters \n \ntheta (Tensor) \u2013 input batch of affine matrices with shape (N\u00d72\u00d73N \\times 2 \\times 3 ) for 2D or (N\u00d73\u00d74N \\times 3 \\times 4 ) for 3D \nsize (torch.Size) \u2013 the target output image size. (N\u00d7C\u00d7H\u00d7WN \\times C \\times H \\times W  for 2D or N\u00d7C\u00d7D\u00d7H\u00d7WN \\times C \\times D \\times H \\times W  for 3D) Example: torch.Size((32, 3, 24, 24)) \nalign_corners (bool, optional) \u2013 if True, consider -1 and 1 to refer to the centers of the corner pixels rather than the image corners. Refer to grid_sample() for a more complete description. A grid generated by affine_grid() should be passed to grid_sample() with the same setting for this option. Default: False\n   Returns \noutput Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )  Return type \noutput (Tensor)    Warning When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().   Warning When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are considered to be at `0 (the center of the input image).  \n DataParallel functions (multi-GPU, distributed) data_parallel  \ntorch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None) [source]\n \nEvaluates module(input) in parallel across the GPUs given in device_ids. This is the functional version of the DataParallel module.  Parameters \n \nmodule (Module) \u2013 the module to evaluate in parallel \ninputs (Tensor) \u2013 inputs to the module \ndevice_ids (list of python:int or torch.device) \u2013 GPU ids on which to replicate module \noutput_device (list of python:int or torch.device) \u2013 GPU location of the output Use -1 to indicate the CPU. (default: device_ids[0])   Returns \na Tensor containing the result of module(input) located on output_device   \n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.adaptive_avg_pool1d(input, output_size) \u2192 Tensor  \nApplies a 1D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool1d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer)   \n"}, {"name": "torch.nn.functional.adaptive_avg_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.adaptive_avg_pool2d(input, output_size) [source]\n \nApplies a 2D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool2d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer or double-integer tuple)   \n"}, {"name": "torch.nn.functional.adaptive_avg_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.adaptive_avg_pool3d(input, output_size) [source]\n \nApplies a 3D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool3d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer or triple-integer tuple)   \n"}, {"name": "torch.nn.functional.adaptive_max_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.adaptive_max_pool1d(*args, **kwargs)  \nApplies a 1D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool1d for details and output shape.  Parameters \n \noutput_size \u2013 the target output size (single integer) \nreturn_indices \u2013 whether to return pooling indices. Default: False\n    \n"}, {"name": "torch.nn.functional.adaptive_max_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.adaptive_max_pool2d(*args, **kwargs)  \nApplies a 2D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool2d for details and output shape.  Parameters \n \noutput_size \u2013 the target output size (single integer or double-integer tuple) \nreturn_indices \u2013 whether to return pooling indices. Default: False\n    \n"}, {"name": "torch.nn.functional.adaptive_max_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.adaptive_max_pool3d(*args, **kwargs)  \nApplies a 3D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool3d for details and output shape.  Parameters \n \noutput_size \u2013 the target output size (single integer or triple-integer tuple) \nreturn_indices \u2013 whether to return pooling indices. Default: False\n    \n"}, {"name": "torch.nn.functional.affine_grid()", "path": "nn.functional#torch.nn.functional.affine_grid", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.affine_grid(theta, size, align_corners=None) [source]\n \nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.  Note This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .   Parameters \n \ntheta (Tensor) \u2013 input batch of affine matrices with shape (N\u00d72\u00d73N \\times 2 \\times 3 ) for 2D or (N\u00d73\u00d74N \\times 3 \\times 4 ) for 3D \nsize (torch.Size) \u2013 the target output image size. (N\u00d7C\u00d7H\u00d7WN \\times C \\times H \\times W  for 2D or N\u00d7C\u00d7D\u00d7H\u00d7WN \\times C \\times D \\times H \\times W  for 3D) Example: torch.Size((32, 3, 24, 24)) \nalign_corners (bool, optional) \u2013 if True, consider -1 and 1 to refer to the centers of the corner pixels rather than the image corners. Refer to grid_sample() for a more complete description. A grid generated by affine_grid() should be passed to grid_sample() with the same setting for this option. Default: False\n   Returns \noutput Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )  Return type \noutput (Tensor)    Warning When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().   Warning When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are considered to be at `0 (the center of the input image).  \n"}, {"name": "torch.nn.functional.alpha_dropout()", "path": "nn.functional#torch.nn.functional.alpha_dropout", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False) [source]\n \nApplies alpha dropout to the input. See AlphaDropout for details. \n"}, {"name": "torch.nn.functional.avg_pool1d()", "path": "nn.functional#torch.nn.functional.avg_pool1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) \u2192 Tensor  \nApplies a 1D average pooling over an input signal composed of several input planes. See AvgPool1d for details and output shape.  Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nkernel_size \u2013 the size of the window. Can be a single number or a tuple (kW,)\n \nstride \u2013 the stride of the window. Can be a single number or a tuple (sW,). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n    Examples: >>> # pool of square window of size=3, stride=2\n>>> input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n>>> F.avg_pool1d(input, kernel_size=3, stride=2)\ntensor([[[ 2.,  4.,  6.]]])\n \n"}, {"name": "torch.nn.functional.avg_pool2d()", "path": "nn.functional#torch.nn.functional.avg_pool2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) \u2192 Tensor  \nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW  regions by step size sH\u00d7sWsH \\times sW  steps. The number of output features is equal to the number of input planes. See AvgPool2d for details and output shape.  Parameters \n \ninput \u2013 input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n"}, {"name": "torch.nn.functional.avg_pool3d()", "path": "nn.functional#torch.nn.functional.avg_pool3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) \u2192 Tensor  \nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW  regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW  steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor . See AvgPool3d for details and output shape.  Parameters \n \ninput \u2013 input tensor (minibatch,in_channels,iT\u00d7iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kT, kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW), Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n"}, {"name": "torch.nn.functional.batch_norm()", "path": "nn.functional#torch.nn.functional.batch_norm", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05) [source]\n \nApplies Batch Normalization for each channel across a batch of data. See BatchNorm1d, BatchNorm2d, BatchNorm3d for details. \n"}, {"name": "torch.nn.functional.bilinear()", "path": "nn.functional#torch.nn.functional.bilinear", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.bilinear(input1, input2, weight, bias=None) [source]\n \nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b  Shape:  input1: (N,\u2217,Hin1)(N, *, H_{in1})  where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}  and \u2217*  means any number of additional dimensions. All but the last dimension of the inputs should be the same. input2: (N,\u2217,Hin2)(N, *, H_{in2})  where Hin2=in2_featuresH_{in2}=\\text{in2\\_features} \n weight: (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features}) \n bias: (out_features)(\\text{out\\_features}) \n output: (N,\u2217,Hout)(N, *, H_{out})  where Hout=out_featuresH_{out}=\\text{out\\_features}  and all but the last dimension are the same shape as the input.  \n"}, {"name": "torch.nn.functional.binary_cross_entropy()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \nFunction that measures the Binary Cross Entropy between the target and the output. See BCELoss for details.  Parameters \n \ninput \u2013 Tensor of arbitrary shape \ntarget \u2013 Tensor of the same shape as input \nweight (Tensor, optional) \u2013 a manual rescaling weight if provided it\u2019s repeated to match input tensor shape \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Examples: >>> input = torch.randn((3, 2), requires_grad=True)\n>>> target = torch.rand((3, 2), requires_grad=False)\n>>> loss = F.binary_cross_entropy(F.sigmoid(input), target)\n>>> loss.backward()\n \n"}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy_with_logits", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) [source]\n \nFunction that measures Binary Cross Entropy between target and output logits. See BCEWithLogitsLoss for details.  Parameters \n \ninput \u2013 Tensor of arbitrary shape \ntarget \u2013 Tensor of the same shape as input \nweight (Tensor, optional) \u2013 a manual rescaling weight if provided it\u2019s repeated to match input tensor shape \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n \npos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.    Examples: >>> input = torch.randn(3, requires_grad=True)\n>>> target = torch.empty(3).random_(2)\n>>> loss = F.binary_cross_entropy_with_logits(input, target)\n>>> loss.backward()\n \n"}, {"name": "torch.nn.functional.celu()", "path": "nn.functional#torch.nn.functional.celu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.celu(input, alpha=1., inplace=False) \u2192 Tensor [source]\n \nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) . See CELU for more details. \n"}, {"name": "torch.nn.functional.conv1d()", "path": "nn.functional#torch.nn.functional.conv1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor  \nApplies a 1D convolution over an input signal composed of several input planes. This operator supports TensorFloat32. See Conv1d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nweight \u2013 filters of shape (out_channels,in_channelsgroups,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None\n \nstride \u2013 the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a one-element tuple (padW,). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a one-element tuple (dW,). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1    Examples: >>> filters = torch.randn(33, 16, 3)\n>>> inputs = torch.randn(20, 16, 50)\n>>> F.conv1d(inputs, filters)\n \n"}, {"name": "torch.nn.functional.conv2d()", "path": "nn.functional#torch.nn.functional.conv2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor  \nApplies a 2D convolution over an input image composed of several input planes. This operator supports TensorFloat32. See Conv2d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nweight \u2013 filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW) \n \nbias \u2013 optional bias tensor of shape (out_channels)(\\text{out\\_channels}) . Default: None\n \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1    Examples: >>> # With square kernels and equal stride\n>>> filters = torch.randn(8,4,3,3)\n>>> inputs = torch.randn(1,4,5,5)\n>>> F.conv2d(inputs, filters, padding=1)\n \n"}, {"name": "torch.nn.functional.conv3d()", "path": "nn.functional#torch.nn.functional.conv3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) \u2192 Tensor  \nApplies a 3D convolution over an input image composed of several input planes. This operator supports TensorFloat32. See Conv3d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW) \n \nweight \u2013 filters of shape (out_channels,in_channelsgroups,kT,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW) \n \nbias \u2013 optional bias tensor of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dT, dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1    Examples: >>> filters = torch.randn(33, 16, 3, 3, 3)\n>>> inputs = torch.randn(20, 16, 50, 10, 20)\n>>> F.conv3d(inputs, filters)\n \n"}, {"name": "torch.nn.functional.conv_transpose1d()", "path": "nn.functional#torch.nn.functional.conv_transpose1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor  \nApplies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d. This operator supports TensorFloat32. See ConvTranspose1d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nweight \u2013 filters of shape (in_channels,out_channelsgroups,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sW,). Default: 1 \npadding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padW,). Default: 0 \noutput_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padW). Default: 0 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dW,). Default: 1    Examples: >>> inputs = torch.randn(20, 16, 50)\n>>> weights = torch.randn(16, 33, 5)\n>>> F.conv_transpose1d(inputs, weights)\n \n"}, {"name": "torch.nn.functional.conv_transpose2d()", "path": "nn.functional#torch.nn.functional.conv_transpose2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor  \nApplies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d. This operator supports TensorFloat32. See ConvTranspose2d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nweight \u2013 filters of shape (in_channels,out_channelsgroups,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 \npadding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padH, padW). Default: 0 \noutput_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padH, out_padW). Default: 0 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1    Examples: >>> # With square kernels and equal stride\n>>> inputs = torch.randn(1, 4, 5, 5)\n>>> weights = torch.randn(4, 8, 3, 3)\n>>> F.conv_transpose2d(inputs, weights, padding=1)\n \n"}, {"name": "torch.nn.functional.conv_transpose3d()", "path": "nn.functional#torch.nn.functional.conv_transpose3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) \u2192 Tensor  \nApplies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d This operator supports TensorFloat32. See ConvTranspose3d for details and output shape.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Parameters \n \ninput \u2013 input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW) \n \nweight \u2013 filters of shape (in_channels,out_channelsgroups,kT,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW) \n \nbias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels}) . Default: None \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1 \npadding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padT, padH, padW). Default: 0 \noutput_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padT, out_padH, out_padW). Default: 0 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dT, dH, dW). Default: 1    Examples: >>> inputs = torch.randn(20, 16, 50, 10, 20)\n>>> weights = torch.randn(16, 33, 3, 3, 3)\n>>> F.conv_transpose3d(inputs, weights)\n \n"}, {"name": "torch.nn.functional.cosine_embedding_loss()", "path": "nn.functional#torch.nn.functional.cosine_embedding_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee CosineEmbeddingLoss for details. \n"}, {"name": "torch.nn.functional.cosine_similarity()", "path": "nn.functional#torch.nn.functional.cosine_similarity", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) \u2192 Tensor  \nReturns cosine similarity between x1 and x2, computed along dim.  similarity=x1\u22c5x2max\u2061(\u2225x1\u22252\u22c5\u2225x2\u22252,\u03f5)\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}  \n Parameters \n \nx1 (Tensor) \u2013 First input. \nx2 (Tensor) \u2013 Second input (of size matching x1). \ndim (int, optional) \u2013 Dimension of vectors. Default: 1 \neps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8     Shape:\n\n Input: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)  where D is at position dim. Output: (\u22171,\u22172)(\\ast_1, \\ast_2)  where 1 is at position dim.    Example: >>> input1 = torch.randn(100, 128)\n>>> input2 = torch.randn(100, 128)\n>>> output = F.cosine_similarity(input1, input2)\n>>> print(output)\n \n"}, {"name": "torch.nn.functional.cross_entropy()", "path": "nn.functional#torch.nn.functional.cross_entropy", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') [source]\n \nThis criterion combines log_softmax and nll_loss in a single function. See CrossEntropyLoss for details.  Parameters \n \ninput (Tensor) \u2013 (N,C)(N, C)  where C = number of classes or (N,C,H,W)(N, C, H, W)  in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  in the case of K-dimensional loss. \ntarget (Tensor) \u2013 (N)(N)  where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  for K-dimensional loss. \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Default: -100 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Examples: >>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.randint(5, (3,), dtype=torch.int64)\n>>> loss = F.cross_entropy(input, target)\n>>> loss.backward()\n \n"}, {"name": "torch.nn.functional.ctc_loss()", "path": "nn.functional#torch.nn.functional.ctc_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False) [source]\n \nThe Connectionist Temporal Classification loss. See CTCLoss for details.  Note In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.   Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \nlog_probs \u2013 (T,N,C)(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size. The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). \ntargets \u2013 (N,S)(N, S)  or (sum(target_lengths)). Targets cannot be blank. In the second form, the targets are assumed to be concatenated. \ninput_lengths \u2013 (N)(N) . Lengths of the inputs (must each be \u2264T\\leq T ) \ntarget_lengths \u2013 (N)(N) . Lengths of the targets \nblank (int, optional) \u2013 Blank label. Default 00 . \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken, 'sum': the output will be summed. Default: 'mean'\n \nzero_infinity (bool, optional) \u2013 Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.    Example: >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n>>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n>>> input_lengths = torch.full((16,), 50, dtype=torch.long)\n>>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n>>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n>>> loss.backward()\n \n"}, {"name": "torch.nn.functional.dropout()", "path": "nn.functional#torch.nn.functional.dropout", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.dropout(input, p=0.5, training=True, inplace=False) [source]\n \nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. See Dropout for details.  Parameters \n \np \u2013 probability of an element to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n"}, {"name": "torch.nn.functional.dropout2d()", "path": "nn.functional#torch.nn.functional.dropout2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False) [source]\n \nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. See Dropout2d for details.  Parameters \n \np \u2013 probability of a channel to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n"}, {"name": "torch.nn.functional.dropout3d()", "path": "nn.functional#torch.nn.functional.dropout3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False) [source]\n \nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. See Dropout3d for details.  Parameters \n \np \u2013 probability of a channel to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n"}, {"name": "torch.nn.functional.elu()", "path": "nn.functional#torch.nn.functional.elu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.elu(input, alpha=1.0, inplace=False) [source]\n \nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) . See ELU for more details. \n"}, {"name": "torch.nn.functional.elu_()", "path": "nn.functional#torch.nn.functional.elu_", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.elu_(input, alpha=1.) \u2192 Tensor  \nIn-place version of elu(). \n"}, {"name": "torch.nn.functional.embedding()", "path": "nn.functional#torch.nn.functional.embedding", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False) [source]\n \nA simple lookup table that looks up embeddings in a fixed dictionary and size. This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings. See torch.nn.Embedding for more details.  Parameters \n \ninput (LongTensor) \u2013 Tensor containing indices into the embedding matrix \nweight (Tensor) \u2013 The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size \npadding_idx (int, optional) \u2013 If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. \nmax_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. Note: this will modify weight in-place. \nnorm_type (float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. \nsparse (bool, optional) \u2013 If True, gradient w.r.t. weight will be a sparse tensor. See Notes under torch.nn.Embedding for more details regarding sparse gradients.     Shape:\n\n Input: LongTensor of arbitrary shape containing the indices to extract \n \nWeight: Embedding matrix of floating point type with shape (V, embedding_dim), \n\nwhere V = maximum index + 1 and embedding_dim = the embedding size    Output: (*, embedding_dim), where * is the input shape    Examples: >>> # a batch of 2 samples of 4 indices each\n>>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])\n>>> # an embedding matrix containing 10 tensors of size 3\n>>> embedding_matrix = torch.rand(10, 3)\n>>> F.embedding(input, embedding_matrix)\ntensor([[[ 0.8490,  0.9625,  0.6753],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.6246,  0.9751,  0.3618],\n         [ 0.4161,  0.2419,  0.7383]],\n\n        [[ 0.6246,  0.9751,  0.3618],\n         [ 0.0237,  0.7794,  0.0528],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.3385,  0.8612,  0.1867]]])\n\n>>> # example with padding_idx\n>>> weights = torch.rand(10, 3)\n>>> weights[0, :].zero_()\n>>> embedding_matrix = weights\n>>> input = torch.tensor([[0,2,0,5]])\n>>> F.embedding(input, embedding_matrix, padding_idx=0)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.5609,  0.5384,  0.8720],\n         [ 0.0000,  0.0000,  0.0000],\n         [ 0.6262,  0.2438,  0.7471]]])\n \n"}, {"name": "torch.nn.functional.embedding_bag()", "path": "nn.functional#torch.nn.functional.embedding_bag", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False) [source]\n \nComputes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings. See torch.nn.EmbeddingBag for more details.  Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ninput (LongTensor) \u2013 Tensor containing bags of indices into the embedding matrix \nweight (Tensor) \u2013 The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size \noffsets (LongTensor, optional) \u2013 Only used when input is 1D. offsets determines the starting index position of each bag (sequence) in input. \nmax_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. Note: this will modify weight in-place. \nnorm_type (float, optional) \u2013 The p in the p-norm to compute for the max_norm option. Default 2. \nscale_grad_by_freq (boolean, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". \nmode (string, optional) \u2013 \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. Default: \"mean\"\n \nsparse (bool, optional) \u2013 if True, gradient w.r.t. weight will be a sparse tensor. See Notes under torch.nn.Embedding for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". \nper_sample_weights (Tensor, optional) \u2013 a tensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None. \ninclude_last_offset (bool, optional) \u2013 if True, the size of offsets is equal to the number of bags + 1. \nlast element is the size of the input, or the ending index position of the last bag (The) \u2013     Shape:  \ninput (LongTensor) and offsets (LongTensor, optional)  \nIf input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  \nIf input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    \nweight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n \nper_sample_weights (Tensor, optional). Has the same shape as input. \noutput: aggregated embedding values of shape (B, embedding_dim)\n  Examples: >>> # an Embedding module containing 10 tensors of size 3\n>>> embedding_matrix = torch.rand(10, 3)\n>>> # a batch of 2 samples of 4 indices each\n>>> input = torch.tensor([1,2,4,5,4,3,2,9])\n>>> offsets = torch.tensor([0,4])\n>>> F.embedding_bag(embedding_matrix, input, offsets)\ntensor([[ 0.3397,  0.3552,  0.5545],\n        [ 0.5893,  0.4386,  0.5882]])\n \n"}, {"name": "torch.nn.functional.feature_alpha_dropout()", "path": "nn.functional#torch.nn.functional.feature_alpha_dropout", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False) [source]\n \nRandomly masks out entire channels (a channel is a feature map, e.g. the jj -th channel of the ii -th sample in the batch input is a tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function. Each element will be masked independently on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance. See FeatureAlphaDropout for details.  Parameters \n \np \u2013 dropout probability of a channel to be zeroed. Default: 0.5 \ntraining \u2013 apply dropout if is True. Default: True\n \ninplace \u2013 If set to True, will do this operation in-place. Default: False\n    \n"}, {"name": "torch.nn.functional.fold()", "path": "nn.functional#torch.nn.functional.fold", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1) [source]\n \nCombines an array of sliding local blocks into a large containing tensor.  Warning Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.  See torch.nn.Fold for details \n"}, {"name": "torch.nn.functional.gelu()", "path": "nn.functional#torch.nn.functional.gelu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.gelu(input) \u2192 Tensor [source]\n \nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)  where \u03a6(x)\\Phi(x)  is the Cumulative Distribution Function for Gaussian Distribution. See Gaussian Error Linear Units (GELUs). \n"}, {"name": "torch.nn.functional.glu()", "path": "nn.functional#torch.nn.functional.glu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.glu(input, dim=-1) \u2192 Tensor [source]\n \nThe gated linear unit. Computes:  GLU(a,b)=a\u2297\u03c3(b)\\text{GLU}(a, b) = a \\otimes \\sigma(b)  \nwhere input is split in half along dim to form a and b, \u03c3\\sigma  is the sigmoid function and \u2297\\otimes  is the element-wise product between matrices. See Language Modeling with Gated Convolutional Networks.  Parameters \n \ninput (Tensor) \u2013 input tensor \ndim (int) \u2013 dimension on which to split the input. Default: -1    \n"}, {"name": "torch.nn.functional.grid_sample()", "path": "nn.functional#torch.nn.functional.grid_sample", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None) [source]\n \nGiven an input and a flow-field grid, computes the output using input values and pixel locations from grid. Currently, only spatial (4-D) and volumetric (5-D) input are supported. In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})  and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out}) . For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels. grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input. If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are  \npadding_mode=\"zeros\": use 0 for out-of-bound grid locations, \npadding_mode=\"border\": use border values for out-of-bound grid locations, \npadding_mode=\"reflection\": use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location x = -3.5 reflects by border -1 and becomes x' = 1.5, then reflects by border 1 and becomes x'' = -0.5.   Note This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .   Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Note NaN values in grid would be interpreted as -1.   Parameters \n \ninput (Tensor) \u2013 input of shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})  (4-D case) or (N,C,Din,Hin,Win)(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})  (5-D case) \ngrid (Tensor) \u2013 flow-field of shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2)  (4-D case) or (N,Dout,Hout,Wout,3)(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)  (5-D case) \nmode (str) \u2013 interpolation mode to calculate output values 'bilinear' | 'nearest' | 'bicubic'. Default: 'bilinear' Note: mode='bicubic' supports only 4-D input. When mode='bilinear' and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear. \npadding_mode (str) \u2013 padding mode for outside grid values 'zeros' | 'border' | 'reflection'. Default: 'zeros'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input as squares rather than points. If set to True, the extrema (-1 and 1) are considered as referring to the center points of the input\u2019s corner pixels. If set to False, they are instead considered as referring to the corner points of the input\u2019s corner pixels, making the sampling more resolution agnostic. This option parallels the align_corners option in interpolate(), and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: False\n   Returns \noutput Tensor  Return type \noutput (Tensor)    Warning When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().   Note mode='bicubic' is implemented using the cubic convolution algorithm with \u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha  might be different from packages to packages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This algorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func: torch.clamp to ensure they are within the valid range.  \n"}, {"name": "torch.nn.functional.gumbel_softmax()", "path": "nn.functional#torch.nn.functional.gumbel_softmax", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1) [source]\n \nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.  Parameters \n \nlogits \u2013 [\u2026, num_features] unnormalized log probabilities \ntau \u2013 non-negative scalar temperature \nhard \u2013 if True, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd \ndim (int) \u2013 A dimension along which softmax will be computed. Default: -1.   Returns \nSampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.    Note This function is here for legacy reasons, may be removed from nn.Functional in the future.   Note The main trick for hard is to do y_hard - y_soft.detach() + y_soft It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)   Examples::\n\n>>> logits = torch.randn(20, 32)\n>>> # Sample soft categorical using reparametrization trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=False)\n>>> # Sample hard categorical using \"Straight-through\" trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=True)\n   \n"}, {"name": "torch.nn.functional.hardshrink()", "path": "nn.functional#torch.nn.functional.hardshrink", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.hardshrink(input, lambd=0.5) \u2192 Tensor [source]\n \nApplies the hard shrinkage function element-wise See Hardshrink for more details. \n"}, {"name": "torch.nn.functional.hardsigmoid()", "path": "nn.functional#torch.nn.functional.hardsigmoid", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.hardsigmoid(input) \u2192 Tensor [source]\n \nApplies the element-wise function  Hardsigmoid(x)={0if x\u2264\u22123,1if x\u2265+3,x/6+1/2otherwise\\text{Hardsigmoid}(x) = \\begin{cases} 0 & \\text{if~} x \\le -3, \\\\ 1 & \\text{if~} x \\ge +3, \\\\ x / 6 + 1 / 2 & \\text{otherwise} \\end{cases}  \n Parameters \ninplace \u2013 If set to True, will do this operation in-place. Default: False   See Hardsigmoid for more details. \n"}, {"name": "torch.nn.functional.hardswish()", "path": "nn.functional#torch.nn.functional.hardswish", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.hardswish(input, inplace=False) [source]\n \nApplies the hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.  Hardswish(x)={0if x\u2264\u22123,xif x\u2265+3,x\u22c5(x+3)/6otherwise\\text{Hardswish}(x) = \\begin{cases} 0 & \\text{if~} x \\le -3, \\\\ x & \\text{if~} x \\ge +3, \\\\ x \\cdot (x + 3) /6 & \\text{otherwise} \\end{cases}  \nSee Hardswish for more details. \n"}, {"name": "torch.nn.functional.hardtanh()", "path": "nn.functional#torch.nn.functional.hardtanh", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) \u2192 Tensor [source]\n \nApplies the HardTanh function element-wise. See Hardtanh for more details. \n"}, {"name": "torch.nn.functional.hardtanh_()", "path": "nn.functional#torch.nn.functional.hardtanh_", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) \u2192 Tensor  \nIn-place version of hardtanh(). \n"}, {"name": "torch.nn.functional.hinge_embedding_loss()", "path": "nn.functional#torch.nn.functional.hinge_embedding_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee HingeEmbeddingLoss for details. \n"}, {"name": "torch.nn.functional.instance_norm()", "path": "nn.functional#torch.nn.functional.instance_norm", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05) [source]\n \nApplies Instance Normalization for each channel in each data sample in a batch. See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details. \n"}, {"name": "torch.nn.functional.interpolate()", "path": "nn.functional#torch.nn.functional.interpolate", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None) [source]\n \nDown/up samples the input to either the given size or the given scale_factor The algorithm used for interpolation is determined by mode. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width. The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area  Parameters \n \ninput (Tensor) \u2013 the input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (str) \u2013 algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'. Default: 'nearest'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False\n \nrecompute_scale_factor (bool, optional) \u2013 recompute the scale_factor for use in the interpolation calculation. When scale_factor is passed as a parameter, it is used to compute the output_size. If recompute_scale_factor is False or not specified, the passed-in scale_factor will be used in the interpolation computation. Otherwise, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly). Note that when scale_factor is floating-point, the recomputed scale_factor may differ from the one passed in due to rounding and precision issues.     Note With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.   Warning When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.   Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  \n"}, {"name": "torch.nn.functional.kl_div()", "path": "nn.functional#torch.nn.functional.kl_div", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False) [source]\n \nThe Kullback-Leibler divergence Loss See KLDivLoss for details.  Parameters \n \ninput \u2013 Tensor of arbitrary shape \ntarget \u2013 Tensor of the same shape as input \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied 'batchmean': the sum of the output will be divided by the batchsize 'sum': the output will be summed 'mean': the output will be divided by the number of elements in the output Default: 'mean'\n \nlog_target (bool) \u2013 A flag indicating whether target is passed in the log space. It is recommended to pass certain distributions (like softmax) in the log space to avoid numerical issues caused by explicit log. Default: False\n     Note size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.   Note :attr:reduction = 'mean' doesn\u2019t return the true kl divergence value, please use :attr:reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as \u2018batchmean\u2019.  \n"}, {"name": "torch.nn.functional.l1_loss()", "path": "nn.functional#torch.nn.functional.l1_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nFunction that takes the mean element-wise absolute value difference. See L1Loss for details. \n"}, {"name": "torch.nn.functional.layer_norm()", "path": "nn.functional#torch.nn.functional.layer_norm", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05) [source]\n \nApplies Layer Normalization for last certain number of dimensions. See LayerNorm for details. \n"}, {"name": "torch.nn.functional.leaky_relu()", "path": "nn.functional#torch.nn.functional.leaky_relu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) \u2192 Tensor [source]\n \nApplies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  See LeakyReLU for more details. \n"}, {"name": "torch.nn.functional.leaky_relu_()", "path": "nn.functional#torch.nn.functional.leaky_relu_", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.leaky_relu_(input, negative_slope=0.01) \u2192 Tensor  \nIn-place version of leaky_relu(). \n"}, {"name": "torch.nn.functional.linear()", "path": "nn.functional#torch.nn.functional.linear", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.linear(input, weight, bias=None) [source]\n \nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b . This operator supports TensorFloat32. Shape:  Input: (N,\u2217,in_features)(N, *, in\\_features)  N is the batch size, * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features) \n Bias: (out_features)(out\\_features) \n Output: (N,\u2217,out_features)(N, *, out\\_features) \n  \n"}, {"name": "torch.nn.functional.local_response_norm()", "path": "nn.functional#torch.nn.functional.local_response_norm", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0) [source]\n \nApplies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels. See LocalResponseNorm for details. \n"}, {"name": "torch.nn.functional.logsigmoid()", "path": "nn.functional#torch.nn.functional.logsigmoid", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.logsigmoid(input) \u2192 Tensor  \nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)  See LogSigmoid for more details. \n"}, {"name": "torch.nn.functional.log_softmax()", "path": "nn.functional#torch.nn.functional.log_softmax", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None) [source]\n \nApplies a softmax followed by a logarithm. While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly. See LogSoftmax for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which log_softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    \n"}, {"name": "torch.nn.functional.lp_pool1d()", "path": "nn.functional#torch.nn.functional.lp_pool1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False) [source]\n \nApplies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well. See LPPool1d for details. \n"}, {"name": "torch.nn.functional.lp_pool2d()", "path": "nn.functional#torch.nn.functional.lp_pool2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False) [source]\n \nApplies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well. See LPPool2d for details. \n"}, {"name": "torch.nn.functional.margin_ranking_loss()", "path": "nn.functional#torch.nn.functional.margin_ranking_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee MarginRankingLoss for details. \n"}, {"name": "torch.nn.functional.max_pool1d()", "path": "nn.functional#torch.nn.functional.max_pool1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.max_pool1d(*args, **kwargs)  \nApplies a 1D max pooling over an input signal composed of several input planes. See MaxPool1d for details. \n"}, {"name": "torch.nn.functional.max_pool2d()", "path": "nn.functional#torch.nn.functional.max_pool2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.max_pool2d(*args, **kwargs)  \nApplies a 2D max pooling over an input signal composed of several input planes. See MaxPool2d for details. \n"}, {"name": "torch.nn.functional.max_pool3d()", "path": "nn.functional#torch.nn.functional.max_pool3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.max_pool3d(*args, **kwargs)  \nApplies a 3D max pooling over an input signal composed of several input planes. See MaxPool3d for details. \n"}, {"name": "torch.nn.functional.max_unpool1d()", "path": "nn.functional#torch.nn.functional.max_unpool1d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None) [source]\n \nComputes a partial inverse of MaxPool1d. See MaxUnpool1d for details. \n"}, {"name": "torch.nn.functional.max_unpool2d()", "path": "nn.functional#torch.nn.functional.max_unpool2d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None) [source]\n \nComputes a partial inverse of MaxPool2d. See MaxUnpool2d for details. \n"}, {"name": "torch.nn.functional.max_unpool3d()", "path": "nn.functional#torch.nn.functional.max_unpool3d", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None) [source]\n \nComputes a partial inverse of MaxPool3d. See MaxUnpool3d for details. \n"}, {"name": "torch.nn.functional.mse_loss()", "path": "nn.functional#torch.nn.functional.mse_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nMeasures the element-wise mean squared error. See MSELoss for details. \n"}, {"name": "torch.nn.functional.multilabel_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_margin_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee MultiLabelMarginLoss for details. \n"}, {"name": "torch.nn.functional.multilabel_soft_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_soft_margin_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) \u2192 Tensor [source]\n \nSee MultiLabelSoftMarginLoss for details. \n"}, {"name": "torch.nn.functional.multi_margin_loss()", "path": "nn.functional#torch.nn.functional.multi_margin_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \n multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,\n\nreduce=None, reduction=\u2019mean\u2019) -> Tensor   See MultiMarginLoss for details. \n"}, {"name": "torch.nn.functional.nll_loss()", "path": "nn.functional#torch.nn.functional.nll_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') [source]\n \nThe negative log likelihood loss. See NLLLoss for details.  Parameters \n \ninput \u2013 (N,C)(N, C)  where C = number of classes or (N,C,H,W)(N, C, H, W)  in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  in the case of K-dimensional loss. \ntarget \u2013 (N)(N)  where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  where K\u22651K \\geq 1  for K-dimensional loss. \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C\n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Default: -100 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Example: >>> # input is of size N x C = 3 x 5\n>>> input = torch.randn(3, 5, requires_grad=True)\n>>> # each element in target has to have 0 <= value < C\n>>> target = torch.tensor([1, 0, 4])\n>>> output = F.nll_loss(F.log_softmax(input), target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.functional.normalize()", "path": "nn.functional#torch.nn.functional.normalize", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None) [source]\n \nPerforms LpL_p  normalization of inputs over specified dimension. For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k) , each ndimn_{dim}  -element vector vv  along dimension dim is transformed as  v=vmax\u2061(\u2225v\u2225p,\u03f5).v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.  \nWith the default arguments it uses the Euclidean norm over vectors along dimension 11  for normalization.  Parameters \n \ninput \u2013 input tensor of any shape \np (float) \u2013 the exponent value in the norm formulation. Default: 2 \ndim (int) \u2013 the dimension to reduce. Default: 1 \neps (float) \u2013 small value to avoid division by zero. Default: 1e-12 \nout (Tensor, optional) \u2013 the output tensor. If out is used, this operation won\u2019t be differentiable.    \n"}, {"name": "torch.nn.functional.one_hot()", "path": "nn.functional#torch.nn.functional.one_hot", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.one_hot(tensor, num_classes=-1) \u2192 LongTensor  \nTakes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. See also One-hot on Wikipedia .  Parameters \n \ntensor (LongTensor) \u2013 class values of any shape. \nnum_classes (int) \u2013 Total number of classes. If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.   Returns \nLongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.   Examples >>> F.one_hot(torch.arange(0, 5) % 3)\ntensor([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0],\n        [0, 1, 0]])\n>>> F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\ntensor([[1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0]])\n>>> F.one_hot(torch.arange(0, 6).view(3,2) % 3)\ntensor([[[1, 0, 0],\n         [0, 1, 0]],\n        [[0, 0, 1],\n         [1, 0, 0]],\n        [[0, 1, 0],\n         [0, 0, 1]]])\n \n"}, {"name": "torch.nn.functional.pad()", "path": "nn.functional#torch.nn.functional.pad", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.pad(input, pad, mode='constant', value=0)  \nPads tensor.  Padding size:\n\nThe padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor  dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right}) ; to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom}) ; to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom}  padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back}) .  Padding mode:\n\nSee torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.    Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters \n \ninput (Tensor) \u2013 N-dimensional tensor \npad (tuple) \u2013 m-elements tuple, where m2\u2264\\frac{m}{2} \\leq  input dimensions and mm  is even. \nmode \u2013 'constant', 'reflect', 'replicate' or 'circular'. Default: 'constant'\n \nvalue \u2013 fill value for 'constant' padding. Default: 0\n    Examples: >>> t4d = torch.empty(3, 3, 4, 2)\n>>> p1d = (1, 1) # pad last dim by 1 on each side\n>>> out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n>>> print(out.size())\ntorch.Size([3, 3, 4, 4])\n>>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n>>> out = F.pad(t4d, p2d, \"constant\", 0)\n>>> print(out.size())\ntorch.Size([3, 3, 8, 4])\n>>> t4d = torch.empty(3, 3, 4, 2)\n>>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n>>> out = F.pad(t4d, p3d, \"constant\", 0)\n>>> print(out.size())\ntorch.Size([3, 9, 7, 3])\n \n"}, {"name": "torch.nn.functional.pairwise_distance()", "path": "nn.functional#torch.nn.functional.pairwise_distance", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False) [source]\n \nSee torch.nn.PairwiseDistance for details \n"}, {"name": "torch.nn.functional.pdist()", "path": "nn.functional#torch.nn.functional.pdist", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.pdist(input, p=2) \u2192 Tensor  \nComputes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous. If input has shape N\u00d7MN \\times M  then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1) . This function is equivalent to scipy.spatial.distance.pdist(input, \u2018minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()).  Parameters \n \ninput \u2013 input tensor of shape N\u00d7MN \\times M . \np \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty] .    \n"}, {"name": "torch.nn.functional.pixel_shuffle()", "path": "nn.functional#torch.nn.functional.pixel_shuffle", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.pixel_shuffle(input, upscale_factor) \u2192 Tensor  \nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is the upscale_factor. See PixelShuffle for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor \nupscale_factor (int) \u2013 factor to increase spatial resolution by    Examples: >>> input = torch.randn(1, 9, 4, 4)\n>>> output = torch.nn.functional.pixel_shuffle(input, 3)\n>>> print(output.size())\ntorch.Size([1, 1, 12, 12])\n \n"}, {"name": "torch.nn.functional.pixel_unshuffle()", "path": "nn.functional#torch.nn.functional.pixel_unshuffle", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.pixel_unshuffle(input, downscale_factor) \u2192 Tensor  \nReverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the downscale_factor. See PixelUnshuffle for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ndownscale_factor (int) \u2013 factor to increase spatial resolution by    Examples: >>> input = torch.randn(1, 1, 12, 12)\n>>> output = torch.nn.functional.pixel_unshuffle(input, 3)\n>>> print(output.size())\ntorch.Size([1, 9, 4, 4])\n \n"}, {"name": "torch.nn.functional.poisson_nll_loss()", "path": "nn.functional#torch.nn.functional.poisson_nll_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') [source]\n \nPoisson negative log likelihood loss. See PoissonNLLLoss for details.  Parameters \n \ninput \u2013 expectation of underlying Poisson distribution. \ntarget \u2013 random sample target\u223cPoisson(input)target \\sim \\text{Poisson}(input) . \nlog_input \u2013 if True the loss is computed as exp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target} * \\text{input} , if False then loss is input\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps}) . Default: True\n \nfull \u2013 whether to compute full loss, i. e. to add the Stirling approximation term. Default: False target\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u2217\u03c0\u2217target)\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target}) . \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \neps (float, optional) \u2013 Small value to avoid evaluation of log\u2061(0)\\log(0)  when log_input`=``False`. Default: 1e-8 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    \n"}, {"name": "torch.nn.functional.prelu()", "path": "nn.functional#torch.nn.functional.prelu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.prelu(input, weight) \u2192 Tensor [source]\n \nApplies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)  where weight is a learnable parameter. See PReLU for more details. \n"}, {"name": "torch.nn.functional.relu()", "path": "nn.functional#torch.nn.functional.relu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.relu(input, inplace=False) \u2192 Tensor [source]\n \nApplies the rectified linear unit function element-wise. See ReLU for more details. \n"}, {"name": "torch.nn.functional.relu6()", "path": "nn.functional#torch.nn.functional.relu6", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.relu6(input, inplace=False) \u2192 Tensor [source]\n \nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6) . See ReLU6 for more details. \n"}, {"name": "torch.nn.functional.relu_()", "path": "nn.functional#torch.nn.functional.relu_", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.relu_(input) \u2192 Tensor  \nIn-place version of relu(). \n"}, {"name": "torch.nn.functional.rrelu()", "path": "nn.functional#torch.nn.functional.rrelu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) \u2192 Tensor [source]\n \nRandomized leaky ReLU. See RReLU for more details. \n"}, {"name": "torch.nn.functional.rrelu_()", "path": "nn.functional#torch.nn.functional.rrelu_", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) \u2192 Tensor  \nIn-place version of rrelu(). \n"}, {"name": "torch.nn.functional.selu()", "path": "nn.functional#torch.nn.functional.selu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.selu(input, inplace=False) \u2192 Tensor [source]\n \nApplies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717  and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 . See SELU for more details. \n"}, {"name": "torch.nn.functional.sigmoid()", "path": "nn.functional#torch.nn.functional.sigmoid", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.sigmoid(input) \u2192 Tensor [source]\n \nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}  See Sigmoid for more details. \n"}, {"name": "torch.nn.functional.silu()", "path": "nn.functional#torch.nn.functional.silu", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.silu(input, inplace=False) [source]\n \nApplies the silu function, element-wise.  silu(x)=x\u2217\u03c3(x),where \u03c3(x) is the logistic sigmoid.\\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}  \n Note See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.  See SiLU for more details. \n"}, {"name": "torch.nn.functional.smooth_l1_loss()", "path": "nn.functional#torch.nn.functional.smooth_l1_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0) [source]\n \nFunction that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. See SmoothL1Loss for details. \n"}, {"name": "torch.nn.functional.softmax()", "path": "nn.functional#torch.nn.functional.softmax", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None) [source]\n \nApplies a softmax function. Softmax is defined as: Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}  It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1. See Softmax for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.     Note This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties).  \n"}, {"name": "torch.nn.functional.softmin()", "path": "nn.functional#torch.nn.functional.softmin", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None) [source]\n \nApplies a softmin function. Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See softmax definition for mathematical formula. See Softmin for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmin will be computed (so every slice along dim will sum to 1). \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    \n"}, {"name": "torch.nn.functional.softplus()", "path": "nn.functional#torch.nn.functional.softplus", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.softplus(input, beta=1, threshold=20) \u2192 Tensor  \nApplies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) . For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold . See Softplus for more details. \n"}, {"name": "torch.nn.functional.softshrink()", "path": "nn.functional#torch.nn.functional.softshrink", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.softshrink(input, lambd=0.5) \u2192 Tensor  \nApplies the soft shrinkage function elementwise See Softshrink for more details. \n"}, {"name": "torch.nn.functional.softsign()", "path": "nn.functional#torch.nn.functional.softsign", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.softsign(input) \u2192 Tensor [source]\n \nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}  See Softsign for more details. \n"}, {"name": "torch.nn.functional.soft_margin_loss()", "path": "nn.functional#torch.nn.functional.soft_margin_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') \u2192 Tensor [source]\n \nSee SoftMarginLoss for details. \n"}, {"name": "torch.nn.functional.tanh()", "path": "nn.functional#torch.nn.functional.tanh", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.tanh(input) \u2192 Tensor [source]\n \nApplies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}  See Tanh for more details. \n"}, {"name": "torch.nn.functional.tanhshrink()", "path": "nn.functional#torch.nn.functional.tanhshrink", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.tanhshrink(input) \u2192 Tensor [source]\n \nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)  See Tanhshrink for more details. \n"}, {"name": "torch.nn.functional.threshold()", "path": "nn.functional#torch.nn.functional.threshold", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.threshold(input, threshold, value, inplace=False)  \nThresholds each element of the input Tensor. See Threshold for more details. \n"}, {"name": "torch.nn.functional.threshold_()", "path": "nn.functional#torch.nn.functional.threshold_", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.threshold_(input, threshold, value) \u2192 Tensor  \nIn-place version of threshold(). \n"}, {"name": "torch.nn.functional.triplet_margin_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') [source]\n \nSee TripletMarginLoss for details \n"}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_with_distance_loss", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean') [source]\n \nSee TripletMarginWithDistanceLoss for details. \n"}, {"name": "torch.nn.functional.unfold()", "path": "nn.functional#torch.nn.functional.unfold", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1) [source]\n \nExtracts sliding local blocks from a batched input tensor.  Warning Currently, only 4-D input tensors (batched image-like tensors) are supported.   Warning More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.  See torch.nn.Unfold for details \n"}, {"name": "torch.nn.functional.upsample()", "path": "nn.functional#torch.nn.functional.upsample", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nUpsamples the input to either the given size or the given scale_factor  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).   Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  The algorithm used for upsampling is determined by mode. Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width. The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)  Parameters \n \ninput (Tensor) \u2013 the input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (string) \u2013 algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear'. Default: 'nearest'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False\n     Note With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.  \n"}, {"name": "torch.nn.functional.upsample_bilinear()", "path": "nn.functional#torch.nn.functional.upsample_bilinear", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using bilinear upsampling.  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).  Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.  Parameters \n \ninput (Tensor) \u2013 input \nsize (int or Tuple[int, int]) \u2013 output spatial size. \nscale_factor (int or Tuple[int, int]) \u2013 multiplier for spatial size     Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  \n"}, {"name": "torch.nn.functional.upsample_nearest()", "path": "nn.functional#torch.nn.functional.upsample_nearest", "type": "torch.nn.functional", "text": " \ntorch.nn.functional.upsample_nearest(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using nearest neighbours\u2019 pixel values.  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').  Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).  Parameters \n \ninput (Tensor) \u2013 input \nsize (int or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatia size. \nscale_factor (int) \u2013 multiplier for spatial size. Has to be an integer.     Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.  \n"}, {"name": "torch.nn.GaussianNLLLoss", "path": "generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss", "type": "torch.nn", "text": " \nclass torch.nn.GaussianNLLLoss(*, full=False, eps=1e-06, reduction='mean') [source]\n \nGaussian negative log likelihood loss. The targets are treated as samples from Gaussian distributions with expectations and variances predicted by the neural network. For a D-dimensional target tensor modelled as having heteroscedastic Gaussian distributions with a D-dimensional tensor of expectations input and a D-dimensional tensor of positive variances var the loss is:  loss=12\u2211i=1D(log\u2061(max(var[i], eps))+(input[i]\u2212target[i])2max(var[i], eps))+const.\\text{loss} = \\frac{1}{2}\\sum_{i=1}^D \\left(\\log\\left(\\text{max}\\left(\\text{var}[i], \\ \\text{eps}\\right)\\right) + \\frac{\\left(\\text{input}[i] - \\text{target}[i]\\right)^2} {\\text{max}\\left(\\text{var}[i], \\ \\text{eps}\\right)}\\right) + \\text{const.}  \nwhere eps is used for stability. By default, the constant term of the loss function is omitted unless full is True. If var is a scalar (implying target tensor has homoscedastic Gaussian distributions) it is broadcasted to be the same size as the input.  Parameters \n \nfull (bool, optional) \u2013 include the constant term in the loss calculation. Default: False. \neps (float, optional) \u2013 value used to clamp var (see note below), for stability. Default: 1e-6. \nreduction (string, optional) \u2013 specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output is the average of all batch member losses, 'sum': the output is the sum of all batch member losses. Default: 'mean'.     Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Var: (N,1)(N, 1)  or (N,\u2217)(N, *) , same shape as the input Output: scalar if reduction is 'mean' (default) or 'sum'. If reduction is 'none', then (N)(N) \n    Examples: >>> loss = nn.GaussianNLLLoss()\n>>> input = torch.randn(5, 2, requires_grad=True)\n>>> target = torch.randn(5, 2)\n>>> var = torch.ones(5, 2, requires_grad=True) #heteroscedastic\n>>> output = loss(input, target, var)\n>>> output.backward()\n\n\n>>> loss = nn.GaussianNLLLoss()\n>>> input = torch.randn(5, 2, requires_grad=True)\n>>> target = torch.randn(5, 2)\n>>> var = torch.ones(5, 1, requires_grad=True) #homoscedastic\n>>> output = loss(input, target, var)\n>>> output.backward()\n  Note The clamping of var is ignored with respect to autograd, and so the gradients are unaffected by it.   Reference:\n\nNix, D. A. and Weigend, A. S., \u201cEstimating the mean and variance of the target probability distribution\u201d, Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN\u201994), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi: 10.1109/ICNN.1994.374138.   \n"}, {"name": "torch.nn.GELU", "path": "generated/torch.nn.gelu#torch.nn.GELU", "type": "torch.nn", "text": " \nclass torch.nn.GELU [source]\n \nApplies the Gaussian Error Linear Units function:  GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)  \nwhere \u03a6(x)\\Phi(x)  is the Cumulative Distribution Function for Gaussian Distribution.  Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.GELU()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.GroupNorm", "path": "generated/torch.nn.groupnorm#torch.nn.GroupNorm", "type": "torch.nn", "text": " \nclass torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True) [source]\n \nApplies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  \nThe input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. \u03b3\\gamma  and \u03b2\\beta  are learnable per-channel affine transform parameter vectors of size num_channels if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). This layer uses statistics computed from input data in both training and evaluation modes.  Parameters \n \nnum_groups (int) \u2013 number of groups to separate the channels into \nnum_channels (int) \u2013 number of channels expected in input \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \naffine \u2013 a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.     Shape:\n\n Input: (N,C,\u2217)(N, C, *)  where C=num_channelsC=\\text{num\\_channels} \n Output: (N,C,\u2217)(N, C, *)  (same shape as input)    Examples: >>> input = torch.randn(20, 6, 10, 10)\n>>> # Separate 6 channels into 3 groups\n>>> m = nn.GroupNorm(3, 6)\n>>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n>>> m = nn.GroupNorm(6, 6)\n>>> # Put all 6 channels into a single group (equivalent with LayerNorm)\n>>> m = nn.GroupNorm(1, 6)\n>>> # Activating the module\n>>> output = m(input)\n \n"}, {"name": "torch.nn.GRU", "path": "generated/torch.nn.gru#torch.nn.GRU", "type": "torch.nn", "text": " \nclass torch.nn.GRU(*args, **kwargs) [source]\n \nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  rt=\u03c3(Wirxt+bir+Whrh(t\u22121)+bhr)zt=\u03c3(Wizxt+biz+Whzh(t\u22121)+bhz)nt=tanh\u2061(Winxt+bin+rt\u2217(Whnh(t\u22121)+bhn))ht=(1\u2212zt)\u2217nt+zt\u2217h(t\u22121)\\begin{array}{ll} r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\ z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\ n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\end{array}  \nwhere hth_t  is the hidden state at time t, xtx_t  is the input at time t, h(t\u22121)h_{(t-1)}  is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_t , ztz_t , ntn_t  are the reset, update, and new gates, respectively. \u03c3\\sigma  is the sigmoid function, and \u2217*  is the Hadamard product. In a multilayer GRU, the input xt(l)x^{(l)}_t  of the ll  -th layer (l>=2l >= 2 ) is the hidden state ht(l\u22121)h^{(l-1)}_t  of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t  where each \u03b4t(l\u22121)\\delta^{(l-1)}_t  is a Bernoulli random variable which is 00  with probability dropout.  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nnum_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1 \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n \nbatch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n \ndropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0 \nbidirectional \u2013 If True, becomes a bidirectional GRU. Default: False\n     Inputs: input, h_0\n\n \ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details. \nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n\n\n \noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  \nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape:\n\n Input1: (L,N,Hin)(L, N, H_{in})  tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}  and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})  tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}  Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}  If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})  where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size} \n Output2: (S,N,Hout)(S, N, H_{out})  tensor containing the next hidden state for each element in the batch     Variables \n \n~GRU.weight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th}  layer (W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0. Otherwise, the shape is (3*hidden_size, num_directions * hidden_size)\n \n~GRU.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th}  layer (W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)\n \n~GRU.bias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th}  layer (b_ir|b_iz|b_in), of shape (3*hidden_size)\n \n~GRU.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th}  layer (b_hr|b_hz|b_hn), of shape (3*hidden_size)\n     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}    Orphan   Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: >>> rnn = nn.GRU(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> output, hn = rnn(input, h0)\n \n"}, {"name": "torch.nn.GRUCell", "path": "generated/torch.nn.grucell#torch.nn.GRUCell", "type": "torch.nn", "text": " \nclass torch.nn.GRUCell(input_size, hidden_size, bias=True) [source]\n \nA gated recurrent unit (GRU) cell  r=\u03c3(Wirx+bir+Whrh+bhr)z=\u03c3(Wizx+biz+Whzh+bhz)n=tanh\u2061(Winx+bin+r\u2217(Whnh+bhn))h\u2032=(1\u2212z)\u2217n+z\u2217h\\begin{array}{ll} r = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\ z = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\ n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\ h' = (1 - z) * n + z * h \\end{array} \nwhere \u03c3\\sigma  is the sigmoid function, and \u2217*  is the Hadamard product.  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n     Inputs: input, hidden\n\n \ninput of shape (batch, input_size): tensor containing input features \nhidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.   Outputs: h\u2019\n\n \nh\u2019 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch   Shape:\n\n Input1: (N,Hin)(N, H_{in})  tensor containing input features where HinH_{in}  = input_size\n Input2: (N,Hout)(N, H_{out})  tensor containing the initial hidden state for each element in the batch where HoutH_{out}  = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})  tensor containing the next hidden state for each element in the batch     Variables \n \n~GRUCell.weight_ih \u2013 the learnable input-hidden weights, of shape (3*hidden_size, input_size)\n \n~GRUCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (3*hidden_size, hidden_size)\n \n~GRUCell.bias_ih \u2013 the learnable input-hidden bias, of shape (3*hidden_size)\n \n~GRUCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (3*hidden_size)\n     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}   Examples: >>> rnn = nn.GRUCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n \n"}, {"name": "torch.nn.Hardshrink", "path": "generated/torch.nn.hardshrink#torch.nn.Hardshrink", "type": "torch.nn", "text": " \nclass torch.nn.Hardshrink(lambd=0.5) [source]\n \nApplies the hard shrinkage function element-wise:  HardShrink(x)={x, if x>\u03bbx, if x<\u2212\u03bb0, otherwise \\text{HardShrink}(x) = \\begin{cases} x, & \\text{ if } x > \\lambda \\\\ x, & \\text{ if } x < -\\lambda \\\\ 0, & \\text{ otherwise } \\end{cases}  \n Parameters \nlambd \u2013 the \u03bb\\lambda  value for the Hardshrink formulation. Default: 0.5    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Hardshrink()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Hardsigmoid", "path": "generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid", "type": "torch.nn", "text": " \nclass torch.nn.Hardsigmoid(inplace=False) [source]\n \nApplies the element-wise function:  Hardsigmoid(x)={0if x\u2264\u22123,1if x\u2265+3,x/6+1/2otherwise\\text{Hardsigmoid}(x) = \\begin{cases} 0 & \\text{if~} x \\le -3, \\\\ 1 & \\text{if~} x \\ge +3, \\\\ x / 6 + 1 / 2 & \\text{otherwise} \\end{cases}  \n Parameters \ninplace \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input    Examples: >>> m = nn.Hardsigmoid()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Hardswish", "path": "generated/torch.nn.hardswish#torch.nn.Hardswish", "type": "torch.nn", "text": " \nclass torch.nn.Hardswish(inplace=False) [source]\n \nApplies the hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.  Hardswish(x)={0if x\u2264\u22123,xif x\u2265+3,x\u22c5(x+3)/6otherwise\\text{Hardswish}(x) = \\begin{cases} 0 & \\text{if~} x \\le -3, \\\\ x & \\text{if~} x \\ge +3, \\\\ x \\cdot (x + 3) /6 & \\text{otherwise} \\end{cases}  \n Parameters \ninplace \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input    Examples: >>> m = nn.Hardswish()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Hardtanh", "path": "generated/torch.nn.hardtanh#torch.nn.Hardtanh", "type": "torch.nn", "text": " \nclass torch.nn.Hardtanh(min_val=-1.0, max_val=1.0, inplace=False, min_value=None, max_value=None) [source]\n \nApplies the HardTanh function element-wise HardTanh is defined as:  HardTanh(x)={1 if x>1\u22121 if x<\u22121x otherwise \\text{HardTanh}(x) = \\begin{cases} 1 & \\text{ if } x > 1 \\\\ -1 & \\text{ if } x < -1 \\\\ x & \\text{ otherwise } \\\\ \\end{cases}  \nThe range of the linear region [\u22121,1][-1, 1]  can be adjusted using min_val and max_val.  Parameters \n \nmin_val \u2013 minimum value of the linear region range. Default: -1 \nmax_val \u2013 maximum value of the linear region range. Default: 1 \ninplace \u2013 can optionally do the operation in-place. Default: False\n    Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val.  Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Hardtanh(-2, 2)\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.HingeEmbeddingLoss", "path": "generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss", "type": "torch.nn", "text": " \nclass torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean') [source]\n \nMeasures the loss given an input tensor xx  and a labels tensor yy  (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as xx , and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for nn -th sample in the mini-batch is  ln={xn,ifyn=1,max\u2061{0,\u0394\u2212xn},ifyn=\u22121,l_n = \\begin{cases} x_n, & \\text{if}\\; y_n = 1,\\\\ \\max \\{0, \\Delta - x_n\\}, & \\text{if}\\; y_n = -1, \\end{cases}  \nand the total loss functions is  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nwhere L={l1,\u2026,lN}\u22a4L = \\{l_1,\\dots,l_N\\}^\\top .  Parameters \n \nmargin (float, optional) \u2013 Has a default value of 1. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (\u2217)(*)  where \u2217*  means, any number of dimensions. The sum operation operates over all the elements. Target: (\u2217)(*) , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input    \n"}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.identity#torch.nn.Identity", "type": "torch.nn", "text": " \nclass torch.nn.Identity(*args, **kwargs) [source]\n \nA placeholder identity operator that is argument-insensitive.  Parameters \n \nargs \u2013 any argument (unused) \nkwargs \u2013 any keyword argument (unused)    Examples: >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 20])\n \n"}, {"name": "torch.nn.init", "path": "nn.init", "type": "torch.nn.init", "text": "torch.nn.init  \ntorch.nn.init.calculate_gain(nonlinearity, param=None) [source]\n \nReturn the recommended gain value for the given nonlinearity function. The values are as follows:   \nnonlinearity gain   \nLinear / Identity 11   \nConv{1,2,3}D 11   \nSigmoid 11   \nTanh 53\\frac{5}{3}   \nReLU 2\\sqrt{2}   \nLeaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}   \nSELU 34\\frac{3}{4}     Parameters \n \nnonlinearity \u2013 the non-linear function (nn.functional name) \nparam \u2013 optional parameter for the non-linear function    Examples >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n \n  \ntorch.nn.init.uniform_(tensor, a=0.0, b=1.0) [source]\n \nFills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b) .  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \na \u2013 the lower bound of the uniform distribution \nb \u2013 the upper bound of the uniform distribution    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)\n \n  \ntorch.nn.init.normal_(tensor, mean=0.0, std=1.0) [source]\n \nFills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \nmean \u2013 the mean of the normal distribution \nstd \u2013 the standard deviation of the normal distribution    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)\n \n  \ntorch.nn.init.constant_(tensor, val) [source]\n \nFills the input Tensor with the value val\\text{val} .  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \nval \u2013 the value to fill the tensor with    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)\n \n  \ntorch.nn.init.ones_(tensor) [source]\n \nFills the input Tensor with the scalar value 1.  Parameters \ntensor \u2013 an n-dimensional torch.Tensor   Examples >>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)\n \n  \ntorch.nn.init.zeros_(tensor) [source]\n \nFills the input Tensor with the scalar value 0.  Parameters \ntensor \u2013 an n-dimensional torch.Tensor   Examples >>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)\n \n  \ntorch.nn.init.eye_(tensor) [source]\n \nFills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.  Parameters \ntensor \u2013 a 2-dimensional torch.Tensor   Examples >>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)\n \n  \ntorch.nn.init.dirac_(tensor, groups=1) [source]\n \nFills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity  Parameters \n \ntensor \u2013 a {3, 4, 5}-dimensional torch.Tensor\n \ngroups (optional) \u2013 number of groups in the conv layer (default: 1)    Examples >>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)\n \n  \ntorch.nn.init.xavier_uniform_(tensor, gain=1.0) [source]\n \nFills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a)  where  a=gain\u00d76fan_in+fan_outa = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}  \nAlso known as Glorot initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \ngain \u2013 an optional scaling factor    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n \n  \ntorch.nn.init.xavier_normal_(tensor, gain=1.0) [source]\n \nFills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where  std=gain\u00d72fan_in+fan_out\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}  \nAlso known as Glorot initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \ngain \u2013 an optional scaling factor    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)\n \n  \ntorch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') [source]\n \nFills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})  where  bound=gain\u00d73fan_mode\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}  \nAlso known as He initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \na \u2013 the negative slope of the rectifier used after this layer (only used with 'leaky_relu') \nmode \u2013 either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. Choosing 'fan_out' preserves the magnitudes in the backwards pass. \nnonlinearity \u2013 the non-linear function (nn.functional name), recommended to use only with 'relu' or 'leaky_relu' (default).    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n \n  \ntorch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') [source]\n \nFills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where  std=gainfan_mode\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}  \nAlso known as He initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \na \u2013 the negative slope of the rectifier used after this layer (only used with 'leaky_relu') \nmode \u2013 either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. Choosing 'fan_out' preserves the magnitudes in the backwards pass. \nnonlinearity \u2013 the non-linear function (nn.functional name), recommended to use only with 'relu' or 'leaky_relu' (default).    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n \n  \ntorch.nn.init.orthogonal_(tensor, gain=1) [source]\n \nFills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor, where n\u22652n \\geq 2 \n \ngain \u2013 optional scaling factor    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)\n \n  \ntorch.nn.init.sparse_(tensor, sparsity, std=0.01) [source]\n \nFills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as described in Deep learning via Hessian-free optimization - Martens, J. (2010).  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \nsparsity \u2013 The fraction of elements in each column to be set to zero \nstd \u2013 the standard deviation of the normal distribution used to generate the non-zero values    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)\n \n\n"}, {"name": "torch.nn.init.calculate_gain()", "path": "nn.init#torch.nn.init.calculate_gain", "type": "torch.nn.init", "text": " \ntorch.nn.init.calculate_gain(nonlinearity, param=None) [source]\n \nReturn the recommended gain value for the given nonlinearity function. The values are as follows:   \nnonlinearity gain   \nLinear / Identity 11   \nConv{1,2,3}D 11   \nSigmoid 11   \nTanh 53\\frac{5}{3}   \nReLU 2\\sqrt{2}   \nLeaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}   \nSELU 34\\frac{3}{4}     Parameters \n \nnonlinearity \u2013 the non-linear function (nn.functional name) \nparam \u2013 optional parameter for the non-linear function    Examples >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n \n"}, {"name": "torch.nn.init.constant_()", "path": "nn.init#torch.nn.init.constant_", "type": "torch.nn.init", "text": " \ntorch.nn.init.constant_(tensor, val) [source]\n \nFills the input Tensor with the value val\\text{val} .  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \nval \u2013 the value to fill the tensor with    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)\n \n"}, {"name": "torch.nn.init.dirac_()", "path": "nn.init#torch.nn.init.dirac_", "type": "torch.nn.init", "text": " \ntorch.nn.init.dirac_(tensor, groups=1) [source]\n \nFills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity  Parameters \n \ntensor \u2013 a {3, 4, 5}-dimensional torch.Tensor\n \ngroups (optional) \u2013 number of groups in the conv layer (default: 1)    Examples >>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)\n \n"}, {"name": "torch.nn.init.eye_()", "path": "nn.init#torch.nn.init.eye_", "type": "torch.nn.init", "text": " \ntorch.nn.init.eye_(tensor) [source]\n \nFills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.  Parameters \ntensor \u2013 a 2-dimensional torch.Tensor   Examples >>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)\n \n"}, {"name": "torch.nn.init.kaiming_normal_()", "path": "nn.init#torch.nn.init.kaiming_normal_", "type": "torch.nn.init", "text": " \ntorch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') [source]\n \nFills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where  std=gainfan_mode\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}  \nAlso known as He initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \na \u2013 the negative slope of the rectifier used after this layer (only used with 'leaky_relu') \nmode \u2013 either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. Choosing 'fan_out' preserves the magnitudes in the backwards pass. \nnonlinearity \u2013 the non-linear function (nn.functional name), recommended to use only with 'relu' or 'leaky_relu' (default).    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n \n"}, {"name": "torch.nn.init.kaiming_uniform_()", "path": "nn.init#torch.nn.init.kaiming_uniform_", "type": "torch.nn.init", "text": " \ntorch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu') [source]\n \nFills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})  where  bound=gain\u00d73fan_mode\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}  \nAlso known as He initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \na \u2013 the negative slope of the rectifier used after this layer (only used with 'leaky_relu') \nmode \u2013 either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. Choosing 'fan_out' preserves the magnitudes in the backwards pass. \nnonlinearity \u2013 the non-linear function (nn.functional name), recommended to use only with 'relu' or 'leaky_relu' (default).    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n \n"}, {"name": "torch.nn.init.normal_()", "path": "nn.init#torch.nn.init.normal_", "type": "torch.nn.init", "text": " \ntorch.nn.init.normal_(tensor, mean=0.0, std=1.0) [source]\n \nFills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \nmean \u2013 the mean of the normal distribution \nstd \u2013 the standard deviation of the normal distribution    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)\n \n"}, {"name": "torch.nn.init.ones_()", "path": "nn.init#torch.nn.init.ones_", "type": "torch.nn.init", "text": " \ntorch.nn.init.ones_(tensor) [source]\n \nFills the input Tensor with the scalar value 1.  Parameters \ntensor \u2013 an n-dimensional torch.Tensor   Examples >>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)\n \n"}, {"name": "torch.nn.init.orthogonal_()", "path": "nn.init#torch.nn.init.orthogonal_", "type": "torch.nn.init", "text": " \ntorch.nn.init.orthogonal_(tensor, gain=1) [source]\n \nFills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor, where n\u22652n \\geq 2 \n \ngain \u2013 optional scaling factor    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)\n \n"}, {"name": "torch.nn.init.sparse_()", "path": "nn.init#torch.nn.init.sparse_", "type": "torch.nn.init", "text": " \ntorch.nn.init.sparse_(tensor, sparsity, std=0.01) [source]\n \nFills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as described in Deep learning via Hessian-free optimization - Martens, J. (2010).  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \nsparsity \u2013 The fraction of elements in each column to be set to zero \nstd \u2013 the standard deviation of the normal distribution used to generate the non-zero values    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)\n \n"}, {"name": "torch.nn.init.uniform_()", "path": "nn.init#torch.nn.init.uniform_", "type": "torch.nn.init", "text": " \ntorch.nn.init.uniform_(tensor, a=0.0, b=1.0) [source]\n \nFills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b) .  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \na \u2013 the lower bound of the uniform distribution \nb \u2013 the upper bound of the uniform distribution    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)\n \n"}, {"name": "torch.nn.init.xavier_normal_()", "path": "nn.init#torch.nn.init.xavier_normal_", "type": "torch.nn.init", "text": " \ntorch.nn.init.xavier_normal_(tensor, gain=1.0) [source]\n \nFills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where  std=gain\u00d72fan_in+fan_out\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}  \nAlso known as Glorot initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \ngain \u2013 an optional scaling factor    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)\n \n"}, {"name": "torch.nn.init.xavier_uniform_()", "path": "nn.init#torch.nn.init.xavier_uniform_", "type": "torch.nn.init", "text": " \ntorch.nn.init.xavier_uniform_(tensor, gain=1.0) [source]\n \nFills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a)  where  a=gain\u00d76fan_in+fan_outa = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}  \nAlso known as Glorot initialization.  Parameters \n \ntensor \u2013 an n-dimensional torch.Tensor\n \ngain \u2013 an optional scaling factor    Examples >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n \n"}, {"name": "torch.nn.init.zeros_()", "path": "nn.init#torch.nn.init.zeros_", "type": "torch.nn.init", "text": " \ntorch.nn.init.zeros_(tensor) [source]\n \nFills the input Tensor with the scalar value 0.  Parameters \ntensor \u2013 an n-dimensional torch.Tensor   Examples >>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)\n \n"}, {"name": "torch.nn.InstanceNorm1d", "path": "generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d", "type": "torch.nn", "text": " \nclass torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) [source]\n \nApplies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.   Note InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don\u2019t apply affine transform.   Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,L)(N, C, L)  or LL  from input of size (N,L)(N, L) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nmomentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False\n     Shape:\n\n Input: (N,C,L)(N, C, L) \n Output: (N,C,L)(N, C, L)  (same shape as input)    Examples: >>> # Without Learnable Parameters\n>>> m = nn.InstanceNorm1d(100)\n>>> # With Learnable Parameters\n>>> m = nn.InstanceNorm1d(100, affine=True)\n>>> input = torch.randn(20, 100, 40)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.InstanceNorm2d", "path": "generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d", "type": "torch.nn", "text": " \nclass torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) [source]\n \nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.   Note InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don\u2019t apply affine transform.   Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,H,W)(N, C, H, W) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nmomentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False\n     Shape:\n\n Input: (N,C,H,W)(N, C, H, W) \n Output: (N,C,H,W)(N, C, H, W)  (same shape as input)    Examples: >>> # Without Learnable Parameters\n>>> m = nn.InstanceNorm2d(100)\n>>> # With Learnable Parameters\n>>> m = nn.InstanceNorm2d(100, affine=True)\n>>> input = torch.randn(20, 100, 35, 45)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.InstanceNorm3d", "path": "generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d", "type": "torch.nn", "text": " \nclass torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) [source]\n \nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.   Note InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don\u2019t apply affine transform.   Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,D,H,W)(N, C, D, H, W) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nmomentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False\n     Shape:\n\n Input: (N,C,D,H,W)(N, C, D, H, W) \n Output: (N,C,D,H,W)(N, C, D, H, W)  (same shape as input)    Examples: >>> # Without Learnable Parameters\n>>> m = nn.InstanceNorm3d(100)\n>>> # With Learnable Parameters\n>>> m = nn.InstanceNorm3d(100, affine=True)\n>>> input = torch.randn(20, 100, 35, 45, 10)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.intrinsic.ConvBn1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.ConvBn1d(conv, bn) [source]\n \nThis is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.nn.intrinsic.ConvBn2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.ConvBn2d(conv, bn) [source]\n \nThis is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.nn.intrinsic.ConvBnReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.ConvBnReLU1d(conv, bn, relu) [source]\n \nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.nn.intrinsic.ConvBnReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.ConvBnReLU2d(conv, bn, relu) [source]\n \nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.nn.intrinsic.ConvReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.ConvReLU1d(conv, relu) [source]\n \nThis is a sequential container which calls the Conv1d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.nn.intrinsic.ConvReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.ConvReLU2d(conv, relu) [source]\n \nThis is a sequential container which calls the Conv2d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.nn.intrinsic.qat.ConvBn2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables \n \n~ConvBn2d.freeze_bn \u2013  \n~ConvBn2d.weight_fake_quant \u2013 fake quant module for weight    \n"}, {"name": "torch.nn.intrinsic.qat.ConvBnReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d and torch.nn.ReLU. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables \n~ConvBnReLU2d.weight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.nn.intrinsic.qat.ConvReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None) [source]\n \nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. We combined the interface of Conv2d and BatchNorm2d.  Variables \n~ConvReLU2d.weight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.nn.intrinsic.qat.LinearReLU", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None) [source]\n \nA LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training. We adopt the same interface as torch.nn.Linear. Similar to torch.nn.intrinsic.LinearReLU, with FakeQuantize modules initialized to default.  Variables \n~LinearReLU.weight \u2013 fake quant module for weight   Examples: >>> m = nn.qat.LinearReLU(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n \n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU2d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nA ConvReLU2d module is a fused module of Conv2d and ReLU We adopt the same interface as torch.nn.quantized.Conv2d.  Variables \nas torch.nn.quantized.Conv2d (Same) \u2013    \n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU3d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nA ConvReLU3d module is a fused module of Conv3d and ReLU We adopt the same interface as torch.nn.quantized.Conv3d. Attributes: Same as torch.nn.quantized.Conv3d \n"}, {"name": "torch.nn.intrinsic.quantized.LinearReLU", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": " \nclass torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8) [source]\n \nA LinearReLU module fused from Linear and ReLU modules We adopt the same interface as torch.nn.quantized.Linear.  Variables \nas torch.nn.quantized.Linear (Same) \u2013    Examples: >>> m = nn.intrinsic.LinearReLU(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n \n"}, {"name": "torch.nn.KLDivLoss", "path": "generated/torch.nn.kldivloss#torch.nn.KLDivLoss", "type": "torch.nn", "text": " \nclass torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False) [source]\n \nThe Kullback-Leibler divergence loss measure Kullback-Leibler divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions. As with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are interpreted as probabilities by default, but could be considered as log-probabilities with log_target set to True. This criterion expects a target Tensor of the same size as the input Tensor. The unreduced (i.e. with reduction set to 'none') loss can be described as:  l(x,y)=L={l1,\u2026,lN},ln=yn\u22c5(log\u2061yn\u2212xn)l(x,y) = L = \\{ l_1,\\dots,l_N \\}, \\quad l_n = y_n \\cdot \\left( \\log y_n - x_n \\right)  \nwhere the index NN  spans all dimensions of input and LL  has the same shape as input. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';} \\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nIn default reduction mode 'mean', the losses are averaged for each minibatch over observations as well as over dimensions. 'batchmean' mode gives the correct KL divergence where losses are averaged over batch dimension only. 'mean' mode\u2019s behavior will be changed to the same as 'batchmean' in the next major release.  Parameters \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied. 'batchmean': the sum of the output will be divided by batchsize. 'sum': the output will be summed. 'mean': the output will be divided by the number of elements in the output. Default: 'mean'\n \nlog_target (bool, optional) \u2013 Specifies whether target is passed in the log space. Default: False\n     Note size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.   Note reduction = 'mean' doesn\u2019t return the true kl divergence value, please use reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as 'batchmean'.   Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Output: scalar by default. If :attr:reduction is 'none', then (N,\u2217)(N, *) , the same shape as the input    \n"}, {"name": "torch.nn.L1Loss", "path": "generated/torch.nn.l1loss#torch.nn.L1Loss", "type": "torch.nn", "text": " \nclass torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that measures the mean absolute error (MAE) between each element in the input xx  and target yy . The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2223xn\u2212yn\u2223,\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left| x_n - y_n \\right|,  \nwhere NN  is the batch size. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nxx  and yy  are tensors of arbitrary shapes with a total of nn  elements each. The sum operation still operates over all the elements, and divides by nn . The division by nn  can be avoided if one sets reduction = 'sum'. Supports real-valued and complex-valued inputs.  Parameters \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *) , same shape as the input    Examples: >>> loss = nn.L1Loss()\n>>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.randn(3, 5)\n>>> output = loss(input, target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.LayerNorm", "path": "generated/torch.nn.layernorm#torch.nn.LayerNorm", "type": "torch.nn", "text": " \nclass torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True) [source]\n \nApplies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  \nThe mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape. \u03b3\\gamma  and \u03b2\\beta  are learnable affine transform parameters of normalized_shape if elementwise_affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).  Note Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine.  This layer uses statistics computed from input data in both training and evaluation modes.  Parameters \n \nnormalized_shape (int or list or torch.Size) \u2013 \ninput shape from an expected input of size  [\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1] \\times \\ldots \\times \\text{normalized\\_shape}[-1]]  \nIf a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.  \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 \nelementwise_affine \u2013 a boolean value that when set to True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.     Shape:\n\n Input: (N,\u2217)(N, *) \n Output: (N,\u2217)(N, *)  (same shape as input)    Examples: >>> input = torch.randn(20, 5, 10, 10)\n>>> # With Learnable Parameters\n>>> m = nn.LayerNorm(input.size()[1:])\n>>> # Without Learnable Parameters\n>>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n>>> # Normalize over last two dimensions\n>>> m = nn.LayerNorm([10, 10])\n>>> # Normalize over last dimension of size 10\n>>> m = nn.LayerNorm(10)\n>>> # Activating the module\n>>> output = m(input)\n \n"}, {"name": "torch.nn.LazyConv1d", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d", "type": "torch.nn", "text": " \nclass torch.nn.LazyConv1d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nA torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).  Parameters \n \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 \npadding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n     See also torch.nn.Conv1d and torch.nn.modules.lazy.LazyModuleMixin   \ncls_to_become  \nalias of Conv1d \n \n"}, {"name": "torch.nn.LazyConv1d.cls_to_become", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of Conv1d \n"}, {"name": "torch.nn.LazyConv2d", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d", "type": "torch.nn", "text": " \nclass torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nA torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).  Parameters \n \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 \npadding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n     See also torch.nn.Conv2d and torch.nn.modules.lazy.LazyModuleMixin   \ncls_to_become  \nalias of Conv2d \n \n"}, {"name": "torch.nn.LazyConv2d.cls_to_become", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of Conv2d \n"}, {"name": "torch.nn.LazyConv3d", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d", "type": "torch.nn", "text": " \nclass torch.nn.LazyConv3d(out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nA torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).  Parameters \n \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 \npadding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n     See also torch.nn.Conv3d and torch.nn.modules.lazy.LazyModuleMixin   \ncls_to_become  \nalias of Conv3d \n \n"}, {"name": "torch.nn.LazyConv3d.cls_to_become", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of Conv3d \n"}, {"name": "torch.nn.LazyConvTranspose1d", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d", "type": "torch.nn", "text": " \nclass torch.nn.LazyConvTranspose1d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros') [source]\n \nA torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).  Parameters \n \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of the input. Default: 0 \noutput_padding (int or tuple, optional) \u2013 Additional size added to one side of the output shape. Default: 0 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     See also torch.nn.ConvTranspose1d and torch.nn.modules.lazy.LazyModuleMixin   \ncls_to_become  \nalias of ConvTranspose1d \n \n"}, {"name": "torch.nn.LazyConvTranspose1d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of ConvTranspose1d \n"}, {"name": "torch.nn.LazyConvTranspose2d", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d", "type": "torch.nn", "text": " \nclass torch.nn.LazyConvTranspose2d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros') [source]\n \nA torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).  Parameters \n \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 \noutput_padding (int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     See also torch.nn.ConvTranspose2d and torch.nn.modules.lazy.LazyModuleMixin   \ncls_to_become  \nalias of ConvTranspose2d \n \n"}, {"name": "torch.nn.LazyConvTranspose2d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of ConvTranspose2d \n"}, {"name": "torch.nn.LazyConvTranspose3d", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d", "type": "torch.nn", "text": " \nclass torch.nn.LazyConvTranspose3d(out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros') [source]\n \nA torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).  Parameters \n \nout_channels (int) \u2013 Number of channels produced by the convolution \nkernel_size (int or tuple) \u2013 Size of the convolving kernel \nstride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 \npadding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 \noutput_padding (int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 \ngroups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 \nbias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True\n \ndilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     See also torch.nn.ConvTranspose3d and torch.nn.modules.lazy.LazyModuleMixin   \ncls_to_become  \nalias of ConvTranspose3d \n \n"}, {"name": "torch.nn.LazyConvTranspose3d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of ConvTranspose3d \n"}, {"name": "torch.nn.LazyLinear", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear", "type": "torch.nn", "text": " \nclass torch.nn.LazyLinear(out_features, bias=True) [source]\n \nA torch.nn.Linear module with lazy initialization. In this module, the weight and bias are of torch.nn.UninitializedParameter class. They will be initialized after the first call to forward is done and the module will become a regular torch.nn.Linear module. Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.  Parameters \n \nout_features \u2013 size of each output sample \nbias \u2013 If set to False, the layer will not learn an additive bias. Default: True\n   Variables \n \n~LazyLinear.weight \u2013 the learnable weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}) . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) , where k=1in_featuresk = \\frac{1}{\\text{in\\_features}} \n \n~LazyLinear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features}) . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1in_featuresk = \\frac{1}{\\text{in\\_features}} \n     \ncls_to_become  \nalias of Linear \n \n"}, {"name": "torch.nn.LazyLinear.cls_to_become", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear.cls_to_become", "type": "torch.nn", "text": " \ncls_to_become  \nalias of Linear \n"}, {"name": "torch.nn.LeakyReLU", "path": "generated/torch.nn.leakyrelu#torch.nn.LeakyReLU", "type": "torch.nn", "text": " \nclass torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) [source]\n \nApplies the element-wise function:  LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  \nor  LeakyRELU(x)={x, if x\u22650negative_slope\u00d7x, otherwise \\text{LeakyRELU}(x) = \\begin{cases} x, & \\text{ if } x \\geq 0 \\\\ \\text{negative\\_slope} \\times x, & \\text{ otherwise } \\end{cases}  \n Parameters \n \nnegative_slope \u2013 Controls the angle of the negative slope. Default: 1e-2 \ninplace \u2013 can optionally do the operation in-place. Default: False\n     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.LeakyReLU(0.1)\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Linear", "path": "generated/torch.nn.linear#torch.nn.Linear", "type": "torch.nn", "text": " \nclass torch.nn.Linear(in_features, out_features, bias=True) [source]\n \nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b  This module supports TensorFloat32.  Parameters \n \nin_features \u2013 size of each input sample \nout_features \u2013 size of each output sample \nbias \u2013 If set to False, the layer will not learn an additive bias. Default: True\n     Shape:\n\n Input: (N,\u2217,Hin)(N, *, H_{in})  where \u2217*  means any number of additional dimensions and Hin=in_featuresH_{in} = \\text{in\\_features} \n Output: (N,\u2217,Hout)(N, *, H_{out})  where all but the last dimension are the same shape as the input and Hout=out_featuresH_{out} = \\text{out\\_features} .     Variables \n \n~Linear.weight \u2013 the learnable weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}) . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) , where k=1in_featuresk = \\frac{1}{\\text{in\\_features}} \n \n~Linear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features}) . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1in_featuresk = \\frac{1}{\\text{in\\_features}} \n    Examples: >>> m = nn.Linear(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n \n"}, {"name": "torch.nn.LocalResponseNorm", "path": "generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm", "type": "torch.nn", "text": " \nclass torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0) [source]\n \nApplies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.  bc=ac(k+\u03b1n\u2211c\u2032=max\u2061(0,c\u2212n/2)min\u2061(N\u22121,c+n/2)ac\u20322)\u2212\u03b2b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} \\sum_{c'=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c'}^2\\right)^{-\\beta}  \n Parameters \n \nsize \u2013 amount of neighbouring channels used for normalization \nalpha \u2013 multiplicative factor. Default: 0.0001 \nbeta \u2013 exponent. Default: 0.75 \nk \u2013 additive factor. Default: 1     Shape:\n\n Input: (N,C,\u2217)(N, C, *) \n Output: (N,C,\u2217)(N, C, *)  (same shape as input)    Examples: >>> lrn = nn.LocalResponseNorm(2)\n>>> signal_2d = torch.randn(32, 5, 24, 24)\n>>> signal_4d = torch.randn(16, 5, 7, 7, 7, 7)\n>>> output_2d = lrn(signal_2d)\n>>> output_4d = lrn(signal_4d)\n \n"}, {"name": "torch.nn.LogSigmoid", "path": "generated/torch.nn.logsigmoid#torch.nn.LogSigmoid", "type": "torch.nn", "text": " \nclass torch.nn.LogSigmoid [source]\n \nApplies the element-wise function:  LogSigmoid(x)=log\u2061(11+exp\u2061(\u2212x))\\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)  \n Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.LogSigmoid()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.LogSoftmax", "path": "generated/torch.nn.logsoftmax#torch.nn.LogSoftmax", "type": "torch.nn", "text": " \nclass torch.nn.LogSoftmax(dim=None) [source]\n \nApplies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))  function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:  LogSoftmax(xi)=log\u2061(exp\u2061(xi)\u2211jexp\u2061(xj))\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)  \n Shape:\n\n Input: (\u2217)(*)  where * means, any number of additional dimensions Output: (\u2217)(*) , same shape as the input     Parameters \ndim (int) \u2013 A dimension along which LogSoftmax will be computed.  Returns \na Tensor of the same dimension and shape as the input with values in the range [-inf, 0)   Examples: >>> m = nn.LogSoftmax()\n>>> input = torch.randn(2, 3)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.LPPool1d", "path": "generated/torch.nn.lppool1d#torch.nn.LPPool1d", "type": "torch.nn", "text": " \nclass torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False) [source]\n \nApplies a 1D power-average pooling over an input signal composed of several input planes. On each window, the function computed is:  f(X)=\u2211x\u2208Xxppf(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}  \n At p = \u221e\\infty , one gets Max Pooling At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)   Note If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.   Parameters \n \nkernel_size \u2013 a single int, the size of the window \nstride \u2013 a single int, the stride of the window. Default value is kernel_size\n \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape:\n\n Input: (N,C,Lin)(N, C, L_{in}) \n \nOutput: (N,C,Lout)(N, C, L_{out}) , where  Lout=\u230aLin\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  \n   Examples::\n\n>>> # power-2 pool of window of length 3, with stride 2.\n>>> m = nn.LPPool1d(2, 3, stride=2)\n>>> input = torch.randn(20, 16, 50)\n>>> output = m(input)\n   \n"}, {"name": "torch.nn.LPPool2d", "path": "generated/torch.nn.lppool2d#torch.nn.LPPool2d", "type": "torch.nn", "text": " \nclass torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False) [source]\n \nApplies a 2D power-average pooling over an input signal composed of several input planes. On each window, the function computed is:  f(X)=\u2211x\u2208Xxppf(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}  \n At p = \u221e\\infty , one gets Max Pooling At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size, stride can either be:  a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension   Note If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.   Parameters \n \nkernel_size \u2013 the size of the window \nstride \u2013 the stride of the window. Default value is kernel_size\n \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where  Hout=\u230aHin\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  \n Wout=\u230aWin\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  \n    Examples: >>> # power-2 pool of square window of size=3, stride=2\n>>> m = nn.LPPool2d(2, 3, stride=2)\n>>> # pool of non-square window of power 1.2\n>>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))\n>>> input = torch.randn(20, 16, 50, 32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.LSTM", "path": "generated/torch.nn.lstm#torch.nn.LSTM", "type": "torch.nn", "text": " \nclass torch.nn.LSTM(*args, **kwargs) [source]\n \nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  it=\u03c3(Wiixt+bii+Whiht\u22121+bhi)ft=\u03c3(Wifxt+bif+Whfht\u22121+bhf)gt=tanh\u2061(Wigxt+big+Whght\u22121+bhg)ot=\u03c3(Wioxt+bio+Whoht\u22121+bho)ct=ft\u2299ct\u22121+it\u2299gtht=ot\u2299tanh\u2061(ct)\\begin{array}{ll} \\\\ i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\ f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\ o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\ c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\ h_t = o_t \\odot \\tanh(c_t) \\\\ \\end{array}  \nwhere hth_t  is the hidden state at time t, ctc_t  is the cell state at time t, xtx_t  is the input at time t, ht\u22121h_{t-1}  is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and iti_t , ftf_t , gtg_t , oto_t  are the input, forget, cell, and output gates, respectively. \u03c3\\sigma  is the sigmoid function, and \u2299\\odot  is the Hadamard product. In a multilayer LSTM, the input xt(l)x^{(l)}_t  of the ll  -th layer (l>=2l >= 2 ) is the hidden state ht(l\u22121)h^{(l-1)}_t  of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t  where each \u03b4t(l\u22121)\\delta^{(l-1)}_t  is a Bernoulli random variable which is 00  with probability dropout. If proj_size > 0 is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the dimension of hth_t  will be changed from hidden_size to proj_size (dimensions of WhiW_{hi}  will be changed accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: ht=Whrhth_t = W_{hr}h_t . Note that as a consequence of this, the output of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nnum_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n \nbatch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n \ndropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0 \nbidirectional \u2013 If True, becomes a bidirectional LSTM. Default: False\n \nproj_size \u2013 If > 0, will use LSTM with projections of corresponding size. Default: 0     Inputs: input, (h_0, c_0)\n\n \ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. \nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1. If proj_size > 0 was specified, the shape has to be (num_layers * num_directions, batch, proj_size). \nc_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.    Outputs: output, (h_n, c_n)\n\n \noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. If proj_size > 0 was specified, output shape will be (seq_len, batch, num_directions * proj_size). For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  \nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. If proj_size > 0 was specified, h_n shape will be (num_layers * num_directions, batch, proj_size). Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n.  \nc_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len.     Variables \n \n~LSTM.weight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th}  layer (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0. Otherwise, the shape is (4*hidden_size, num_directions * hidden_size)\n \n~LSTM.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th}  layer (W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size). If proj_size > 0 was specified, the shape will be (4*hidden_size, proj_size). \n~LSTM.bias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th}  layer (b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)\n \n~LSTM.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th}  layer (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)\n \n~LSTM.weight_hr_l[k] \u2013 the learnable projection weights of the kth\\text{k}^{th}  layer of shape (proj_size, hidden_size). Only present when proj_size > 0 was specified.     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}    Warning There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables: On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1. This may affect performance. On CUDA 10.2 or later, set environment variable (note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2. See the cuDNN 8 Release Notes for more information.   Orphan   Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: >>> rnn = nn.LSTM(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> c0 = torch.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))\n \n"}, {"name": "torch.nn.LSTMCell", "path": "generated/torch.nn.lstmcell#torch.nn.LSTMCell", "type": "torch.nn", "text": " \nclass torch.nn.LSTMCell(input_size, hidden_size, bias=True) [source]\n \nA long short-term memory (LSTM) cell.  i=\u03c3(Wiix+bii+Whih+bhi)f=\u03c3(Wifx+bif+Whfh+bhf)g=tanh\u2061(Wigx+big+Whgh+bhg)o=\u03c3(Wiox+bio+Whoh+bho)c\u2032=f\u2217c+i\u2217gh\u2032=o\u2217tanh\u2061(c\u2032)\\begin{array}{ll} i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\ f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\ g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\ o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\ c' = f * c + i * g \\\\ h' = o * \\tanh(c') \\\\ \\end{array} \nwhere \u03c3\\sigma  is the sigmoid function, and \u2217*  is the Hadamard product.  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n     Inputs: input, (h_0, c_0)\n\n \ninput of shape (batch, input_size): tensor containing input features \nh_0 of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. \nc_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.    Outputs: (h_1, c_1)\n\n \nh_1 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch \nc_1 of shape (batch, hidden_size): tensor containing the next cell state for each element in the batch     Variables \n \n~LSTMCell.weight_ih \u2013 the learnable input-hidden weights, of shape (4*hidden_size, input_size)\n \n~LSTMCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (4*hidden_size, hidden_size)\n \n~LSTMCell.bias_ih \u2013 the learnable input-hidden bias, of shape (4*hidden_size)\n \n~LSTMCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (4*hidden_size)\n     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}   Examples: >>> rnn = nn.LSTMCell(10, 20)\n>>> input = torch.randn(3, 10)\n>>> hx = torch.randn(3, 20)\n>>> cx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx, cx = rnn(input[i], (hx, cx))\n        output.append(hx)\n \n"}, {"name": "torch.nn.MarginRankingLoss", "path": "generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss", "type": "torch.nn", "text": " \nclass torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy  (containing 1 or -1). If y=1y = 1  then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1 . The loss function for each pair of samples in the mini-batch is:  loss(x1,x2,y)=max\u2061(0,\u2212y\u2217(x1\u2212x2)+margin)\\text{loss}(x1, x2, y) = \\max(0, -y * (x1 - x2) + \\text{margin})  \n Parameters \n \nmargin (float, optional) \u2013 Has a default value of 00 . \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input1: (N)(N)  where N is the batch size. Input2: (N)(N) , same shape as the Input1. Target: (N)(N) , same shape as the inputs. Output: scalar. If reduction is 'none', then (N)(N) .    Examples: >>> loss = nn.MarginRankingLoss()\n>>> input1 = torch.randn(3, requires_grad=True)\n>>> input2 = torch.randn(3, requires_grad=True)\n>>> target = torch.randn(3).sign()\n>>> output = loss(input1, input2, target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.MaxPool1d", "path": "generated/torch.nn.maxpool1d#torch.nn.MaxPool1d", "type": "torch.nn", "text": " \nclass torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) [source]\n \nApplies a 1D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L)  and output (N,C,Lout)(N, C, L_{out})  can be precisely described as:  out(Ni,Cj,k)=max\u2061m=0,\u2026,kernel_size\u22121input(Ni,Cj,stride\u00d7k+m)out(N_i, C_j, k) = \\max_{m=0, \\ldots, \\text{kernel\\_size} - 1} input(N_i, C_j, stride \\times k + m)  \nIf padding is non-zero, then the input is implicitly padded with negative infinity on both sides for padding number of points. dilation is the stride between the elements within the sliding window. This link has a nice visualization of the pooling parameters.  Note When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.   Parameters \n \nkernel_size \u2013 The size of the sliding window, must be > 0. \nstride \u2013 The stride of the sliding window, must be > 0. Default value is kernel_size. \npadding \u2013 Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2. \ndilation \u2013 The stride between elements within a sliding window, must be > 0. \nreturn_indices \u2013 If True, will return the argmax along with the max values. Useful for torch.nn.MaxUnpool1d later \nceil_mode \u2013 If True, will use ceil instead of floor to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window.     Shape:\n\n Input: (N,C,Lin)(N, C, L_{in}) \n \nOutput: (N,C,Lout)(N, C, L_{out}) , where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  \n    Examples: >>> # pool of size=3, stride=2\n>>> m = nn.MaxPool1d(3, stride=2)\n>>> input = torch.randn(20, 16, 50)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.MaxPool2d", "path": "generated/torch.nn.maxpool2d#torch.nn.MaxPool2d", "type": "torch.nn", "text": " \nclass torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) [source]\n \nApplies a 2D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  and kernel_size (kH,kW)(kH, kW)  can be precisely described as:  out(Ni,Cj,h,w)=max\u2061m=0,\u2026,kH\u22121max\u2061n=0,\u2026,kW\u22121input(Ni,Cj,stride[0]\u00d7h+m,stride[1]\u00d7w+n)\\begin{aligned} out(N_i, C_j, h, w) ={} & \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\ & \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m, \\text{stride[1]} \\times w + n) \\end{aligned}  \nIf padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.  Note When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size, stride, padding, dilation can either be:  a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension   Parameters \n \nkernel_size \u2013 the size of the window to take a max over \nstride \u2013 the stride of the window. Default value is kernel_size\n \npadding \u2013 implicit zero padding to be added on both sides \ndilation \u2013 a parameter that controls the stride of elements in the window \nreturn_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where  Hout=\u230aHin+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]} \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor  \n Wout=\u230aWin+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]} \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor  \n    Examples: >>> # pool of square window of size=3, stride=2\n>>> m = nn.MaxPool2d(3, stride=2)\n>>> # pool of non-square window\n>>> m = nn.MaxPool2d((3, 2), stride=(2, 1))\n>>> input = torch.randn(20, 16, 50, 32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.MaxPool3d", "path": "generated/torch.nn.maxpool3d#torch.nn.MaxPool3d", "type": "torch.nn", "text": " \nclass torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) [source]\n \nApplies a 3D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  and kernel_size (kD,kH,kW)(kD, kH, kW)  can be precisely described as:  out(Ni,Cj,d,h,w)=max\u2061k=0,\u2026,kD\u22121max\u2061m=0,\u2026,kH\u22121max\u2061n=0,\u2026,kW\u22121input(Ni,Cj,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\\begin{aligned} \\text{out}(N_i, C_j, d, h, w) ={} & \\max_{k=0, \\ldots, kD-1} \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\ & \\text{input}(N_i, C_j, \\text{stride[0]} \\times d + k, \\text{stride[1]} \\times h + m, \\text{stride[2]} \\times w + n) \\end{aligned}  \nIf padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.  Note When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.  The parameters kernel_size, stride, padding, dilation can either be:  a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension   Parameters \n \nkernel_size \u2013 the size of the window to take a max over \nstride \u2013 the stride of the window. Default value is kernel_size\n \npadding \u2013 implicit zero padding to be added on all three sides \ndilation \u2013 a parameter that controls the stride of elements in the window \nreturn_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool3d later \nceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape:\n\n Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in}) \n \nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  \n Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  \n Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  \n    Examples: >>> # pool of square window of size=3, stride=2\n>>> m = nn.MaxPool3d(3, stride=2)\n>>> # pool of non-square window\n>>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))\n>>> input = torch.randn(20, 16, 50,44, 31)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.MaxUnpool1d", "path": "generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d", "type": "torch.nn", "text": " \nclass torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0) [source]\n \nComputes a partial inverse of MaxPool1d. MaxPool1d is not fully invertible, since the non-maximal values are lost. MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool1d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.   Parameters \n \nkernel_size (int or tuple) \u2013 Size of the max pooling window. \nstride (int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. \npadding (int or tuple) \u2013 Padding that was added to the input     Inputs:\n\n \ninput: the input Tensor to invert \nindices: the indices given out by MaxPool1d\n \noutput_size (optional): the targeted output size   Shape:\n\n Input: (N,C,Hin)(N, C, H_{in}) \n \nOutput: (N,C,Hout)(N, C, H_{out}) , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]  \nor as given by output_size in the call operator     Example: >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool1d(2, stride=2)\n>>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n>>> output, indices = pool(input)\n>>> unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n>>> # Example showcasing the use of output_size\n>>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])\n>>> output, indices = pool(input)\n>>> unpool(output, indices, output_size=input.size())\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])\n\n>>> unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n \n"}, {"name": "torch.nn.MaxUnpool2d", "path": "generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d", "type": "torch.nn", "text": " \nclass torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0) [source]\n \nComputes a partial inverse of MaxPool2d. MaxPool2d is not fully invertible, since the non-maximal values are lost. MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool2d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.   Parameters \n \nkernel_size (int or tuple) \u2013 Size of the max pooling window. \nstride (int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. \npadding (int or tuple) \u2013 Padding that was added to the input     Inputs:\n\n \ninput: the input Tensor to invert \nindices: the indices given out by MaxPool2d\n \noutput_size (optional): the targeted output size   Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  \n Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  \nor as given by output_size in the call operator     Example: >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool2d(2, stride=2)\n>>> input = torch.tensor([[[[ 1.,  2,  3,  4],\n                            [ 5,  6,  7,  8],\n                            [ 9, 10, 11, 12],\n                            [13, 14, 15, 16]]]])\n>>> output, indices = pool(input)\n>>> unpool(output, indices)\ntensor([[[[  0.,   0.,   0.,   0.],\n          [  0.,   6.,   0.,   8.],\n          [  0.,   0.,   0.,   0.],\n          [  0.,  14.,   0.,  16.]]]])\n\n>>> # specify a different output size than input size\n>>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))\ntensor([[[[  0.,   0.,   0.,   0.,   0.],\n          [  6.,   0.,   8.,   0.,   0.],\n          [  0.,   0.,   0.,  14.,   0.],\n          [ 16.,   0.,   0.,   0.,   0.],\n          [  0.,   0.,   0.,   0.,   0.]]]])\n \n"}, {"name": "torch.nn.MaxUnpool3d", "path": "generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d", "type": "torch.nn", "text": " \nclass torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0) [source]\n \nComputes a partial inverse of MaxPool3d. MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool3d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs section below.   Parameters \n \nkernel_size (int or tuple) \u2013 Size of the max pooling window. \nstride (int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. \npadding (int or tuple) \u2013 Padding that was added to the input     Inputs:\n\n \ninput: the input Tensor to invert \nindices: the indices given out by MaxPool3d\n \noutput_size (optional): the targeted output size   Shape:\n\n Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in}) \n \nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where  Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  \n Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  \n Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}  \nor as given by output_size in the call operator     Example: >>> # pool of square window of size=3, stride=2\n>>> pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n>>> unpool = nn.MaxUnpool3d(3, stride=2)\n>>> output, indices = pool(torch.randn(20, 16, 51, 33, 15))\n>>> unpooled_output = unpool(output, indices)\n>>> unpooled_output.size()\ntorch.Size([20, 16, 51, 33, 15])\n \n"}, {"name": "torch.nn.Module", "path": "generated/torch.nn.module#torch.nn.Module", "type": "torch.nn", "text": " \nclass torch.nn.Module [source]\n \nBase class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n Submodules assigned in this way will be registered, and will have their parameters converted too when you call to(), etc.  Variables \ntraining (bool) \u2013 Boolean represents whether this module is in training or evaluation mode.    \nadd_module(name, module) [source]\n \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n  \napply(fn) [source]\n \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n  \nbfloat16() [source]\n \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n  \nbuffers(recurse=True) [source]\n \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nchildren() [source]\n \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n  \ncpu() [source]\n \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n  \ncuda(device=None) [source]\n \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \ndouble() [source]\n \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n  \ndump_patches: bool = False  \nThis allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. \n  \neval() [source]\n \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n  \nextra_repr() [source]\n \nSet the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \n  \nfloat() [source]\n \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n  \nforward(*input)  \nDefines the computation performed at every call. Should be overridden by all subclasses.  Note Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.  \n  \nhalf() [source]\n \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n  \nload_state_dict(state_dict, strict=True) [source]\n \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n  \nmodules() [source]\n \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n  \nnamed_buffers(prefix='', recurse=True) [source]\n \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n  \nnamed_children() [source]\n \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n  \nnamed_modules(memo=None, prefix='') [source]\n \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n  \nnamed_parameters(prefix='', recurse=True) [source]\n \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n  \nparameters(recurse=True) [source]\n \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nregister_backward_hook(hook) [source]\n \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_buffer(name, tensor, persistent=True) [source]\n \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n  \nregister_forward_hook(hook) [source]\n \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_forward_pre_hook(hook) [source]\n \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_full_backward_hook(hook) [source]\n \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_parameter(name, param) [source]\n \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n  \nrequires_grad_(requires_grad=True) [source]\n \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n  \nstate_dict(destination=None, prefix='', keep_vars=False) [source]\n \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n  \nto(*args, **kwargs) [source]\n \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) [source]\n\n  \nto(dtype, non_blocking=False) [source]\n\n  \nto(tensor, non_blocking=False) [source]\n\n  \nto(memory_format=torch.channels_last) [source]\n\n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n  \ntrain(mode=True) [source]\n \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n  \ntype(dst_type) [source]\n \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n  \nxpu(device=None) [source]\n \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \nzero_grad(set_to_none=False) [source]\n \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n \n"}, {"name": "torch.nn.Module.add_module()", "path": "generated/torch.nn.module#torch.nn.Module.add_module", "type": "torch.nn", "text": " \nadd_module(name, module) [source]\n \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n"}, {"name": "torch.nn.Module.apply()", "path": "generated/torch.nn.module#torch.nn.Module.apply", "type": "torch.nn", "text": " \napply(fn) [source]\n \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n"}, {"name": "torch.nn.Module.bfloat16()", "path": "generated/torch.nn.module#torch.nn.Module.bfloat16", "type": "torch.nn", "text": " \nbfloat16() [source]\n \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.buffers()", "path": "generated/torch.nn.module#torch.nn.Module.buffers", "type": "torch.nn", "text": " \nbuffers(recurse=True) [source]\n \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.nn.Module.children()", "path": "generated/torch.nn.module#torch.nn.Module.children", "type": "torch.nn", "text": " \nchildren() [source]\n \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n"}, {"name": "torch.nn.Module.cpu()", "path": "generated/torch.nn.module#torch.nn.Module.cpu", "type": "torch.nn", "text": " \ncpu() [source]\n \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.cuda()", "path": "generated/torch.nn.module#torch.nn.Module.cuda", "type": "torch.nn", "text": " \ncuda(device=None) [source]\n \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.double()", "path": "generated/torch.nn.module#torch.nn.Module.double", "type": "torch.nn", "text": " \ndouble() [source]\n \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.dump_patches", "path": "generated/torch.nn.module#torch.nn.Module.dump_patches", "type": "torch.nn", "text": " \ndump_patches: bool = False  \nThis allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. \n"}, {"name": "torch.nn.Module.eval()", "path": "generated/torch.nn.module#torch.nn.Module.eval", "type": "torch.nn", "text": " \neval() [source]\n \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.extra_repr()", "path": "generated/torch.nn.module#torch.nn.Module.extra_repr", "type": "torch.nn", "text": " \nextra_repr() [source]\n \nSet the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. \n"}, {"name": "torch.nn.Module.float()", "path": "generated/torch.nn.module#torch.nn.Module.float", "type": "torch.nn", "text": " \nfloat() [source]\n \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.forward()", "path": "generated/torch.nn.module#torch.nn.Module.forward", "type": "torch.nn", "text": " \nforward(*input)  \nDefines the computation performed at every call. Should be overridden by all subclasses.  Note Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.  \n"}, {"name": "torch.nn.Module.half()", "path": "generated/torch.nn.module#torch.nn.Module.half", "type": "torch.nn", "text": " \nhalf() [source]\n \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.load_state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.load_state_dict", "type": "torch.nn", "text": " \nload_state_dict(state_dict, strict=True) [source]\n \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n"}, {"name": "torch.nn.Module.modules()", "path": "generated/torch.nn.module#torch.nn.Module.modules", "type": "torch.nn", "text": " \nmodules() [source]\n \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n"}, {"name": "torch.nn.Module.named_buffers()", "path": "generated/torch.nn.module#torch.nn.Module.named_buffers", "type": "torch.nn", "text": " \nnamed_buffers(prefix='', recurse=True) [source]\n \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n"}, {"name": "torch.nn.Module.named_children()", "path": "generated/torch.nn.module#torch.nn.Module.named_children", "type": "torch.nn", "text": " \nnamed_children() [source]\n \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n"}, {"name": "torch.nn.Module.named_modules()", "path": "generated/torch.nn.module#torch.nn.Module.named_modules", "type": "torch.nn", "text": " \nnamed_modules(memo=None, prefix='') [source]\n \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n"}, {"name": "torch.nn.Module.named_parameters()", "path": "generated/torch.nn.module#torch.nn.Module.named_parameters", "type": "torch.nn", "text": " \nnamed_parameters(prefix='', recurse=True) [source]\n \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n"}, {"name": "torch.nn.Module.parameters()", "path": "generated/torch.nn.module#torch.nn.Module.parameters", "type": "torch.nn", "text": " \nparameters(recurse=True) [source]\n \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.nn.Module.register_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_backward_hook", "type": "torch.nn", "text": " \nregister_backward_hook(hook) [source]\n \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Module.register_buffer()", "path": "generated/torch.nn.module#torch.nn.Module.register_buffer", "type": "torch.nn", "text": " \nregister_buffer(name, tensor, persistent=True) [source]\n \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n"}, {"name": "torch.nn.Module.register_forward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_hook", "type": "torch.nn", "text": " \nregister_forward_hook(hook) [source]\n \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Module.register_forward_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_pre_hook", "type": "torch.nn", "text": " \nregister_forward_pre_hook(hook) [source]\n \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Module.register_full_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_full_backward_hook", "type": "torch.nn", "text": " \nregister_full_backward_hook(hook) [source]\n \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Module.register_parameter()", "path": "generated/torch.nn.module#torch.nn.Module.register_parameter", "type": "torch.nn", "text": " \nregister_parameter(name, param) [source]\n \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n"}, {"name": "torch.nn.Module.requires_grad_()", "path": "generated/torch.nn.module#torch.nn.Module.requires_grad_", "type": "torch.nn", "text": " \nrequires_grad_(requires_grad=True) [source]\n \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.state_dict", "type": "torch.nn", "text": " \nstate_dict(destination=None, prefix='', keep_vars=False) [source]\n \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n"}, {"name": "torch.nn.Module.to()", "path": "generated/torch.nn.module#torch.nn.Module.to", "type": "torch.nn", "text": " \nto(*args, **kwargs) [source]\n \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) [source]\n\n  \nto(dtype, non_blocking=False) [source]\n\n  \nto(tensor, non_blocking=False) [source]\n\n  \nto(memory_format=torch.channels_last) [source]\n\n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n"}, {"name": "torch.nn.Module.train()", "path": "generated/torch.nn.module#torch.nn.Module.train", "type": "torch.nn", "text": " \ntrain(mode=True) [source]\n \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.type()", "path": "generated/torch.nn.module#torch.nn.Module.type", "type": "torch.nn", "text": " \ntype(dst_type) [source]\n \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.xpu()", "path": "generated/torch.nn.module#torch.nn.Module.xpu", "type": "torch.nn", "text": " \nxpu(device=None) [source]\n \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Module.zero_grad()", "path": "generated/torch.nn.module#torch.nn.Module.zero_grad", "type": "torch.nn", "text": " \nzero_grad(set_to_none=False) [source]\n \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n"}, {"name": "torch.nn.ModuleDict", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict", "type": "torch.nn", "text": " \nclass torch.nn.ModuleDict(modules=None) [source]\n \nHolds submodules in a dictionary. ModuleDict can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all Module methods. ModuleDict is an ordered dictionary that respects  the order of insertion, and in update(), the order of the merged OrderedDict, dict (started from Python 3.6) or another ModuleDict (the argument to update()).  Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict before Python version 3.6) does not preserve the order of the merged mapping.  Parameters \nmodules (iterable, optional) \u2013 a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)   Example: class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n  \nclear() [source]\n \nRemove all items from the ModuleDict. \n  \nitems() [source]\n \nReturn an iterable of the ModuleDict key/value pairs. \n  \nkeys() [source]\n \nReturn an iterable of the ModuleDict keys. \n  \npop(key) [source]\n \nRemove key from the ModuleDict and return its module.  Parameters \nkey (string) \u2013 key to pop from the ModuleDict   \n  \nupdate(modules) [source]\n \nUpdate the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters \nmodules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)   \n  \nvalues() [source]\n \nReturn an iterable of the ModuleDict values. \n \n"}, {"name": "torch.nn.ModuleDict.clear()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.clear", "type": "torch.nn", "text": " \nclear() [source]\n \nRemove all items from the ModuleDict. \n"}, {"name": "torch.nn.ModuleDict.items()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.items", "type": "torch.nn", "text": " \nitems() [source]\n \nReturn an iterable of the ModuleDict key/value pairs. \n"}, {"name": "torch.nn.ModuleDict.keys()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.keys", "type": "torch.nn", "text": " \nkeys() [source]\n \nReturn an iterable of the ModuleDict keys. \n"}, {"name": "torch.nn.ModuleDict.pop()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.pop", "type": "torch.nn", "text": " \npop(key) [source]\n \nRemove key from the ModuleDict and return its module.  Parameters \nkey (string) \u2013 key to pop from the ModuleDict   \n"}, {"name": "torch.nn.ModuleDict.update()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.update", "type": "torch.nn", "text": " \nupdate(modules) [source]\n \nUpdate the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters \nmodules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)   \n"}, {"name": "torch.nn.ModuleDict.values()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.values", "type": "torch.nn", "text": " \nvalues() [source]\n \nReturn an iterable of the ModuleDict values. \n"}, {"name": "torch.nn.ModuleList", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList", "type": "torch.nn", "text": " \nclass torch.nn.ModuleList(modules=None) [source]\n \nHolds submodules in a list. ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.  Parameters \nmodules (iterable, optional) \u2013 an iterable of modules to add   Example: class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n  \nappend(module) [source]\n \nAppends a given module to the end of the list.  Parameters \nmodule (nn.Module) \u2013 module to append   \n  \nextend(modules) [source]\n \nAppends modules from a Python iterable to the end of the list.  Parameters \nmodules (iterable) \u2013 iterable of modules to append   \n  \ninsert(index, module) [source]\n \nInsert a given module before a given index in the list.  Parameters \n \nindex (int) \u2013 index to insert. \nmodule (nn.Module) \u2013 module to insert    \n \n"}, {"name": "torch.nn.ModuleList.append()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.append", "type": "torch.nn", "text": " \nappend(module) [source]\n \nAppends a given module to the end of the list.  Parameters \nmodule (nn.Module) \u2013 module to append   \n"}, {"name": "torch.nn.ModuleList.extend()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.extend", "type": "torch.nn", "text": " \nextend(modules) [source]\n \nAppends modules from a Python iterable to the end of the list.  Parameters \nmodules (iterable) \u2013 iterable of modules to append   \n"}, {"name": "torch.nn.ModuleList.insert()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.insert", "type": "torch.nn", "text": " \ninsert(index, module) [source]\n \nInsert a given module before a given index in the list.  Parameters \n \nindex (int) \u2013 index to insert. \nmodule (nn.Module) \u2013 module to insert    \n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin", "type": "torch.nn", "text": " \nclass torch.nn.modules.lazy.LazyModuleMixin(*args, **kwargs) [source]\n \nA mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d Modules that lazily initialize parameters, or \u201clazy modules\u201d, derive the shapes of their parameters from the first input(s) to their forward method. Until that first forward they contain torch.nn.UninitializedParameter`s that should not be accessed\nor used, and afterward they contain regular :class:`torch.nn.Parameter`s.\nLazy modules are convenient since they don't require computing some\nmodule arguments, like the `in_features argument of a typical torch.nn.Linear. After construction, networks with lazy modules should first be converted to the desired dtype and placed on the desired device. The lazy modules should then be initialized with one or more \u201cdry runs\u201d. These \u201cdry runs\u201d send inputs of the correct size, dtype, and device through the network and to each one of its lazy modules. After this the network can be used as usual. >>> class LazyMLP(torch.nn.Module):\n...    def __init__(self):\n...        super().__init__()\n...        self.fc1 = torch.nn.LazyLinear(10)\n...        self.relu1 = torch.nn.ReLU()\n...        self.fc2 = torch.nn.LazyLinear(1)\n...        self.relu2 = torch.nn.ReLU()\n...\n...    def forward(self, input):\n...        x = self.relu1(self.fc1(input))\n...        y = self.relu2(self.fc2(x))\n...        return y\n>>> # constructs a network with lazy modules\n>>> lazy_mlp = LazyMLP()\n>>> # transforms the network's device and dtype\n>>> # NOTE: these transforms can and should be applied after construction and before any 'dry runs'\n>>> lazy_mlp = mlp.cuda().double()\n>>> lazy_mlp\nLazyMLP(\n  (fc1): LazyLinear(in_features=0, out_features=10, bias=True)\n  (relu1): ReLU()\n  (fc2): LazyLinear(in_features=0, out_features=1, bias=True)\n  (relu2): ReLU()\n)\n>>> # performs a dry run to initialize the network's lazy modules\n>>> lazy_mlp(torch.ones(10,10).cuda())\n>>> # after initialization, LazyLinear modules become regular Linear modules\n>>> lazy_mlp\nLazyMLP(\n  (fc1): Linear(in_features=10, out_features=10, bias=True)\n  (relu1): ReLU()\n  (fc2): Linear(in_features=10, out_features=1, bias=True)\n  (relu2): ReLU()\n)\n>>> # attaches an optimizer, since parameters can now be used as usual\n>>> optim = torch.optim.SGD(mlp.parameters(), lr=0.01)\n A final caveat when using lazy modules is that the order of initialization of a network\u2019s parameters may change, since the lazy modules are always initialized after other modules. This can cause the parameters of a network using lazy modules to be initialized differently than the parameters of a network without lazy modules. For example, if the LazyMLP class defined above had a torch.nn.LazyLinear module first and then a regular torch.nn.Linear second, the second module would be initialized on construction and the first module would be initialized during the first dry run. Lazy modules can be serialized with a state dict like other modules. For example: >>> lazy_mlp = LazyMLP()\n>>> # The state dict shows the uninitialized parameters\n>>> lazy_mlp.state_dict()\nOrderedDict([('fc1.weight', Uninitialized parameter),\n             ('fc1.bias',\n              tensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,\n                       4.5637e-41, -1.8788e+22,  4.5636e-41, -2.0042e-31,  4.5637e-41])),\n             ('fc2.weight', Uninitialized parameter),\n             ('fc2.bias', tensor([0.0019]))])\n Lazy modules can also load regular torch.nn.Parameter s, which replace their torch.nn.UninitializedParameter s: >>> full_mlp = LazyMLP()\n>>> # Dry run to initialize another module\n>>> full_mlp.forward(torch.ones(10, 1))\n>>> # Load an initialized state into a lazy module\n>>> lazy_mlp.load_state_dict(full_mlp.state_dict())\n>>> # The state dict now holds valid values\n>>> lazy_mlp.state_dict()\nOrderedDict([('fc1.weight',\n              tensor([[-0.3837],\n                      [ 0.0907],\n                      [ 0.6708],\n                      [-0.5223],\n                      [-0.9028],\n                      [ 0.2851],\n                      [-0.4537],\n                      [ 0.6813],\n                      [ 0.5766],\n                      [-0.8678]])),\n             ('fc1.bias',\n              tensor([-1.8832e+25,  4.5636e-41, -1.8832e+25,  4.5636e-41, -6.1598e-30,\n                       4.5637e-41, -1.8788e+22,  4.5636e-41, -2.0042e-31,  4.5637e-41])),\n             ('fc2.weight',\n              tensor([[ 0.1320,  0.2938,  0.0679,  0.2793,  0.1088, -0.1795, -0.2301,  0.2807,\n                        0.2479,  0.1091]])),\n             ('fc2.bias', tensor([0.0019]))])\n Note, however, that lazy modules cannot validate that the shape of parameters they load is correct.  \nhas_uninitialized_params() [source]\n \nCheck if a module has parameters that are not initialized \n  \ninitialize_parameters(*args, **kwargs) [source]\n \nInitialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference. \n \n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params", "type": "torch.nn", "text": " \nhas_uninitialized_params() [source]\n \nCheck if a module has parameters that are not initialized \n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters", "type": "torch.nn", "text": " \ninitialize_parameters(*args, **kwargs) [source]\n \nInitialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference. \n"}, {"name": "torch.nn.modules.module.register_module_backward_hook()", "path": "generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook", "type": "torch.nn", "text": " \ntorch.nn.modules.module.register_module_backward_hook(hook) [source]\n \nRegisters a backward hook common to all the modules. This function is deprecated in favor of nn.module.register_module_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.modules.module.register_module_forward_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook", "type": "torch.nn", "text": " \ntorch.nn.modules.module.register_module_forward_hook(hook) [source]\n \nRegisters a global forward hook for all the modules  Warning This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.  The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   This hook will be executed before specific module hooks registered with register_forward_hook. \n"}, {"name": "torch.nn.modules.module.register_module_forward_pre_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook", "type": "torch.nn", "text": " \ntorch.nn.modules.module.register_module_forward_pre_hook(hook) [source]\n \nRegisters a forward pre-hook common to all modules.  Warning This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.  The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). This hook has precedence over the specific module hooks registered with register_forward_pre_hook.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.MSELoss", "path": "generated/torch.nn.mseloss#torch.nn.MSELoss", "type": "torch.nn", "text": " \nclass torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx  and target yy . The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=(xn\u2212yn)2,\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2,  \nwhere NN  is the batch size. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nxx  and yy  are tensors of arbitrary shapes with a total of nn  elements each. The mean operation still operates over all the elements, and divides by nn . The division by nn  can be avoided if one sets reduction = 'sum'.  Parameters \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input    Examples: >>> loss = nn.MSELoss()\n>>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.randn(3, 5)\n>>> output = loss(input, target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.MultiheadAttention", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention", "type": "torch.nn", "text": " \nclass torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None) [source]\n \nAllows the model to jointly attend to information from different representation subspaces. See Attention Is All You Need  MultiHead(Q,K,V)=Concat(head1,\u2026,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O  \nwhere headi=Attention(QWiQ,KWiK,VWiV)head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) .  Parameters \n \nembed_dim \u2013 total dimension of the model. \nnum_heads \u2013 parallel attention heads. \ndropout \u2013 a Dropout layer on attn_output_weights. Default: 0.0. \nbias \u2013 add bias as module parameter. Default: True. \nadd_bias_kv \u2013 add bias to the key and value sequences at dim=0. \nadd_zero_attn \u2013 add a new batch of zeros to the key and value sequences at dim=1. \nkdim \u2013 total number of features in key. Default: None. \nvdim \u2013 total number of features in value. Default: None.    Note that if kdim and vdim are None, they will be set to embed_dim such that query, key, and value have the same number of features. Examples: >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n>>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n  \nforward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None) [source]\n \n Parameters \n \nkey, value (query,) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nkey_padding_mask \u2013 if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored \nneed_weights \u2013 output attn_output_weights. \nattn_mask \u2013 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.     Shapes for inputs:\n\n query: (L,N,E)(L, N, E)  where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E) , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)  where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)  where N is the batch size, S is the source sequence length. If a ByteTensor is provided, the non-zero positions will be ignored while the position with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. \nattn_mask: if a 2D mask: (L,S)(L, S)  where L is the target sequence length, S is the source sequence length. If a 3D mask: (N\u22c5num_heads,L,S)(N\\cdot\\text{num\\_heads}, L, S)  where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.    Shapes for outputs:\n\n attn_output: (L,N,E)(L, N, E)  where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)  where N is the batch size, L is the target sequence length, S is the source sequence length.    \n \n"}, {"name": "torch.nn.MultiheadAttention.forward()", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention.forward", "type": "torch.nn", "text": " \nforward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None) [source]\n \n Parameters \n \nkey, value (query,) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nkey_padding_mask \u2013 if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored \nneed_weights \u2013 output attn_output_weights. \nattn_mask \u2013 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.     Shapes for inputs:\n\n query: (L,N,E)(L, N, E)  where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E) , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)  where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)  where N is the batch size, S is the source sequence length. If a ByteTensor is provided, the non-zero positions will be ignored while the position with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. \nattn_mask: if a 2D mask: (L,S)(L, S)  where L is the target sequence length, S is the source sequence length. If a 3D mask: (N\u22c5num_heads,L,S)(N\\cdot\\text{num\\_heads}, L, S)  where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.    Shapes for outputs:\n\n attn_output: (L,N,E)(L, N, E)  where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)  where N is the batch size, L is the target sequence length, S is the source sequence length.    \n"}, {"name": "torch.nn.MultiLabelMarginLoss", "path": "generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss", "type": "torch.nn", "text": " \nclass torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 2D Tensor of target class indices). For each sample in the mini-batch:  loss(x,y)=\u2211ijmax\u2061(0,1\u2212(x[y[j]]\u2212x[i]))x.size(0)\\text{loss}(x, y) = \\sum_{ij}\\frac{\\max(0, 1 - (x[y[j]] - x[i]))}{\\text{x.size}(0)}  \nwhere x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\} , y\u2208{0,\u22ef,y.size(0)\u22121}y \\in \\left\\{0, \\; \\cdots , \\; \\text{y.size}(0) - 1\\right\\} , 0\u2264y[j]\u2264x.size(0)\u221210 \\leq y[j] \\leq \\text{x.size}(0)-1 , and i\u2260y[j]i \\neq y[j]  for all ii  and jj . yy  and xx  must have the same size. The criterion only considers a contiguous block of non-negative targets that starts at the front. This allows for different samples to have variable amounts of target classes.  Parameters \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (C)(C)  or (N,C)(N, C)  where N is the batch size and C is the number of classes. Target: (C)(C)  or (N,C)(N, C) , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N) .    Examples: >>> loss = nn.MultiLabelMarginLoss()\n>>> x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])\n>>> # for target y, only consider labels 3 and 0, not after label -1\n>>> y = torch.LongTensor([[3, 0, -1, 1]])\n>>> loss(x, y)\n>>> # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))\ntensor(0.8500)\n \n"}, {"name": "torch.nn.MultiLabelSoftMarginLoss", "path": "generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss", "type": "torch.nn", "text": " \nclass torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx  and target yy  of size (N,C)(N, C) . For each sample in the minibatch:  loss(x,y)=\u22121C\u2217\u2211iy[i]\u2217log\u2061((1+exp\u2061(\u2212x[i]))\u22121)+(1\u2212y[i])\u2217log\u2061(exp\u2061(\u2212x[i])(1+exp\u2061(\u2212x[i])))loss(x, y) = - \\frac{1}{C} * \\sum_i y[i] * \\log((1 + \\exp(-x[i]))^{-1}) + (1-y[i]) * \\log\\left(\\frac{\\exp(-x[i])}{(1 + \\exp(-x[i]))}\\right)  \nwhere i\u2208{0,\u22ef,x.nElement()\u22121}i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.nElement}() - 1\\right\\} , y[i]\u2208{0,1}y[i] \\in \\left\\{0, \\; 1\\right\\} .  Parameters \n \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,C)(N, C)  where N is the batch size and C is the number of classes. Target: (N,C)(N, C) , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N) .    \n"}, {"name": "torch.nn.MultiMarginLoss", "path": "generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss", "type": "torch.nn", "text": " \nclass torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-1 ): For each mini-batch sample, the loss in terms of the 1D input xx  and scalar output yy  is:  loss(x,y)=\u2211imax\u2061(0,margin\u2212x[y]+x[i]))px.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, \\text{margin} - x[y] + x[i]))^p}{\\text{x.size}(0)}  \nwhere x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}  and i\u2260yi \\neq y . Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor. The loss function then becomes:  loss(x,y)=\u2211imax\u2061(0,w[y]\u2217(margin\u2212x[y]+x[i]))p)x.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, w[y] * (\\text{margin} - x[y] + x[i]))^p)}{\\text{x.size}(0)}  \n Parameters \n \np (int, optional) \u2013 Has a default value of 11 . 11  and 22  are the only supported values. \nmargin (float, optional) \u2013 Has a default value of 11 . \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    \n"}, {"name": "torch.nn.NLLLoss", "path": "generated/torch.nn.nllloss#torch.nn.NLLLoss", "type": "torch.nn", "text": " \nclass torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') [source]\n \nThe negative log likelihood loss. It is useful to train a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)  or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  for the K-dimensional case (described later). Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. The target that this loss expects should be a class index in the range [0,C\u22121][0, C-1]  where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range). The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wynxn,yn,wc=weight[c]\u22c51{c=\u0338ignore_index},\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},  \nwhere xx  is the input, yy  is the target, ww  is the weight, and NN  is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={\u2211n=1N1\u2211n=1Nwynln,if reduction=\u2018mean\u2019;\u2211n=1Nln,if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, & \\text{if reduction} = \\text{`mean';}\\\\ \\sum_{n=1}^N l_n, & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nCan also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1 , where KK  is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.  Parameters \n \nweight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the weighted mean of the output is taken, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,C)(N, C)  where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  in the case of K-dimensional loss. Target: (N)(N)  where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-1 , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N) , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  in the case of K-dimensional loss.    Examples: >>> m = nn.LogSoftmax(dim=1)\n>>> loss = nn.NLLLoss()\n>>> # input is of size N x C = 3 x 5\n>>> input = torch.randn(3, 5, requires_grad=True)\n>>> # each element in target has to have 0 <= value < C\n>>> target = torch.tensor([1, 0, 4])\n>>> output = loss(m(input), target)\n>>> output.backward()\n>>>\n>>>\n>>> # 2D loss example (used, for example, with image inputs)\n>>> N, C = 5, 4\n>>> loss = nn.NLLLoss()\n>>> # input is of size N x C x height x width\n>>> data = torch.randn(N, 16, 10, 10)\n>>> conv = nn.Conv2d(16, C, (3, 3))\n>>> m = nn.LogSoftmax(dim=1)\n>>> # each element in target has to have 0 <= value < C\n>>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n>>> output = loss(m(conv(data)), target)\n>>> output.backward()\n \n"}, {"name": "torch.nn.PairwiseDistance", "path": "generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance", "type": "torch.nn", "text": " \nclass torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False) [source]\n \nComputes the batchwise pairwise distance between vectors v1v_1 , v2v_2  using the p-norm:  \u2225x\u2225p=(\u2211i=1n\u2223xi\u2223p)1/p.\\Vert x \\Vert _p = \\left( \\sum_{i=1}^n \\vert x_i \\vert ^ p \\right) ^ {1/p}.  \n Parameters \n \np (real) \u2013 the norm degree. Default: 2 \neps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-6 \nkeepdim (bool, optional) \u2013 Determines whether or not to keep the vector dimension. Default: False     Shape:\n\n Input1: (N,D)(N, D)  where D = vector dimension\n Input2: (N,D)(N, D) , same shape as the Input1 Output: (N)(N) . If keepdim is True, then (N,1)(N, 1) .   Examples::\n\n>>> pdist = nn.PairwiseDistance(p=2)\n>>> input1 = torch.randn(100, 128)\n>>> input2 = torch.randn(100, 128)\n>>> output = pdist(input1, input2)\n   \n"}, {"name": "torch.nn.parallel.data_parallel()", "path": "nn.functional#torch.nn.parallel.data_parallel", "type": "torch.nn.functional", "text": " \ntorch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None) [source]\n \nEvaluates module(input) in parallel across the GPUs given in device_ids. This is the functional version of the DataParallel module.  Parameters \n \nmodule (Module) \u2013 the module to evaluate in parallel \ninputs (Tensor) \u2013 inputs to the module \ndevice_ids (list of python:int or torch.device) \u2013 GPU ids on which to replicate module \noutput_device (list of python:int or torch.device) \u2013 GPU location of the output Use -1 to indicate the CPU. (default: device_ids[0])   Returns \na Tensor containing the result of module(input) located on output_device   \n"}, {"name": "torch.nn.parallel.DistributedDataParallel", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel", "type": "torch.nn", "text": " \nclass torch.nn.parallel.DistributedDataParallel(module, device_ids=None, output_device=None, dim=0, broadcast_buffers=True, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=False) [source]\n \nImplements distributed data parallelism that is based on torch.distributed package at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. The batch size should be larger than the number of GPUs used locally. See also: Basics and Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel. The same constraints on input as in torch.nn.DataParallel apply. Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group(). DistributedDataParallel is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training. To use DistributedDataParallel on a host with N GPUs, you should spawn up N processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting CUDA_VISIBLE_DEVICES for every process or by calling: >>> torch.cuda.set_device(i)\n where i is from 0 to N-1. In each process, you should refer the following to construct this module: >>> torch.distributed.init_process_group(\n>>>     backend='nccl', world_size=N, init_method='...'\n>>> )\n>>> model = DistributedDataParallel(model, device_ids=[i], output_device=i)\n In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn.  Note Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.   Note nccl backend is currently the fastest and highly recommended backend when using GPUs. This applies to both single-node and multi-node distributed training.   Note This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine.   Note If you use torch.save on one process to checkpoint the module, and torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location, torch.load would recover the module to devices where the module was saved from.   Note When a model is trained on M nodes with batch=N, the gradient will be M times smaller when compared to the same model trained on a single node with batch=M*N if the loss is summed (NOT averaged as usual) across instances in a batch (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart. But in most cases, you can just treat a DistributedDataParallel wrapped model, a DataParallel wrapped model and an ordinary model on a single GPU as the same (E.g. using the same learning rate for equivalent batch size).   Note Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.   Note If you are using DistributedDataParallel in conjunction with the Distributed RPC Framework, you should always use torch.distributed.autograd.backward() to compute gradients and torch.distributed.optim.DistributedOptimizer for optimizing parameters. Example: >>> import torch.distributed.autograd as dist_autograd\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>> from torch.distributed.rpc import RRef\n>>>\n>>> t1 = torch.rand((3, 3), requires_grad=True)\n>>> t2 = torch.rand((3, 3), requires_grad=True)\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(t1, t2))\n>>> ddp_model = DDP(my_model)\n>>>\n>>> # Setup optimizer\n>>> optimizer_params = [rref]\n>>> for param in ddp_model.parameters():\n>>>     optimizer_params.append(RRef(param))\n>>>\n>>> dist_optim = DistributedOptimizer(\n>>>     optim.SGD,\n>>>     optimizer_params,\n>>>     lr=0.05,\n>>> )\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>     pred = ddp_model(rref.to_here())\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n>>>     dist_optim.step()\n   Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.   Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.   Warning This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient allreduce following the reverse order of the registered parameters of the model. In other words, it is users\u2019 responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.   Warning This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose torch.memory_format is torch.contiguous_format and others whose format is torch.channels_last. However, corresponding parameters in different processes must have the same strides.   Warning This module doesn\u2019t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters).   Warning If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don\u2019t change this setting.   Warning Forward and backward hooks defined on module and its submodules won\u2019t be invoked anymore, unless the hooks are initialized in the forward() method.   Warning You should never try to change your model\u2019s parameters after wrapping up your model with DistributedDataParallel. Because, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model\u2019s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.   Warning Using DistributedDataParallel in conjunction with the Distributed RPC Framework is experimental and subject to change.   Warning The gradient_as_bucket_view mode does not yet work with Automatic Mixed Precision (AMP). AMP maintains stashed gradients that are used for unscaling gradients. With gradient_as_bucket_view=True, these stashed gradients will point to communication buckets in the first iteration. In the next iteration, the communication buckets are mutated and thus these stashed gradients will be unexpectedly mutated as well, which might lead to wrong results.   Parameters \n \nmodule (Module) \u2013 module to be parallelized \ndevice_ids (list of python:int or torch.device) \u2013 CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the i\u2019th module replica is placed on device_ids[i]. For multi-device modules and CPU modules, device_ids must be None or an empty list, and input data for the forward pass must be placed on the correct device. (default: all visible devices for single-device modules) \noutput_device (int or torch.device) \u2013 Device location of output for single-device CUDA modules. For multi-device modules and CPU modules, it must be None, and the module itself dictates the output location. (default: device_ids[0] for single-device modules) \nbroadcast_buffers (bool) \u2013 Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True) \nprocess_group \u2013 The process group to be used for distributed data all-reduction. If None, the default process group, which is created by torch.distributed.init_process_group(), will be used. (default: None) \nbucket_cap_mb \u2013 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB). (default: 25) \nfind_unused_parameters (bool) \u2013 Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach. (default: False) \ncheck_reduction \u2013 This argument is deprecated. \ngradient_as_bucket_view (bool) \u2013 This is a prototype feature and subject to changes. When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution.   Variables \n~DistributedDataParallel.module (Module) \u2013 the module to be parallelized.   Example: >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n>>> net = torch.nn.parallel.DistributedDataParallel(model, pg)\n  \njoin(divide_by_initial_world_size=True, enable=True) [source]\n \nA context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes. This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes. Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP). To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.  Warning This module works only with the multi-process, single-device usage of torch.nn.parallel.DistributedDataParallel, which means that a single process works on a single GPU.   Warning This module currently does not support custom distributed collective operations in the forward pass, such as SyncBatchNorm or other custom defined collectives in the model\u2019s forward pass.   Parameters \n \ndivide_by_initial_world_size (bool) \u2013 If True, will divide gradients by the initial world_size DDP training was launched with. If False, will compute the effective world size (number of ranks that have not depleted their inputs yet) and divide gradients by that during allreduce. Set divide_by_initial_world_size=True to ensure every input sample including the uneven inputs have equal weight in terms of how much they contribute to the global gradient. This is achieved by always dividing the gradient by the initial world_size even when we encounter uneven inputs. If you set this to False, we divide the gradient by the remaining number of nodes. This ensures parity with training on a smaller world_size although it also means the uneven inputs would contribute more towards the global gradient. Typically, you would want to set this to True for cases where the last few inputs of your training job are uneven. In extreme cases, where there is a large discrepancy in the number of inputs, setting this to False might provide better results. \nenable (bool) \u2013 Whether to enable uneven input detection or not. Pass in enable=False to disable in cases where you know that inputs are even across participating processes. Default is True.    Example: >>>  import torch\n>>>  import torch.distributed as dist\n>>>  import os\n>>>  import torch.multiprocessing as mp\n>>>  import torch.nn as nn\n>>>  # On each spawned worker\n>>>  def worker(rank):\n>>>      dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n>>>      torch.cuda.set_device(rank)\n>>>      model = nn.Linear(1, 1, bias=False).to(rank)\n>>>      model = torch.nn.parallel.DistributedDataParallel(\n>>>          model, device_ids=[rank], output_device=rank\n>>>      )\n>>>      # Rank 1 gets one more input than rank 0.\n>>>      inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]\n>>>      with model.join():\n>>>          for _ in range(5):\n>>>              for inp in inputs:\n>>>                  loss = model(inp).sum()\n>>>                  loss.backward()\n>>>  # Without the join() API, the below synchronization will hang\n>>>  # blocking for rank 1's allreduce to complete.\n>>>  torch.cuda.synchronize(device=rank)\n \n  \nno_sync() [source]\n \nA context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context. Example: >>> ddp = torch.nn.parallel.DistributedDataParallel(model, pg)\n>>> with ddp.no_sync():\n>>>   for input in inputs:\n>>>     ddp(input).backward()  # no synchronization, accumulate grads\n>>> ddp(another_input).backward()  # synchronize grads\n \n  \nregister_comm_hook(state, hook) [source]\n \nRegisters a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers. This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.  Parameters \n \nstate (object) \u2013 \nPassed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.  \nhook (callable) \u2013 \nAverages gradient tensors across workers and defined as: hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future: This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it can also just return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters. We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.work.      Warning Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.   Warning DDP communication hook can only be registered once and should be registered before calling backward.   Warning The Future object that hook returns should contain a result that has the same shape with the tensors inside grad bucket.   Warning DDP communication hook does not support single-process multiple-device mode. Gradbucket tensors should consist of only a single tensor.   Warning get_future API supports only NCCL backend and will return a torch._C.Future which is an internal type and should be used with caution. It can still be used by register_comm_hook API, but it is subject to some subtle differences compared to torch.futures.Future.   Warning DDP communication hook is experimental and subject to change.   Example::\n\nBelow is an example of a noop hook that returns the same tensors. >>> def noop(state: object, bucket: dist._GradBucket): -> torch.futures.Future\n>>>     fut = torch.futures.Future()\n>>>     fut.set_result(bucket.get_tensors())\n>>>     return fut\n >>> ddp.register_comm_hook(state = None, hook = noop)\n  Example::\n\nBelow is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce. >>> def encode_and_decode(state: object, bucket: dist._GradBucket): -> torch.futures.Future\n>>>     tensors = [t / process_group.world_size for t in bucket.get_tensors()]\n>>>     encoded_tensors = encode(tensors) # encode gradients\n>>>     fut = process_group.allreduce(encoded_tensors).get_future()\n>>>     # Define the then callback to decode.\n>>>     def decode(fut):\n>>>         decoded_tensors = decode(fut.value()) # decode gradients\n>>>         return decoded_tensors\n>>>     return fut.then(decode)\n >>> ddp.register_comm_hook(state = None, hook = encode_and_decode)\n   \n \n"}, {"name": "torch.nn.parallel.DistributedDataParallel.join()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.join", "type": "torch.nn", "text": " \njoin(divide_by_initial_world_size=True, enable=True) [source]\n \nA context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes. This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes. Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP). To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.  Warning This module works only with the multi-process, single-device usage of torch.nn.parallel.DistributedDataParallel, which means that a single process works on a single GPU.   Warning This module currently does not support custom distributed collective operations in the forward pass, such as SyncBatchNorm or other custom defined collectives in the model\u2019s forward pass.   Parameters \n \ndivide_by_initial_world_size (bool) \u2013 If True, will divide gradients by the initial world_size DDP training was launched with. If False, will compute the effective world size (number of ranks that have not depleted their inputs yet) and divide gradients by that during allreduce. Set divide_by_initial_world_size=True to ensure every input sample including the uneven inputs have equal weight in terms of how much they contribute to the global gradient. This is achieved by always dividing the gradient by the initial world_size even when we encounter uneven inputs. If you set this to False, we divide the gradient by the remaining number of nodes. This ensures parity with training on a smaller world_size although it also means the uneven inputs would contribute more towards the global gradient. Typically, you would want to set this to True for cases where the last few inputs of your training job are uneven. In extreme cases, where there is a large discrepancy in the number of inputs, setting this to False might provide better results. \nenable (bool) \u2013 Whether to enable uneven input detection or not. Pass in enable=False to disable in cases where you know that inputs are even across participating processes. Default is True.    Example: >>>  import torch\n>>>  import torch.distributed as dist\n>>>  import os\n>>>  import torch.multiprocessing as mp\n>>>  import torch.nn as nn\n>>>  # On each spawned worker\n>>>  def worker(rank):\n>>>      dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n>>>      torch.cuda.set_device(rank)\n>>>      model = nn.Linear(1, 1, bias=False).to(rank)\n>>>      model = torch.nn.parallel.DistributedDataParallel(\n>>>          model, device_ids=[rank], output_device=rank\n>>>      )\n>>>      # Rank 1 gets one more input than rank 0.\n>>>      inputs = [torch.tensor([1]).float() for _ in range(10 + rank)]\n>>>      with model.join():\n>>>          for _ in range(5):\n>>>              for inp in inputs:\n>>>                  loss = model(inp).sum()\n>>>                  loss.backward()\n>>>  # Without the join() API, the below synchronization will hang\n>>>  # blocking for rank 1's allreduce to complete.\n>>>  torch.cuda.synchronize(device=rank)\n \n"}, {"name": "torch.nn.parallel.DistributedDataParallel.no_sync()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.no_sync", "type": "torch.nn", "text": " \nno_sync() [source]\n \nA context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context. Example: >>> ddp = torch.nn.parallel.DistributedDataParallel(model, pg)\n>>> with ddp.no_sync():\n>>>   for input in inputs:\n>>>     ddp(input).backward()  # no synchronization, accumulate grads\n>>> ddp(another_input).backward()  # synchronize grads\n \n"}, {"name": "torch.nn.parallel.DistributedDataParallel.register_comm_hook()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.register_comm_hook", "type": "torch.nn", "text": " \nregister_comm_hook(state, hook) [source]\n \nRegisters a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers. This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.  Parameters \n \nstate (object) \u2013 \nPassed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.  \nhook (callable) \u2013 \nAverages gradient tensors across workers and defined as: hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future: This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it can also just return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters. We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.work.      Warning Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.   Warning DDP communication hook can only be registered once and should be registered before calling backward.   Warning The Future object that hook returns should contain a result that has the same shape with the tensors inside grad bucket.   Warning DDP communication hook does not support single-process multiple-device mode. Gradbucket tensors should consist of only a single tensor.   Warning get_future API supports only NCCL backend and will return a torch._C.Future which is an internal type and should be used with caution. It can still be used by register_comm_hook API, but it is subject to some subtle differences compared to torch.futures.Future.   Warning DDP communication hook is experimental and subject to change.   Example::\n\nBelow is an example of a noop hook that returns the same tensors. >>> def noop(state: object, bucket: dist._GradBucket): -> torch.futures.Future\n>>>     fut = torch.futures.Future()\n>>>     fut.set_result(bucket.get_tensors())\n>>>     return fut\n >>> ddp.register_comm_hook(state = None, hook = noop)\n  Example::\n\nBelow is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce. >>> def encode_and_decode(state: object, bucket: dist._GradBucket): -> torch.futures.Future\n>>>     tensors = [t / process_group.world_size for t in bucket.get_tensors()]\n>>>     encoded_tensors = encode(tensors) # encode gradients\n>>>     fut = process_group.allreduce(encoded_tensors).get_future()\n>>>     # Define the then callback to decode.\n>>>     def decode(fut):\n>>>         decoded_tensors = decode(fut.value()) # decode gradients\n>>>         return decoded_tensors\n>>>     return fut.then(decode)\n >>> ddp.register_comm_hook(state = None, hook = encode_and_decode)\n   \n"}, {"name": "torch.nn.parameter.Parameter", "path": "generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter", "type": "torch.nn", "text": " \nclass torch.nn.parameter.Parameter [source]\n \nA kind of Tensor that is to be considered a module parameter. Parameters are Tensor subclasses, that have a very special property when used with Module s - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn\u2019t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.  Parameters \n \ndata (Tensor) \u2013 parameter tensor. \nrequires_grad (bool, optional) \u2013 if the parameter requires gradient. See Excluding subgraphs from backward for more details. Default: True\n    \n"}, {"name": "torch.nn.parameter.UninitializedParameter", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter", "type": "torch.nn", "text": " \nclass torch.nn.parameter.UninitializedParameter [source]\n \nA parameter that is not initialized. Unitialized Parameters are a a special case of torch.nn.Parameter where the shape of the data is still unknown. Unlikely a torch.nn.Parameter, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular torch.nn.Parameter.  \nmaterialize(shape, device=None, dtype=None) [source]\n \nCreate a Parameter with the same properties of the uninitialized one. Given a shape, it materializes a parameter in the same device and with the same dtype as the current one or the specified ones in the arguments.  Parameters \n \nshape \u2013 (tuple): the shape for the materialized tensor. \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module. Optional. \ndtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module. Optional.    \n \n"}, {"name": "torch.nn.parameter.UninitializedParameter.materialize()", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter.materialize", "type": "torch.nn", "text": " \nmaterialize(shape, device=None, dtype=None) [source]\n \nCreate a Parameter with the same properties of the uninitialized one. Given a shape, it materializes a parameter in the same device and with the same dtype as the current one or the specified ones in the arguments.  Parameters \n \nshape \u2013 (tuple): the shape for the materialized tensor. \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module. Optional. \ndtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module. Optional.    \n"}, {"name": "torch.nn.ParameterDict", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict", "type": "torch.nn", "text": " \nclass torch.nn.ParameterDict(parameters=None) [source]\n \nHolds parameters in a dictionary. ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods. ParameterDict is an ordered dictionary that respects  the order of insertion, and in update(), the order of the merged OrderedDict or another ParameterDict (the argument to update()).  Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict) does not preserve the order of the merged mapping.  Parameters \nparameters (iterable, optional) \u2013 a mapping (dictionary) of (string : Parameter) or an iterable of key-value pairs of type (string, Parameter)   Example: class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n  \nclear() [source]\n \nRemove all items from the ParameterDict. \n  \nitems() [source]\n \nReturn an iterable of the ParameterDict key/value pairs. \n  \nkeys() [source]\n \nReturn an iterable of the ParameterDict keys. \n  \npop(key) [source]\n \nRemove key from the ParameterDict and return its parameter.  Parameters \nkey (string) \u2013 key to pop from the ParameterDict   \n  \nupdate(parameters) [source]\n \nUpdate the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters \nparameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)   \n  \nvalues() [source]\n \nReturn an iterable of the ParameterDict values. \n \n"}, {"name": "torch.nn.ParameterDict.clear()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.clear", "type": "torch.nn", "text": " \nclear() [source]\n \nRemove all items from the ParameterDict. \n"}, {"name": "torch.nn.ParameterDict.items()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.items", "type": "torch.nn", "text": " \nitems() [source]\n \nReturn an iterable of the ParameterDict key/value pairs. \n"}, {"name": "torch.nn.ParameterDict.keys()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.keys", "type": "torch.nn", "text": " \nkeys() [source]\n \nReturn an iterable of the ParameterDict keys. \n"}, {"name": "torch.nn.ParameterDict.pop()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.pop", "type": "torch.nn", "text": " \npop(key) [source]\n \nRemove key from the ParameterDict and return its parameter.  Parameters \nkey (string) \u2013 key to pop from the ParameterDict   \n"}, {"name": "torch.nn.ParameterDict.update()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.update", "type": "torch.nn", "text": " \nupdate(parameters) [source]\n \nUpdate the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters \nparameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)   \n"}, {"name": "torch.nn.ParameterDict.values()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.values", "type": "torch.nn", "text": " \nvalues() [source]\n \nReturn an iterable of the ParameterDict values. \n"}, {"name": "torch.nn.ParameterList", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList", "type": "torch.nn", "text": " \nclass torch.nn.ParameterList(parameters=None) [source]\n \nHolds parameters in a list. ParameterList can be indexed like a regular Python list, but parameters it contains are properly registered, and will be visible by all Module methods.  Parameters \nparameters (iterable, optional) \u2013 an iterable of Parameter to add   Example: class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n  \nappend(parameter) [source]\n \nAppends a given parameter at the end of the list.  Parameters \nparameter (nn.Parameter) \u2013 parameter to append   \n  \nextend(parameters) [source]\n \nAppends parameters from a Python iterable to the end of the list.  Parameters \nparameters (iterable) \u2013 iterable of parameters to append   \n \n"}, {"name": "torch.nn.ParameterList.append()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.append", "type": "torch.nn", "text": " \nappend(parameter) [source]\n \nAppends a given parameter at the end of the list.  Parameters \nparameter (nn.Parameter) \u2013 parameter to append   \n"}, {"name": "torch.nn.ParameterList.extend()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.extend", "type": "torch.nn", "text": " \nextend(parameters) [source]\n \nAppends parameters from a Python iterable to the end of the list.  Parameters \nparameters (iterable) \u2013 iterable of parameters to append   \n"}, {"name": "torch.nn.PixelShuffle", "path": "generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle", "type": "torch.nn", "text": " \nclass torch.nn.PixelShuffle(upscale_factor) [source]\n \nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is an upscale factor. This is useful for implementing efficient sub-pixel convolution with a stride of 1/r1/r . See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.  Parameters \nupscale_factor (int) \u2013 factor to increase spatial resolution by    Shape:\n\n Input: (\u2217,Cin,Hin,Win)(*, C_{in}, H_{in}, W_{in}) , where * is zero or more batch dimensions Output: (\u2217,Cout,Hout,Wout)(*, C_{out}, H_{out}, W_{out}) , where     Cout=Cin\u00f7upscale_factor2C_{out} = C_{in} \\div \\text{upscale\\_factor}^2  \n Hout=Hin\u00d7upscale_factorH_{out} = H_{in} \\times \\text{upscale\\_factor}  \n Wout=Win\u00d7upscale_factorW_{out} = W_{in} \\times \\text{upscale\\_factor}  \nExamples: >>> pixel_shuffle = nn.PixelShuffle(3)\n>>> input = torch.randn(1, 9, 4, 4)\n>>> output = pixel_shuffle(input)\n>>> print(output.size())\ntorch.Size([1, 1, 12, 12])\n \n"}, {"name": "torch.nn.PixelUnshuffle", "path": "generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle", "type": "torch.nn", "text": " \nclass torch.nn.PixelUnshuffle(downscale_factor) [source]\n \nReverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor. See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.  Parameters \ndownscale_factor (int) \u2013 factor to decrease spatial resolution by    Shape:\n\n Input: (\u2217,Cin,Hin,Win)(*, C_{in}, H_{in}, W_{in}) , where * is zero or more batch dimensions Output: (\u2217,Cout,Hout,Wout)(*, C_{out}, H_{out}, W_{out}) , where     Cout=Cin\u00d7downscale_factor2C_{out} = C_{in} \\times \\text{downscale\\_factor}^2  \n Hout=Hin\u00f7downscale_factorH_{out} = H_{in} \\div \\text{downscale\\_factor}  \n Wout=Win\u00f7downscale_factorW_{out} = W_{in} \\div \\text{downscale\\_factor}  \nExamples: >>> pixel_unshuffle = nn.PixelUnshuffle(3)\n>>> input = torch.randn(1, 1, 12, 12)\n>>> output = pixel_unshuffle(input)\n>>> print(output.size())\ntorch.Size([1, 9, 4, 4])\n \n"}, {"name": "torch.nn.PoissonNLLLoss", "path": "generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss", "type": "torch.nn", "text": " \nclass torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean') [source]\n \nNegative log likelihood loss with Poisson distribution of target. The loss can be described as:  target\u223cPoisson(input)loss(input,target)=input\u2212target\u2217log\u2061(input)+log\u2061(target!)\\text{target} \\sim \\mathrm{Poisson}(\\text{input}) \\text{loss}(\\text{input}, \\text{target}) = \\text{input} - \\text{target} * \\log(\\text{input}) + \\log(\\text{target!}) \nThe last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.  Parameters \n \nlog_input (bool, optional) \u2013 if True the loss is computed as exp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target}*\\text{input} , if False the loss is input\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps}) . \nfull (bool, optional) \u2013 \nwhether to compute full loss, i. e. to add the Stirling approximation term  target\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u03c0target).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).  \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \neps (float, optional) \u2013 Small value to avoid evaluation of log\u2061(0)\\log(0)  when log_input = False. Default: 1e-8 \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n    Examples: >>> loss = nn.PoissonNLLLoss()\n>>> log_input = torch.randn(5, 2, requires_grad=True)\n>>> target = torch.randn(5, 2)\n>>> output = loss(log_input, target)\n>>> output.backward()\n  Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Output: scalar by default. If reduction is 'none', then (N,\u2217)(N, *) , the same shape as the input    \n"}, {"name": "torch.nn.PReLU", "path": "generated/torch.nn.prelu#torch.nn.PReLU", "type": "torch.nn", "text": " \nclass torch.nn.PReLU(num_parameters=1, init=0.25) [source]\n \nApplies the element-wise function:  PReLU(x)=max\u2061(0,x)+a\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)  \nor  PReLU(x)={x, if x\u22650ax, otherwise \\text{PReLU}(x) = \\begin{cases} x, & \\text{ if } x \\geq 0 \\\\ ax, & \\text{ otherwise } \\end{cases}  \nHere aa  is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter aa  across all input channels. If called with nn.PReLU(nChannels), a separate aa  is used for each input channel.  Note weight decay should not be used when learning aa  for good performance.   Note Channel dim is the 2nd dim of input. When input has dims < 2, then there is no channel dim and the number of channels = 1.   Parameters \n \nnum_parameters (int) \u2013 number of aa  to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. Default: 1 \ninit (float) \u2013 the initial value of aa . Default: 0.25     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Variables \n~PReLU.weight (Tensor) \u2013 the learnable weights of shape (num_parameters).    Examples: >>> m = nn.PReLU()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.qat.Conv2d", "path": "torch.nn.qat#torch.nn.qat.Conv2d", "type": "Quantization", "text": " \nclass torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None) [source]\n \nA Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Conv2d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for documentation. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables \n~Conv2d.weight_fake_quant \u2013 fake quant module for weight    \nclassmethod from_float(mod) [source]\n \nCreate a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user \n \n"}, {"name": "torch.nn.qat.Conv2d.from_float()", "path": "torch.nn.qat#torch.nn.qat.Conv2d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user \n"}, {"name": "torch.nn.qat.Linear", "path": "torch.nn.qat#torch.nn.qat.Linear", "type": "Quantization", "text": " \nclass torch.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None) [source]\n \nA linear module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, with FakeQuantize modules initialized to default.  Variables \n~Linear.weight \u2013 fake quant module for weight    \nclassmethod from_float(mod) [source]\n \nCreate a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user \n \n"}, {"name": "torch.nn.qat.Linear.from_float()", "path": "torch.nn.qat#torch.nn.qat.Linear.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user \n"}, {"name": "torch.nn.quantized.BatchNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm2d", "type": "Quantization", "text": " \nclass torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1) [source]\n \nThis is the quantized version of BatchNorm2d. \n"}, {"name": "torch.nn.quantized.BatchNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm3d", "type": "Quantization", "text": " \nclass torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1) [source]\n \nThis is the quantized version of BatchNorm3d. \n"}, {"name": "torch.nn.quantized.Conv1d", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d", "type": "Quantization", "text": " \nclass torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nApplies a 1D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv1d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables \n \n~Conv1d.weight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \n~Conv1d.scale (Tensor) \u2013 scalar for the output scale \n~Conv1d.zero_point (Tensor) \u2013 scalar for the output zero point    See Conv1d for other attributes. Examples: >>> m = nn.quantized.Conv1d(16, 33, 3, stride=2)\n>>> input = torch.randn(20, 16, 100)\n>>> # quantize input to quint8\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0,\n                                        dtype=torch.quint8)\n>>> output = m(q_input)\n  \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n \n"}, {"name": "torch.nn.quantized.Conv1d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n"}, {"name": "torch.nn.quantized.Conv2d", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d", "type": "Quantization", "text": " \nclass torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nApplies a 2D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv2d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables \n \n~Conv2d.weight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \n~Conv2d.scale (Tensor) \u2013 scalar for the output scale \n~Conv2d.zero_point (Tensor) \u2013 scalar for the output zero point    See Conv2d for other attributes. Examples: >>> # With square kernels and equal stride\n>>> m = nn.quantized.Conv2d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> # non-square kernels and unequal stride and with padding and dilation\n>>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n>>> input = torch.randn(20, 16, 50, 100)\n>>> # quantize input to quint8\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n  \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n \n"}, {"name": "torch.nn.quantized.Conv2d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n"}, {"name": "torch.nn.quantized.Conv3d", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d", "type": "Quantization", "text": " \nclass torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros') [source]\n \nApplies a 3D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv3d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables \n \n~Conv3d.weight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \n~Conv3d.scale (Tensor) \u2013 scalar for the output scale \n~Conv3d.zero_point (Tensor) \u2013 scalar for the output zero point    See Conv3d for other attributes. Examples: >>> # With square kernels and equal stride\n>>> m = nn.quantized.Conv3d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n>>> # non-square kernels and unequal stride and with padding and dilation\n>>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))\n>>> input = torch.randn(20, 16, 56, 56, 56)\n>>> # quantize input to quint8\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n  \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n \n"}, {"name": "torch.nn.quantized.Conv3d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n"}, {"name": "torch.nn.quantized.DeQuantize", "path": "torch.nn.quantized#torch.nn.quantized.DeQuantize", "type": "Quantization", "text": " \nclass torch.nn.quantized.DeQuantize [source]\n \nDequantizes an incoming tensor  Examples::\n\n>>> input = torch.tensor([[1., -1.], [1., -1.]])\n>>> scale, zero_point, dtype = 1.0, 2, torch.qint8\n>>> qm = Quantize(scale, zero_point, dtype)\n>>> quantized_input = qm(input)\n>>> dqm = DeQuantize()\n>>> dequantized = dqm(quantized_input)\n>>> print(dequantized)\ntensor([[ 1., -1.],\n        [ 1., -1.]], dtype=torch.float32)\n   \n"}, {"name": "torch.nn.quantized.dynamic", "path": "torch.nn.quantized.dynamic", "type": "torch.nn.quantized.dynamic", "text": "torch.nn.quantized.dynamic Linear  \nclass torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8) [source]\n \nA dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n \n~Linear.weight (Tensor) \u2013 the non-learnable quantized weights of the module which are of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}) . \n~Linear.bias (Tensor) \u2013 the non-learnable floating point bias of the module of shape (out_features)(\\text{out\\_features}) . If bias is True, the values are initialized to zero.    Examples: >>> m = nn.quantized.dynamic.Linear(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n  \nclassmethod from_float(mod) [source]\n \nCreate a dynamic quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n \n LSTM  \nclass torch.nn.quantized.dynamic.LSTM(*args, **kwargs) [source]\n \nA dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation. Examples: >>> rnn = nn.LSTM(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> c0 = torch.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))\n \n LSTMCell  \nclass torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs) [source]\n \nA long short-term memory (LSTM) cell. A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation. Examples: >>> rnn = nn.LSTMCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> cx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx, cx = rnn(input[i], (hx, cx))\n        output.append(hx)\n \n GRUCell  \nclass torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8) [source]\n \nA gated recurrent unit (GRU) cell A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation. Examples: >>> rnn = nn.GRUCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n \n RNNCell  \nclass torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8) [source]\n \nAn Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation. Examples: >>> rnn = nn.RNNCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n \n\n"}, {"name": "torch.nn.quantized.dynamic.GRUCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.GRUCell", "type": "torch.nn.quantized.dynamic", "text": " \nclass torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8) [source]\n \nA gated recurrent unit (GRU) cell A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation. Examples: >>> rnn = nn.GRUCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n \n"}, {"name": "torch.nn.quantized.dynamic.Linear", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear", "type": "torch.nn.quantized.dynamic", "text": " \nclass torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8) [source]\n \nA dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n \n~Linear.weight (Tensor) \u2013 the non-learnable quantized weights of the module which are of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}) . \n~Linear.bias (Tensor) \u2013 the non-learnable floating point bias of the module of shape (out_features)(\\text{out\\_features}) . If bias is True, the values are initialized to zero.    Examples: >>> m = nn.quantized.dynamic.Linear(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n  \nclassmethod from_float(mod) [source]\n \nCreate a dynamic quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n \n"}, {"name": "torch.nn.quantized.dynamic.Linear.from_float()", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear.from_float", "type": "torch.nn.quantized.dynamic", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a dynamic quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n"}, {"name": "torch.nn.quantized.dynamic.LSTM", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTM", "type": "torch.nn.quantized.dynamic", "text": " \nclass torch.nn.quantized.dynamic.LSTM(*args, **kwargs) [source]\n \nA dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation. Examples: >>> rnn = nn.LSTM(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> c0 = torch.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))\n \n"}, {"name": "torch.nn.quantized.dynamic.LSTMCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTMCell", "type": "torch.nn.quantized.dynamic", "text": " \nclass torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs) [source]\n \nA long short-term memory (LSTM) cell. A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation. Examples: >>> rnn = nn.LSTMCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> cx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx, cx = rnn(input[i], (hx, cx))\n        output.append(hx)\n \n"}, {"name": "torch.nn.quantized.dynamic.RNNCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.RNNCell", "type": "torch.nn.quantized.dynamic", "text": " \nclass torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8) [source]\n \nAn Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation. Examples: >>> rnn = nn.RNNCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n \n"}, {"name": "torch.nn.quantized.ELU", "path": "torch.nn.quantized#torch.nn.quantized.ELU", "type": "Quantization", "text": " \nclass torch.nn.quantized.ELU(scale, zero_point, alpha=1.0) [source]\n \nThis is the quantized equivalent of ELU.  Parameters \n \nscale \u2013 quantization scale of the output tensor \nzero_point \u2013 quantization zero point of the output tensor \nalpha \u2013 the alpha constant    \n"}, {"name": "torch.nn.quantized.Embedding", "path": "torch.nn.quantized#torch.nn.quantized.Embedding", "type": "Quantization", "text": " \nclass torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, dtype=torch.quint8) [source]\n \nA quantized Embedding module with quantized packed weights as inputs. We adopt the same interface as torch.nn.Embedding, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation. Similar to Embedding, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n~Embedding.weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}) .    Examples::\n\n>>> m = nn.quantized.Embedding(num_embeddings=10, embedding_dim=12)\n>>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8])\n>>> output = m(indices)\n>>> print(output.size())\ntorch.Size([9, 12]\n    \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user   \n \n"}, {"name": "torch.nn.quantized.Embedding.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Embedding.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user   \n"}, {"name": "torch.nn.quantized.EmbeddingBag", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag", "type": "Quantization", "text": " \nclass torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='sum', sparse=False, _weight=None, include_last_offset=False, dtype=torch.quint8) [source]\n \nA quantized EmbeddingBag module with quantized packed weights as inputs. We adopt the same interface as torch.nn.EmbeddingBag, please see https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for documentation. Similar to EmbeddingBag, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n~EmbeddingBag.weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}) .    Examples::\n\n>>> m = nn.quantized.EmbeddingBag(num_embeddings=10, embedding_dim=12, include_last_offset=True, mode='sum')\n>>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8, 6, 6, 9, 1, 6, 8, 8, 3, 2, 3, 6, 3, 6, 5, 7, 0, 8, 4, 6, 5, 8, 2, 3])\n>>> offsets = torch.tensor([0, 19, 20, 28, 28, 32])\n>>> output = m(indices, offsets)\n>>> print(output.size())\ntorch.Size([5, 12]\n    \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding_bag module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user   \n \n"}, {"name": "torch.nn.quantized.EmbeddingBag.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding_bag module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user   \n"}, {"name": "torch.nn.quantized.FloatFunctional", "path": "torch.nn.quantized#torch.nn.quantized.FloatFunctional", "type": "Quantization", "text": " \nclass torch.nn.quantized.FloatFunctional [source]\n \nState collector class for float operations. The instance of this class can be used instead of the torch. prefix for some operations. See example usage below.  Note This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).  Examples: >>> f_add = FloatFunctional()\n>>> a = torch.tensor(3.0)\n>>> b = torch.tensor(4.0)\n>>> f_add.add(a, b)  # Equivalent to ``torch.add(a, b)``\n  Valid operation names:\n\n add cat mul add_relu add_scalar mul_scalar    \n"}, {"name": "torch.nn.quantized.functional.adaptive_avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.adaptive_avg_pool2d(input, output_size) [source]\n \nApplies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters propagate to the output.  See AdaptiveAvgPool2d for details and output shape.  Parameters \noutput_size \u2013 the target output size (single integer or double-integer tuple)   \n"}, {"name": "torch.nn.quantized.functional.avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) [source]\n \nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW  regions by step size sH\u00d7sWsH \\times sW  steps. The number of output features is equal to the number of input planes.  Note The input quantization parameters propagate to the output.  See AvgPool2d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n"}, {"name": "torch.nn.quantized.functional.conv1d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv1d", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8) [source]\n \nApplies a 1D convolution over a quantized 1D input composed of several input planes. See Conv1d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW) \n \nweight \u2013 quantized filters of shape (out_channels,in_channelsgroups,iW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , iW) \n \nbias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels}) . The tensor type must be torch.float. \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sW,). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dW,). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \npadding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d \nscale \u2013 quantization scale for the output. Default: 1.0 \nzero_point \u2013 quantization zero_point for the output. Default: 0 \ndtype \u2013 quantization data type to use. Default: torch.quint8\n    Examples: >>> from torch.nn.quantized import functional as qF\n>>> filters = torch.randn(33, 16, 3, dtype=torch.float)\n>>> inputs = torch.randn(20, 16, 50, dtype=torch.float)\n>>> bias = torch.randn(33, dtype=torch.float)\n>>>\n>>> scale, zero_point = 1.0, 0\n>>> dtype_inputs = torch.quint8\n>>> dtype_filters = torch.qint8\n>>>\n>>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)\n>>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)\n>>> qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)\n \n"}, {"name": "torch.nn.quantized.functional.conv2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv2d", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8) [source]\n \nApplies a 2D convolution over a quantized 2D input composed of several input planes. See Conv2d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW) \n \nweight \u2013 quantized filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW) \n \nbias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels}) . The tensor type must be torch.float. \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \npadding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d \nscale \u2013 quantization scale for the output. Default: 1.0 \nzero_point \u2013 quantization zero_point for the output. Default: 0 \ndtype \u2013 quantization data type to use. Default: torch.quint8\n    Examples: >>> from torch.nn.quantized import functional as qF\n>>> filters = torch.randn(8, 4, 3, 3, dtype=torch.float)\n>>> inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)\n>>> bias = torch.randn(8, dtype=torch.float)\n>>>\n>>> scale, zero_point = 1.0, 0\n>>> dtype_inputs = torch.quint8\n>>> dtype_filters = torch.qint8\n>>>\n>>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)\n>>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)\n>>> qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)\n \n"}, {"name": "torch.nn.quantized.functional.conv3d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv3d", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8) [source]\n \nApplies a 3D convolution over a quantized 3D input composed of several input planes. See Conv3d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor of shape (minibatch,in_channels,iD,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iD , iH , iW) \n \nweight \u2013 quantized filters of shape (out_channels,in_channelsgroups,kD,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kD , kH , kW) \n \nbias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels}) . The tensor type must be torch.float. \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padD, padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dD, dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels}  should be divisible by the number of groups. Default: 1 \npadding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d \nscale \u2013 quantization scale for the output. Default: 1.0 \nzero_point \u2013 quantization zero_point for the output. Default: 0 \ndtype \u2013 quantization data type to use. Default: torch.quint8\n    Examples: >>> from torch.nn.quantized import functional as qF\n>>> filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float)\n>>> inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float)\n>>> bias = torch.randn(8, dtype=torch.float)\n>>>\n>>> scale, zero_point = 1.0, 0\n>>> dtype_inputs = torch.quint8\n>>> dtype_filters = torch.qint8\n>>>\n>>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)\n>>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)\n>>> qF.conv3d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)\n \n"}, {"name": "torch.nn.quantized.functional.hardswish()", "path": "torch.nn.quantized#torch.nn.quantized.functional.hardswish", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.hardswish(input, scale, zero_point) [source]\n \nThis is the quantized version of hardswish().  Parameters \n \ninput \u2013 quantized input \nscale \u2013 quantization scale of the output tensor \nzero_point \u2013 quantization zero point of the output tensor    \n"}, {"name": "torch.nn.quantized.functional.interpolate()", "path": "torch.nn.quantized#torch.nn.quantized.functional.interpolate", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nDown/up samples the input to either the given size or the given scale_factor See torch.nn.functional.interpolate() for implementation details. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.  Note The input quantization parameters propagate to the output.   Note Only 2D/3D input is supported for quantized inputs   Note Only the following modes are supported for the quantized inputs:  bilinear nearest    Parameters \n \ninput (Tensor) \u2013 the input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (str) \u2013 algorithm used for upsampling: 'nearest' | 'bilinear'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'bilinear'. Default: False\n    \n"}, {"name": "torch.nn.quantized.functional.linear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.linear", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None) [source]\n \nApplies a linear transformation to the incoming quantized data: y=xAT+by = xA^T + b . See Linear  Note Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use Linear.   Parameters \n \ninput (Tensor) \u2013 Quantized input of type torch.quint8\n \nweight (Tensor) \u2013 Quantized weight of type torch.qint8\n \nbias (Tensor) \u2013 None or fp32 bias of type torch.float\n \nscale (double) \u2013 output scale. If None, derived from the input scale \nzero_point (long) \u2013 output zero point. If None, derived from the input zero_point     Shape:\n\n Input: (N,\u2217,in_features)(N, *, in\\_features)  where * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features) \n Bias: (out_features)(out\\_features) \n Output: (N,\u2217,out_features)(N, *, out\\_features) \n    \n"}, {"name": "torch.nn.quantized.functional.max_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False) [source]\n \nApplies a 2D max pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters are propagated to the output.  See MaxPool2d for details. \n"}, {"name": "torch.nn.quantized.functional.upsample()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nUpsamples the input to either the given size or the given scale_factor  Warning This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(...).  See torch.nn.functional.interpolate() for implementation details. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.  Note The input quantization parameters propagate to the output.   Note Only 2D input is supported for quantized inputs   Note Only the following modes are supported for the quantized inputs:  bilinear nearest    Parameters \n \ninput (Tensor) \u2013 quantized input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to be an integer. \nmode (string) \u2013 algorithm used for upsampling: 'nearest' | 'bilinear'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'bilinear'. Default: False\n     Warning With align_corners = True, the linearly interpolating modes (bilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.  \n"}, {"name": "torch.nn.quantized.functional.upsample_bilinear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using bilinear upsampling.  Warning This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True).   Note The input quantization parameters propagate to the output.   Note Only 2D inputs are supported   Parameters \n \ninput (Tensor) \u2013 quantized input \nsize (int or Tuple[int, int]) \u2013 output spatial size. \nscale_factor (int or Tuple[int, int]) \u2013 multiplier for spatial size    \n"}, {"name": "torch.nn.quantized.functional.upsample_nearest()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": " \ntorch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using nearest neighbours\u2019 pixel values.  Warning This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='nearest').   Note The input quantization parameters propagate to the output.   Note Only 2D inputs are supported   Parameters \n \ninput (Tensor) \u2013 quantized input \nsize (int or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (int) \u2013 multiplier for spatial size. Has to be an integer.    \n"}, {"name": "torch.nn.quantized.GroupNorm", "path": "torch.nn.quantized#torch.nn.quantized.GroupNorm", "type": "Quantization", "text": " \nclass torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True) [source]\n \nThis is the quantized version of GroupNorm.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.nn.quantized.Hardswish", "path": "torch.nn.quantized#torch.nn.quantized.Hardswish", "type": "Quantization", "text": " \nclass torch.nn.quantized.Hardswish(scale, zero_point) [source]\n \nThis is the quantized version of Hardswish.  Parameters \n \nscale \u2013 quantization scale of the output tensor \nzero_point \u2013 quantization zero point of the output tensor    \n"}, {"name": "torch.nn.quantized.InstanceNorm1d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": " \nclass torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) [source]\n \nThis is the quantized version of InstanceNorm1d.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.nn.quantized.InstanceNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": " \nclass torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) [source]\n \nThis is the quantized version of InstanceNorm2d.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.nn.quantized.InstanceNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": " \nclass torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False) [source]\n \nThis is the quantized version of InstanceNorm3d.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.nn.quantized.LayerNorm", "path": "torch.nn.quantized#torch.nn.quantized.LayerNorm", "type": "Quantization", "text": " \nclass torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True) [source]\n \nThis is the quantized version of LayerNorm.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.nn.quantized.Linear", "path": "torch.nn.quantized#torch.nn.quantized.Linear", "type": "Quantization", "text": " \nclass torch.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8) [source]\n \nA quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n \n~Linear.weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}) . \n~Linear.bias (Tensor) \u2013 the non-learnable bias of the module of shape (out_features)(\\text{out\\_features}) . If bias is True, the values are initialized to zero. \n~Linear.scale \u2013 scale parameter of output Quantized Tensor, type: double \n~Linear.zero_point \u2013 zero_point parameter for output Quantized Tensor, type: long    Examples: >>> m = nn.quantized.Linear(20, 30)\n>>> input = torch.randn(128, 20)\n>>> input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n  \nclassmethod from_float(mod) [source]\n \nCreate a quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n \n"}, {"name": "torch.nn.quantized.Linear.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Linear.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   \n"}, {"name": "torch.nn.quantized.QFunctional", "path": "torch.nn.quantized#torch.nn.quantized.QFunctional", "type": "Quantization", "text": " \nclass torch.nn.quantized.QFunctional [source]\n \nWrapper class for quantized operations. The instance of this class can be used instead of the torch.ops.quantized prefix. See example usage below.  Note This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).  Examples: >>> q_add = QFunctional()\n>>> a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)\n>>> b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)\n>>> q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(a, b, 1.0, 0)``\n  Valid operation names:\n\n add cat mul add_relu add_scalar mul_scalar    \n"}, {"name": "torch.nn.quantized.Quantize", "path": "torch.nn.quantized#torch.nn.quantized.Quantize", "type": "Quantization", "text": " \nclass torch.nn.quantized.Quantize(scale, zero_point, dtype) [source]\n \nQuantizes an incoming tensor  Parameters \n \nscale \u2013 scale of the output Quantized Tensor \nzero_point \u2013 zero_point of output Quantized Tensor \ndtype \u2013 data type of output Quantized Tensor   Variables \nzero_point, dtype (`scale`,) \u2013     Examples::\n\n>>> t = torch.tensor([[1., -1.], [1., -1.]])\n>>> scale, zero_point, dtype = 1.0, 2, torch.qint8\n>>> qm = Quantize(scale, zero_point, dtype)\n>>> qt = qm(t)\n>>> print(qt)\ntensor([[ 1., -1.],\n        [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)\n   \n"}, {"name": "torch.nn.quantized.ReLU6", "path": "torch.nn.quantized#torch.nn.quantized.ReLU6", "type": "Quantization", "text": " \nclass torch.nn.quantized.ReLU6(inplace=False) [source]\n \nApplies the element-wise function: ReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6)) , where x0x_0  is the zero_point, and q(6)q(6)  is the quantized representation of number 6.  Parameters \ninplace \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.quantized.ReLU6()\n>>> input = torch.randn(2)\n>>> input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.ReflectionPad1d", "path": "generated/torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d", "type": "torch.nn", "text": " \nclass torch.nn.ReflectionPad1d(padding) [source]\n \nPads the input tensor using the reflection of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} )    Shape:\n\n Input: (N,C,Win)(N, C, W_{in}) \n \nOutput: (N,C,Wout)(N, C, W_{out})  where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ReflectionPad1d(2)\n>>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n>>> input\ntensor([[[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]])\n>>> m(input)\ntensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\n         [6., 5., 4., 5., 6., 7., 6., 5.]]])\n>>> # using different paddings for different sides\n>>> m = nn.ReflectionPad1d((3, 1))\n>>> m(input)\ntensor([[[3., 2., 1., 0., 1., 2., 3., 2.],\n         [7., 6., 5., 4., 5., 6., 7., 6.]]])\n \n"}, {"name": "torch.nn.ReflectionPad2d", "path": "generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d", "type": "torch.nn", "text": " \nclass torch.nn.ReflectionPad2d(padding) [source]\n \nPads the input tensor using the reflection of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )    Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}  Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ReflectionPad2d(2)\n>>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n>>> input\ntensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]]]])\n>>> m(input)\ntensor([[[[8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.]]]])\n>>> # using different paddings for different sides\n>>> m = nn.ReflectionPad2d((1, 1, 2, 0))\n>>> m(input)\ntensor([[[[7., 6., 7., 8., 7.],\n          [4., 3., 4., 5., 4.],\n          [1., 0., 1., 2., 1.],\n          [4., 3., 4., 5., 4.],\n          [7., 6., 7., 8., 7.]]]])\n \n"}, {"name": "torch.nn.ReLU", "path": "generated/torch.nn.relu#torch.nn.ReLU", "type": "torch.nn", "text": " \nclass torch.nn.ReLU(inplace=False) [source]\n \nApplies the rectified linear unit function element-wise: ReLU(x)=(x)+=max\u2061(0,x)\\text{ReLU}(x) = (x)^+ = \\max(0, x)   Parameters \ninplace \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples:   >>> m = nn.ReLU()\n  >>> input = torch.randn(2)\n  >>> output = m(input)\n\n\nAn implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n  >>> m = nn.ReLU()\n  >>> input = torch.randn(2).unsqueeze(0)\n  >>> output = torch.cat((m(input),m(-input)))\n \n"}, {"name": "torch.nn.ReLU6", "path": "generated/torch.nn.relu6#torch.nn.ReLU6", "type": "torch.nn", "text": " \nclass torch.nn.ReLU6(inplace=False) [source]\n \nApplies the element-wise function:  ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)  \n Parameters \ninplace \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.ReLU6()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.ReplicationPad1d", "path": "generated/torch.nn.replicationpad1d#torch.nn.ReplicationPad1d", "type": "torch.nn", "text": " \nclass torch.nn.ReplicationPad1d(padding) [source]\n \nPads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} )    Shape:\n\n Input: (N,C,Win)(N, C, W_{in}) \n \nOutput: (N,C,Wout)(N, C, W_{out})  where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ReplicationPad1d(2)\n>>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n>>> input\ntensor([[[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]])\n>>> m(input)\ntensor([[[0., 0., 0., 1., 2., 3., 3., 3.],\n         [4., 4., 4., 5., 6., 7., 7., 7.]]])\n>>> # using different paddings for different sides\n>>> m = nn.ReplicationPad1d((3, 1))\n>>> m(input)\ntensor([[[0., 0., 0., 0., 1., 2., 3., 3.],\n         [4., 4., 4., 4., 5., 6., 7., 7.]]])\n \n"}, {"name": "torch.nn.ReplicationPad2d", "path": "generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d", "type": "torch.nn", "text": " \nclass torch.nn.ReplicationPad2d(padding) [source]\n \nPads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )    Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}  Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ReplicationPad2d(2)\n>>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n>>> input\ntensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]]]])\n>>> m(input)\ntensor([[[[0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [3., 3., 3., 4., 5., 5., 5.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.]]]])\n>>> # using different paddings for different sides\n>>> m = nn.ReplicationPad2d((1, 1, 2, 0))\n>>> m(input)\ntensor([[[[0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [3., 3., 4., 5., 5.],\n          [6., 6., 7., 8., 8.]]]])\n \n"}, {"name": "torch.nn.ReplicationPad3d", "path": "generated/torch.nn.replicationpad3d#torch.nn.ReplicationPad3d", "type": "torch.nn", "text": " \nclass torch.nn.ReplicationPad3d(padding) [source]\n \nPads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} , padding_front\\text{padding\\_front} , padding_back\\text{padding\\_back} )    Shape:\n\n Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in}) \n \nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}  Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}  Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ReplicationPad3d(3)\n>>> input = torch.randn(16, 3, 8, 320, 480)\n>>> output = m(input)\n>>> # using different paddings for different sides\n>>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))\n>>> output = m(input)\n \n"}, {"name": "torch.nn.RNN", "path": "generated/torch.nn.rnn#torch.nn.RNN", "type": "torch.nn", "text": " \nclass torch.nn.RNN(*args, **kwargs) [source]\n \nApplies a multi-layer Elman RNN with tanh\u2061\\tanh  or ReLU\\text{ReLU}  non-linearity to an input sequence. For each element in the input sequence, each layer computes the following function:  ht=tanh\u2061(Wihxt+bih+Whhh(t\u22121)+bhh)h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})  \nwhere hth_t  is the hidden state at time t, xtx_t  is the input at time t, and h(t\u22121)h_{(t-1)}  is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is 'relu', then ReLU\\text{ReLU}  is used instead of tanh\u2061\\tanh .  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nnum_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 \nnonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n \nbatch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n \ndropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0 \nbidirectional \u2013 If True, becomes a bidirectional RNN. Default: False\n     Inputs: input, h_0\n\n \ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. \nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n\n\n \noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  \nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape:\n\n Input1: (L,N,Hin)(L, N, H_{in})  tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}  and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})  tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}  Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}  If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})  where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size} \n Output2: (S,N,Hout)(S, N, H_{out})  tensor containing the next hidden state for each element in the batch     Variables \n \n~RNN.weight_ih_l[k] \u2013 the learnable input-hidden weights of the k-th layer, of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is (hidden_size, num_directions * hidden_size)\n \n~RNN.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size, hidden_size)\n \n~RNN.bias_ih_l[k] \u2013 the learnable input-hidden bias of the k-th layer, of shape (hidden_size)\n \n~RNN.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)\n     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}    Warning There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables: On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1. This may affect performance. On CUDA 10.2 or later, set environment variable (note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2. See the cuDNN 8 Release Notes for more information.   Orphan   Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: >>> rnn = nn.RNN(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> output, hn = rnn(input, h0)\n \n"}, {"name": "torch.nn.RNNBase", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase", "type": "torch.nn", "text": " \nclass torch.nn.RNNBase(mode, input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, proj_size=0) [source]\n \n \nflatten_parameters() [source]\n \nResets parameter data pointer so that they can use faster code paths. Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op. \n \n"}, {"name": "torch.nn.RNNBase.flatten_parameters()", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase.flatten_parameters", "type": "torch.nn", "text": " \nflatten_parameters() [source]\n \nResets parameter data pointer so that they can use faster code paths. Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op. \n"}, {"name": "torch.nn.RNNCell", "path": "generated/torch.nn.rnncell#torch.nn.RNNCell", "type": "torch.nn", "text": " \nclass torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh') [source]\n \nAn Elman RNN cell with tanh or ReLU non-linearity.  h\u2032=tanh\u2061(Wihx+bih+Whhh+bhh)h' = \\tanh(W_{ih} x + b_{ih} + W_{hh} h + b_{hh}) \nIf nonlinearity is \u2018relu\u2019, then ReLU is used in place of tanh.  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n \nnonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n     Inputs: input, hidden\n\n \ninput of shape (batch, input_size): tensor containing input features \nhidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.   Outputs: h\u2019\n\n \nh\u2019 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch   Shape:\n\n Input1: (N,Hin)(N, H_{in})  tensor containing input features where HinH_{in}  = input_size\n Input2: (N,Hout)(N, H_{out})  tensor containing the initial hidden state for each element in the batch where HoutH_{out}  = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})  tensor containing the next hidden state for each element in the batch     Variables \n \n~RNNCell.weight_ih \u2013 the learnable input-hidden weights, of shape (hidden_size, input_size)\n \n~RNNCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (hidden_size, hidden_size)\n \n~RNNCell.bias_ih \u2013 the learnable input-hidden bias, of shape (hidden_size)\n \n~RNNCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (hidden_size)\n     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}   Examples: >>> rnn = nn.RNNCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n \n"}, {"name": "torch.nn.RReLU", "path": "generated/torch.nn.rrelu#torch.nn.RReLU", "type": "torch.nn", "text": " \nclass torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False) [source]\n \nApplies the randomized leaky rectified liner unit function, element-wise, as described in the paper: Empirical Evaluation of Rectified Activations in Convolutional Network. The function is defined as:  RReLU(x)={xif x\u22650ax otherwise \\text{RReLU}(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ ax & \\text{ otherwise } \\end{cases}  \nwhere aa  is randomly sampled from uniform distribution U(lower,upper)\\mathcal{U}(\\text{lower}, \\text{upper}) . See: https://arxiv.org/pdf/1505.00853.pdf  Parameters \n \nlower \u2013 lower bound of the uniform distribution. Default: 18\\frac{1}{8} \n \nupper \u2013 upper bound of the uniform distribution. Default: 13\\frac{1}{3} \n \ninplace \u2013 can optionally do the operation in-place. Default: False\n     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input    Examples: >>> m = nn.RReLU(0.1, 0.3)\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.SELU", "path": "generated/torch.nn.selu#torch.nn.SELU", "type": "torch.nn", "text": " \nclass torch.nn.SELU(inplace=False) [source]\n \nApplied element-wise, as:  SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))  \nwith \u03b1=1.6732632423543772848170429916717\\alpha = 1.6732632423543772848170429916717  and scale=1.0507009873554804934193349852946\\text{scale} = 1.0507009873554804934193349852946 . More details can be found in the paper Self-Normalizing Neural Networks .  Parameters \ninplace (bool, optional) \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.SELU()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Sequential", "path": "generated/torch.nn.sequential#torch.nn.Sequential", "type": "torch.nn", "text": " \nclass torch.nn.Sequential(*args) [source]\n \nA sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in. To make it easier to understand, here is a small example: # Example of using Sequential\nmodel = nn.Sequential(\n          nn.Conv2d(1,20,5),\n          nn.ReLU(),\n          nn.Conv2d(20,64,5),\n          nn.ReLU()\n        )\n\n# Example of using Sequential with OrderedDict\nmodel = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ]))\n \n"}, {"name": "torch.nn.Sigmoid", "path": "generated/torch.nn.sigmoid#torch.nn.Sigmoid", "type": "torch.nn", "text": " \nclass torch.nn.Sigmoid [source]\n \nApplies the element-wise function:  Sigmoid(x)=\u03c3(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}  \n Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Sigmoid()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.SiLU", "path": "generated/torch.nn.silu#torch.nn.SiLU", "type": "torch.nn", "text": " \nclass torch.nn.SiLU(inplace=False) [source]\n \nApplies the silu function, element-wise.  silu(x)=x\u2217\u03c3(x),where \u03c3(x) is the logistic sigmoid.\\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}  \n Note See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.   Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input    Examples: >>> m = nn.SiLU()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.SmoothL1Loss", "path": "generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss", "type": "torch.nn", "text": " \nclass torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0) [source]\n \nCreates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the torch.nn.MSELoss and in some cases prevents exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick). Omitting a scaling factor of beta, this loss is also known as the Huber loss:  loss(x,y)=1n\u2211izi\\text{loss}(x, y) = \\frac{1}{n} \\sum_{i} z_{i}  \nwhere ziz_{i}  is given by:  zi={0.5(xi\u2212yi)2/beta,if \u2223xi\u2212yi\u2223<beta\u2223xi\u2212yi\u2223\u22120.5\u2217beta,otherwise z_{i} = \\begin{cases} 0.5 (x_i - y_i)^2 / beta, & \\text{if } |x_i - y_i| < beta \\\\ |x_i - y_i| - 0.5 * beta, & \\text{otherwise } \\end{cases}  \nxx  and yy  arbitrary shapes with a total of nn  elements each the sum operation still operates over all the elements, and divides by nn . beta is an optional parameter that defaults to 1. Note: When beta is set to 0, this is equivalent to L1Loss. Passing a negative value in for beta will result in an exception. The division by nn  can be avoided if sets reduction = 'sum'.  Parameters \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n \nbeta (float, optional) \u2013 Specifies the threshold at which to change between L1 and L2 loss. This value defaults to 1.0.     Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  means, any number of additional dimensions Target: (N,\u2217)(N, *) , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *) , same shape as the input    \n"}, {"name": "torch.nn.SoftMarginLoss", "path": "generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss", "type": "torch.nn", "text": " \nclass torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that optimizes a two-class classification logistic loss between input tensor xx  and target tensor yy  (containing 1 or -1).  loss(x,y)=\u2211ilog\u2061(1+exp\u2061(\u2212y[i]\u2217x[i]))x.nelement()\\text{loss}(x, y) = \\sum_i \\frac{\\log(1 + \\exp(-y[i]*x[i]))}{\\text{x.nelement}()}  \n Parameters \n \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (\u2217)(*)  where \u2217*  means, any number of additional dimensions Target: (\u2217)(*) , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input    \n"}, {"name": "torch.nn.Softmax", "path": "generated/torch.nn.softmax#torch.nn.Softmax", "type": "torch.nn", "text": " \nclass torch.nn.Softmax(dim=None) [source]\n \nApplies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. Softmax is defined as:  Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}  \nWhen the input Tensor is a sparse tensor then the unspecifed values are treated as -inf.  Shape:\n\n Input: (\u2217)(*)  where * means, any number of additional dimensions Output: (\u2217)(*) , same shape as the input     Returns \na Tensor of the same dimension and shape as the input with values in the range [0, 1]  Parameters \ndim (int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1).    Note This module doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it\u2019s faster and has better numerical properties).  Examples: >>> m = nn.Softmax(dim=1)\n>>> input = torch.randn(2, 3)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Softmax2d", "path": "generated/torch.nn.softmax2d#torch.nn.Softmax2d", "type": "torch.nn", "text": " \nclass torch.nn.Softmax2d [source]\n \nApplies SoftMax over features to each spatial location. When given an image of Channels x Height x Width, it will apply Softmax to each location (Channels,hi,wj)(Channels, h_i, w_j)   Shape:\n\n Input: (N,C,H,W)(N, C, H, W) \n Output: (N,C,H,W)(N, C, H, W)  (same shape as input)     Returns \na Tensor of the same dimension and shape as the input with values in the range [0, 1]   Examples: >>> m = nn.Softmax2d()\n>>> # you softmax over the 2nd dimension\n>>> input = torch.randn(2, 3, 12, 13)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Softmin", "path": "generated/torch.nn.softmin#torch.nn.Softmin", "type": "torch.nn", "text": " \nclass torch.nn.Softmin(dim=None) [source]\n \nApplies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1. Softmin is defined as:  Softmin(xi)=exp\u2061(\u2212xi)\u2211jexp\u2061(\u2212xj)\\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}  \n Shape:\n\n Input: (\u2217)(*)  where * means, any number of additional dimensions Output: (\u2217)(*) , same shape as the input     Parameters \ndim (int) \u2013 A dimension along which Softmin will be computed (so every slice along dim will sum to 1).  Returns \na Tensor of the same dimension and shape as the input, with values in the range [0, 1]   Examples: >>> m = nn.Softmin()\n>>> input = torch.randn(2, 3)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Softplus", "path": "generated/torch.nn.softplus#torch.nn.Softplus", "type": "torch.nn", "text": " \nclass torch.nn.Softplus(beta=1, threshold=20) [source]\n \nApplies the element-wise function:  Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))  \nSoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive. For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold .  Parameters \n \nbeta \u2013 the \u03b2\\beta  value for the Softplus formulation. Default: 1 \nthreshold \u2013 values above this revert to a linear function. Default: 20     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Softplus()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Softshrink", "path": "generated/torch.nn.softshrink#torch.nn.Softshrink", "type": "torch.nn", "text": " \nclass torch.nn.Softshrink(lambd=0.5) [source]\n \nApplies the soft shrinkage function elementwise:  SoftShrinkage(x)={x\u2212\u03bb, if x>\u03bbx+\u03bb, if x<\u2212\u03bb0, otherwise \\text{SoftShrinkage}(x) = \\begin{cases} x - \\lambda, & \\text{ if } x > \\lambda \\\\ x + \\lambda, & \\text{ if } x < -\\lambda \\\\ 0, & \\text{ otherwise } \\end{cases}  \n Parameters \nlambd \u2013 the \u03bb\\lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5    Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Softshrink()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Softsign", "path": "generated/torch.nn.softsign#torch.nn.Softsign", "type": "torch.nn", "text": " \nclass torch.nn.Softsign [source]\n \nApplies the element-wise function:  SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}  \n Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Softsign()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.SyncBatchNorm", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm", "type": "torch.nn", "text": " \nclass torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, process_group=None) [source]\n \nApplies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta \nThe mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are sampled from U(0,1)\\mathcal{U}(0, 1)  and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.  Because the Batch Normalization is done for each channel in the C dimension, computing statistics on (N, +) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization. Currently SyncBatchNorm only supports DistributedDataParallel (DDP) with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm*D layer to SyncBatchNorm before wrapping Network with DDP.  Parameters \n \nnum_features \u2013 CC  from an expected input of size (N,C,+)(N, C, +) \n \neps \u2013 a value added to the denominator for numerical stability. Default: 1e-5\n \nmomentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 \naffine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True\n \ntrack_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics, and initializes statistics buffers running_mean and running_var as None. When these buffers are None, this module always uses batch statistics. in both training and eval modes. Default: True\n \nprocess_group \u2013 synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world     Shape:\n\n Input: (N,C,+)(N, C, +) \n Output: (N,C,+)(N, C, +)  (same shape as input)    Examples: >>> # With Learnable Parameters\n>>> m = nn.SyncBatchNorm(100)\n>>> # creating process group (optional)\n>>> # ranks is a list of int identifying rank ids.\n>>> ranks = list(range(8))\n>>> r1, r2 = ranks[:4], ranks[4:]\n>>> # Note: every rank calls into new_group for every\n>>> # process group created, even if that rank is not\n>>> # part of the group.\n>>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]\n>>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]\n>>> # Without Learnable Parameters\n>>> m = nn.BatchNorm3d(100, affine=False, process_group=process_group)\n>>> input = torch.randn(20, 100, 35, 45, 10)\n>>> output = m(input)\n\n>>> # network is nn.BatchNorm layer\n>>> sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)\n>>> # only single gpu per process is currently supported\n>>> ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(\n>>>                         sync_bn_network,\n>>>                         device_ids=[args.local_rank],\n>>>                         output_device=args.local_rank)\n  \nclassmethod convert_sync_batchnorm(module, process_group=None) [source]\n \nHelper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.  Parameters \n \nmodule (nn.Module) \u2013 module containing one or more attr:BatchNorm*D layers \nprocess_group (optional) \u2013 process group to scope synchronization, default is the whole world   Returns \nThe original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.   Example: >>> # Network with nn.BatchNorm layer\n>>> module = torch.nn.Sequential(\n>>>            torch.nn.Linear(20, 100),\n>>>            torch.nn.BatchNorm1d(100),\n>>>          ).cuda()\n>>> # creating process group (optional)\n>>> # ranks is a list of int identifying rank ids.\n>>> ranks = list(range(8))\n>>> r1, r2 = ranks[:4], ranks[4:]\n>>> # Note: every rank calls into new_group for every\n>>> # process group created, even if that rank is not\n>>> # part of the group.\n>>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]\n>>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]\n>>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\n \n \n"}, {"name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm()", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm.convert_sync_batchnorm", "type": "torch.nn", "text": " \nclassmethod convert_sync_batchnorm(module, process_group=None) [source]\n \nHelper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.  Parameters \n \nmodule (nn.Module) \u2013 module containing one or more attr:BatchNorm*D layers \nprocess_group (optional) \u2013 process group to scope synchronization, default is the whole world   Returns \nThe original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.   Example: >>> # Network with nn.BatchNorm layer\n>>> module = torch.nn.Sequential(\n>>>            torch.nn.Linear(20, 100),\n>>>            torch.nn.BatchNorm1d(100),\n>>>          ).cuda()\n>>> # creating process group (optional)\n>>> # ranks is a list of int identifying rank ids.\n>>> ranks = list(range(8))\n>>> r1, r2 = ranks[:4], ranks[4:]\n>>> # Note: every rank calls into new_group for every\n>>> # process group created, even if that rank is not\n>>> # part of the group.\n>>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]\n>>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1]\n>>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\n \n"}, {"name": "torch.nn.Tanh", "path": "generated/torch.nn.tanh#torch.nn.Tanh", "type": "torch.nn", "text": " \nclass torch.nn.Tanh [source]\n \nApplies the element-wise function:  Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}  \n Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Tanh()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Tanhshrink", "path": "generated/torch.nn.tanhshrink#torch.nn.Tanhshrink", "type": "torch.nn", "text": " \nclass torch.nn.Tanhshrink [source]\n \nApplies the element-wise function:  Tanhshrink(x)=x\u2212tanh\u2061(x)\\text{Tanhshrink}(x) = x - \\tanh(x)  \n Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input     Examples: >>> m = nn.Tanhshrink()\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Threshold", "path": "generated/torch.nn.threshold#torch.nn.Threshold", "type": "torch.nn", "text": " \nclass torch.nn.Threshold(threshold, value, inplace=False) [source]\n \nThresholds each element of the input Tensor. Threshold is defined as:  y={x, if x>thresholdvalue, otherwise y = \\begin{cases} x, &\\text{ if } x > \\text{threshold} \\\\ \\text{value}, &\\text{ otherwise } \\end{cases}  \n Parameters \n \nthreshold \u2013 The value to threshold at \nvalue \u2013 The value to replace with \ninplace \u2013 can optionally do the operation in-place. Default: False\n     Shape:\n\n Input: (N,\u2217)(N, *)  where * means, any number of additional dimensions Output: (N,\u2217)(N, *) , same shape as the input    Examples: >>> m = nn.Threshold(0.1, 20)\n>>> input = torch.randn(2)\n>>> output = m(input)\n \n"}, {"name": "torch.nn.Transformer", "path": "generated/torch.nn.transformer#torch.nn.Transformer", "type": "torch.nn", "text": " \nclass torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation='relu', custom_encoder=None, custom_decoder=None) [source]\n \nA transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.  Parameters \n \nd_model \u2013 the number of expected features in the encoder/decoder inputs (default=512). \nnhead \u2013 the number of heads in the multiheadattention models (default=8). \nnum_encoder_layers \u2013 the number of sub-encoder-layers in the encoder (default=6). \nnum_decoder_layers \u2013 the number of sub-decoder-layers in the decoder (default=6). \ndim_feedforward \u2013 the dimension of the feedforward network model (default=2048). \ndropout \u2013 the dropout value (default=0.1). \nactivation \u2013 the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). \ncustom_encoder \u2013 custom encoder (default=None). \ncustom_decoder \u2013 custom decoder (default=None).     Examples::\n\n>>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n>>> src = torch.rand((10, 32, 512))\n>>> tgt = torch.rand((20, 32, 512))\n>>> out = transformer_model(src, tgt)\n   Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model  \nforward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None) [source]\n \nTake in and process masked source/target sequences.  Parameters \n \nsrc \u2013 the sequence to the encoder (required). \ntgt \u2013 the sequence to the decoder (required). \nsrc_mask \u2013 the additive mask for the src sequence (optional). \ntgt_mask \u2013 the additive mask for the tgt sequence (optional). \nmemory_mask \u2013 the additive mask for the encoder output (optional). \nsrc_key_padding_mask \u2013 the ByteTensor mask for src keys per batch (optional). \ntgt_key_padding_mask \u2013 the ByteTensor mask for tgt keys per batch (optional). \nmemory_key_padding_mask \u2013 the ByteTensor mask for memory keys per batch (optional).     Shape:\n\n src: (S,N,E)(S, N, E) . tgt: (T,N,E)(T, N, E) . src_mask: (S,S)(S, S) . tgt_mask: (T,T)(T, T) . memory_mask: (T,S)(T, S) . src_key_padding_mask: (N,S)(N, S) . tgt_key_padding_mask: (N,T)(N, T) . memory_key_padding_mask: (N,S)(N, S) .  Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.  output: (T,N,E)(T, N, E) .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number   Examples >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n \n  \ngenerate_square_subsequent_mask(sz) [source]\n \nGenerate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0). \n \n"}, {"name": "torch.nn.Transformer.forward()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.forward", "type": "torch.nn", "text": " \nforward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None) [source]\n \nTake in and process masked source/target sequences.  Parameters \n \nsrc \u2013 the sequence to the encoder (required). \ntgt \u2013 the sequence to the decoder (required). \nsrc_mask \u2013 the additive mask for the src sequence (optional). \ntgt_mask \u2013 the additive mask for the tgt sequence (optional). \nmemory_mask \u2013 the additive mask for the encoder output (optional). \nsrc_key_padding_mask \u2013 the ByteTensor mask for src keys per batch (optional). \ntgt_key_padding_mask \u2013 the ByteTensor mask for tgt keys per batch (optional). \nmemory_key_padding_mask \u2013 the ByteTensor mask for memory keys per batch (optional).     Shape:\n\n src: (S,N,E)(S, N, E) . tgt: (T,N,E)(T, N, E) . src_mask: (S,S)(S, S) . tgt_mask: (T,T)(T, T) . memory_mask: (T,S)(T, S) . src_key_padding_mask: (N,S)(N, S) . tgt_key_padding_mask: (N,T)(N, T) . memory_key_padding_mask: (N,S)(N, S) .  Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.  output: (T,N,E)(T, N, E) .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number   Examples >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n \n"}, {"name": "torch.nn.Transformer.generate_square_subsequent_mask()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.generate_square_subsequent_mask", "type": "torch.nn", "text": " \ngenerate_square_subsequent_mask(sz) [source]\n \nGenerate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0). \n"}, {"name": "torch.nn.TransformerDecoder", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder", "type": "torch.nn", "text": " \nclass torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None) [source]\n \nTransformerDecoder is a stack of N decoder layers  Parameters \n \ndecoder_layer \u2013 an instance of the TransformerDecoderLayer() class (required). \nnum_layers \u2013 the number of sub-decoder-layers in the decoder (required). \nnorm \u2013 the layer normalization component (optional).     Examples::\n\n>>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n>>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n>>> memory = torch.rand(10, 32, 512)\n>>> tgt = torch.rand(20, 32, 512)\n>>> out = transformer_decoder(tgt, memory)\n    \nforward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None) [source]\n \nPass the inputs (and mask) through the decoder layer in turn.  Parameters \n \ntgt \u2013 the sequence to the decoder (required). \nmemory \u2013 the sequence from the last layer of the encoder (required). \ntgt_mask \u2013 the mask for the tgt sequence (optional). \nmemory_mask \u2013 the mask for the memory sequence (optional). \ntgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). \nmemory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n \n"}, {"name": "torch.nn.TransformerDecoder.forward()", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder.forward", "type": "torch.nn", "text": " \nforward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None) [source]\n \nPass the inputs (and mask) through the decoder layer in turn.  Parameters \n \ntgt \u2013 the sequence to the decoder (required). \nmemory \u2013 the sequence from the last layer of the encoder (required). \ntgt_mask \u2013 the mask for the tgt sequence (optional). \nmemory_mask \u2013 the mask for the memory sequence (optional). \ntgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). \nmemory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n"}, {"name": "torch.nn.TransformerDecoderLayer", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer", "type": "torch.nn", "text": " \nclass torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu') [source]\n \nTransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.  Parameters \n \nd_model \u2013 the number of expected features in the input (required). \nnhead \u2013 the number of heads in the multiheadattention models (required). \ndim_feedforward \u2013 the dimension of the feedforward network model (default=2048). \ndropout \u2013 the dropout value (default=0.1). \nactivation \u2013 the activation function of intermediate layer, relu or gelu (default=relu).     Examples::\n\n>>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n>>> memory = torch.rand(10, 32, 512)\n>>> tgt = torch.rand(20, 32, 512)\n>>> out = decoder_layer(tgt, memory)\n    \nforward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None) [source]\n \nPass the inputs (and mask) through the decoder layer.  Parameters \n \ntgt \u2013 the sequence to the decoder layer (required). \nmemory \u2013 the sequence from the last layer of the encoder (required). \ntgt_mask \u2013 the mask for the tgt sequence (optional). \nmemory_mask \u2013 the mask for the memory sequence (optional). \ntgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). \nmemory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n \n"}, {"name": "torch.nn.TransformerDecoderLayer.forward()", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer.forward", "type": "torch.nn", "text": " \nforward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None) [source]\n \nPass the inputs (and mask) through the decoder layer.  Parameters \n \ntgt \u2013 the sequence to the decoder layer (required). \nmemory \u2013 the sequence from the last layer of the encoder (required). \ntgt_mask \u2013 the mask for the tgt sequence (optional). \nmemory_mask \u2013 the mask for the memory sequence (optional). \ntgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). \nmemory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n"}, {"name": "torch.nn.TransformerEncoder", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder", "type": "torch.nn", "text": " \nclass torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None) [source]\n \nTransformerEncoder is a stack of N encoder layers  Parameters \n \nencoder_layer \u2013 an instance of the TransformerEncoderLayer() class (required). \nnum_layers \u2013 the number of sub-encoder-layers in the encoder (required). \nnorm \u2013 the layer normalization component (optional).     Examples::\n\n>>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n>>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n>>> src = torch.rand(10, 32, 512)\n>>> out = transformer_encoder(src)\n    \nforward(src, mask=None, src_key_padding_mask=None) [source]\n \nPass the input through the encoder layers in turn.  Parameters \n \nsrc \u2013 the sequence to the encoder (required). \nmask \u2013 the mask for the src sequence (optional). \nsrc_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n \n"}, {"name": "torch.nn.TransformerEncoder.forward()", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder.forward", "type": "torch.nn", "text": " \nforward(src, mask=None, src_key_padding_mask=None) [source]\n \nPass the input through the encoder layers in turn.  Parameters \n \nsrc \u2013 the sequence to the encoder (required). \nmask \u2013 the mask for the src sequence (optional). \nsrc_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n"}, {"name": "torch.nn.TransformerEncoderLayer", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer", "type": "torch.nn", "text": " \nclass torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu') [source]\n \nTransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.  Parameters \n \nd_model \u2013 the number of expected features in the input (required). \nnhead \u2013 the number of heads in the multiheadattention models (required). \ndim_feedforward \u2013 the dimension of the feedforward network model (default=2048). \ndropout \u2013 the dropout value (default=0.1). \nactivation \u2013 the activation function of intermediate layer, relu or gelu (default=relu).     Examples::\n\n>>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n>>> src = torch.rand(10, 32, 512)\n>>> out = encoder_layer(src)\n    \nforward(src, src_mask=None, src_key_padding_mask=None) [source]\n \nPass the input through the encoder layer.  Parameters \n \nsrc \u2013 the sequence to the encoder layer (required). \nsrc_mask \u2013 the mask for the src sequence (optional). \nsrc_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n \n"}, {"name": "torch.nn.TransformerEncoderLayer.forward()", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer.forward", "type": "torch.nn", "text": " \nforward(src, src_mask=None, src_key_padding_mask=None) [source]\n \nPass the input through the encoder layer.  Parameters \n \nsrc \u2013 the sequence to the encoder layer (required). \nsrc_mask \u2013 the mask for the src sequence (optional). \nsrc_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:\n\nsee the docs in Transformer class.   \n"}, {"name": "torch.nn.TripletMarginLoss", "path": "generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss", "type": "torch.nn", "text": " \nclass torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean') [source]\n \nCreates a criterion that measures the triplet loss given an input tensors x1x1 , x2x2 , x3x3  and a margin with a value greater than 00 . This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n (i.e., anchor, positive examples and negative examples respectively). The shapes of all input tensors should be (N,D)(N, D) . The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. The loss function for each sample in the mini-batch is:  L(a,p,n)=max\u2061{d(ai,pi)\u2212d(ai,ni)+margin,0}L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}  \nwhere  d(xi,yi)=\u2225xi\u2212yi\u2225pd(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p  \nSee also TripletMarginWithDistanceLoss, which computes the triplet margin loss for input tensors using a custom distance function.  Parameters \n \nmargin (float, optional) \u2013 Default: 11 . \np (int, optional) \u2013 The norm degree for pairwise distance. Default: 22 . \nswap (bool, optional) \u2013 The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. Default: False. \nsize_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True\n \nreduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True\n \nreduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'\n     Shape:\n\n Input: (N,D)(N, D)  where DD  is the vector dimension. \n \nOutput: A Tensor of shape (N)(N)  if reduction is 'none', or a scalar \n\notherwise.       >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n>>> anchor = torch.randn(100, 128, requires_grad=True)\n>>> positive = torch.randn(100, 128, requires_grad=True)\n>>> negative = torch.randn(100, 128, requires_grad=True)\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n \n"}, {"name": "torch.nn.TripletMarginWithDistanceLoss", "path": "generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss", "type": "torch.nn", "text": " \nclass torch.nn.TripletMarginWithDistanceLoss(*, distance_function=None, margin=1.0, swap=False, reduction='mean') [source]\n \nCreates a criterion that measures the triplet loss given input tensors aa , pp , and nn  (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d). The unreduced loss (i.e., with reduction set to 'none') can be described as:  \u2113(a,p,n)=L={l1,\u2026,lN}\u22a4,li=max\u2061{d(ai,pi)\u2212d(ai,ni)+margin,0}\\ell(a, p, n) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_i = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}  \nwhere NN  is the batch size; dd  is a nonnegative, real-valued function quantifying the closeness of two tensors, referred to as the distance_function; and marginmargin  is a nonnegative margin representing the minimum difference between the positive and negative distances that is required for the loss to be 0. The input tensors have NN  elements each and can be of any shape that the distance function can handle. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if reduction=\u2018mean\u2019;sum\u2061(L),if reduction=\u2018sum\u2019.\\ell(x, y) = \\begin{cases} \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\ \\operatorname{sum}(L), & \\text{if reduction} = \\text{`sum'.} \\end{cases}  \nSee also TripletMarginLoss, which computes the triplet loss for input tensors using the lpl_p  distance as the distance function.  Parameters \n \ndistance_function (callable, optional) \u2013 A nonnegative, real-valued function that quantifies the closeness of two tensors. If not specified, nn.PairwiseDistance will be used. Default: None\n \nmargin (float, optional) \u2013 A nonnegative margin representing the minimum difference between the positive and negative distances required for the loss to be 0. Larger margins penalize cases where the negative examples are not distant enough from the anchors, relative to the positives. Default: 11 . \nswap (bool, optional) \u2013 Whether to use the distance swap described in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. If True, and if the positive example is closer to the negative example than the anchor is, swaps the positive example and the anchor in the loss computation. Default: False. \nreduction (string, optional) \u2013 Specifies the (optional) reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Default: 'mean'\n     Shape:\n\n Input: (N,\u2217)(N, *)  where \u2217*  represents any number of additional dimensions as supported by the distance function. Output: A Tensor of shape (N)(N)  if reduction is 'none', or a scalar otherwise.    Examples: >>> # Initialize embeddings\n>>> embedding = nn.Embedding(1000, 128)\n>>> anchor_ids = torch.randint(0, 1000, (1,))\n>>> positive_ids = torch.randint(0, 1000, (1,))\n>>> negative_ids = torch.randint(0, 1000, (1,))\n>>> anchor = embedding(anchor_ids)\n>>> positive = embedding(positive_ids)\n>>> negative = embedding(negative_ids)\n>>>\n>>> # Built-in Distance Function\n>>> triplet_loss = \\\n>>>     nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n>>>\n>>> # Custom Distance Function\n>>> def l_infinity(x1, x2):\n>>>     return torch.max(torch.abs(x1 - x2), dim=1).values\n>>>\n>>> triplet_loss = \\\n>>>     nn.TripletMarginWithDistanceLoss(distance_function=l_infinity, margin=1.5)\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n>>>\n>>> # Custom Distance Function (Lambda)\n>>> triplet_loss = \\\n>>>     nn.TripletMarginWithDistanceLoss(\n>>>         distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n>>> output = triplet_loss(anchor, positive, negative)\n>>> output.backward()\n  Reference:\n\nV. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: http://www.bmva.org/bmvc/2016/papers/paper119/index.html   \n"}, {"name": "torch.nn.Unflatten", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten", "type": "torch.nn", "text": " \nclass torch.nn.Unflatten(dim, unflattened_size) [source]\n \nUnflattens a tensor dim expanding it to a desired shape. For use with Sequential.  \ndim specifies the dimension of the input tensor to be unflattened, and it can be either int or str when Tensor or NamedTensor is used, respectively. \nunflattened_size is the new shape of the unflattened dimension of the tensor and it can be a tuple of ints or a list of ints or torch.Size for Tensor input; a NamedShape (tuple of (name, size) tuples) for NamedTensor input.   Shape:\n\n Input: (N,\u2217dims)(N, *dims) \n Output: (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}}) \n     Parameters \n \ndim (Union[int, str]) \u2013 Dimension to be unflattened \nunflattened_size (Union[torch.Size, Tuple, List, NamedShape]) \u2013 New shape of the unflattened dimension    Examples >>> input = torch.randn(2, 50)\n>>> # With tuple of ints\n>>> m = nn.Sequential(\n>>>     nn.Linear(50, 50),\n>>>     nn.Unflatten(1, (2, 5, 5))\n>>> )\n>>> output = m(input)\n>>> output.size()\ntorch.Size([2, 2, 5, 5])\n>>> # With torch.Size\n>>> m = nn.Sequential(\n>>>     nn.Linear(50, 50),\n>>>     nn.Unflatten(1, torch.Size([2, 5, 5]))\n>>> )\n>>> output = m(input)\n>>> output.size()\ntorch.Size([2, 2, 5, 5])\n>>> # With namedshape (tuple of tuples)\n>>> input = torch.randn(2, 50, names=('N', 'features'))\n>>> unflatten = nn.Unflatten('features', (('C', 2), ('H', 5), ('W', 5)))\n>>> output = unflatten(input)\n>>> output.size()\ntorch.Size([2, 2, 5, 5])\n  \nadd_module(name, module)  \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n  \napply(fn)  \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n  \nbfloat16()  \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n  \nbuffers(recurse=True)  \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nchildren()  \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n  \ncpu()  \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n  \ncuda(device=None)  \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \ndouble()  \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n  \neval()  \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n  \nfloat()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n  \nhalf()  \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n  \nload_state_dict(state_dict, strict=True)  \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n  \nmodules()  \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n  \nnamed_buffers(prefix='', recurse=True)  \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n  \nnamed_children()  \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n  \nnamed_modules(memo=None, prefix='')  \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n  \nnamed_parameters(prefix='', recurse=True)  \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n  \nparameters(recurse=True)  \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n  \nregister_backward_hook(hook)  \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_buffer(name, tensor, persistent=True)  \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n  \nregister_forward_hook(hook)  \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_forward_pre_hook(hook)  \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_full_backward_hook(hook)  \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n  \nregister_parameter(name, param)  \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n  \nrequires_grad_(requires_grad=True)  \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n  \nstate_dict(destination=None, prefix='', keep_vars=False)  \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n  \nto(*args, **kwargs)  \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) \n  \nto(dtype, non_blocking=False) \n  \nto(tensor, non_blocking=False) \n  \nto(memory_format=torch.channels_last) \n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n  \ntrain(mode=True)  \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n  \ntype(dst_type)  \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n  \nxpu(device=None)  \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n  \nzero_grad(set_to_none=False)  \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n \n"}, {"name": "torch.nn.Unflatten.add_module()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.add_module", "type": "torch.nn", "text": " \nadd_module(name, module)  \nAdds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters \n \nname (string) \u2013 name of the child module. The child module can be accessed from this module using the given name \nmodule (Module) \u2013 child module to be added to the module.    \n"}, {"name": "torch.nn.Unflatten.apply()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.apply", "type": "torch.nn", "text": " \napply(fn)  \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   Example: >>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n \n"}, {"name": "torch.nn.Unflatten.bfloat16()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.bfloat16", "type": "torch.nn", "text": " \nbfloat16()  \nCasts all floating point parameters and buffers to bfloat16 datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.buffers", "type": "torch.nn", "text": " \nbuffers(recurse=True)  \nReturns an iterator over module buffers.  Parameters \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields \ntorch.Tensor \u2013 module buffer   Example: >>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.nn.Unflatten.children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.children", "type": "torch.nn", "text": " \nchildren()  \nReturns an iterator over immediate children modules.  Yields \nModule \u2013 a child module   \n"}, {"name": "torch.nn.Unflatten.cpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cpu", "type": "torch.nn", "text": " \ncpu()  \nMoves all model parameters and buffers to the CPU.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.cuda()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cuda", "type": "torch.nn", "text": " \ncuda(device=None)  \nMoves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.double()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.double", "type": "torch.nn", "text": " \ndouble()  \nCasts all floating point parameters and buffers to double datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.eval()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.eval", "type": "torch.nn", "text": " \neval()  \nSets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.float()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.float", "type": "torch.nn", "text": " \nfloat()  \nCasts all floating point parameters and buffers to float datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.half()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.half", "type": "torch.nn", "text": " \nhalf()  \nCasts all floating point parameters and buffers to half datatype.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.load_state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.load_state_dict", "type": "torch.nn", "text": " \nload_state_dict(state_dict, strict=True)  \nCopies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters \n \nstate_dict (dict) \u2013 a dict containing parameters and persistent buffers. \nstrict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True\n   Returns \n \nmissing_keys is a list of str containing the missing keys \nunexpected_keys is a list of str containing the unexpected keys   Return type \nNamedTuple with missing_keys and unexpected_keys fields   \n"}, {"name": "torch.nn.Unflatten.modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.modules", "type": "torch.nn", "text": " \nmodules()  \nReturns an iterator over all modules in the network.  Yields \nModule \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n\n0 -> Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n \n"}, {"name": "torch.nn.Unflatten.named_buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_buffers", "type": "torch.nn", "text": " \nnamed_buffers(prefix='', recurse=True)  \nReturns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all buffer names. \nrecurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields \n(string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: >>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n \n"}, {"name": "torch.nn.Unflatten.named_children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_children", "type": "torch.nn", "text": " \nnamed_children()  \nReturns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple containing a name and child module   Example: >>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n \n"}, {"name": "torch.nn.Unflatten.named_modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_modules", "type": "torch.nn", "text": " \nnamed_modules(memo=None, prefix='')  \nReturns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields \n(string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: >>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n        print(idx, '->', m)\n\n0 -> ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n \n"}, {"name": "torch.nn.Unflatten.named_parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_parameters", "type": "torch.nn", "text": " \nnamed_parameters(prefix='', recurse=True)  \nReturns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters \n \nprefix (str) \u2013 prefix to prepend to all parameter names. \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields \n(string, Parameter) \u2013 Tuple containing the name and parameter   Example: >>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n \n"}, {"name": "torch.nn.Unflatten.parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.parameters", "type": "torch.nn", "text": " \nparameters(recurse=True)  \nReturns an iterator over module parameters. This is typically passed to an optimizer.  Parameters \nrecurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields \nParameter \u2013 module parameter   Example: >>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n \n"}, {"name": "torch.nn.Unflatten.register_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_backward_hook", "type": "torch.nn", "text": " \nregister_backward_hook(hook)  \nRegisters a backward hook on the module. This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Unflatten.register_buffer()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_buffer", "type": "torch.nn", "text": " \nregister_buffer(name, tensor, persistent=True)  \nAdds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters \n \nname (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name \ntensor (Tensor) \u2013 buffer to be registered. \npersistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: >>> self.register_buffer('running_mean', torch.zeros(num_features))\n \n"}, {"name": "torch.nn.Unflatten.register_forward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_hook", "type": "torch.nn", "text": " \nregister_forward_hook(hook)  \nRegisters a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -> None or modified output\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Unflatten.register_forward_pre_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_pre_hook", "type": "torch.nn", "text": " \nregister_forward_pre_hook(hook)  \nRegisters a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -> None or modified input\n The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Unflatten.register_full_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_full_backward_hook", "type": "torch.nn", "text": " \nregister_full_backward_hook(hook)  \nRegisters a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.  Warning Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.   Returns \na handle that can be used to remove the added hook by calling handle.remove()  Return type \ntorch.utils.hooks.RemovableHandle   \n"}, {"name": "torch.nn.Unflatten.register_parameter()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_parameter", "type": "torch.nn", "text": " \nregister_parameter(name, param)  \nAdds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters \n \nname (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name \nparam (Parameter) \u2013 parameter to be added to the module.    \n"}, {"name": "torch.nn.Unflatten.requires_grad_()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.requires_grad_", "type": "torch.nn", "text": " \nrequires_grad_(requires_grad=True)  \nChange if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters \nrequires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.state_dict", "type": "torch.nn", "text": " \nstate_dict(destination=None, prefix='', keep_vars=False)  \nReturns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns \na dictionary containing a whole state of the module  Return type \ndict   Example: >>> module.state_dict().keys()\n['bias', 'weight']\n \n"}, {"name": "torch.nn.Unflatten.to()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.to", "type": "torch.nn", "text": " \nto(*args, **kwargs)  \nMoves and/or casts the parameters and buffers. This can be called as  \nto(device=None, dtype=None, non_blocking=False) \n  \nto(dtype, non_blocking=False) \n  \nto(tensor, non_blocking=False) \n  \nto(memory_format=torch.channels_last) \n Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters \n \ndevice (torch.device) \u2013 the desired device of the parameters and buffers in this module \ndtype (torch.dtype) \u2013 the desired floating point or complex dtype of the parameters and buffers in this module \ntensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module \nmemory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns \nself  Return type \nModule   Examples: >>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n>>> gpu1 = torch.device(\"cuda:1\")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n>>> cpu = torch.device(\"cpu\")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j],\n        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n \n"}, {"name": "torch.nn.Unflatten.train()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.train", "type": "torch.nn", "text": " \ntrain(mode=True)  \nSets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters \nmode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.type()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.type", "type": "torch.nn", "text": " \ntype(dst_type)  \nCasts all parameters and buffers to dst_type.  Parameters \ndst_type (type or string) \u2013 the desired type  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.xpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.xpu", "type": "torch.nn", "text": " \nxpu(device=None)  \nMoves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.  Parameters \ndevice (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns \nself  Return type \nModule   \n"}, {"name": "torch.nn.Unflatten.zero_grad()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.zero_grad", "type": "torch.nn", "text": " \nzero_grad(set_to_none=False)  \nSets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details.   \n"}, {"name": "torch.nn.Unfold", "path": "generated/torch.nn.unfold#torch.nn.Unfold", "type": "torch.nn", "text": " \nclass torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1) [source]\n \nExtracts sliding local blocks from a batched input tensor. Consider a batched input tensor of shape (N,C,\u2217)(N, C, *) , where NN  is the batch dimension, CC  is the channel dimension, and \u2217*  represent arbitrary spatial dimensions. This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L) , where C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})  is the total number of values within each block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})  spatial locations each containing a CC -channeled vector), and LL  is the total number of such blocks:  L=\u220fd\u230aspatial_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121stride[d]+1\u230b,L = \\prod_d \\left\\lfloor\\frac{\\text{spatial\\_size}[d] + 2 \\times \\text{padding}[d] % - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,  \nwhere spatial_size\\text{spatial\\_size}  is formed by the spatial dimensions of input (\u2217*  above), and dd  is over all spatial dimensions. Therefore, indexing output at the last dimension (column dimension) gives all values within a certain block. The padding, stride and dilation arguments specify how the sliding blocks are retrieved.  \nstride controls the stride for the sliding blocks. \npadding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping. \ndilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.   Parameters \n \nkernel_size (int or tuple) \u2013 the size of the sliding blocks \nstride (int or tuple, optional) \u2013 the stride of the sliding blocks in the input spatial dimensions. Default: 1 \npadding (int or tuple, optional) \u2013 implicit zero padding to be added on both sides of input. Default: 0 \ndilation (int or tuple, optional) \u2013 a parameter that controls the stride of elements within the neighborhood. Default: 1     If kernel_size, dilation, padding or stride is an int or a tuple of length 1, their values will be replicated across all spatial dimensions. For the case of two input spatial dimensions this operation is sometimes called im2col.   Note Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other. In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters: >>> fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)\n>>> fold = nn.Fold(output_size=..., **fold_params)\n>>> unfold = nn.Unfold(**fold_params)\n Then for any (supported) input tensor the following equality holds: fold(unfold(input)) == divisor * input\n where divisor is a tensor that depends only on the shape and dtype of the input: >>> input_ones = torch.ones(input.shape, dtype=input.dtype)\n>>> divisor = fold(unfold(input_ones))\n When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).   Warning Currently, only 4-D input tensors (batched image-like tensors) are supported.   Shape:\n\n Input: (N,C,\u2217)(N, C, *) \n Output: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)  as described above    Examples: >>> unfold = nn.Unfold(kernel_size=(2, 3))\n>>> input = torch.randn(2, 5, 3, 4)\n>>> output = unfold(input)\n>>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels)\n>>> # 4 blocks (2x3 kernels) in total in the 3x4 input\n>>> output.size()\ntorch.Size([2, 30, 4])\n\n>>> # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)\n>>> inp = torch.randn(1, 3, 10, 12)\n>>> w = torch.randn(2, 3, 4, 5)\n>>> inp_unf = torch.nn.functional.unfold(inp, (4, 5))\n>>> out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n>>> out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))\n>>> # or equivalently (and avoiding a copy),\n>>> # out = out_unf.view(1, 2, 7, 8)\n>>> (torch.nn.functional.conv2d(inp, w) - out).abs().max()\ntensor(1.9073e-06)\n \n"}, {"name": "torch.nn.Upsample", "path": "generated/torch.nn.upsample#torch.nn.Upsample", "type": "torch.nn", "text": " \nclass torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor. The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively. One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous)  Parameters \n \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) \u2013 output spatial sizes \nscale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (str, optional) \u2013 the upsampling algorithm: one of 'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. Default: 'nearest'\n \nalign_corners (bool, optional) \u2013 if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when mode is 'linear', 'bilinear', or 'trilinear'. Default: False\n     Shape:\n\n Input: (N,C,Win)(N, C, W_{in}) , (N,C,Hin,Win)(N, C, H_{in}, W_{in})  or (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in}) \n Output: (N,C,Wout)(N, C, W_{out}) , (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  or (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where     Dout=\u230aDin\u00d7scale_factor\u230bD_{out} = \\left\\lfloor D_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \n Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \n Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \n Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, bicubic, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs.   Note If you want downsampling/general resizing, you should use interpolate().  Examples: >>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n>>> input\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n\n>>> m = nn.Upsample(scale_factor=2, mode='nearest')\n>>> m(input)\ntensor([[[[ 1.,  1.,  2.,  2.],\n          [ 1.,  1.,  2.,  2.],\n          [ 3.,  3.,  4.,  4.],\n          [ 3.,  3.,  4.,  4.]]]])\n\n>>> m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False\n>>> m(input)\ntensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],\n          [ 1.5000,  1.7500,  2.2500,  2.5000],\n          [ 2.5000,  2.7500,  3.2500,  3.5000],\n          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])\n\n>>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n>>> m(input)\ntensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],\n          [ 1.6667,  2.0000,  2.3333,  2.6667],\n          [ 2.3333,  2.6667,  3.0000,  3.3333],\n          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])\n\n>>> # Try scaling the same data in a larger tensor\n>>>\n>>> input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3)\n>>> input_3x3[:, :, :2, :2].copy_(input)\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n>>> input_3x3\ntensor([[[[ 1.,  2.,  0.],\n          [ 3.,  4.,  0.],\n          [ 0.,  0.,  0.]]]])\n\n>>> m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False\n>>> # Notice that values in top left corner are the same with the small input (except at boundary)\n>>> m(input_3x3)\ntensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],\n          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],\n          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],\n          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],\n          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n\n>>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n>>> # Notice that values in top left corner are now changed\n>>> m(input_3x3)\ntensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],\n          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],\n          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],\n          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],\n          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n \n"}, {"name": "torch.nn.UpsamplingBilinear2d", "path": "generated/torch.nn.upsamplingbilinear2d#torch.nn.UpsamplingBilinear2d", "type": "torch.nn", "text": " \nclass torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None) [source]\n \nApplies a 2D bilinear upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument. When size is given, it is the output size of the image (h, w).  Parameters \n \nsize (int or Tuple[int, int], optional) \u2013 output spatial sizes \nscale_factor (float or Tuple[float, float], optional) \u2013 multiplier for spatial size.     Warning This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True).   Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where     Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \n Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \nExamples: >>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n>>> input\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n\n>>> m = nn.UpsamplingBilinear2d(scale_factor=2)\n>>> m(input)\ntensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],\n          [ 1.6667,  2.0000,  2.3333,  2.6667],\n          [ 2.3333,  2.6667,  3.0000,  3.3333],\n          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])\n \n"}, {"name": "torch.nn.UpsamplingNearest2d", "path": "generated/torch.nn.upsamplingnearest2d#torch.nn.UpsamplingNearest2d", "type": "torch.nn", "text": " \nclass torch.nn.UpsamplingNearest2d(size=None, scale_factor=None) [source]\n \nApplies a 2D nearest neighbor upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument. When size is given, it is the output size of the image (h, w).  Parameters \n \nsize (int or Tuple[int, int], optional) \u2013 output spatial sizes \nscale_factor (float or Tuple[float, float], optional) \u2013 multiplier for spatial size.     Warning This class is deprecated in favor of interpolate().   Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where     Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \n Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  \nExamples: >>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n>>> input\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n\n>>> m = nn.UpsamplingNearest2d(scale_factor=2)\n>>> m(input)\ntensor([[[[ 1.,  1.,  2.,  2.],\n          [ 1.,  1.,  2.,  2.],\n          [ 3.,  3.,  4.,  4.],\n          [ 3.,  3.,  4.,  4.]]]])\n \n"}, {"name": "torch.nn.utils.clip_grad_norm_()", "path": "generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_", "type": "torch.nn", "text": " \ntorch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0) [source]\n \nClips gradient norm of an iterable of parameters. The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.  Parameters \n \nparameters (Iterable[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized \nmax_norm (float or int) \u2013 max norm of the gradients \nnorm_type (float or int) \u2013 type of the used p-norm. Can be 'inf' for infinity norm.   Returns \nTotal norm of the parameters (viewed as a single vector).   \n"}, {"name": "torch.nn.utils.clip_grad_value_()", "path": "generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_", "type": "torch.nn", "text": " \ntorch.nn.utils.clip_grad_value_(parameters, clip_value) [source]\n \nClips gradient of an iterable of parameters at specified value. Gradients are modified in-place.  Parameters \n \nparameters (Iterable[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized \nclip_value (float or int) \u2013 maximum allowed value of the gradients. The gradients are clipped in the range [-clip_value,clip_value]\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right] \n    \n"}, {"name": "torch.nn.utils.parameters_to_vector()", "path": "generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector", "type": "torch.nn", "text": " \ntorch.nn.utils.parameters_to_vector(parameters) [source]\n \nConvert parameters to one vector  Parameters \nparameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.  Returns \nThe parameters represented by a single vector   \n"}, {"name": "torch.nn.utils.prune.BasePruningMethod", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.BasePruningMethod [source]\n \nAbstract base class for creation of new pruning techniques. Provides a skeleton for customization requiring the overriding of methods such as compute_mask() and apply().  \nclassmethod apply(module, name, *args, importance_scores=None, **kwargs) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \nargs \u2013 arguments passed on to a subclass of BasePruningMethod\n \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the parameter will be used in its place. \nkwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod\n    \n  \napply_mask(module) [source]\n \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \nabstract compute_mask(t, default_mask) [source]\n \nComputes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.  Parameters \n \nt (torch.Tensor) \u2013 tensor representing the importance scores of the \nto prune. (parameter) \u2013  \ndefault_mask (torch.Tensor) \u2013 Base mask from previous pruning \nthat need to be respected after the new mask is (iterations,) \u2013  \nSame dims as t. (applied.) \u2013    Returns \nmask to apply to t, of same dims as t  Return type \nmask (torch.Tensor)   \n  \nprune(t, default_mask=None, importance_scores=None) [source]\n \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module) [source]\n \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, *args, importance_scores=None, **kwargs) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \nargs \u2013 arguments passed on to a subclass of BasePruningMethod\n \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the parameter will be used in its place. \nkwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod\n    \n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply_mask", "type": "torch.nn", "text": " \napply_mask(module) [source]\n \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.compute_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.compute_mask", "type": "torch.nn", "text": " \nabstract compute_mask(t, default_mask) [source]\n \nComputes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.  Parameters \n \nt (torch.Tensor) \u2013 tensor representing the importance scores of the \nto prune. (parameter) \u2013  \ndefault_mask (torch.Tensor) \u2013 Base mask from previous pruning \nthat need to be respected after the new mask is (iterations,) \u2013  \nSame dims as t. (applied.) \u2013    Returns \nmask to apply to t, of same dims as t  Return type \nmask (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.prune()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None) [source]\n \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.remove()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.remove", "type": "torch.nn", "text": " \nremove(module) [source]\n \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.CustomFromMask", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.CustomFromMask(mask) [source]\n \n \nclassmethod apply(module, name, mask) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act.    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.CustomFromMask.apply()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, mask) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act.    \n"}, {"name": "torch.nn.utils.prune.CustomFromMask.apply_mask()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.CustomFromMask.prune()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.CustomFromMask.remove()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.custom_from_mask()", "path": "generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.custom_from_mask(module, name, mask) [source]\n \nPrunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \nmask (Tensor) \u2013 binary mask to be applied to the parameter.   Returns \nmodified (i.e. pruned) version of the input module  Return type \nmodule (nn.Module)   Examples >>> m = prune.custom_from_mask(\n        nn.Linear(5, 3), name='bias', mask=torch.Tensor([0, 1, 0])\n    )\n>>> print(m.bias_mask)\ntensor([0., 1., 0.])\n \n"}, {"name": "torch.nn.utils.prune.global_unstructured()", "path": "generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.global_unstructured(parameters, pruning_method, importance_scores=None, **kwargs) [source]\n \nGlobally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method. Modifies modules in place by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters \n \nparameters (Iterable of (module, name) tuples) \u2013 parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune. module must be of type nn.Module, and name must be a string. \npruning_method (function) \u2013 a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has PRUNING_TYPE='unstructured'. \nimportance_scores (dict) \u2013 a dictionary mapping (module, name) tuples to the corresponding parameter\u2019s importance scores tensor. The tensor should be the same shape as the parameter, and is used for computing mask for pruning. If unspecified or None, the parameter will be used in place of its importance scores. \nkwargs \u2013 other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Raises \nTypeError \u2013 if PRUNING_TYPE != 'unstructured'    Note Since global structured pruning doesn\u2019t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.  Examples >>> net = nn.Sequential(OrderedDict([\n        ('first', nn.Linear(10, 4)),\n        ('second', nn.Linear(4, 1)),\n    ]))\n>>> parameters_to_prune = (\n        (net.first, 'weight'),\n        (net.second, 'weight'),\n    )\n>>> prune.global_unstructured(\n        parameters_to_prune,\n        pruning_method=prune.L1Unstructured,\n        amount=10,\n    )\n>>> print(sum(torch.nn.utils.parameters_to_vector(net.buffers()) == 0))\ntensor(10, dtype=torch.uint8)\n \n"}, {"name": "torch.nn.utils.prune.Identity", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.Identity [source]\n \nUtility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.  \nclassmethod apply(module, name) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act.    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.Identity.apply()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act.    \n"}, {"name": "torch.nn.utils.prune.Identity.apply_mask()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.Identity.prune()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.Identity.remove()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.is_pruned()", "path": "generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.is_pruned(module) [source]\n \nCheck whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.  Parameters \nmodule (nn.Module) \u2013 object that is either pruned or unpruned  Returns \nbinary answer to whether module is pruned.   Examples >>> m = nn.Linear(5, 7)\n>>> print(prune.is_pruned(m))\nFalse\n>>> prune.random_unstructured(m, name='weight', amount=0.2)\n>>> print(prune.is_pruned(m))\nTrue\n \n"}, {"name": "torch.nn.utils.prune.L1Unstructured", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.L1Unstructured(amount) [source]\n \nPrune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  Parameters \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    \nclassmethod apply(module, name, amount, importance_scores=None) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place.    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.L1Unstructured.apply()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, amount, importance_scores=None) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place.    \n"}, {"name": "torch.nn.utils.prune.L1Unstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.L1Unstructured.prune()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.L1Unstructured.remove()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.l1_unstructured()", "path": "generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.l1_unstructured(module, name, amount, importance_scores=None) [source]\n \nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place.   Returns \nmodified (i.e. pruned) version of the input module  Return type \nmodule (nn.Module)   Examples >>> m = prune.l1_unstructured(nn.Linear(2, 3), 'weight', amount=0.2)\n>>> m.state_dict().keys()\nodict_keys(['bias', 'weight_orig', 'weight_mask'])\n \n"}, {"name": "torch.nn.utils.prune.LnStructured", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.LnStructured(amount, n, dim=-1) [source]\n \nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.  Parameters \n \namount (int or float) \u2013 quantity of channels to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nn (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). \ndim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.     \nclassmethod apply(module, name, amount, n, dim, importance_scores=None) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nn (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). \ndim (int) \u2013 index of the dim along which we define channels to prune. \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place.    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \ncompute_mask(t, default_mask) [source]\n \nComputes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.  Parameters \n \nt (torch.Tensor) \u2013 tensor representing the parameter to prune \ndefault_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns \nmask to apply to t, of same dims as t  Return type \nmask (torch.Tensor)  Raises \nIndexError \u2013 if self.dim >= len(t.shape)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.LnStructured.apply()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, amount, n, dim, importance_scores=None) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nn (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). \ndim (int) \u2013 index of the dim along which we define channels to prune. \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place.    \n"}, {"name": "torch.nn.utils.prune.LnStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.LnStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.compute_mask", "type": "torch.nn", "text": " \ncompute_mask(t, default_mask) [source]\n \nComputes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.  Parameters \n \nt (torch.Tensor) \u2013 tensor representing the parameter to prune \ndefault_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns \nmask to apply to t, of same dims as t  Return type \nmask (torch.Tensor)  Raises \nIndexError \u2013 if self.dim >= len(t.shape)   \n"}, {"name": "torch.nn.utils.prune.LnStructured.prune()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.LnStructured.remove()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.ln_structured()", "path": "generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.ln_structured(module, name, amount, n, dim, importance_scores=None) [source]\n \nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \nn (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). \ndim (int) \u2013 index of the dim along which we define channels to prune. \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place.   Returns \nmodified (i.e. pruned) version of the input module  Return type \nmodule (nn.Module)   Examples >>> m = prune.ln_structured(\n       nn.Conv2d(5, 3, 2), 'weight', amount=0.3, dim=1, n=float('-inf')\n    )\n \n"}, {"name": "torch.nn.utils.prune.PruningContainer", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.PruningContainer(*args) [source]\n \nContainer holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls. Accepts as argument an instance of a BasePruningMethod or an iterable of them.  \nadd_pruning_method(method) [source]\n \nAdds a child pruning method to the container.  Parameters \nmethod (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.   \n  \nclassmethod apply(module, name, *args, importance_scores=None, **kwargs)  \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \nargs \u2013 arguments passed on to a subclass of BasePruningMethod\n \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the parameter will be used in its place. \nkwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod\n    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \ncompute_mask(t, default_mask) [source]\n \nApplies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):  for \u2018unstructured\u2019, the mask will be computed from the raveled list of nonmasked entries; for \u2018structured\u2019, the mask will be computed from the nonmasked channels in the tensor; for \u2018global\u2019, the mask will be computed across all entries.   Parameters \n \nt (torch.Tensor) \u2013 tensor representing the parameter to prune (of same dimensions as default_mask). \ndefault_mask (torch.Tensor) \u2013 mask from previous pruning iteration.   Returns \nnew mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).  Return type \nmask (torch.Tensor)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.PruningContainer.add_pruning_method()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.add_pruning_method", "type": "torch.nn", "text": " \nadd_pruning_method(method) [source]\n \nAdds a child pruning method to the container.  Parameters \nmethod (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.   \n"}, {"name": "torch.nn.utils.prune.PruningContainer.apply()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, *args, importance_scores=None, **kwargs)  \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \nargs \u2013 arguments passed on to a subclass of BasePruningMethod\n \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the parameter will be used in its place. \nkwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod\n    \n"}, {"name": "torch.nn.utils.prune.PruningContainer.apply_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.PruningContainer.compute_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.compute_mask", "type": "torch.nn", "text": " \ncompute_mask(t, default_mask) [source]\n \nApplies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):  for \u2018unstructured\u2019, the mask will be computed from the raveled list of nonmasked entries; for \u2018structured\u2019, the mask will be computed from the nonmasked channels in the tensor; for \u2018global\u2019, the mask will be computed across all entries.   Parameters \n \nt (torch.Tensor) \u2013 tensor representing the parameter to prune (of same dimensions as default_mask). \ndefault_mask (torch.Tensor) \u2013 mask from previous pruning iteration.   Returns \nnew mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).  Return type \nmask (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.PruningContainer.prune()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.PruningContainer.remove()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.RandomStructured", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.RandomStructured(amount, dim=-1) [source]\n \nPrune entire (currently unpruned) channels in a tensor at random.  Parameters \n \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \ndim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.     \nclassmethod apply(module, name, amount, dim=-1) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \ndim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \ncompute_mask(t, default_mask) [source]\n \nComputes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.  Parameters \n \nt (torch.Tensor) \u2013 tensor representing the parameter to prune \ndefault_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns \nmask to apply to t, of same dims as t  Return type \nmask (torch.Tensor)  Raises \nIndexError \u2013 if self.dim >= len(t.shape)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.RandomStructured.apply()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, amount, dim=-1) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \ndim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.    \n"}, {"name": "torch.nn.utils.prune.RandomStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.RandomStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.compute_mask", "type": "torch.nn", "text": " \ncompute_mask(t, default_mask) [source]\n \nComputes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.  Parameters \n \nt (torch.Tensor) \u2013 tensor representing the parameter to prune \ndefault_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns \nmask to apply to t, of same dims as t  Return type \nmask (torch.Tensor)  Raises \nIndexError \u2013 if self.dim >= len(t.shape)   \n"}, {"name": "torch.nn.utils.prune.RandomStructured.prune()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.RandomStructured.remove()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.RandomUnstructured", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured", "type": "torch.nn", "text": " \nclass torch.nn.utils.prune.RandomUnstructured(amount) [source]\n \nPrune (currently unpruned) units in a tensor at random.  Parameters \n \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.     \nclassmethod apply(module, name, amount) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    \n  \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n  \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n  \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n \n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply", "type": "torch.nn", "text": " \nclassmethod apply(module, name, amount) [source]\n \nAdds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    \n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply_mask", "type": "torch.nn", "text": " \napply_mask(module)  \nSimply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters \nmodule (nn.Module) \u2013 module containing the tensor to prune  Returns \npruned version of the input tensor  Return type \npruned_tensor (torch.Tensor)   \n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.prune()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.prune", "type": "torch.nn", "text": " \nprune(t, default_mask=None, importance_scores=None)  \nComputes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters \n \nt (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). \nimportance_scores (torch.Tensor) \u2013 tensor of importance scores (of same shape as t) used to compute mask for pruning t. The values in this tensor indicate the importance of the corresponding elements in the t that is being pruned. If unspecified or None, the tensor t will be used in its place. \ndefault_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns \npruned version of tensor t.   \n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.remove()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.remove", "type": "torch.nn", "text": " \nremove(module)  \nRemoves the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  \n"}, {"name": "torch.nn.utils.prune.random_structured()", "path": "generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.random_structured(module, name, amount, dim) [source]\n \nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. \ndim (int) \u2013 index of the dim along which we define channels to prune.   Returns \nmodified (i.e. pruned) version of the input module  Return type \nmodule (nn.Module)   Examples >>> m = prune.random_structured(\n        nn.Linear(5, 3), 'weight', amount=3, dim=1\n    )\n>>> columns_pruned = int(sum(torch.sum(m.weight, dim=0) == 0))\n>>> print(columns_pruned)\n3\n \n"}, {"name": "torch.nn.utils.prune.random_unstructured()", "path": "generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.random_unstructured(module, name, amount) [source]\n \nPrunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act. \namount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Returns \nmodified (i.e. pruned) version of the input module  Return type \nmodule (nn.Module)   Examples >>> m = prune.random_unstructured(nn.Linear(2, 3), 'weight', amount=1)\n>>> torch.sum(m.weight_mask == 0)\ntensor(1)\n \n"}, {"name": "torch.nn.utils.prune.remove()", "path": "generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove", "type": "torch.nn", "text": " \ntorch.nn.utils.prune.remove(module, name) [source]\n \nRemoves the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!   Parameters \n \nmodule (nn.Module) \u2013 module containing the tensor to prune \nname (str) \u2013 parameter name within module on which pruning will act.    Examples >>> m = random_unstructured(nn.Linear(5, 7), name='weight', amount=0.2)\n>>> m = remove(m, name='weight')\n \n"}, {"name": "torch.nn.utils.remove_spectral_norm()", "path": "generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm", "type": "torch.nn", "text": " \ntorch.nn.utils.remove_spectral_norm(module, name='weight') [source]\n \nRemoves the spectral normalization reparameterization from a module.  Parameters \n \nmodule (Module) \u2013 containing module \nname (str, optional) \u2013 name of weight parameter    Example >>> m = spectral_norm(nn.Linear(40, 10))\n>>> remove_spectral_norm(m)\n \n"}, {"name": "torch.nn.utils.remove_weight_norm()", "path": "generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm", "type": "torch.nn", "text": " \ntorch.nn.utils.remove_weight_norm(module, name='weight') [source]\n \nRemoves the weight normalization reparameterization from a module.  Parameters \n \nmodule (Module) \u2013 containing module \nname (str, optional) \u2013 name of weight parameter    Example >>> m = weight_norm(nn.Linear(20, 40))\n>>> remove_weight_norm(m)\n \n"}, {"name": "torch.nn.utils.rnn.PackedSequence", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence", "type": "torch.nn", "text": " \nclass torch.nn.utils.rnn.PackedSequence [source]\n \nHolds the data and list of batch_sizes of a packed sequence. All RNN modules accept packed sequences as inputs.  Note Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence(). Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence(). For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1].   Variables \n \n~PackedSequence.data (Tensor) \u2013 Tensor containing packed sequence \n~PackedSequence.batch_sizes (Tensor) \u2013 Tensor of integers holding information about the batch size at each sequence step \n~PackedSequence.sorted_indices (Tensor, optional) \u2013 Tensor of integers holding how this PackedSequence is constructed from sequences. \n~PackedSequence.unsorted_indices (Tensor, optional) \u2013 Tensor of integers holding how this to recover the original sequences with correct order.     Note data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data. However, batch_sizes should always be a CPU torch.int64 tensor. This invariant is maintained throughout PackedSequence class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).   \nproperty batch_sizes  \nAlias for field number 1 \n  \ncount()  \nReturn number of occurrences of value. \n  \nproperty data  \nAlias for field number 0 \n  \nindex()  \nReturn first index of value. Raises ValueError if the value is not present. \n  \nproperty is_cuda  \nReturns true if self.data stored on a gpu \n  \nis_pinned() [source]\n \nReturns true if self.data stored on in pinned memory \n  \nproperty sorted_indices  \nAlias for field number 2 \n  \nto(*args, **kwargs) [source]\n \nPerforms dtype and/or device conversion on self.data. It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.  Note If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.  \n  \nproperty unsorted_indices  \nAlias for field number 3 \n \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.batch_sizes()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.batch_sizes", "type": "torch.nn", "text": " \nproperty batch_sizes  \nAlias for field number 1 \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.count()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.count", "type": "torch.nn", "text": " \ncount()  \nReturn number of occurrences of value. \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.data()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.data", "type": "torch.nn", "text": " \nproperty data  \nAlias for field number 0 \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.index()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.index", "type": "torch.nn", "text": " \nindex()  \nReturn first index of value. Raises ValueError if the value is not present. \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.is_cuda()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_cuda", "type": "torch.nn", "text": " \nproperty is_cuda  \nReturns true if self.data stored on a gpu \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.is_pinned()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_pinned", "type": "torch.nn", "text": " \nis_pinned() [source]\n \nReturns true if self.data stored on in pinned memory \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.sorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.sorted_indices", "type": "torch.nn", "text": " \nproperty sorted_indices  \nAlias for field number 2 \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.to()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.to", "type": "torch.nn", "text": " \nto(*args, **kwargs) [source]\n \nPerforms dtype and/or device conversion on self.data. It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.  Note If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.  \n"}, {"name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.unsorted_indices", "type": "torch.nn", "text": " \nproperty unsorted_indices  \nAlias for field number 3 \n"}, {"name": "torch.nn.utils.rnn.pack_padded_sequence()", "path": "generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence", "type": "torch.nn", "text": " \ntorch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True) [source]\n \nPacks a Tensor containing padded sequences of variable length. input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected. For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.  Note This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute.   Parameters \n \ninput (Tensor) \u2013 padded batch of variable length sequences. \nlengths (Tensor or list(int)) \u2013 list of sequence lengths of each batch element (must be on the CPU if provided as a tensor). \nbatch_first (bool, optional) \u2013 if True, the input is expected in B x T x * format. \nenforce_sorted (bool, optional) \u2013 if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally. Default: True.   Returns \na PackedSequence object   \n"}, {"name": "torch.nn.utils.rnn.pack_sequence()", "path": "generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence", "type": "torch.nn", "text": " \ntorch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True) [source]\n \nPacks a list of variable length Tensors sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero. For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted in the order of decreasing length. enforce_sorted = True is only necessary for ONNX export. Example >>> from torch.nn.utils.rnn import pack_sequence\n>>> a = torch.tensor([1,2,3])\n>>> b = torch.tensor([4,5])\n>>> c = torch.tensor([6])\n>>> pack_sequence([a, b, c])\nPackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))\n  Parameters \n \nsequences (list[Tensor]) \u2013 A list of sequences of decreasing length. \nenforce_sorted (bool, optional) \u2013 if True, checks that the input contains sequences sorted by length in a decreasing order. If False, this condition is not checked. Default: True.   Returns \na PackedSequence object   \n"}, {"name": "torch.nn.utils.rnn.pad_packed_sequence()", "path": "generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence", "type": "torch.nn", "text": " \ntorch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None) [source]\n \nPads a packed batch of variable length sequences. It is an inverse operation to pack_padded_sequence(). The returned Tensor\u2019s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format. Example >>> from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n>>> seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]])\n>>> lens = [2, 1, 3]\n>>> packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False)\n>>> packed\nPackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]),\n               sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))\n>>> seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n>>> seq_unpacked\ntensor([[1, 2, 0],\n        [3, 0, 0],\n        [4, 5, 6]])\n>>> lens_unpacked\ntensor([2, 1, 3])\n  Note total_length is useful to implement the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details.   Parameters \n \nsequence (PackedSequence) \u2013 batch to pad \nbatch_first (bool, optional) \u2013 if True, the output will be in B x T x * format. \npadding_value (float, optional) \u2013 values for padded elements. \ntotal_length (int, optional) \u2013 if not None, the output will be padded to have length total_length. This method will throw ValueError if total_length is less than the max sequence length in sequence.   Returns \nTuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to pack_padded_sequence or pack_sequence.   \n"}, {"name": "torch.nn.utils.rnn.pad_sequence()", "path": "generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence", "type": "torch.nn", "text": " \ntorch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0) [source]\n \nPad a list of variable length Tensors with padding_value pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise. B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none. Example >>> from torch.nn.utils.rnn import pad_sequence\n>>> a = torch.ones(25, 300)\n>>> b = torch.ones(22, 300)\n>>> c = torch.ones(15, 300)\n>>> pad_sequence([a, b, c]).size()\ntorch.Size([25, 3, 300])\n  Note This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.   Parameters \n \nsequences (list[Tensor]) \u2013 list of variable length sequences. \nbatch_first (bool, optional) \u2013 output will be in B x T x * if True, or in T x B x * otherwise \npadding_value (float, optional) \u2013 value for padded elements. Default: 0.   Returns \nTensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise   \n"}, {"name": "torch.nn.utils.spectral_norm()", "path": "generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm", "type": "torch.nn", "text": " \ntorch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1, eps=1e-12, dim=None) [source]\n \nApplies spectral normalization to a parameter in the given module.  WSN=W\u03c3(W),\u03c3(W)=max\u2061h:h\u22600\u2225Wh\u22252\u2225h\u22252\\mathbf{W}_{SN} = \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})}, \\sigma(\\mathbf{W}) = \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}  \nSpectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \u03c3\\sigma  of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call. See Spectral Normalization for Generative Adversarial Networks .  Parameters \n \nmodule (nn.Module) \u2013 containing module \nname (str, optional) \u2013 name of weight parameter \nn_power_iterations (int, optional) \u2013 number of power iterations to calculate spectral norm \neps (float, optional) \u2013 epsilon for numerical stability in calculating norms \ndim (int, optional) \u2013 dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose{1,2,3}d, when it is 1\n   Returns \nThe original module with the spectral norm hook   Example: >>> m = spectral_norm(nn.Linear(20, 40))\n>>> m\nLinear(in_features=20, out_features=40, bias=True)\n>>> m.weight_u.size()\ntorch.Size([40])\n \n"}, {"name": "torch.nn.utils.vector_to_parameters()", "path": "generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters", "type": "torch.nn", "text": " \ntorch.nn.utils.vector_to_parameters(vec, parameters) [source]\n \nConvert one vector to the parameters  Parameters \n \nvec (Tensor) \u2013 a single vector represents the parameters of a model. \nparameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.    \n"}, {"name": "torch.nn.utils.weight_norm()", "path": "generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm", "type": "torch.nn", "text": " \ntorch.nn.utils.weight_norm(module, name='weight', dim=0) [source]\n \nApplies weight normalization to a parameter in the given module.  w=gv\u2225v\u2225\\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}  \nWeight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call. By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None. See https://arxiv.org/abs/1602.07868  Parameters \n \nmodule (Module) \u2013 containing module \nname (str, optional) \u2013 name of weight parameter \ndim (int, optional) \u2013 dimension over which to compute the norm   Returns \nThe original module with the weight norm hook   Example: >>> m = weight_norm(nn.Linear(20, 40), name='weight')\n>>> m\nLinear(in_features=20, out_features=40, bias=True)\n>>> m.weight_g.size()\ntorch.Size([40, 1])\n>>> m.weight_v.size()\ntorch.Size([40, 20])\n \n"}, {"name": "torch.nn.ZeroPad2d", "path": "generated/torch.nn.zeropad2d#torch.nn.ZeroPad2d", "type": "torch.nn", "text": " \nclass torch.nn.ZeroPad2d(padding) [source]\n \nPads the input tensor boundaries with zero. For N-dimensional padding, use torch.nn.functional.pad().  Parameters \npadding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )    Shape:\n\n Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in}) \n \nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}  Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}      Examples: >>> m = nn.ZeroPad2d(2)\n>>> input = torch.randn(1, 1, 3, 3)\n>>> input\ntensor([[[[-0.1678, -0.4418,  1.9466],\n          [ 0.9604, -0.4219, -0.5241],\n          [-0.9162, -0.5436, -0.6446]]]])\n>>> m(input)\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],\n          [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n>>> # using different paddings for different sides\n>>> m = nn.ZeroPad2d((1, 1, 2, 0))\n>>> m(input)\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],\n          [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],\n          [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])\n \n"}, {"name": "torch.nonzero()", "path": "generated/torch.nonzero#torch.nonzero", "type": "torch", "text": " \ntorch.nonzero(input, *, out=None, as_tuple=False) \u2192 LongTensor or tuple of LongTensors  \n Note torch.nonzero(..., as_tuple=False) (default) returns a 2-D tensor where each row is the index for a nonzero value. torch.nonzero(..., as_tuple=True) returns a tuple of 1-D index tensors, allowing for advanced indexing, so x[x.nonzero(as_tuple=True)] gives all nonzero values of tensor x. Of the returned tuple, each index tensor contains nonzero indices for a certain dimension. See below for more details on the two behaviors. When input is on CUDA, torch.nonzero() causes host-device synchronization.  When as_tuple is ``False`` (default): Returns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style). If input has nn  dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n) , where zz  is the total number of non-zero elements in the input tensor. When as_tuple is ``True``: Returns a tuple of 1-D tensors, one for each dimension in input, each containing the indices (in that dimension) of all non-zero elements of input . If input has nn  dimensions, then the resulting tuple contains nn  tensors of size zz , where zz  is the total number of non-zero elements in the input tensor. As a special case, when input has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (LongTensor, optional) \u2013 the output tensor containing indices  Returns \nIf as_tuple is False, the output tensor containing indices. If as_tuple is True, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.  Return type \nLongTensor or tuple of LongTensor   Example: >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 4]])\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3]])\n>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n(tensor([0, 1, 2, 4]),)\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n>>> torch.nonzero(torch.tensor(5), as_tuple=True)\n(tensor([0]),)\n \n"}, {"name": "torch.norm()", "path": "generated/torch.norm#torch.norm", "type": "torch", "text": " \ntorch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None) [source]\n \nReturns the matrix norm or vector norm of a given tensor.  Warning torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm() instead, but note that torch.linalg.norm() has a different signature and slightly different behavior that is more consistent with NumPy\u2019s numpy.linalg.norm.   Parameters \n \ninput (Tensor) \u2013 The input tensor. Its data type must be either a floating point or complex type. For complex inputs, the norm is calculated using the absolute value of each element. If the input is complex and neither dtype nor out is specified, the result\u2019s data type will be the corresponding floating point type (e.g. float if input is complexfloat). \np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 \nthe order of norm. Default: 'fro' The following norms can be calculated:   \nord matrix norm vector norm   \n\u2019fro\u2019 Frobenius norm \u2013  \n\u2018nuc\u2019 nuclear norm \u2013  \nNumber \u2013 sum(abs(x)**ord)**(1./ord)   The vector norm can be calculated across any number of dimensions. The corresponding dimensions of input are flattened into one dimension, and the norm is calculated on the flattened dimension. Frobenius norm produces the same result as p=2 in all cases except when dim is a list of three or more dims, in which case Frobenius norm throws an error. Nuclear norm can only be calculated across exactly two dimensions.  \ndim (int, tuple of python:ints, list of python:ints, optional) \u2013 Specifies which dimension or dimensions of input to calculate the norm across. If dim is None, the norm will be calculated across all dimensions of input. If the norm type indicated by p does not support the specified number of dimensions, an error will occur. \nkeepdim (bool, optional) \u2013 whether the output tensors have dim retained or not. Ignored if dim = None and out = None. Default: False\n \nout (Tensor, optional) \u2013 the output tensor. Ignored if dim = None and out = None. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to :attr:\u2019dtype\u2019 while performing the operation. Default: None.     Note Even though p='fro' supports any number of dimensions, the true mathematical definition of Frobenius norm only applies to tensors with exactly two dimensions. torch.linalg.norm() with ord='fro' aligns with the mathematical definition, since it can only be applied across exactly two dimensions.  Example: >>> import torch\n>>> a = torch.arange(9, dtype= torch.float) - 4\n>>> b = a.reshape((3, 3))\n>>> torch.norm(a)\ntensor(7.7460)\n>>> torch.norm(b)\ntensor(7.7460)\n>>> torch.norm(a, float('inf'))\ntensor(4.)\n>>> torch.norm(b, float('inf'))\ntensor(4.)\n>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n>>> torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n>>> torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n \n"}, {"name": "torch.normal()", "path": "generated/torch.normal#torch.normal", "type": "torch", "text": " \ntorch.normal(mean, std, *, generator=None, out=None) \u2192 Tensor  \nReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given. The mean is a tensor with the mean of each output element\u2019s normal distribution The std is a tensor with the standard deviation of each output element\u2019s normal distribution The shapes of mean and std don\u2019t need to match, but the total number of elements in each tensor need to be the same.  Note When the shapes do not match, the shape of mean is used as the shape for the returned output tensor   Parameters \n \nmean (Tensor) \u2013 the tensor of per-element means \nstd (Tensor) \u2013 the tensor of per-element standard deviations   Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])\n  \ntorch.normal(mean=0.0, std, *, out=None) \u2192 Tensor \n Similar to the function above, but the means are shared among all drawn elements.  Parameters \n \nmean (float, optional) \u2013 the mean for all distributions \nstd (Tensor) \u2013 the tensor of per-element standard deviations   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n  \ntorch.normal(mean, std=1.0, *, out=None) \u2192 Tensor \n Similar to the function above, but the standard-deviations are shared among all drawn elements.  Parameters \n \nmean (Tensor) \u2013 the tensor of per-element means \nstd (float, optional) \u2013 the standard deviation for all distributions   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor   Example: >>> torch.normal(mean=torch.arange(1., 6.))\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n  \ntorch.normal(mean, std, size, *, out=None) \u2192 Tensor \n Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by size.  Parameters \n \nmean (float) \u2013 the mean for all distributions \nstd (float) \u2013 the standard deviation for all distributions \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.normal(2, 3, size=(1, 4))\ntensor([[-1.3987, -1.9544,  3.6048,  0.7909]])\n \n"}, {"name": "torch.not_equal()", "path": "generated/torch.not_equal#torch.not_equal", "type": "torch", "text": " \ntorch.not_equal(input, other, *, out=None) \u2192 Tensor  \nAlias for torch.ne(). \n"}, {"name": "torch.no_grad", "path": "generated/torch.no_grad#torch.no_grad", "type": "torch", "text": " \nclass torch.no_grad [source]\n \nContext-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n \n"}, {"name": "torch.numel()", "path": "generated/torch.numel#torch.numel", "type": "torch", "text": " \ntorch.numel(input) \u2192 int  \nReturns the total number of elements in the input tensor.  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(1, 2, 3, 4, 5)\n>>> torch.numel(a)\n120\n>>> a = torch.zeros(4,4)\n>>> torch.numel(a)\n16\n \n"}, {"name": "torch.ones()", "path": "generated/torch.ones#torch.ones", "type": "torch", "text": " \ntorch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n>>> torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])\n \n"}, {"name": "torch.ones_like()", "path": "generated/torch.ones_like#torch.ones_like", "type": "torch", "text": " \ntorch.ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor filled with the scalar value 1, with the same size as input. torch.ones_like(input) is equivalent to torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Warning As of 0.4, this function does not support an out keyword. As an alternative, the old torch.ones_like(input, out=output) is equivalent to torch.ones(input.size(), out=output).   Parameters \ninput (Tensor) \u2013 the size of input will determine size of the output tensor.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    Example: >>> input = torch.empty(2, 3)\n>>> torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n \n"}, {"name": "torch.onnx", "path": "onnx", "type": "torch.onnx", "text": "torch.onnx  Example: End-to-end AlexNet from PyTorch to ONNX Tracing vs Scripting Write PyTorch model in Torch way Using dictionaries to handle Named Arguments as model inputs \nIndexing  Getter Setter   TorchVision support Limitations Supported operators \nAdding support for operators  ATen operators Non-ATen operators Custom operators   \nOperator Export Type  ONNX ONNX_ATEN ONNX_ATEN_FALLBACK RAW ONNX_FALLTHROUGH   Frequently Asked Questions Use external data format Training Functions  Example: End-to-end AlexNet from PyTorch to ONNX Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX. It runs a single round of inference and then saves the resulting traced model to alexnet.onnx: import torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device='cuda')\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\n# Providing input and output names sets the display names for values\n# within the model's graph. Setting these does not change the semantics\n# of the graph; it is only for readability.\n#\n# The inputs to the network consist of the flat list of inputs (i.e.\n# the values you would pass to the forward() method) followed by the\n# flat list of parameters. You can partially specify names, i.e. provide\n# a list here shorter than the number of inputs to the model, and we will\n# only set that subset of names, starting from the beginning.\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n The resulting alexnet.onnx is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument verbose=True causes the exporter to print out a human-readable representation of the network: # These are the inputs and parameters to the network, which have taken on\n# the names we specified earlier.\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\n      %learned_0 : Float(64, 3, 11, 11)\n      %learned_1 : Float(64)\n      %learned_2 : Float(192, 64, 5, 5)\n      %learned_3 : Float(192)\n      # ---- omitted for brevity ----\n      %learned_14 : Float(1000, 4096)\n      %learned_15 : Float(1000)) {\n  # Every statement consists of some output tensors (and their types),\n  # the operator to be run (with its attributes, e.g., kernels, strides,\n  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  # ---- omitted for brevity ----\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  # Dynamic means that the shape is not known. This may be because of a\n  # limitation of our implementation (which we would like to fix in a\n  # future release) or shapes which are truly dynamic.\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n  # ---- omitted for brevity ----\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%output1);\n}\n You can also verify the protobuf using the ONNX library. You can install ONNX with conda: conda install -c conda-forge onnx\n Then, you can run: import onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)\n To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions. Once these are installed, you can use the backend for Caffe2: # ...continuing from above\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\nprint(outputs[0])\n You can also run the exported model with ONNX Runtime, you will need to install ONNX Runtime: please follow these instructions. Once these are installed, you can use the backend for ONNX Runtime: # ...continuing from above\nimport onnxruntime as ort\n\nort_session = ort.InferenceSession('alexnet.onnx')\n\noutputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\n\nprint(outputs[0])\n Here is another tutorial of exporting the SuperResolution model to ONNX.. In the future, there will be backends for other frameworks as well. Tracing vs Scripting The ONNX exporter can be both trace-based and script-based exporter.  \ntrace-based means that it operates by executing your model once, and exporting the operators which were actually run during this run. This means that if your model is dynamic, e.g., changes behavior depending on input data, the export won\u2019t be accurate. Similarly, a trace is likely to be valid only for a specific input size (which is one reason why we require explicit inputs on tracing.) We recommend examining the model trace and making sure the traced operators look reasonable. If your model contains control flows like for loops and if conditions, trace-based exporter will unroll the loops and if conditions, exporting a static graph that is exactly the same as this run. If you want to export your model with dynamic control flows, you will need to use the script-based exporter. \nscript-based means that the model you are trying to export is a ScriptModule. ScriptModule is the core data structure in TorchScript, and TorchScript is a subset of Python language, that creates serializable and optimizable models from PyTorch code.  We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example: import torch\n\n# Trace-based only\n\nclass LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        for i in range(y):\n            x = x + i\n        return x\n\nmodel = LoopModel()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\n\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)\n With trace-based exporter, we get the result ONNX graph which unrolls the for loop: graph(%0 : Long(2, 3),\n      %1 : Long()):\n  %2 : Tensor = onnx::Constant[value={1}]()\n  %3 : Tensor = onnx::Add(%0, %2)\n  %4 : Tensor = onnx::Constant[value={2}]()\n  %5 : Tensor = onnx::Add(%3, %4)\n  %6 : Tensor = onnx::Constant[value={3}]()\n  %7 : Tensor = onnx::Add(%5, %6)\n  %8 : Tensor = onnx::Constant[value={4}]()\n  %9 : Tensor = onnx::Add(%7, %8)\n  return (%9)\n To utilize script-based exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module: # Mixing tracing and scripting\n\n@torch.jit.script\ndef loop(x, y):\n    for i in range(int(y)):\n        x = x + i\n    return x\n\nclass LoopModel2(torch.nn.Module):\n    def forward(self, x, y):\n        return loop(x, y)\n\nmodel = LoopModel2()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,\n                  input_names=['input_data', 'loop_range'])\n Now the exported ONNX graph becomes: graph(%input_data : Long(2, 3),\n      %loop_range : Long()):\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\n  %3 : Tensor = onnx::Cast[to=9](%2)\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop # custom_loop.py:240:5\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop # custom_loop.py:241:13\n      %9 : Tensor = onnx::Cast[to=9](%2)\n      -> (%9, %8)\n  return (%4)\n The dynamic control flow is captured correctly. We can verify in backends with different loop range. import caffe2.python.onnx.backend as backend\nimport numpy as np\nimport onnx\nmodel = onnx.load('loop.onnx')\n\nrep = backend.prepare(model)\noutputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))\nprint(outputs[0])\n#[[37 37 37]\n# [37 37 37]]\n\n\nimport onnxruntime as ort\nort_sess = ort.InferenceSession('loop.onnx')\noutputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\n                              'loop_range': np.array(9).astype(np.int64)})\nprint(outputs)\n#[array([[37, 37, 37],\n#       [37, 37, 37]], dtype=int64)]\n To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers. E.g.: class LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        res = []\n        arr = x.split(2, 0)\n        for i in range(int(y)):\n            res += [arr[i].sum(0, False)]\n        return torch.stack(res)\n\nmodel = torch.jit.script(LoopModel())\ninputs = (torch.randn(16), torch.tensor(8))\n\nout = model(*inputs)\ntorch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n Write PyTorch model in Torch way PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model. For the trace-based exporter, tracing treats the numpy values as the constant node, therefore it calculates the wrong result if we change the input. So the PyTorch model need implement using torch operators. For example, do not use numpy operators on numpy tensors: np.concatenate((x, y, z), axis=1)\n do not convert to numpy types: y = x.astype(np.int)\n Always use torch tensors and torch operators: torch.concat, etc. In addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., class MyModule(nn.Module):\n    def __init__(self):\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)\n Using dictionaries to handle Named Arguments as model inputs There are two ways to handle models which consist of named parameters or keyword arguments as inputs:  The first method is to pass all the inputs in the same order as required by the model and pass None values for the keyword arguments that do not require a value to be passed The second and more intuitive method is to represent the keyword arguments as key-value pairs where the key represents the name of the argument in the model signature and the value represents the value of the argument to be passed  For example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)\n There are two ways of exporting the model:  \nNot using a dictionary for the keyword arguments and passing all the inputs in the same order as required by the model torch.onnx.export(model, (x, None, z), \u2018test.onnx\u2019)\n  \nUsing a dictionary to represent the keyword arguments. This dictionary is always passed in addition to the non-keyword arguments and is always the last argument in the args tuple. torch.onnx.export(model, (x, {'y': None, 'z': z}), \u2018test.onnx\u2019)\n   For cases in which there are no keyword arguments, models can be exported with either an empty or no dictionary. For example, torch.onnx.export(model, (x, {}), \u2018test.onnx\u2019)\nor\ntorch.onnx.export(model, (x, ), \u2018test.onnx\u2019)\n An exception to this rule are cases in which the last input is also of a dictionary type. In these cases it is mandatory to have an empty dictionary as the last argument in the args tuple. For example, class Model(torch.nn.Module):\n  def forward(self, k, x):\n    ...\n    return x\nm = Model()\nk =\u202ftorch.randn(2, 3)\nx = {torch.tensor(1.):\u202ftorch.randn(2, 3)}\n Without the presence of the empty dictionary, the export call assumes that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)\n Indexing Tensor indexing in PyTorch is very flexible and complicated. There are two categories of indexing. Both are largely supported in exporting today. If you are experiencing issues exporting indexing that belongs to the supported patterns below, please double check that you are exporting with the latest opset (opset_version=12). Getter This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: data = torch.randn(3, 4)\nindex = torch.tensor([1, 2])\n\n# RHS indexing is supported in ONNX opset >= 11.\nclass RHSIndexing(torch.nn.Module):\n    def forward(self, data, index):\n        return data[index]\n\nout = RHSIndexing()(data, index)\n\ntorch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('indexing.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: data.numpy(),\n    sess.get_inputs()[1].name: index.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\n Below is the list of supported patterns for RHS indexing. # Scalar indices\ndata[0, 1]\n\n# Slice indices\ndata[:3]\n\n# Tensor indices\ndata[torch.tensor([[1, 2], [2, 3]])]\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])]\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])]\n\n# Ellipsis\n# Not supported in scripting\n# i.e. torch.jit.script(model) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[...]\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]\n\n# Boolean mask (supported for ONNX opset version >= 11)\ndata[data != 1]\n And below is the list of unsupported patterns for RHS indexing. # Tensor indices that includes negative values.\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\n Setter In code, this type of indexing occurs on the LHS. Export is supported for ONNX opset version >= 11. E.g.: data = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\n\n# LHS indexing is supported in ONNX opset >= 11.\nclass LHSIndexing(torch.nn.Module):\n    def forward(self, data, new_data):\n        data[1] = new_data\n        return data\n\nout = LHSIndexing()(data, new_data)\n\ndata = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\ntorch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('inplace_assign.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),\n    sess.get_inputs()[1].name: new_data.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\n Below is the list of supported patterns for LHS indexing. # Scalar indices\ndata[0, 1] = new_data\n\n# Slice indices\ndata[:3] = new_data\n\n# Tensor indices\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\n\n# Ellipsis\n# Not supported to export in script modules\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[...] = new_data\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\n\n# Boolean mask\ndata[data != 1] = new_data\n And below is the list of unsupported patterns for LHS indexing. # Multiple tensor indices if any has rank >= 2\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n\n# Multiple tensor indices that are not consecutive\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n\n# Tensor indices that includes negative values.\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\n If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that you are exporting with the latest opset (opset_version=12). TorchVision support All TorchVision models, except for quantized versions, are exportable to ONNX. More details can be found in TorchVision. Limitations  Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted but their usage is not recommended. Users need to verify their dict inputs carefully, and keep in mind that dynamic lookups are not available. PyTorch and ONNX backends(Caffe2, ONNX Runtime, etc) often have implementations of operators with some numeric differences. Depending on model structure, these differences may be negligible, but they can also cause major divergences in behavior (especially on untrained models.) We allow Caffe2 to call directly to Torch implementations of operators, to help you smooth over these differences when precision is important, and to also document these differences.  Supported operators The following operators are supported:  BatchNorm ConstantPadNd Conv Dropout Embedding (no optional arguments supported) EmbeddingBag FeatureDropout (training mode not supported) Index MaxPool1d MaxPool2d MaxPool3d RNN abs absolute acos adaptive_avg_pool1d adaptive_avg_pool2d adaptive_avg_pool3d adaptive_max_pool1d adaptive_max_pool2d adaptive_max_pool3d add (nonzero alpha not supported) addmm and arange argmax argmin asin atan avg_pool1d avg_pool2d avg_pool2d avg_pool3d as_strided baddbmm bitshift cat ceil celu clamp clamp_max clamp_min concat copy cos cumsum det dim_arange div dropout einsum elu empty empty_like eq erf exp expand expand_as eye flatten floor floor_divide frobenius_norm full full_like gather ge gelu glu group_norm gt hardswish hardtanh im2col index_copy index_fill index_put index_select instance_norm interpolate isnan KLDivLoss layer_norm le leaky_relu len log log1p log2 log_sigmoid log_softmax logdet logsumexp lt masked_fill masked_scatter masked_select max mean min mm mul multinomial narrow ne neg new_empty new_full new_zeros nll_loss nonzero norm ones ones_like or permute pixel_shuffle pow prelu (single weight shared among input channels not supported) prod rand randn randn_like reciprocal reflection_pad relu repeat replication_pad reshape reshape_as round rrelu rsqrt rsub scalar_tensor scatter scatter_add select selu sigmoid sign sin size slice softmax softplus sort split sqrt squeeze stack std sub (nonzero alpha not supported) sum t tan tanh threshold (non-zero threshold/non-zero value not supported) to topk transpose true_divide type_as unbind unfold (experimental support with ATen-Caffe2 integration) unique unsqueeze upsample_nearest1d upsample_nearest2d upsample_nearest3d view weight_norm where zeros zeros_like  The operator set above is sufficient to export the following models:  AlexNet DCGAN DenseNet Inception (warning: this model is highly sensitive to changes in operator implementation) ResNet SuperResolution VGG word_language_model  Adding support for operators Adding export support for operators is an advance usage. To achieve this, developers need to touch the source code of PyTorch. Please follow the instructions for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the ONNX operator list. ATen operators If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below:  Define the symbolic function in torch/onnx/symbolic_opset<version>.py, for example torch/onnx/symbolic_opset9.py. Make sure the function has the same name as the ATen operator/function defined in VariableType.h. The first parameter is always the exported ONNX graph. Parameter names must EXACTLY match the names in VariableType.h, because dispatch is done with keyword arguments. Parameter ordering does NOT necessarily match what is in VariableType.h, tensors (inputs) are always first, then non-tensor arguments. In the symbolic function, if the operator is already standardized in ONNX, we only need to create a node to represent the ONNX operator in the graph. If the input argument is a tensor, but ONNX asks for a scalar, we have to explicitly do the conversion. The helper function _scalar can convert a scalar tensor into a python scalar, and _if_scalar_type_as can turn a Python scalar into a PyTorch tensor.  Non-ATen operators If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions:  Create a symbolic function named symbolic in the corresponding Function class. The first parameter is always the exported ONNX graph. Parameter names except the first must EXACTLY match the names in forward. The output tuple size must match the outputs of forward. In the symbolic function, if the operator is already standardized in ONNX, we just need to create a node to represent the ONNX operator in the graph.  Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this: def operator/symbolic(g, *inputs):\n  \"\"\"\n  Modifies Graph (e.g., using \"op\"), adding the ONNX operations representing\n  this PyTorch function, and returning a Value or tuple of Values specifying the\n  ONNX outputs whose values correspond to the original PyTorch return values\n  of the autograd Function (or None if an output is not supported by ONNX).\n\n  Args:\n    g (Graph): graph to write the ONNX representation into\n    inputs (Value...): list of values representing the variables which contain\n        the inputs for this function\n  \"\"\"\n\nclass Value(object):\n  \"\"\"Represents an intermediate tensor value computed in ONNX.\"\"\"\n  def type(self):\n    \"\"\"Returns the Type of the value.\"\"\"\n\nclass Type(object):\n  def sizes(self):\n    \"\"\"Returns a tuple of ints representing the shape of a tensor this describes.\"\"\"\n\nclass Graph(object):\n  def op(self, opname, *inputs, **attrs):\n    \"\"\"\n    Create an ONNX operator 'opname', taking 'args' as inputs\n    and attributes 'kwargs' and add it as a node to the current graph,\n    returning the value representing the single output of this\n    operator (see the `outputs` keyword argument for multi-return\n    nodes).\n\n    The set of operators and the inputs/attributes they take\n    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md\n\n    Args:\n        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.\n        args (Value...): The inputs to the operator; usually provided\n            as arguments to the `symbolic` definition.\n        kwargs: The attributes of the ONNX operator, with keys named\n            according to the following convention: `alpha_f` indicates\n            the `alpha` attribute with type `f`.  The valid type specifiers are\n            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute\n            specified with type float accepts either a single float, or a\n            list of floats (e.g., you would say `dims_i` for a `dims` attribute\n            that takes a list of integers).\n        outputs (int, optional):  The number of outputs this operator returns;\n            by default an operator is assumed to return a single output.\n            If `outputs` is greater than one, this functions returns a tuple\n            of output `Value`, representing each output of the ONNX operator\n            in positional.\n    \"\"\"\n The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h. Here is an example of handling missing symbolic function for elu operator. We try to export the model and see the error message as below: UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator elu\n The export fails because PyTorch does not support exporting elu operator. We find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override; in VariableType.h. This means elu is an ATen operator. We check the ONNX operator list, and confirm that Elu is standardized in ONNX. We add the following lines to symbolic_opset9.py: def elu(g, input, alpha, inplace=False):\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))\n Now PyTorch is able to export elu operator. There are more examples in symbolic_opset9.py, symbolic_opset10.py. The interface for specifying operator definitions is experimental; adventurous users should note that the APIs will probably change in a future interface. Custom operators Following this tutorial Extending TorchScript with Custom C++ Operators, you can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: # Create custom symbolic function\nfrom torch.onnx.symbolic_helper import parse_args\n@parse_args('v', 'v', 'f', 'i')\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\n    return g.op(\"Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\n# Register custom symbolic function\nfrom torch.onnx import register_custom_op_symbolic\nregister_custom_op_symbolic('custom_ops::foo_forward', symbolic_foo_forward, 9)\n\nclass FooModel(torch.nn.Module):\n    def __init__(self, attr1, attr2):\n        super(FooModule, self).__init__()\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def forward(self, input1, input2):\n        # Calling custom op\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n\nmodel = FooModel(attr1, attr2)\ntorch.onnx.export(model, (dummy_input1, dummy_input2), 'model.onnx', custom_opsets={\"custom_domain\": 2})\n Depending on the custom operator, you can export it as one or a combination of existing ONNX ops. You can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain and version (custom opset) using the custom_opsets dictionary at export. If not explicitly specified, the custom opset version is set to 1 by default. Using custom ONNX ops, you will need to extend the backend of your choice with matching custom ops implementation, e.g. Caffe2 custom ops, ONNX Runtime custom ops. Operator Export Type Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API. This flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX. ONNX This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)\n    return (%2)\n ONNX_ATEN This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"exp\"](%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"div\"](%0, %1)\n    return (%2)\n ONNX_ATEN_FALLBACK To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. Example torch ir graph:\n\n  graph(%0 : Float):\n    %3 : int = prim::Constant[value=0]()\n    %4 : Float = aten::triu(%0, %3) # unsupported op\n    %5 : Float = aten::mul(%4, %0) # registered op\n    return (%5)\n\nis exported as:\n\n  graph(%0 : Float):\n    %1 : Long() = onnx::Constant[value={0}]()\n    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1) # unsupported op\n    %3 : Float = onnx::Mul(%2, %0) # registered op\n    return (%3)\n RAW To export a raw ir. Example torch ir graph:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n\nis exported as:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n ONNX_FALLTHROUGH This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op\n    return (%7))\n\nis exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op\n    return (%3\n Frequently Asked Questions Q: I have exported my lstm model, but its input size seems to be fixed? The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. layer_count = 4\n\nmodel = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)\nmodel.eval()\n\nwith torch.no_grad():\n    input = torch.randn(5, 3, 10)\n    h0 = torch.randn(layer_count * 2, 3, 20)\n    c0 = torch.randn(layer_count * 2, 3, 20)\n    output, (hn, cn) = model(input, (h0, c0))\n\n    # default export\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape [5, 3, 10]\n    print(onnx_model.graph.input[0])\n\n    # export with `dynamic_axes`\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',\n                    input_names=['input', 'h0', 'c0'],\n                    output_names=['output', 'hn', 'cn'],\n                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape ['sequence', 3, 10]\n    print(onnx_model.graph.input[0])\n Q: How to export models with loops in it? Please checkout Tracing vs Scripting. Q: Does ONNX support implicit scalar datatype casting? No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future. class ImplicitCastType(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        # Exporter knows x is float32, will export '2' as float32 as well.\n        y = x + 2\n        # Without type propagation, exporter doesn't know the datatype of y.\n        # Thus '3' is exported as int64 by default.\n        return y + 3\n        # The following will export correctly.\n        # return y + torch.tensor([3], dtype=torch.float32)\n\nx = torch.tensor([1.0], dtype=torch.float32)\ntorch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',\n                  example_outputs=ImplicitCastType()(x))\n Q: Is tensor in-place indexed assignment like data[index] = new_data supported? Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing. Q: Is tensor list exportable to ONNX? Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.: class ListLoopModel(torch.nn.Module):\n    def forward(self, x):\n        res = []\n        res1 = []\n        arr = x.split(2, 0)\n        res2 = torch.zeros(3, 4, dtype=torch.long)\n        for i in range(len(arr)):\n            res += [arr[i].sum(0, False)]\n            res1 += [arr[-1 - i].sum(0, False)]\n            res2 += 1\n        return torch.stack(res), torch.stack(res1), res2\n\nmodel = torch.jit.script(ListLoopModel())\ninputs = torch.randn(16)\n\nout = model(inputs)\ntorch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('loop_and_list.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: inputs.numpy(),\n})\n\nassert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]\n Use external data format use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. model = torchvision.models.mobilenet_v2(pretrained=True)\ninput = torch.randn(2, 3, 224, 224, requires_grad=True)\ntorch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)\n This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models. Training Training argument in export API allows users to export models in a training-friendly mode. TrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model optimizations which might interfere with model parameter training. TrainingMode.PRESERVE exports the model in inference mode if model.training is False. Otherwise, it exports the model in a training-friendly mode. The default mode for this argument is TrainingMode.EVAL which exports the model in inference mode. Functions  \ntorch.onnx.export(model, args, f, export_params=True, verbose=False, training=<TrainingMode.EVAL: 0>, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False) [source]\n \nExport a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)  Parameters \n \nmodel (torch.nn.Module) \u2013 the model to be exported. \nargs (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013 \na dictionary to specify the input to the corresponding named parameter: - KEY: str, named parameter - VALUE: corresponding input args can be structured either as:  \nONLY A TUPLE OF ARGUMENTS or torch.Tensor: \u2018\u2019args = (x, y, z)\u2019'\n   The inputs to the model, e.g., such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args. If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.  \nA TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: \u2018\u2019args = (x,\n        {\n        \u2018y\u2019: input_y,\n        \u2018z\u2019: input_z\n        }) \u2018\u2019\n   The inputs to the model are structured as a tuple consisting of non-keyword arguments and the last value of this tuple being a dictionary consisting of named parameters and the corresponding inputs as key-value pairs. If certain named argument is not present in the dictionary, it is assigned the default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example.  class Model(torch.nn.Module):\n\n def forward(self, k, x):\n\n\u2026 return x     m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)  \nf \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name. A binary Protobuf will be written to this file. \nexport_params (bool, default True) \u2013 if specified, all parameters will be exported. Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values()\n \nverbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. \ntraining (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. \ninput_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order \noutput_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order \naten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops. \nexport_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. \noperator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 \nOperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph: graph(%0 : Float)::\n  %3 : int = prim::Constant[value=0]()\n  %4 : Float = aten::triu(%0, %3) # missing op\n  %5 : Float = aten::mul(%4, %0) # registered op\n  return (%5)\n is exported as: graph(%0 : Float)::\n  %1 : Long() = onnx::Constant[value={0}]()\n  %2 : Float = aten::ATen[operator=\"triu\"](%0, %1)  # missing op\n  %3 : Float = onnx::Mul(%2, %0) # registered op\n  return (%3)\n In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph: graph(%x.1 : Long(1, strides=[1]))::\n  %1 : None = prim::Constant()\n  %2 : Tensor = aten::sum(%x.1, %1)\n  %y.1 : Tensor[] = prim::ListConstruct(%2)\n  return (%y.1)\n is exported as: graph(%x.1 : Long(1, strides=[1]))::\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\n  %y.1 : Long() = prim::ListConstruct(%1)\n  return (%y.1)\n In the above example, prim::ListConstruct is not supported, hence exporter falls through.  \nopset_version (int, default is 9) \u2013 by default we export the model to the opset version of the onnx submodule. Since ONNX\u2019s latest opset may evolve before next stable release, by default we export to one stable opset version. Right now, supported stable opset version is 9. The opset_version must be _onnx_main_opset or in _onnx_stable_opsets which are defined in torch/onnx/symbolic_helper.py \ndo_constant_folding (bool, default False) \u2013 If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes. \nexample_outputs (tuple of Tensors, default None) \u2013 Model\u2019s example outputs being exported. example_outputs must be provided when exporting a ScriptModule or TorchScript Function. \nstrip_doc_string (bool, default True) \u2013 if True, strips the field \u201cdoc_string\u201d from the exported model, which information about the stack trace. \ndynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013 \na dictionary to specify dynamic axes of input/output, such that: - KEY: input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export. Example. if we have the following shape for inputs and outputs: shape(input_1) = ('b', 3, 'w', 'h')\nand shape(input_2) = ('b', 4)\nand shape(output)  = ('b', 'd', 5)\n Then dynamic axes can be defined either as:  \nONLY INDICES: ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':[0],\n                  'output':[0, 1]}``\nwhere automatic names will be generated for exported dynamic axes\n  \nINDICES WITH CORRESPONDING NAMES: ``dynamic_axes = {'input_1':{0:'batch',\n                             1:'width',\n                             2:'height'},\n                  'input_2':{0:'batch'},\n                  'output':{0:'batch',\n                            1:'detections'}}``\nwhere provided names will be applied to exported dynamic axes\n  \nMIXED MODE OF (1) and (2): ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':{0:'batch'},\n                  'output':[0,1]}``\n    \nkeep_initializers_as_inputs (bool, default None) \u2013 \nIf True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version < 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.  \ncustom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate custom opset domain and version at export. If model contains a custom opset, it is optional to specify the domain and opset version in the dictionary: - KEY: opset domain name - VALUE: opset version If the custom opset is not provided in this dictionary, opset version is set to 1 by default. \nenable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run as part of the export, to ensure the exported model is a valid ONNX model. \nexternal_data_format (bool, default False) \u2013 If True, then the model is exported in ONNX external data format, in which case some of the model parameters are stored in external binary files and not in the ONNX model file itself. See link for format details: https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423 Also, in this case, argument \u2018f\u2019 must be a string specifying the location of the model. The external binary files will be stored in the same location specified by the model location \u2018f\u2019. If False, then the model is stored in regular format, i.e. model and parameters are all in one file. This argument is ignored for all export types other than ONNX.    \n  \ntorch.onnx.export_to_pretty_string(*args, **kwargs) [source]\n\n  \ntorch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version) [source]\n\n  \ntorch.onnx.operators.shape_as_tensor(x) [source]\n\n  \ntorch.onnx.select_model_mode_for_export(model, mode) [source]\n \nA context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019, resetting it when we exit the with-block. A no-op if mode is None. In version 1.6 changed to this from set_training \n  \ntorch.onnx.is_in_onnx_export() [source]\n \nCheck whether it\u2019s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread. \n\n"}, {"name": "torch.onnx.export()", "path": "onnx#torch.onnx.export", "type": "torch.onnx", "text": " \ntorch.onnx.export(model, args, f, export_params=True, verbose=False, training=<TrainingMode.EVAL: 0>, input_names=None, output_names=None, aten=False, export_raw_ir=False, operator_export_type=None, opset_version=None, _retain_param_name=True, do_constant_folding=True, example_outputs=None, strip_doc_string=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, enable_onnx_checker=True, use_external_data_format=False) [source]\n \nExport a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)  Parameters \n \nmodel (torch.nn.Module) \u2013 the model to be exported. \nargs (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013 \na dictionary to specify the input to the corresponding named parameter: - KEY: str, named parameter - VALUE: corresponding input args can be structured either as:  \nONLY A TUPLE OF ARGUMENTS or torch.Tensor: \u2018\u2019args = (x, y, z)\u2019'\n   The inputs to the model, e.g., such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args. If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.  \nA TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: \u2018\u2019args = (x,\n        {\n        \u2018y\u2019: input_y,\n        \u2018z\u2019: input_z\n        }) \u2018\u2019\n   The inputs to the model are structured as a tuple consisting of non-keyword arguments and the last value of this tuple being a dictionary consisting of named parameters and the corresponding inputs as key-value pairs. If certain named argument is not present in the dictionary, it is assigned the default value, or None if default value is not provided. Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example.  class Model(torch.nn.Module):\n\n def forward(self, k, x):\n\n\u2026 return x     m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)} In the previous iteration, the call to export API would look like torch.onnx.export(model, (k, x), \u2018test.onnx\u2019) This would work as intended. However, the export function would now assume that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)  \nf \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name. A binary Protobuf will be written to this file. \nexport_params (bool, default True) \u2013 if specified, all parameters will be exported. Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values()\n \nverbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. \ntraining (enum, default TrainingMode.EVAL) \u2013 TrainingMode.EVAL: export the model in inference mode. TrainingMode.PRESERVE: export the model in inference mode if model.training is False and to a training friendly mode if model.training is True. TrainingMode.TRAINING: export the model in a training friendly mode. \ninput_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order \noutput_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order \naten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops. \nexport_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. \noperator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 \nOperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph: graph(%0 : Float)::\n  %3 : int = prim::Constant[value=0]()\n  %4 : Float = aten::triu(%0, %3) # missing op\n  %5 : Float = aten::mul(%4, %0) # registered op\n  return (%5)\n is exported as: graph(%0 : Float)::\n  %1 : Long() = onnx::Constant[value={0}]()\n  %2 : Float = aten::ATen[operator=\"triu\"](%0, %1)  # missing op\n  %3 : Float = onnx::Mul(%2, %0) # registered op\n  return (%3)\n In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph: graph(%x.1 : Long(1, strides=[1]))::\n  %1 : None = prim::Constant()\n  %2 : Tensor = aten::sum(%x.1, %1)\n  %y.1 : Tensor[] = prim::ListConstruct(%2)\n  return (%y.1)\n is exported as: graph(%x.1 : Long(1, strides=[1]))::\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\n  %y.1 : Long() = prim::ListConstruct(%1)\n  return (%y.1)\n In the above example, prim::ListConstruct is not supported, hence exporter falls through.  \nopset_version (int, default is 9) \u2013 by default we export the model to the opset version of the onnx submodule. Since ONNX\u2019s latest opset may evolve before next stable release, by default we export to one stable opset version. Right now, supported stable opset version is 9. The opset_version must be _onnx_main_opset or in _onnx_stable_opsets which are defined in torch/onnx/symbolic_helper.py \ndo_constant_folding (bool, default False) \u2013 If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes. \nexample_outputs (tuple of Tensors, default None) \u2013 Model\u2019s example outputs being exported. example_outputs must be provided when exporting a ScriptModule or TorchScript Function. \nstrip_doc_string (bool, default True) \u2013 if True, strips the field \u201cdoc_string\u201d from the exported model, which information about the stack trace. \ndynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013 \na dictionary to specify dynamic axes of input/output, such that: - KEY: input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export. Example. if we have the following shape for inputs and outputs: shape(input_1) = ('b', 3, 'w', 'h')\nand shape(input_2) = ('b', 4)\nand shape(output)  = ('b', 'd', 5)\n Then dynamic axes can be defined either as:  \nONLY INDICES: ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':[0],\n                  'output':[0, 1]}``\nwhere automatic names will be generated for exported dynamic axes\n  \nINDICES WITH CORRESPONDING NAMES: ``dynamic_axes = {'input_1':{0:'batch',\n                             1:'width',\n                             2:'height'},\n                  'input_2':{0:'batch'},\n                  'output':{0:'batch',\n                            1:'detections'}}``\nwhere provided names will be applied to exported dynamic axes\n  \nMIXED MODE OF (1) and (2): ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':{0:'batch'},\n                  'output':[0,1]}``\n    \nkeep_initializers_as_inputs (bool, default None) \u2013 \nIf True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version < 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.  \ncustom_opsets (dict<string, int>, default empty dict) \u2013 A dictionary to indicate custom opset domain and version at export. If model contains a custom opset, it is optional to specify the domain and opset version in the dictionary: - KEY: opset domain name - VALUE: opset version If the custom opset is not provided in this dictionary, opset version is set to 1 by default. \nenable_onnx_checker (bool, default True) \u2013 If True the onnx model checker will be run as part of the export, to ensure the exported model is a valid ONNX model. \nexternal_data_format (bool, default False) \u2013 If True, then the model is exported in ONNX external data format, in which case some of the model parameters are stored in external binary files and not in the ONNX model file itself. See link for format details: https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423 Also, in this case, argument \u2018f\u2019 must be a string specifying the location of the model. The external binary files will be stored in the same location specified by the model location \u2018f\u2019. If False, then the model is stored in regular format, i.e. model and parameters are all in one file. This argument is ignored for all export types other than ONNX.    \n"}, {"name": "torch.onnx.export_to_pretty_string()", "path": "onnx#torch.onnx.export_to_pretty_string", "type": "torch.onnx", "text": " \ntorch.onnx.export_to_pretty_string(*args, **kwargs) [source]\n\n"}, {"name": "torch.onnx.is_in_onnx_export()", "path": "onnx#torch.onnx.is_in_onnx_export", "type": "torch.onnx", "text": " \ntorch.onnx.is_in_onnx_export() [source]\n \nCheck whether it\u2019s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread. \n"}, {"name": "torch.onnx.operators.shape_as_tensor()", "path": "onnx#torch.onnx.operators.shape_as_tensor", "type": "torch.onnx", "text": " \ntorch.onnx.operators.shape_as_tensor(x) [source]\n\n"}, {"name": "torch.onnx.register_custom_op_symbolic()", "path": "onnx#torch.onnx.register_custom_op_symbolic", "type": "torch.onnx", "text": " \ntorch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version) [source]\n\n"}, {"name": "torch.onnx.select_model_mode_for_export()", "path": "onnx#torch.onnx.select_model_mode_for_export", "type": "torch.onnx", "text": " \ntorch.onnx.select_model_mode_for_export(model, mode) [source]\n \nA context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019, resetting it when we exit the with-block. A no-op if mode is None. In version 1.6 changed to this from set_training \n"}, {"name": "torch.optim", "path": "optim", "type": "torch.optim", "text": "torch.optim torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future. How to use an optimizer To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients. Constructing it To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.  Note If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call. In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.  Example: optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n Per-parameter options Optimizer s also support specifying per-parameter options. To do this, instead of passing an iterable of Variable s, pass in an iterable of dict s. Each of them will define a separate parameter group, and should contain a params key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.  Note You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn\u2019t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.  For example, this is very useful when one wants to specify per-layer learning rates: optim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)\n This means that model.base\u2019s parameters will use the default learning rate of 1e-2, model.classifier\u2019s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters. Taking an optimization step All optimizers implement a step() method, that updates the parameters. It can be used in two ways: optimizer.step() This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. backward(). Example: for input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n optimizer.step(closure) Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it. Example: for input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n Algorithms  \nclass torch.optim.Optimizer(params, defaults) [source]\n \nBase class for all optimizers.  Warning Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries.   Parameters \n \nparams (iterable) \u2013 an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized. \ndefaults \u2013 (dict): a dict containing default values of optimization options (used when a parameter group doesn\u2019t specify them).     \nadd_param_group(param_group) [source]\n \nAdd a param group to the Optimizer s param_groups. This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.  Parameters \n \nparam_group (dict) \u2013 Specifies what Tensors should be optimized along with group \noptimization options. (specific) \u2013     \n  \nload_state_dict(state_dict) [source]\n \nLoads the optimizer state.  Parameters \nstate_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().   \n  \nstate_dict() [source]\n \nReturns the state of the optimizer as a dict. It contains two entries:  \n state - a dict holding current optimization state. Its content\n\ndiffers between optimizer classes.    param_groups - a dict containing all parameter groups  \n  \nstep(closure) [source]\n \nPerforms a single optimization step (parameter update).  Parameters \nclosure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.    Note Unless otherwise specified, this function should not modify the .grad field of the parameters.  \n  \nzero_grad(set_to_none=False) [source]\n \nSets the gradients of all optimized torch.Tensor s to zero.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).   \n \n  \nclass torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) [source]\n \nImplements Adadelta algorithm. It has been proposed in ADADELTA: An Adaptive Learning Rate Method.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nrho (float, optional) \u2013 coefficient used for computing a running average of squared gradients (default: 0.9) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-6) \nlr (float, optional) \u2013 coefficient that scale delta before it is applied to the parameters (default: 1.0) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10) [source]\n \nImplements Adagrad algorithm. It has been proposed in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \nlr_decay (float, optional) \u2013 learning rate decay (default: 0) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-10)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) [source]\n \nImplements Adam algorithm. It has been proposed in Adam: A Method for Stochastic Optimization. The implementation of the L2 penalty follows changes proposed in Decoupled Weight Decay Regularization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0) \namsgrad (boolean, optional) \u2013 whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False) [source]\n \nImplements AdamW algorithm. The original Adam algorithm was proposed in Adam: A Method for Stochastic Optimization. The AdamW variant was proposed in Decoupled Weight Decay Regularization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \nweight_decay (float, optional) \u2013 weight decay coefficient (default: 1e-2) \namsgrad (boolean, optional) \u2013 whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) [source]\n \nImplements lazy version of Adam algorithm suitable for sparse tensors. In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) [source]\n \nImplements Adamax algorithm (a variant of Adam based on infinity norm). It has been proposed in Adam: A Method for Stochastic Optimization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 2e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) [source]\n \nImplements Averaged Stochastic Gradient Descent. It has been proposed in Acceleration of stochastic approximation by averaging.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \nlambd (float, optional) \u2013 decay term (default: 1e-4) \nalpha (float, optional) \u2013 power for eta update (default: 0.75) \nt0 (float, optional) \u2013 point at which to start averaging (default: 1e6) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None) [source]\n \nImplements L-BFGS algorithm, heavily inspired by minFunc <https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>.  Warning This optimizer doesn\u2019t support per-parameter options and parameter groups (there can be only one).   Warning Right now all parameters have to be on a single device. This will be improved in the future.   Note This is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn\u2019t fit in memory try reducing the history size, or use a different algorithm.   Parameters \n \nlr (float) \u2013 learning rate (default: 1) \nmax_iter (int) \u2013 maximal number of iterations per optimization step (default: 20) \nmax_eval (int) \u2013 maximal number of function evaluations per optimization step (default: max_iter * 1.25). \ntolerance_grad (float) \u2013 termination tolerance on first order optimality (default: 1e-5). \ntolerance_change (float) \u2013 termination tolerance on function value/parameter changes (default: 1e-9). \nhistory_size (int) \u2013 update history size (default: 100). \nline_search_fn (str) \u2013 either \u2018strong_wolfe\u2019 or None (default: None).     \nstep(closure) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) [source]\n \nImplements RMSprop algorithm. Proposed by G. Hinton in his course. The centered version first appears in Generating Sequences With Recurrent Neural Networks. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon)  where \u03b1\\alpha  is the scheduled learning rate and vv  is the weighted moving average of the squared gradient.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \nmomentum (float, optional) \u2013 momentum factor (default: 0) \nalpha (float, optional) \u2013 smoothing constant (default: 0.99) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \ncentered (bool, optional) \u2013 if True, compute the centered RMSProp, the gradient is normalized by an estimation of its variance \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) [source]\n \nImplements the resilient backpropagation algorithm.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \netas (Tuple[float, float], optional) \u2013 pair of (etaminus, etaplis), that are multiplicative increase and decrease factors (default: (0.5, 1.2)) \nstep_sizes (Tuple[float, float], optional) \u2013 a pair of minimal and maximal allowed step sizes (default: (1e-6, 50))     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n  \nclass torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False) [source]\n \nImplements stochastic gradient descent (optionally with momentum). Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float) \u2013 learning rate \nmomentum (float, optional) \u2013 momentum factor (default: 0) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0) \ndampening (float, optional) \u2013 dampening for momentum (default: 0) \nnesterov (bool, optional) \u2013 enables Nesterov momentum (default: False)    Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> optimizer.zero_grad()\n>>> loss_fn(model(input), target).backward()\n>>> optimizer.step()\n  Note The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks. Considering the specific case of Momentum, the update can be written as  vt+1=\u03bc\u2217vt+gt+1,pt+1=pt\u2212lr\u2217vt+1,\\begin{aligned} v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\ p_{t+1} & = p_{t} - \\text{lr} * v_{t+1}, \\end{aligned}  \nwhere pp , gg , vv  and \u03bc\\mu  denote the parameters, gradient, velocity, and momentum respectively. This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form  vt+1=\u03bc\u2217vt+lr\u2217gt+1,pt+1=pt\u2212vt+1.\\begin{aligned} v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\ p_{t+1} & = p_{t} - v_{t+1}. \\end{aligned}  \nThe Nesterov version is analogously modified.   \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n How to adjust learning rate torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you should write your code this way: >>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n  Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling scheduler.step()) before the optimizer\u2019s update (calling optimizer.step()), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling scheduler.step() at the wrong time.   \nclass torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False) [source]\n \nSets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nlr_lambda (function or list) \u2013 A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> # Assuming optimizer has two groups.\n>>> lambda1 = lambda epoch: epoch // 30\n>>> lambda2 = lambda epoch: 0.95 ** epoch\n>>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n  \nload_state_dict(state_dict) [source]\n \nLoads the schedulers state. When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.  Parameters \nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   \n  \nstate_dict() [source]\n \nReturns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. When saving or loading the scheduler, please make sure to also save or load the state of the optimizer. \n \n  \nclass torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False) [source]\n \nMultiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nlr_lambda (function or list) \u2013 A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> lmbda = lambda epoch: 0.95\n>>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n  \nload_state_dict(state_dict) [source]\n \nLoads the schedulers state.  Parameters \nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   \n  \nstate_dict() [source]\n \nReturns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. \n \n  \nclass torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False) [source]\n \nDecays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nstep_size (int) \u2013 Period of learning rate decay. \ngamma (float) \u2013 Multiplicative factor of learning rate decay. Default: 0.1. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.005    if 30 <= epoch < 60\n>>> # lr = 0.0005   if 60 <= epoch < 90\n>>> # ...\n>>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n \n  \nclass torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False) [source]\n \nDecays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nmilestones (list) \u2013 List of epoch indices. Must be increasing. \ngamma (float) \u2013 Multiplicative factor of learning rate decay. Default: 0.1. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.005    if 30 <= epoch < 80\n>>> # lr = 0.0005   if epoch >= 80\n>>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n \n  \nclass torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False) [source]\n \nDecays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \ngamma (float) \u2013 Multiplicative factor of learning rate decay. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    \n  \nclass torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False) [source]\n \nSet the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr and TcurT_{cur}  is the number of epochs since the last restart in SGDR:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTmax\u03c0)),Tcur\u2260(2k+1)Tmax;\u03b7t+1=\u03b7t+12(\u03b7max\u2212\u03b7min)(1\u2212cos\u2061(1Tmax\u03c0)),Tcur=(2k+1)Tmax.\\begin{aligned} \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right), & T_{cur} \\neq (2k+1)T_{max}; \\\\ \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min}) \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right), & T_{cur} = (2k+1)T_{max}. \\end{aligned}  \nWhen last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTmax\u03c0))\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)  \nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nT_max (int) \u2013 Maximum number of iterations. \neta_min (float) \u2013 Minimum learning rate. Default: 0. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    \n  \nclass torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False) [source]\n \nReduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nmode (str) \u2013 One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: \u2018min\u2019. \nfactor (float) \u2013 Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1. \npatience (int) \u2013 Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn\u2019t improved then. Default: 10. \nthreshold (float) \u2013 Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4. \nthreshold_mode (str) \u2013 One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in \u2018max\u2019 mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: \u2018rel\u2019. \ncooldown (int) \u2013 Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0. \nmin_lr (float or list) \u2013 A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0. \neps (float) \u2013 Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n>>> for epoch in range(10):\n>>>     train(...)\n>>>     val_loss = validate(...)\n>>>     # Note that step should be called after validate()\n>>>     scheduler.step(val_loss)\n \n  \nclass torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False) [source]\n \nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis. Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This class has three built-in policies, as put forth in the paper:  \u201ctriangular\u201d: A basic triangular cycle without amplitude scaling. \u201ctriangular2\u201d: A basic triangular cycle that scales initial amplitude by half each cycle. \u201cexp_range\u201d: A cycle that scales initial amplitude by gammacycle iterations\\text{gamma}^{\\text{cycle iterations}}  at each cycle iteration.  This implementation was adapted from the github repo: bckenstler/CLR  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nbase_lr (float or list) \u2013 Initial learning rate which is the lower boundary in the cycle for each parameter group. \nmax_lr (float or list) \u2013 Upper learning rate boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. \nstep_size_up (int) \u2013 Number of training iterations in the increasing half of a cycle. Default: 2000 \nstep_size_down (int) \u2013 Number of training iterations in the decreasing half of a cycle. If step_size_down is None, it is set to step_size_up. Default: None \nmode (str) \u2013 One of {triangular, triangular2, exp_range}. Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. Default: \u2018triangular\u2019 \ngamma (float) \u2013 Constant in \u2018exp_range\u2019 scaling function: gamma**(cycle iterations) Default: 1.0 \nscale_fn (function) \u2013 Custom scaling policy defined by a single argument lambda function, where 0 <= scale_fn(x) <= 1 for all x >= 0. If specified, then \u2018mode\u2019 is ignored. Default: None \nscale_mode (str) \u2013 {\u2018cycle\u2019, \u2018iterations\u2019}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default: \u2018cycle\u2019 \ncycle_momentum (bool) \u2013 If True, momentum is cycled inversely to learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019. Default: True \nbase_momentum (float or list) \u2013 Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is \u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019. Default: 0.8 \nmax_momentum (float or list) \u2013 Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). The momentum at any cycle is the difference of max_momentum and some scaling of the amplitude; therefore base_momentum may not actually be reached depending on scaling function. Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019 and learning rate is \u2018base_lr\u2019 Default: 0.9 \nlast_epoch (int) \u2013 The index of the last batch. This parameter is used when resuming a training job. Since step() should be invoked after each batch instead of after each epoch, this number represents the total number of batches computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1 \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n>>> data_loader = torch.utils.data.DataLoader(...)\n>>> for epoch in range(10):\n>>>     for batch in data_loader:\n>>>         train_batch(...)\n>>>         scheduler.step()\n  \nget_lr() [source]\n \nCalculates the learning rate at batch index. This function treats self.last_epoch as the last batch index. If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum. \n \n  \nclass torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=-1, verbose=False) [source]\n \nSets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):  A value for total_steps is explicitly provided. A number of epochs (epochs) and a number of steps per epoch (steps_per_epoch) are provided. In this case, the number of total steps is inferred by total_steps = epochs * steps_per_epoch  You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch. The default behaviour of this scheduler follows the fastai implementation of 1cycle, which claims that \u201cunpublished work has shown even better results by using only two phases\u201d. To mimic the behaviour of the original paper instead, set three_phase=True.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nmax_lr (float or list) \u2013 Upper learning rate boundaries in the cycle for each parameter group. \ntotal_steps (int) \u2013 The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None \nepochs (int) \u2013 The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None \nsteps_per_epoch (int) \u2013 The number of steps per epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None \npct_start (float) \u2013 The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3 \nanneal_strategy (str) \u2013 {\u2018cos\u2019, \u2018linear\u2019} Specifies the annealing strategy: \u201ccos\u201d for cosine annealing, \u201clinear\u201d for linear annealing. Default: \u2018cos\u2019 \ncycle_momentum (bool) \u2013 If True, momentum is cycled inversely to learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019. Default: True \nbase_momentum (float or list) \u2013 Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is \u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019. Default: 0.85 \nmax_momentum (float or list) \u2013 Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019 and learning rate is \u2018base_lr\u2019 Default: 0.95 \ndiv_factor (float) \u2013 Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25 \nfinal_div_factor (float) \u2013 Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Default: 1e4 \nthree_phase (bool) \u2013 If True, use a third phase of the schedule to annihilate the learning rate according to \u2018final_div_factor\u2019 instead of modifying the second phase (the first two phases will be symmetrical about the step indicated by \u2018pct_start\u2019). \nlast_epoch (int) \u2013 The index of the last batch. This parameter is used when resuming a training job. Since step() should be invoked after each batch instead of after each epoch, this number represents the total number of batches computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1 \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> data_loader = torch.utils.data.DataLoader(...)\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n>>> for epoch in range(10):\n>>>     for batch in data_loader:\n>>>         train_batch(...)\n>>>         scheduler.step()\n \n  \nclass torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False) [source]\n \nSet the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr, TcurT_{cur}  is the number of epochs since the last restart and TiT_{i}  is the number of epochs between two warm restarts in SGDR:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTi\u03c0))\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)  \nWhen Tcur=TiT_{cur}=T_{i} , set \u03b7t=\u03b7min\\eta_t = \\eta_{min} . When Tcur=0T_{cur}=0  after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max} . It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nT_0 (int) \u2013 Number of iterations for the first restart. \nT_mult (int, optional) \u2013 A factor increases TiT_{i}  after a restart. Default: 1. \neta_min (float, optional) \u2013 Minimum learning rate. Default: 0. \nlast_epoch (int, optional) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.     \nstep(epoch=None) [source]\n \nStep could be called after every batch update Example >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n>>> iters = len(dataloader)\n>>> for epoch in range(20):\n>>>     for i, sample in enumerate(dataloader):\n>>>         inputs, labels = sample['inputs'], sample['labels']\n>>>         optimizer.zero_grad()\n>>>         outputs = net(inputs)\n>>>         loss = criterion(outputs, labels)\n>>>         loss.backward()\n>>>         optimizer.step()\n>>>         scheduler.step(epoch + i / iters)\n This function can be called in an interleaved way. Example >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n>>> for epoch in range(20):\n>>>     scheduler.step()\n>>> scheduler.step(26)\n>>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n \n \n Stochastic Weight Averaging torch.optim.swa_utils implements Stochastic Weight Averaging (SWA). In particular, torch.optim.swa_utils.AveragedModel class implements SWA models, torch.optim.swa_utils.SWALR implements the SWA learning rate scheduler and torch.optim.swa_utils.update_bn() is a utility function used to update SWA batch normalization statistics at the end of training. SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization. Constructing averaged models AveragedModel class serves to compute the weights of the SWA model. You can create an averaged model by running: >>> swa_model = AveragedModel(model)\n Here the model model can be an arbitrary torch.nn.Module object. swa_model will keep track of the running averages of the parameters of the model. To update these averages, you can use the update_parameters() function: >>> swa_model.update_parameters(model)\n SWA learning rate schedules Typically, in SWA the learning rate is set to a high constant value. SWALR is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group: >>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n You can also use cosine annealing to a fixed value instead of linear annealing by setting anneal_strategy=\"cos\". Taking care of batch normalization update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader loader at the end of training: >>> torch.optim.swa_utils.update_bn(loader, swa_model)\n update_bn() applies the swa_model to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.  Warning update_bn() assumes that each batch in the dataloader loader is either a tensors or a list of tensors where the first element is the tensor that the network swa_model should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the swa_model by doing a forward pass with the swa_model on each element of the dataset.  Custom averaging strategies By default, torch.optim.swa_utils.AveragedModel computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the avg_fn parameter. In the following example ema_model computes an exponential moving average. Example: >>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n Putting it all together In the example below, swa_model is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160: >>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)\n\n"}, {"name": "torch.optim.Adadelta", "path": "optim#torch.optim.Adadelta", "type": "torch.optim", "text": " \nclass torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0) [source]\n \nImplements Adadelta algorithm. It has been proposed in ADADELTA: An Adaptive Learning Rate Method.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nrho (float, optional) \u2013 coefficient used for computing a running average of squared gradients (default: 0.9) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-6) \nlr (float, optional) \u2013 coefficient that scale delta before it is applied to the parameters (default: 1.0) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.Adadelta.step()", "path": "optim#torch.optim.Adadelta.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.Adagrad", "path": "optim#torch.optim.Adagrad", "type": "torch.optim", "text": " \nclass torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10) [source]\n \nImplements Adagrad algorithm. It has been proposed in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \nlr_decay (float, optional) \u2013 learning rate decay (default: 0) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-10)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.Adagrad.step()", "path": "optim#torch.optim.Adagrad.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.Adam", "path": "optim#torch.optim.Adam", "type": "torch.optim", "text": " \nclass torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) [source]\n \nImplements Adam algorithm. It has been proposed in Adam: A Method for Stochastic Optimization. The implementation of the L2 penalty follows changes proposed in Decoupled Weight Decay Regularization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0) \namsgrad (boolean, optional) \u2013 whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.Adam.step()", "path": "optim#torch.optim.Adam.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.Adamax", "path": "optim#torch.optim.Adamax", "type": "torch.optim", "text": " \nclass torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) [source]\n \nImplements Adamax algorithm (a variant of Adam based on infinity norm). It has been proposed in Adam: A Method for Stochastic Optimization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 2e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.Adamax.step()", "path": "optim#torch.optim.Adamax.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.AdamW", "path": "optim#torch.optim.AdamW", "type": "torch.optim", "text": " \nclass torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False) [source]\n \nImplements AdamW algorithm. The original Adam algorithm was proposed in Adam: A Method for Stochastic Optimization. The AdamW variant was proposed in Decoupled Weight Decay Regularization.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \nweight_decay (float, optional) \u2013 weight decay coefficient (default: 1e-2) \namsgrad (boolean, optional) \u2013 whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.AdamW.step()", "path": "optim#torch.optim.AdamW.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.ASGD", "path": "optim#torch.optim.ASGD", "type": "torch.optim", "text": " \nclass torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0) [source]\n \nImplements Averaged Stochastic Gradient Descent. It has been proposed in Acceleration of stochastic approximation by averaging.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \nlambd (float, optional) \u2013 decay term (default: 1e-4) \nalpha (float, optional) \u2013 power for eta update (default: 0.75) \nt0 (float, optional) \u2013 point at which to start averaging (default: 1e6) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.ASGD.step()", "path": "optim#torch.optim.ASGD.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.LBFGS", "path": "optim#torch.optim.LBFGS", "type": "torch.optim", "text": " \nclass torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, history_size=100, line_search_fn=None) [source]\n \nImplements L-BFGS algorithm, heavily inspired by minFunc <https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>.  Warning This optimizer doesn\u2019t support per-parameter options and parameter groups (there can be only one).   Warning Right now all parameters have to be on a single device. This will be improved in the future.   Note This is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn\u2019t fit in memory try reducing the history size, or use a different algorithm.   Parameters \n \nlr (float) \u2013 learning rate (default: 1) \nmax_iter (int) \u2013 maximal number of iterations per optimization step (default: 20) \nmax_eval (int) \u2013 maximal number of function evaluations per optimization step (default: max_iter * 1.25). \ntolerance_grad (float) \u2013 termination tolerance on first order optimality (default: 1e-5). \ntolerance_change (float) \u2013 termination tolerance on function value/parameter changes (default: 1e-9). \nhistory_size (int) \u2013 update history size (default: 100). \nline_search_fn (str) \u2013 either \u2018strong_wolfe\u2019 or None (default: None).     \nstep(closure) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.LBFGS.step()", "path": "optim#torch.optim.LBFGS.step", "type": "torch.optim", "text": " \nstep(closure) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False) [source]\n \nSet the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr and TcurT_{cur}  is the number of epochs since the last restart in SGDR:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTmax\u03c0)),Tcur\u2260(2k+1)Tmax;\u03b7t+1=\u03b7t+12(\u03b7max\u2212\u03b7min)(1\u2212cos\u2061(1Tmax\u03c0)),Tcur=(2k+1)Tmax.\\begin{aligned} \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right), & T_{cur} \\neq (2k+1)T_{max}; \\\\ \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min}) \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right), & T_{cur} = (2k+1)T_{max}. \\end{aligned}  \nWhen last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTmax\u03c0))\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)  \nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nT_max (int) \u2013 Maximum number of iterations. \neta_min (float) \u2013 Minimum learning rate. Default: 0. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    \n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False) [source]\n \nSet the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr, TcurT_{cur}  is the number of epochs since the last restart and TiT_{i}  is the number of epochs between two warm restarts in SGDR:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTi\u03c0))\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)  \nWhen Tcur=TiT_{cur}=T_{i} , set \u03b7t=\u03b7min\\eta_t = \\eta_{min} . When Tcur=0T_{cur}=0  after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max} . It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nT_0 (int) \u2013 Number of iterations for the first restart. \nT_mult (int, optional) \u2013 A factor increases TiT_{i}  after a restart. Default: 1. \neta_min (float, optional) \u2013 Minimum learning rate. Default: 0. \nlast_epoch (int, optional) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.     \nstep(epoch=None) [source]\n \nStep could be called after every batch update Example >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n>>> iters = len(dataloader)\n>>> for epoch in range(20):\n>>>     for i, sample in enumerate(dataloader):\n>>>         inputs, labels = sample['inputs'], sample['labels']\n>>>         optimizer.zero_grad()\n>>>         outputs = net(inputs)\n>>>         loss = criterion(outputs, labels)\n>>>         loss.backward()\n>>>         optimizer.step()\n>>>         scheduler.step(epoch + i / iters)\n This function can be called in an interleaved way. Example >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n>>> for epoch in range(20):\n>>>     scheduler.step()\n>>> scheduler.step(26)\n>>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n \n \n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step()", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step", "type": "torch.optim", "text": " \nstep(epoch=None) [source]\n \nStep could be called after every batch update Example >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n>>> iters = len(dataloader)\n>>> for epoch in range(20):\n>>>     for i, sample in enumerate(dataloader):\n>>>         inputs, labels = sample['inputs'], sample['labels']\n>>>         optimizer.zero_grad()\n>>>         outputs = net(inputs)\n>>>         loss = criterion(outputs, labels)\n>>>         loss.backward()\n>>>         optimizer.step()\n>>>         scheduler.step(epoch + i / iters)\n This function can be called in an interleaved way. Example >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n>>> for epoch in range(20):\n>>>     scheduler.step()\n>>> scheduler.step(26)\n>>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n \n"}, {"name": "torch.optim.lr_scheduler.CyclicLR", "path": "optim#torch.optim.lr_scheduler.CyclicLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False) [source]\n \nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis. Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This class has three built-in policies, as put forth in the paper:  \u201ctriangular\u201d: A basic triangular cycle without amplitude scaling. \u201ctriangular2\u201d: A basic triangular cycle that scales initial amplitude by half each cycle. \u201cexp_range\u201d: A cycle that scales initial amplitude by gammacycle iterations\\text{gamma}^{\\text{cycle iterations}}  at each cycle iteration.  This implementation was adapted from the github repo: bckenstler/CLR  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nbase_lr (float or list) \u2013 Initial learning rate which is the lower boundary in the cycle for each parameter group. \nmax_lr (float or list) \u2013 Upper learning rate boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. \nstep_size_up (int) \u2013 Number of training iterations in the increasing half of a cycle. Default: 2000 \nstep_size_down (int) \u2013 Number of training iterations in the decreasing half of a cycle. If step_size_down is None, it is set to step_size_up. Default: None \nmode (str) \u2013 One of {triangular, triangular2, exp_range}. Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. Default: \u2018triangular\u2019 \ngamma (float) \u2013 Constant in \u2018exp_range\u2019 scaling function: gamma**(cycle iterations) Default: 1.0 \nscale_fn (function) \u2013 Custom scaling policy defined by a single argument lambda function, where 0 <= scale_fn(x) <= 1 for all x >= 0. If specified, then \u2018mode\u2019 is ignored. Default: None \nscale_mode (str) \u2013 {\u2018cycle\u2019, \u2018iterations\u2019}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default: \u2018cycle\u2019 \ncycle_momentum (bool) \u2013 If True, momentum is cycled inversely to learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019. Default: True \nbase_momentum (float or list) \u2013 Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is \u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019. Default: 0.8 \nmax_momentum (float or list) \u2013 Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). The momentum at any cycle is the difference of max_momentum and some scaling of the amplitude; therefore base_momentum may not actually be reached depending on scaling function. Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019 and learning rate is \u2018base_lr\u2019 Default: 0.9 \nlast_epoch (int) \u2013 The index of the last batch. This parameter is used when resuming a training job. Since step() should be invoked after each batch instead of after each epoch, this number represents the total number of batches computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1 \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n>>> data_loader = torch.utils.data.DataLoader(...)\n>>> for epoch in range(10):\n>>>     for batch in data_loader:\n>>>         train_batch(...)\n>>>         scheduler.step()\n  \nget_lr() [source]\n \nCalculates the learning rate at batch index. This function treats self.last_epoch as the last batch index. If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum. \n \n"}, {"name": "torch.optim.lr_scheduler.CyclicLR.get_lr()", "path": "optim#torch.optim.lr_scheduler.CyclicLR.get_lr", "type": "torch.optim", "text": " \nget_lr() [source]\n \nCalculates the learning rate at batch index. This function treats self.last_epoch as the last batch index. If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum. \n"}, {"name": "torch.optim.lr_scheduler.ExponentialLR", "path": "optim#torch.optim.lr_scheduler.ExponentialLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False) [source]\n \nDecays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \ngamma (float) \u2013 Multiplicative factor of learning rate decay. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    \n"}, {"name": "torch.optim.lr_scheduler.LambdaLR", "path": "optim#torch.optim.lr_scheduler.LambdaLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False) [source]\n \nSets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nlr_lambda (function or list) \u2013 A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> # Assuming optimizer has two groups.\n>>> lambda1 = lambda epoch: epoch // 30\n>>> lambda2 = lambda epoch: 0.95 ** epoch\n>>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n  \nload_state_dict(state_dict) [source]\n \nLoads the schedulers state. When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.  Parameters \nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   \n  \nstate_dict() [source]\n \nReturns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. When saving or loading the scheduler, please make sure to also save or load the state of the optimizer. \n \n"}, {"name": "torch.optim.lr_scheduler.LambdaLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.load_state_dict", "type": "torch.optim", "text": " \nload_state_dict(state_dict) [source]\n \nLoads the schedulers state. When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.  Parameters \nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   \n"}, {"name": "torch.optim.lr_scheduler.LambdaLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.state_dict", "type": "torch.optim", "text": " \nstate_dict() [source]\n \nReturns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. When saving or loading the scheduler, please make sure to also save or load the state of the optimizer. \n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False) [source]\n \nMultiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nlr_lambda (function or list) \u2013 A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> lmbda = lambda epoch: 0.95\n>>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n  \nload_state_dict(state_dict) [source]\n \nLoads the schedulers state.  Parameters \nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   \n  \nstate_dict() [source]\n \nReturns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. \n \n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict", "type": "torch.optim", "text": " \nload_state_dict(state_dict) [source]\n \nLoads the schedulers state.  Parameters \nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   \n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.state_dict", "type": "torch.optim", "text": " \nstate_dict() [source]\n \nReturns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. \n"}, {"name": "torch.optim.lr_scheduler.MultiStepLR", "path": "optim#torch.optim.lr_scheduler.MultiStepLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False) [source]\n \nDecays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nmilestones (list) \u2013 List of epoch indices. Must be increasing. \ngamma (float) \u2013 Multiplicative factor of learning rate decay. Default: 0.1. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.005    if 30 <= epoch < 80\n>>> # lr = 0.0005   if epoch >= 80\n>>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n \n"}, {"name": "torch.optim.lr_scheduler.OneCycleLR", "path": "optim#torch.optim.lr_scheduler.OneCycleLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, three_phase=False, last_epoch=-1, verbose=False) [source]\n \nSets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):  A value for total_steps is explicitly provided. A number of epochs (epochs) and a number of steps per epoch (steps_per_epoch) are provided. In this case, the number of total steps is inferred by total_steps = epochs * steps_per_epoch  You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch. The default behaviour of this scheduler follows the fastai implementation of 1cycle, which claims that \u201cunpublished work has shown even better results by using only two phases\u201d. To mimic the behaviour of the original paper instead, set three_phase=True.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nmax_lr (float or list) \u2013 Upper learning rate boundaries in the cycle for each parameter group. \ntotal_steps (int) \u2013 The total number of steps in the cycle. Note that if a value is not provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None \nepochs (int) \u2013 The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None \nsteps_per_epoch (int) \u2013 The number of steps per epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None \npct_start (float) \u2013 The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3 \nanneal_strategy (str) \u2013 {\u2018cos\u2019, \u2018linear\u2019} Specifies the annealing strategy: \u201ccos\u201d for cosine annealing, \u201clinear\u201d for linear annealing. Default: \u2018cos\u2019 \ncycle_momentum (bool) \u2013 If True, momentum is cycled inversely to learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019. Default: True \nbase_momentum (float or list) \u2013 Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is \u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019. Default: 0.85 \nmax_momentum (float or list) \u2013 Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019 and learning rate is \u2018base_lr\u2019 Default: 0.95 \ndiv_factor (float) \u2013 Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25 \nfinal_div_factor (float) \u2013 Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Default: 1e4 \nthree_phase (bool) \u2013 If True, use a third phase of the schedule to annihilate the learning rate according to \u2018final_div_factor\u2019 instead of modifying the second phase (the first two phases will be symmetrical about the step indicated by \u2018pct_start\u2019). \nlast_epoch (int) \u2013 The index of the last batch. This parameter is used when resuming a training job. Since step() should be invoked after each batch instead of after each epoch, this number represents the total number of batches computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1 \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> data_loader = torch.utils.data.DataLoader(...)\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n>>> for epoch in range(10):\n>>>     for batch in data_loader:\n>>>         train_batch(...)\n>>>         scheduler.step()\n \n"}, {"name": "torch.optim.lr_scheduler.ReduceLROnPlateau", "path": "optim#torch.optim.lr_scheduler.ReduceLROnPlateau", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False) [source]\n \nReduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nmode (str) \u2013 One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: \u2018min\u2019. \nfactor (float) \u2013 Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1. \npatience (int) \u2013 Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn\u2019t improved then. Default: 10. \nthreshold (float) \u2013 Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4. \nthreshold_mode (str) \u2013 One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in \u2018max\u2019 mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: \u2018rel\u2019. \ncooldown (int) \u2013 Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0. \nmin_lr (float or list) \u2013 A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0. \neps (float) \u2013 Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n>>> for epoch in range(10):\n>>>     train(...)\n>>>     val_loss = validate(...)\n>>>     # Note that step should be called after validate()\n>>>     scheduler.step(val_loss)\n \n"}, {"name": "torch.optim.lr_scheduler.StepLR", "path": "optim#torch.optim.lr_scheduler.StepLR", "type": "torch.optim", "text": " \nclass torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False) [source]\n \nDecays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.  Parameters \n \noptimizer (Optimizer) \u2013 Wrapped optimizer. \nstep_size (int) \u2013 Period of learning rate decay. \ngamma (float) \u2013 Multiplicative factor of learning rate decay. Default: 0.1. \nlast_epoch (int) \u2013 The index of last epoch. Default: -1. \nverbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False.    Example >>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.005    if 30 <= epoch < 60\n>>> # lr = 0.0005   if 60 <= epoch < 90\n>>> # ...\n>>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n \n"}, {"name": "torch.optim.Optimizer", "path": "optim#torch.optim.Optimizer", "type": "torch.optim", "text": " \nclass torch.optim.Optimizer(params, defaults) [source]\n \nBase class for all optimizers.  Warning Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries.   Parameters \n \nparams (iterable) \u2013 an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized. \ndefaults \u2013 (dict): a dict containing default values of optimization options (used when a parameter group doesn\u2019t specify them).     \nadd_param_group(param_group) [source]\n \nAdd a param group to the Optimizer s param_groups. This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.  Parameters \n \nparam_group (dict) \u2013 Specifies what Tensors should be optimized along with group \noptimization options. (specific) \u2013     \n  \nload_state_dict(state_dict) [source]\n \nLoads the optimizer state.  Parameters \nstate_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().   \n  \nstate_dict() [source]\n \nReturns the state of the optimizer as a dict. It contains two entries:  \n state - a dict holding current optimization state. Its content\n\ndiffers between optimizer classes.    param_groups - a dict containing all parameter groups  \n  \nstep(closure) [source]\n \nPerforms a single optimization step (parameter update).  Parameters \nclosure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.    Note Unless otherwise specified, this function should not modify the .grad field of the parameters.  \n  \nzero_grad(set_to_none=False) [source]\n \nSets the gradients of all optimized torch.Tensor s to zero.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).   \n \n"}, {"name": "torch.optim.Optimizer.add_param_group()", "path": "optim#torch.optim.Optimizer.add_param_group", "type": "torch.optim", "text": " \nadd_param_group(param_group) [source]\n \nAdd a param group to the Optimizer s param_groups. This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.  Parameters \n \nparam_group (dict) \u2013 Specifies what Tensors should be optimized along with group \noptimization options. (specific) \u2013     \n"}, {"name": "torch.optim.Optimizer.load_state_dict()", "path": "optim#torch.optim.Optimizer.load_state_dict", "type": "torch.optim", "text": " \nload_state_dict(state_dict) [source]\n \nLoads the optimizer state.  Parameters \nstate_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().   \n"}, {"name": "torch.optim.Optimizer.state_dict()", "path": "optim#torch.optim.Optimizer.state_dict", "type": "torch.optim", "text": " \nstate_dict() [source]\n \nReturns the state of the optimizer as a dict. It contains two entries:  \n state - a dict holding current optimization state. Its content\n\ndiffers between optimizer classes.    param_groups - a dict containing all parameter groups  \n"}, {"name": "torch.optim.Optimizer.step()", "path": "optim#torch.optim.Optimizer.step", "type": "torch.optim", "text": " \nstep(closure) [source]\n \nPerforms a single optimization step (parameter update).  Parameters \nclosure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.    Note Unless otherwise specified, this function should not modify the .grad field of the parameters.  \n"}, {"name": "torch.optim.Optimizer.zero_grad()", "path": "optim#torch.optim.Optimizer.zero_grad", "type": "torch.optim", "text": " \nzero_grad(set_to_none=False) [source]\n \nSets the gradients of all optimized torch.Tensor s to zero.  Parameters \nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).   \n"}, {"name": "torch.optim.RMSprop", "path": "optim#torch.optim.RMSprop", "type": "torch.optim", "text": " \nclass torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) [source]\n \nImplements RMSprop algorithm. Proposed by G. Hinton in his course. The centered version first appears in Generating Sequences With Recurrent Neural Networks. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon)  where \u03b1\\alpha  is the scheduled learning rate and vv  is the weighted moving average of the squared gradient.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \nmomentum (float, optional) \u2013 momentum factor (default: 0) \nalpha (float, optional) \u2013 smoothing constant (default: 0.99) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) \ncentered (bool, optional) \u2013 if True, compute the centered RMSProp, the gradient is normalized by an estimation of its variance \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.RMSprop.step()", "path": "optim#torch.optim.RMSprop.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.Rprop", "path": "optim#torch.optim.Rprop", "type": "torch.optim", "text": " \nclass torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50)) [source]\n \nImplements the resilient backpropagation algorithm.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-2) \netas (Tuple[float, float], optional) \u2013 pair of (etaminus, etaplis), that are multiplicative increase and decrease factors (default: (0.5, 1.2)) \nstep_sizes (Tuple[float, float], optional) \u2013 a pair of minimal and maximal allowed step sizes (default: (1e-6, 50))     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.Rprop.step()", "path": "optim#torch.optim.Rprop.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.SGD", "path": "optim#torch.optim.SGD", "type": "torch.optim", "text": " \nclass torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False) [source]\n \nImplements stochastic gradient descent (optionally with momentum). Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float) \u2013 learning rate \nmomentum (float, optional) \u2013 momentum factor (default: 0) \nweight_decay (float, optional) \u2013 weight decay (L2 penalty) (default: 0) \ndampening (float, optional) \u2013 dampening for momentum (default: 0) \nnesterov (bool, optional) \u2013 enables Nesterov momentum (default: False)    Example >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> optimizer.zero_grad()\n>>> loss_fn(model(input), target).backward()\n>>> optimizer.step()\n  Note The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks. Considering the specific case of Momentum, the update can be written as  vt+1=\u03bc\u2217vt+gt+1,pt+1=pt\u2212lr\u2217vt+1,\\begin{aligned} v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\ p_{t+1} & = p_{t} - \\text{lr} * v_{t+1}, \\end{aligned}  \nwhere pp , gg , vv  and \u03bc\\mu  denote the parameters, gradient, velocity, and momentum respectively. This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form  vt+1=\u03bc\u2217vt+lr\u2217gt+1,pt+1=pt\u2212vt+1.\\begin{aligned} v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\ p_{t+1} & = p_{t} - v_{t+1}. \\end{aligned}  \nThe Nesterov version is analogously modified.   \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.SGD.step()", "path": "optim#torch.optim.SGD.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.optim.SparseAdam", "path": "optim#torch.optim.SparseAdam", "type": "torch.optim", "text": " \nclass torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08) [source]\n \nImplements lazy version of Adam algorithm suitable for sparse tensors. In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.  Parameters \n \nparams (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups \nlr (float, optional) \u2013 learning rate (default: 1e-3) \nbetas (Tuple[float, float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) \neps (float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8)     \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n \n"}, {"name": "torch.optim.SparseAdam.step()", "path": "optim#torch.optim.SparseAdam.step", "type": "torch.optim", "text": " \nstep(closure=None) [source]\n \nPerforms a single optimization step.  Parameters \nclosure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   \n"}, {"name": "torch.orgqr()", "path": "generated/torch.orgqr#torch.orgqr", "type": "torch", "text": " \ntorch.orgqr(input, input2) \u2192 Tensor  \nComputes the orthogonal matrix Q of a QR factorization, from the (input, input2) tuple returned by torch.geqrf(). This directly calls the underlying LAPACK function ?orgqr. See LAPACK documentation for orgqr for further details.  Parameters \n \ninput (Tensor) \u2013 the a from torch.geqrf(). \ninput2 (Tensor) \u2013 the tau from torch.geqrf().    \n"}, {"name": "torch.ormqr()", "path": "generated/torch.ormqr#torch.ormqr", "type": "torch", "text": " \ntorch.ormqr(input, input2, input3, left=True, transpose=False) \u2192 Tensor  \nMultiplies mat (given by input3) by the orthogonal Q matrix of the QR factorization formed by torch.geqrf() that is represented by (a, tau) (given by (input, input2)). This directly calls the underlying LAPACK function ?ormqr. See LAPACK documentation for ormqr for further details.  Parameters \n \ninput (Tensor) \u2013 the a from torch.geqrf(). \ninput2 (Tensor) \u2013 the tau from torch.geqrf(). \ninput3 (Tensor) \u2013 the matrix to be multiplied.    \n"}, {"name": "torch.outer()", "path": "generated/torch.outer#torch.outer", "type": "torch", "text": " \ntorch.outer(input, vec2, *, out=None) \u2192 Tensor  \nOuter product of input and vec2. If input is a vector of size nn  and vec2 is a vector of size mm , then out must be a matrix of size (n\u00d7m)(n \\times m) .  Note This function does not broadcast.   Parameters \n \ninput (Tensor) \u2013 1-D input vector \nvec2 (Tensor) \u2013 1-D input vector   Keyword Arguments \nout (Tensor, optional) \u2013 optional output matrix   Example: >>> v1 = torch.arange(1., 5.)\n>>> v2 = torch.arange(1., 4.)\n>>> torch.outer(v1, v2)\ntensor([[  1.,   2.,   3.],\n        [  2.,   4.,   6.],\n        [  3.,   6.,   9.],\n        [  4.,   8.,  12.]])\n \n"}, {"name": "torch.overrides", "path": "torch.overrides", "type": "torch.overrides", "text": "torch.overrides This module exposes various helper functions for the __torch_function__ protocol. See Extending torch for more detail on the __torch_function__ protocol. Functions  \ntorch.overrides.get_ignored_functions() [source]\n \nReturn public functions that cannot be overridden by __torch_function__.  Returns \nA tuple of functions that are publicly available in the torch API but cannot be overridden with __torch_function__. Mostly this is because none of the arguments of these functions are tensors or tensor-likes.  Return type \nSet[Callable]   Examples >>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse\n \n  \ntorch.overrides.get_overridable_functions() [source]\n \nList functions that are overridable via __torch_function__  Returns \nA dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.  Return type \nDict[Any, List[Callable]]   \n  \ntorch.overrides.get_testing_overrides() [source]\n \nReturn a dict containing dummy overrides for all overridable functions  Returns \nA dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines __torch_function__.  Return type \nDict[Callable, Callable]   Examples >>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>\n \n  \ntorch.overrides.handle_torch_function(public_api, relevant_args, *args, **kwargs) [source]\n \nImplement a function with checks for __torch_function__ overrides. See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.  Parameters \n \npublic_api (function) \u2013 Function exposed by the public torch API originally called like public_api(*args, **kwargs) on which arguments are now being checked. \nrelevant_args (iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. \nargs (tuple) \u2013 Arbitrary positional arguments originally passed into public_api. \nkwargs (tuple) \u2013 Arbitrary keyword arguments originally passed into public_api.   Returns \nResult from calling implementation or an __torch_function__ method, as appropriate.  Return type \nobject   :raises TypeError : if no implementation is found.: Example >>> def func(a):\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0\n \n  \ntorch.overrides.has_torch_function()  \nCheck for __torch_function__ implementations in the elements of an iterable. Considers exact Tensor s and Parameter s non-dispatchable. :param relevant_args: Iterable or aguments to check for __torch_function__ methods. :type relevant_args: iterable  Returns \nTrue if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.  Return type \nbool    See also  \ntorch.is_tensor_like() \n\nChecks if something is a Tensor-like, including an exact Tensor.    \n  \ntorch.overrides.is_tensor_like(inp) [source]\n \nReturns True if the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a __torch_function__ attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. >>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue\n Built-in or user types aren\u2019t usually Tensor-like. >>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse\n But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue\n \n  \ntorch.overrides.is_tensor_method_or_property(func) [source]\n \nReturns True if the function passed in is a handler for a method or property belonging to torch.Tensor, as passed into __torch_function__.  Note For properties, their __get__ method must be passed in.  This may be needed, in particular, for the following reasons:  Methods/properties sometimes don\u2019t contain a __module__ slot. They require that the first passed-in argument is an instance of torch.Tensor.  Examples >>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse\n \n  \ntorch.overrides.wrap_torch_function(dispatcher) [source]\n \nWraps a given function with __torch_function__ -related functionality.  Parameters \ndispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.    Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available.  Examples >>> def dispatcher(a): # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a): # This will make func dispatchable by __torch_function__\n...     return a + 0\n \n\n"}, {"name": "torch.overrides.get_ignored_functions()", "path": "torch.overrides#torch.overrides.get_ignored_functions", "type": "torch.overrides", "text": " \ntorch.overrides.get_ignored_functions() [source]\n \nReturn public functions that cannot be overridden by __torch_function__.  Returns \nA tuple of functions that are publicly available in the torch API but cannot be overridden with __torch_function__. Mostly this is because none of the arguments of these functions are tensors or tensor-likes.  Return type \nSet[Callable]   Examples >>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse\n \n"}, {"name": "torch.overrides.get_overridable_functions()", "path": "torch.overrides#torch.overrides.get_overridable_functions", "type": "torch.overrides", "text": " \ntorch.overrides.get_overridable_functions() [source]\n \nList functions that are overridable via __torch_function__  Returns \nA dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.  Return type \nDict[Any, List[Callable]]   \n"}, {"name": "torch.overrides.get_testing_overrides()", "path": "torch.overrides#torch.overrides.get_testing_overrides", "type": "torch.overrides", "text": " \ntorch.overrides.get_testing_overrides() [source]\n \nReturn a dict containing dummy overrides for all overridable functions  Returns \nA dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines __torch_function__.  Return type \nDict[Callable, Callable]   Examples >>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>\n \n"}, {"name": "torch.overrides.handle_torch_function()", "path": "torch.overrides#torch.overrides.handle_torch_function", "type": "torch.overrides", "text": " \ntorch.overrides.handle_torch_function(public_api, relevant_args, *args, **kwargs) [source]\n \nImplement a function with checks for __torch_function__ overrides. See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.  Parameters \n \npublic_api (function) \u2013 Function exposed by the public torch API originally called like public_api(*args, **kwargs) on which arguments are now being checked. \nrelevant_args (iterable) \u2013 Iterable of arguments to check for __torch_function__ methods. \nargs (tuple) \u2013 Arbitrary positional arguments originally passed into public_api. \nkwargs (tuple) \u2013 Arbitrary keyword arguments originally passed into public_api.   Returns \nResult from calling implementation or an __torch_function__ method, as appropriate.  Return type \nobject   :raises TypeError : if no implementation is found.: Example >>> def func(a):\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0\n \n"}, {"name": "torch.overrides.has_torch_function()", "path": "torch.overrides#torch.overrides.has_torch_function", "type": "torch.overrides", "text": " \ntorch.overrides.has_torch_function()  \nCheck for __torch_function__ implementations in the elements of an iterable. Considers exact Tensor s and Parameter s non-dispatchable. :param relevant_args: Iterable or aguments to check for __torch_function__ methods. :type relevant_args: iterable  Returns \nTrue if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.  Return type \nbool    See also  \ntorch.is_tensor_like() \n\nChecks if something is a Tensor-like, including an exact Tensor.    \n"}, {"name": "torch.overrides.is_tensor_like()", "path": "torch.overrides#torch.overrides.is_tensor_like", "type": "torch.overrides", "text": " \ntorch.overrides.is_tensor_like(inp) [source]\n \nReturns True if the passed-in input is a Tensor-like. Currently, this occurs whenever there\u2019s a __torch_function__ attribute on the type of the input. Examples A subclass of tensor is generally a Tensor-like. >>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue\n Built-in or user types aren\u2019t usually Tensor-like. >>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse\n But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue\n \n"}, {"name": "torch.overrides.is_tensor_method_or_property()", "path": "torch.overrides#torch.overrides.is_tensor_method_or_property", "type": "torch.overrides", "text": " \ntorch.overrides.is_tensor_method_or_property(func) [source]\n \nReturns True if the function passed in is a handler for a method or property belonging to torch.Tensor, as passed into __torch_function__.  Note For properties, their __get__ method must be passed in.  This may be needed, in particular, for the following reasons:  Methods/properties sometimes don\u2019t contain a __module__ slot. They require that the first passed-in argument is an instance of torch.Tensor.  Examples >>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse\n \n"}, {"name": "torch.overrides.wrap_torch_function()", "path": "torch.overrides#torch.overrides.wrap_torch_function", "type": "torch.overrides", "text": " \ntorch.overrides.wrap_torch_function(dispatcher) [source]\n \nWraps a given function with __torch_function__ -related functionality.  Parameters \ndispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.    Note This decorator may reduce the performance of your code. Generally, it\u2019s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available.  Examples >>> def dispatcher(a): # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a): # This will make func dispatchable by __torch_function__\n...     return a + 0\n \n"}, {"name": "torch.pca_lowrank()", "path": "generated/torch.pca_lowrank#torch.pca_lowrank", "type": "torch", "text": " \ntorch.pca_lowrank(A, q=None, center=True, niter=2) [source]\n \nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix. This function returns a namedtuple (U, S, V) which is the nearly optimal approximation of a singular value decomposition of a centered matrix AA  such that A=Udiag(S)VTA = U diag(S) V^T .  Note The relation of (U, S, V) to PCA is as follows:  \nAA  is a data matrix with m samples and n features the VV  columns represent the principal directions \nS\u2217\u22172/(m\u22121)S ** 2 / (m - 1)  contains the eigenvalues of ATA/(m\u22121)A^T A / (m - 1)  which is the covariance of A when center=True is provided. \nmatmul(A, V[:, :k]) projects data to the first k principal components    Note Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:  \nUU  is m x q matrix \nSS  is q-vector \nVV  is n x q matrix    Note To obtain repeatable results, reset the seed for the pseudorandom number generator   Parameters \n \nA (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n) \n \nq (int, optional) \u2013 a slightly overestimated rank of AA . By default, q = min(6, m,\nn). \ncenter (bool, optional) \u2013 if True, center the input tensor, otherwise, assume that the input is centered. \nniter (int, optional) \u2013 the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.    References: - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n  structure with randomness: probabilistic algorithms for\n  constructing approximate matrix decompositions,\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n  `arXiv <http://arxiv.org/abs/0909.4061>`_).\n \n"}, {"name": "torch.pinverse()", "path": "generated/torch.pinverse#torch.pinverse", "type": "torch", "text": " \ntorch.pinverse(input, rcond=1e-15) \u2192 Tensor  \nCalculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at Moore-Penrose inverse for more details  Note torch.pinverse() is deprecated. Please use torch.linalg.pinv() instead which includes new parameters hermitian and out.   Note This method is implemented using the Singular Value Decomposition.   Note The pseudo-inverse is not necessarily a continuous function in the elements of the matrix [1]. Therefore, derivatives are not always existent, and exist for a constant rank only [2]. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See svd() for more details.   Note Supports real and complex inputs. Batched version for complex inputs is only supported on the CPU.   Parameters \n \ninput (Tensor) \u2013 The input tensor of size (\u2217,m,n)(*, m, n)  where \u2217*  is zero or more batch dimensions. \nrcond (float, optional) \u2013 A floating point value to determine the cutoff for small singular values. Default: 1e-15.   Returns \nThe pseudo-inverse of input of dimensions (\u2217,n,m)(*, n, m)    Example: >>> input = torch.randn(3, 5)\n>>> input\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.pinverse(input)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n>>> # Batched pinverse example\n>>> a = torch.randn(2,6,3)\n>>> b = torch.pinverse(a)\n>>> torch.matmul(b, a)\ntensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],\n        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],\n        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],\n\n        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],\n        [-2.2352e-07,  1.0000e+00,  1.1921e-07],\n        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])\n \n"}, {"name": "torch.poisson()", "path": "generated/torch.poisson#torch.poisson", "type": "torch", "text": " \ntorch.poisson(input, generator=None) \u2192 Tensor  \nReturns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,  outi\u223cPoisson(inputi)\\text{out}_i \\sim \\text{Poisson}(\\text{input}_i)  \n Parameters \ninput (Tensor) \u2013 the input tensor containing the rates of the Poisson distribution  Keyword Arguments \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling   Example: >>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5\n>>> torch.poisson(rates)\ntensor([[9., 1., 3., 5.],\n        [8., 6., 6., 0.],\n        [0., 4., 5., 3.],\n        [2., 1., 4., 2.]])\n \n"}, {"name": "torch.polar()", "path": "generated/torch.polar#torch.polar", "type": "torch", "text": " \ntorch.polar(abs, angle, *, out=None) \u2192 Tensor  \nConstructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.  out=abs\u22c5cos\u2061(angle)+abs\u22c5sin\u2061(angle)\u22c5j\\text{out} = \\text{abs} \\cdot \\cos(\\text{angle}) + \\text{abs} \\cdot \\sin(\\text{angle}) \\cdot j  \n Parameters \n \nabs (Tensor) \u2013 The absolute value the complex tensor. Must be float or double. \nangle (Tensor) \u2013 The angle of the complex tensor. Must be same dtype as abs.   Keyword Arguments \nout (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.    Example::\n\n>>> import numpy as np\n>>> abs = torch.tensor([1, 2], dtype=torch.float64)\n>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n>>> z = torch.polar(abs, angle)\n>>> z\ntensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)\n   \n"}, {"name": "torch.polygamma()", "path": "generated/torch.polygamma#torch.polygamma", "type": "torch", "text": " \ntorch.polygamma(n, input, *, out=None) \u2192 Tensor  \nComputes the nthn^{th}  derivative of the digamma function on input. n\u22650n \\geq 0  is called the order of the polygamma function.  \u03c8(n)(x)=d(n)dx(n)\u03c8(x)\\psi^{(n)}(x) = \\frac{d^{(n)}}{dx^{(n)}} \\psi(x)  \n Note This function is implemented only for nonnegative integers n\u22650n \\geq 0 .   Parameters \n \nn (int) \u2013 the order of the polygamma function \ninput (Tensor) \u2013 the input tensor.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.tensor([1, 0.5])\n>>> torch.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.polygamma(4, a)\ntensor([ -24.8863, -771.4742])\n   \n"}, {"name": "torch.pow()", "path": "generated/torch.pow#torch.pow", "type": "torch", "text": " \ntorch.pow(input, exponent, *, out=None) \u2192 Tensor  \nTakes the power of each element in input with exponent and returns a tensor with the result. exponent can be either a single float number or a Tensor with the same number of elements as input. When exponent is a scalar value, the operation applied is:  outi=xiexponent\\text{out}_i = x_i ^ \\text{exponent}  \nWhen exponent is a tensor, the operation applied is:  outi=xiexponenti\\text{out}_i = x_i ^ {\\text{exponent}_i}  \nWhen exponent is a tensor, the shapes of input and exponent must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nexponent (float or tensor) \u2013 the exponent value   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n>>> torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n>>> exp = torch.arange(1., 5.)\n\n>>> a = torch.arange(1., 5.)\n>>> a\ntensor([ 1.,  2.,  3.,  4.])\n>>> exp\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])\n  \ntorch.pow(self, exponent, *, out=None) \u2192 Tensor \n self is a scalar float value, and exponent is a tensor. The returned tensor out is of the same shape as exponent The operation applied is:  outi=selfexponenti\\text{out}_i = \\text{self} ^ {\\text{exponent}_i}  \n Parameters \n \nself (float) \u2013 the scalar base value for the power operation \nexponent (Tensor) \u2013 the exponent tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> exp = torch.arange(1., 5.)\n>>> base = 2\n>>> torch.pow(base, exp)\ntensor([  2.,   4.,   8.,  16.])\n \n"}, {"name": "torch.prod()", "path": "generated/torch.prod#torch.prod", "type": "torch", "text": " \ntorch.prod(input, *, dtype=None) \u2192 Tensor  \nReturns the product of all elements in the input tensor.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.8020,  0.5428, -1.5854]])\n>>> torch.prod(a)\ntensor(0.6902)\n  \ntorch.prod(input, dim, keepdim=False, *, dtype=None) \u2192 Tensor \n Returns the product of each row of the input tensor in the given dimension dim. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.   Example: >>> a = torch.randn(4, 2)\n>>> a\ntensor([[ 0.5261, -0.3837],\n        [ 1.1857, -0.2498],\n        [-1.1646,  0.0705],\n        [ 1.1131, -1.0629]])\n>>> torch.prod(a, 1)\ntensor([-0.2018, -0.2962, -0.0821, -1.1831])\n \n"}, {"name": "torch.promote_types()", "path": "generated/torch.promote_types#torch.promote_types", "type": "torch", "text": " \ntorch.promote_types(type1, type2) \u2192 dtype  \nReturns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2. See type promotion documentation for more information on the type promotion logic.  Parameters \n \ntype1 (torch.dtype) \u2013  \ntype2 (torch.dtype) \u2013     Example: >>> torch.promote_types(torch.int32, torch.float32)\ntorch.float32\n>>> torch.promote_types(torch.uint8, torch.long)\ntorch.long\n \n"}, {"name": "torch.qr()", "path": "generated/torch.qr#torch.qr", "type": "torch", "text": " \ntorch.qr(input, some=True, *, out=None) -> (Tensor, Tensor)  \nComputes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices. If some is True, then this function returns the thin (reduced) QR factorization. Otherwise, if some is False, this function returns the complete QR factorization.  Warning torch.qr is deprecated. Please use torch.linalg.qr() instead. Differences with torch.linalg.qr:  \ntorch.linalg.qr takes a string parameter mode instead of some:  \nsome=True is equivalent of mode='reduced': both are the default \nsome=False is equivalent of mode='complete'.      Warning If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))  columns of input are linearly independent. This behavior will propably change once QR supports pivoting.   Note This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m\u00d7nm \\times n . \nsome (bool, optional) \u2013 \nSet to True for reduced QR decomposition and False for complete QR decomposition. If k = min(m, n) then:  \nsome=True : returns (Q, R) with dimensions (m, k), (k, n) (default) \n'some=False': returns (Q, R) with dimensions (m, m), (m, n)     Keyword Arguments \nout (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of some above.   Example: >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.qr(a, some=False)\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue\n \n"}, {"name": "torch.quantile()", "path": "generated/torch.quantile#torch.quantile", "type": "torch", "text": " \ntorch.quantile(input, q) \u2192 Tensor  \nReturns the q-th quantiles of all elements in the input tensor, doing a linear interpolation when the q-th quantile lies between two data points.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nq (float or Tensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1]    Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.0700, -0.5446,  0.9214]])\n>>> q = torch.tensor([0, 0.5, 1])\n>>> torch.quantile(a, q)\ntensor([-0.5446,  0.0700,  0.9214])\n  \ntorch.quantile(input, q, dim=None, keepdim=False, *, out=None) \u2192 Tensor \n Returns the q-th quantiles of each row of the input tensor along the dimension dim, doing a linear interpolation when the q-th quantile lies between two data points. By default, dim is None resulting in the input tensor being flattened before computation. If keepdim is True, the output dimensions are of the same size as input except in the dimensions being reduced (dim or all if dim is None) where they have size 1. Otherwise, the dimensions being reduced are squeezed (see torch.squeeze()). If q is a 1D tensor, an extra dimension is prepended to the output tensor with the same size as q which represents the quantiles.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nq (float or Tensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(2, 3)\n>>> a\ntensor([[ 0.0795, -1.2117,  0.9765],\n        [ 1.1707,  0.6706,  0.4884]])\n>>> q = torch.tensor([0.25, 0.5, 0.75])\n>>> torch.quantile(a, q, dim=1, keepdim=True)\ntensor([[[-0.5661],\n        [ 0.5795]],\n\n        [[ 0.0795],\n        [ 0.6706]],\n\n        [[ 0.5280],\n        [ 0.9206]]])\n>>> torch.quantile(a, q, dim=1, keepdim=True).shape\ntorch.Size([3, 2, 1])\n \n"}, {"name": "torch.quantization", "path": "torch.quantization", "type": "torch.quantization", "text": "torch.quantization This module implements the functions you call directly to convert your model from FP32 to quantized form. For example the prepare() is used in post training quantization to prepares your model for the calibration step and convert() actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu. Top-level quantization APIs  \ntorch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False) [source]\n \nQuantize the input float model with post training static quantization. First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.  Parameters \n \nmodel \u2013 input float model \nrun_fn \u2013 a calibration function for calibrating the prepared model \nrun_args \u2013 positional arguments for run_fn\n \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nmapping \u2013 correspondence between original module types and quantized counterparts   Returns \nQuantized model.   \n  \ntorch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False) [source]\n \nConverts a float model to dynamic (i.e. weights-only) quantized model. Replaces specified modules with dynamic weight-only quantized versions and output the quantized model. For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants. Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.  Parameters \n \nmodel \u2013 input model \nqconfig_spec \u2013 \nEither:  A dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute). Entries in the dictionary need to be QConfigDynamic instances. A set of types and/or submodule names to apply dynamic quantization to, in which case the dtype argument is used to specify the bit-width   \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nmapping \u2013 maps type of a submodule to a type of corresponding dynamically quantized version with which the submodule needs to be replaced    \n  \ntorch.quantization.quantize_qat(model, run_fn, run_args, inplace=False) [source]\n \nDo quantization aware training and output a quantized model  Parameters \n \nmodel \u2013 input model \nrun_fn \u2013 a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop \nrun_args \u2013 positional arguments for run_fn\n   Returns \nQuantized model.   \n  \ntorch.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None) [source]\n \nPrepares a copy of the model for quantization calibration or quantization-aware training. Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute. The model will be attached with observer or fake quant modules, and qconfig will be propagated.  Parameters \n \nmodel \u2013 input model to be modified in-place \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nallow_list \u2013 list of quantizable modules \nobserver_non_leaf_module_list \u2013 list of non-leaf modules we want to add observer \nprepare_custom_config_dict \u2013 customization configuration dictionary for prepare function    # Example of prepare_custom_config_dict:\nprepare_custom_config_dict = {\n    # user will manually define the corresponding observed\n    # module class which has a from_float class method that converts\n    # float custom module to observed custom module\n    \"float_to_observed_custom_module_class\": {\n        CustomModule: ObservedCustomModule\n    }\n }\n \n  \ntorch.quantization.prepare_qat(model, mapping=None, inplace=False) [source]\n \nPrepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version. Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.  Parameters \n \nmodel \u2013 input model to be modified in-place \nmapping \u2013 dictionary that maps float modules to quantized modules to be replaced. \ninplace \u2013 carry out model transformations in-place, the original module is mutated    \n  \ntorch.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True, convert_custom_config_dict=None) [source]\n \nConverts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.  Parameters \n \nmodule \u2013 prepared and calibrated module \nmapping \u2013 a dictionary that maps from source module type to target module type, can be overwritten to allow swapping user defined Modules \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nconvert_custom_config_dict \u2013 custom configuration dictionary for convert function    # Example of convert_custom_config_dict:\nconvert_custom_config_dict = {\n    # user will manually define the corresponding quantized\n    # module class which has a from_observed class method that converts\n    # observed custom module to quantized custom module\n    \"observed_to_quantized_custom_module_class\": {\n        ObservedCustomModule: QuantizedCustomModule\n    }\n}\n \n  \nclass torch.quantization.QConfig [source]\n \nDescribes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively. Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial): my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8), weight=default_observer.with_args(dtype=torch.qint8)) \n  \nclass torch.quantization.QConfigDynamic [source]\n \nDescribes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights. It\u2019s like QConfig, but for dynamic quantization. Note that QConfigDynamic needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial): my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8)) \n Preparing model for quantization  \ntorch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None) [source]\n \nFuses a list of modules into a single module Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.  Parameters \n \nmodel \u2013 Model containing the modules to be fused \nmodules_to_fuse \u2013 list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse. \ninplace \u2013 bool specifying if fusion happens in place on the model, by default a new model is returned \nfuser_func \u2013 Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules \nfuse_custom_config_dict \u2013 custom configuration for fusion    # Example of fuse_custom_config_dict\nfuse_custom_config_dict = {\n    # Additional fuser_method mapping\n    \"additional_fuser_method_mapping\": {\n        (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n    },\n}\n  Returns \nmodel with fused modules. A new copy is created if inplace=True.   Examples: >>> m = myModel()\n>>> # m is a module containing  the sub-modules below\n>>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]\n>>> fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)\n>>> output = fused_m(input)\n\n>>> m = myModel()\n>>> # Alternately provide a single list of modules to fuse\n>>> modules_to_fuse = ['conv1', 'bn1', 'relu1']\n>>> fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)\n>>> output = fused_m(input)\n \n  \nclass torch.quantization.QuantStub(qconfig=None) [source]\n \nQuantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.  Parameters \nqconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules   \n  \nclass torch.quantization.DeQuantStub [source]\n \nDequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert. \n  \nclass torch.quantization.QuantWrapper(module) [source]\n \nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules. This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub. \n  \ntorch.quantization.add_quant_dequant(module) [source]\n \nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.  Parameters \n \nmodule \u2013 input module with qconfig attributes for all the leaf modules \nwe want to quantize (that) \u2013    Returns \nEither the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.   \n Utility functions  \ntorch.quantization.add_observer_(module, qconfig_propagation_list=None, non_leaf_module_list=None, device=None, custom_module_class_mapping=None) [source]\n \nAdd observer for the leaf child of the module. This function insert observer module to all leaf child module that has a valid qconfig attribute.  Parameters \n \nmodule \u2013 input module with qconfig attributes for all the leaf modules that we want to quantize \ndevice \u2013 parent device, if any \nnon_leaf_module_list \u2013 list of non-leaf modules we want to add observer   Returns \nNone, module is modified inplace with added observer modules and forward_hooks   \n  \ntorch.quantization.swap_module(mod, mapping, custom_module_class_mapping) [source]\n \nSwaps the module if it has a quantized counterpart and it has an observer attached.  Parameters \n \nmod \u2013 input module \nmapping \u2013 a dictionary that maps from nn module to nnq module   Returns \nThe corresponding quantized module of mod   \n  \ntorch.quantization.propagate_qconfig_(module, qconfig_dict=None, allow_list=None) [source]\n \nPropagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module  Parameters \n \nmodule \u2013 input module \nqconfig_dict \u2013 dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)   Returns \nNone, module is modified inplace with qconfig attached   \n  \ntorch.quantization.default_eval_fn(model, calib_data) [source]\n \nDefault evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset \n Observers  \nclass torch.quantization.ObserverBase(dtype) [source]\n \nBase observer Module. Any observer implementation should derive from this class. Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.  Parameters \ndtype \u2013 Quantized data type    \nclassmethod with_args(**kwargs)  \nWrapper that allows creation of class factories. This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Example: >>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n>>> foo_instance1 = foo_builder()\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1) == id(foo_instance2)\nFalse\n \n \n  \nclass torch.quantization.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the running min and max values. This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    Given running min/max as xminx_\\text{min}  and xmaxx_\\text{max} , scale ss  and zero point zz  are computed as: The running minimum/maximum xmin/maxx_\\text{min/max}  is computed as:  xmin={min\u2061(X)if xmin=Nonemin\u2061(xmin,min\u2061(X))otherwisexmax={max\u2061(X)if xmax=Nonemax\u2061(xmax,max\u2061(X))otherwise\\begin{array}{ll} x_\\text{min} &= \\begin{cases} \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\ \\min\\left(x_\\text{min}, \\min(X)\\right) & \\text{otherwise} \\end{cases}\\\\ x_\\text{max} &= \\begin{cases} \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\ \\max\\left(x_\\text{max}, \\max(X)\\right) & \\text{otherwise} \\end{cases}\\\\ \\end{array} \nwhere XX  is the observed tensor. The scale ss  and zero point zz  are then computed as:  if Symmetric:s=2max\u2061(\u2223xmin\u2223,xmax)/(Qmax\u2212Qmin)z={0if dtype is qint8128otherwiseOtherwise:s=(xmax\u2212xmin)/(Qmax\u2212Qmin)z=Qmin\u2212round(xmin/s)\\begin{aligned} \\text{if Symmetric:}&\\\\ &s = 2 \\max(|x_\\text{min}|, x_\\text{max}) / \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\ &z = \\begin{cases} 0 & \\text{if dtype is qint8} \\\\ 128 & \\text{otherwise} \\end{cases}\\\\ \\text{Otherwise:}&\\\\ &s = \\left( x_\\text{max} - x_\\text{min} \\right ) / \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\ &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s) \\end{aligned} \nwhere QminQ_\\text{min}  and QmaxQ_\\text{max}  are the minimum and maximum of the quantized data type.  Warning Only works with torch.per_tensor_symmetric quantization scheme   Warning dtype can only take torch.qint8 or torch.quint8.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  \n  \nclass torch.quantization.MovingAverageMinMaxObserver(averaging_constant=0.01, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the moving average of the min and max values. This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \naveraging_constant \u2013 Averaging constant for min/max. \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    The moving average min/max is computed as follows  xmin={min\u2061(X)if xmin=None(1\u2212c)xmin+cmin\u2061(X)otherwisexmax={max\u2061(X)if xmax=None(1\u2212c)xmax+cmax\u2061(X)otherwise\\begin{array}{ll} x_\\text{min} = \\begin{cases} \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\ (1 - c) x_\\text{min} + c \\min(X) & \\text{otherwise} \\end{cases}\\\\ x_\\text{max} = \\begin{cases} \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\ (1 - c) x_\\text{max} + c \\max(X) & \\text{otherwise} \\end{cases}\\\\ \\end{array} \nwhere xmin/maxx_\\text{min/max}  is the running average min/max, XX  is is the incoming tensor, and cc  is the averaging_constant. The scale and zero point are then computed as in MinMaxObserver.  Note Only works with torch.per_tensor_affine quantization scheme.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  \n  \nclass torch.quantization.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \nch_axis \u2013 Channel axis \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  \n  \nclass torch.quantization.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01, ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \naveraging_constant \u2013 Averaging constant for min/max. \nch_axis \u2013 Channel axis \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  \n  \nclass torch.quantization.HistogramObserver(bins=2048, upsample_rate=128, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False) [source]\n \nThe module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.  Parameters \n \nbins \u2013 Number of bins to use for the histogram \nupsample_rate \u2013 Factor by which the histograms are upsampled, this is used to interpolate histograms with varying ranges across observations \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit    The scale and zero point are computed as follows:  \n Create the histogram of the incoming inputs.\n\nThe histogram is computed continuously, and the ranges per bin change with every new tensor observed.    \n Search the distribution in the histogram for optimal min/max values.\n\nThe search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.    \n Compute the scale and zero point the same way as in the\n\nMinMaxObserver     \n  \nclass torch.quantization.FakeQuantize(observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, **observer_kwargs) [source]\n \nSimulate the quantize and dequantize operations in training time. The output of this module is given by x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale  \nscale defines the scale factor used for quantization. \nzero_point specifies the quantized value to which 0 in floating point maps to \nquant_min specifies the minimum allowable quantized value. \nquant_max specifies the maximum allowable quantized value. \nfake_quant_enable controls the application of fake quantization on tensors, note that statistics can still be updated. \nobserver_enable controls statistics collection on tensors \n \ndtype specifies the quantized dtype that is being emulated with fake-quantization, \n\nallowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype      Parameters \n \nobserver (module) \u2013 Module for observing statistics on input tensors and calculating scale and zero-point. \nquant_min (int) \u2013 The minimum allowable quantized value. \nquant_max (int) \u2013 The maximum allowable quantized value. \nobserver_kwargs (optional) \u2013 Arguments for the observer module   Variables \n~FakeQuantize.observer (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.   \n  \nclass torch.quantization.NoopObserver(dtype=torch.float16, custom_op_name='') [source]\n \nObserver that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float(). Primarily used for quantization to float16 which doesn\u2019t require determining ranges.  Parameters \n \ndtype \u2013 Quantized data type \ncustom_op_name \u2013 (temporary) specify this observer for an operator that doesn\u2019t require any observation (Can be used in Graph Mode Passes for special case ops).    \n Debugging utilities  \ntorch.quantization.get_observer_dict(mod, target_dict, prefix='') [source]\n \nTraverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers \n  \nclass torch.quantization.RecordingObserver(**kwargs) [source]\n \nThe module is mainly for debug and records the tensor values during runtime.  Parameters \n \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit    \n  \nnn.intrinsic   \n"}, {"name": "torch.quantization.add_observer_()", "path": "torch.quantization#torch.quantization.add_observer_", "type": "torch.quantization", "text": " \ntorch.quantization.add_observer_(module, qconfig_propagation_list=None, non_leaf_module_list=None, device=None, custom_module_class_mapping=None) [source]\n \nAdd observer for the leaf child of the module. This function insert observer module to all leaf child module that has a valid qconfig attribute.  Parameters \n \nmodule \u2013 input module with qconfig attributes for all the leaf modules that we want to quantize \ndevice \u2013 parent device, if any \nnon_leaf_module_list \u2013 list of non-leaf modules we want to add observer   Returns \nNone, module is modified inplace with added observer modules and forward_hooks   \n"}, {"name": "torch.quantization.add_quant_dequant()", "path": "torch.quantization#torch.quantization.add_quant_dequant", "type": "torch.quantization", "text": " \ntorch.quantization.add_quant_dequant(module) [source]\n \nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.  Parameters \n \nmodule \u2013 input module with qconfig attributes for all the leaf modules \nwe want to quantize (that) \u2013    Returns \nEither the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.   \n"}, {"name": "torch.quantization.convert()", "path": "torch.quantization#torch.quantization.convert", "type": "torch.quantization", "text": " \ntorch.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True, convert_custom_config_dict=None) [source]\n \nConverts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.  Parameters \n \nmodule \u2013 prepared and calibrated module \nmapping \u2013 a dictionary that maps from source module type to target module type, can be overwritten to allow swapping user defined Modules \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nconvert_custom_config_dict \u2013 custom configuration dictionary for convert function    # Example of convert_custom_config_dict:\nconvert_custom_config_dict = {\n    # user will manually define the corresponding quantized\n    # module class which has a from_observed class method that converts\n    # observed custom module to quantized custom module\n    \"observed_to_quantized_custom_module_class\": {\n        ObservedCustomModule: QuantizedCustomModule\n    }\n}\n \n"}, {"name": "torch.quantization.default_eval_fn()", "path": "torch.quantization#torch.quantization.default_eval_fn", "type": "torch.quantization", "text": " \ntorch.quantization.default_eval_fn(model, calib_data) [source]\n \nDefault evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset \n"}, {"name": "torch.quantization.DeQuantStub", "path": "torch.quantization#torch.quantization.DeQuantStub", "type": "torch.quantization", "text": " \nclass torch.quantization.DeQuantStub [source]\n \nDequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert. \n"}, {"name": "torch.quantization.FakeQuantize", "path": "torch.quantization#torch.quantization.FakeQuantize", "type": "torch.quantization", "text": " \nclass torch.quantization.FakeQuantize(observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, **observer_kwargs) [source]\n \nSimulate the quantize and dequantize operations in training time. The output of this module is given by x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale  \nscale defines the scale factor used for quantization. \nzero_point specifies the quantized value to which 0 in floating point maps to \nquant_min specifies the minimum allowable quantized value. \nquant_max specifies the maximum allowable quantized value. \nfake_quant_enable controls the application of fake quantization on tensors, note that statistics can still be updated. \nobserver_enable controls statistics collection on tensors \n \ndtype specifies the quantized dtype that is being emulated with fake-quantization, \n\nallowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype      Parameters \n \nobserver (module) \u2013 Module for observing statistics on input tensors and calculating scale and zero-point. \nquant_min (int) \u2013 The minimum allowable quantized value. \nquant_max (int) \u2013 The maximum allowable quantized value. \nobserver_kwargs (optional) \u2013 Arguments for the observer module   Variables \n~FakeQuantize.observer (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.   \n"}, {"name": "torch.quantization.fuse_modules()", "path": "torch.quantization#torch.quantization.fuse_modules", "type": "torch.quantization", "text": " \ntorch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None) [source]\n \nFuses a list of modules into a single module Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.  Parameters \n \nmodel \u2013 Model containing the modules to be fused \nmodules_to_fuse \u2013 list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse. \ninplace \u2013 bool specifying if fusion happens in place on the model, by default a new model is returned \nfuser_func \u2013 Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules \nfuse_custom_config_dict \u2013 custom configuration for fusion    # Example of fuse_custom_config_dict\nfuse_custom_config_dict = {\n    # Additional fuser_method mapping\n    \"additional_fuser_method_mapping\": {\n        (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n    },\n}\n  Returns \nmodel with fused modules. A new copy is created if inplace=True.   Examples: >>> m = myModel()\n>>> # m is a module containing  the sub-modules below\n>>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]\n>>> fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)\n>>> output = fused_m(input)\n\n>>> m = myModel()\n>>> # Alternately provide a single list of modules to fuse\n>>> modules_to_fuse = ['conv1', 'bn1', 'relu1']\n>>> fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)\n>>> output = fused_m(input)\n \n"}, {"name": "torch.quantization.get_observer_dict()", "path": "torch.quantization#torch.quantization.get_observer_dict", "type": "torch.quantization", "text": " \ntorch.quantization.get_observer_dict(mod, target_dict, prefix='') [source]\n \nTraverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers \n"}, {"name": "torch.quantization.HistogramObserver", "path": "torch.quantization#torch.quantization.HistogramObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.HistogramObserver(bins=2048, upsample_rate=128, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False) [source]\n \nThe module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.  Parameters \n \nbins \u2013 Number of bins to use for the histogram \nupsample_rate \u2013 Factor by which the histograms are upsampled, this is used to interpolate histograms with varying ranges across observations \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit    The scale and zero point are computed as follows:  \n Create the histogram of the incoming inputs.\n\nThe histogram is computed continuously, and the ranges per bin change with every new tensor observed.    \n Search the distribution in the histogram for optimal min/max values.\n\nThe search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.    \n Compute the scale and zero point the same way as in the\n\nMinMaxObserver     \n"}, {"name": "torch.quantization.MinMaxObserver", "path": "torch.quantization#torch.quantization.MinMaxObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the running min and max values. This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    Given running min/max as xminx_\\text{min}  and xmaxx_\\text{max} , scale ss  and zero point zz  are computed as: The running minimum/maximum xmin/maxx_\\text{min/max}  is computed as:  xmin={min\u2061(X)if xmin=Nonemin\u2061(xmin,min\u2061(X))otherwisexmax={max\u2061(X)if xmax=Nonemax\u2061(xmax,max\u2061(X))otherwise\\begin{array}{ll} x_\\text{min} &= \\begin{cases} \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\ \\min\\left(x_\\text{min}, \\min(X)\\right) & \\text{otherwise} \\end{cases}\\\\ x_\\text{max} &= \\begin{cases} \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\ \\max\\left(x_\\text{max}, \\max(X)\\right) & \\text{otherwise} \\end{cases}\\\\ \\end{array} \nwhere XX  is the observed tensor. The scale ss  and zero point zz  are then computed as:  if Symmetric:s=2max\u2061(\u2223xmin\u2223,xmax)/(Qmax\u2212Qmin)z={0if dtype is qint8128otherwiseOtherwise:s=(xmax\u2212xmin)/(Qmax\u2212Qmin)z=Qmin\u2212round(xmin/s)\\begin{aligned} \\text{if Symmetric:}&\\\\ &s = 2 \\max(|x_\\text{min}|, x_\\text{max}) / \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\ &z = \\begin{cases} 0 & \\text{if dtype is qint8} \\\\ 128 & \\text{otherwise} \\end{cases}\\\\ \\text{Otherwise:}&\\\\ &s = \\left( x_\\text{max} - x_\\text{min} \\right ) / \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\ &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s) \\end{aligned} \nwhere QminQ_\\text{min}  and QmaxQ_\\text{max}  are the minimum and maximum of the quantized data type.  Warning Only works with torch.per_tensor_symmetric quantization scheme   Warning dtype can only take torch.qint8 or torch.quint8.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  \n"}, {"name": "torch.quantization.MovingAverageMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAverageMinMaxObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.MovingAverageMinMaxObserver(averaging_constant=0.01, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the moving average of the min and max values. This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \naveraging_constant \u2013 Averaging constant for min/max. \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    The moving average min/max is computed as follows  xmin={min\u2061(X)if xmin=None(1\u2212c)xmin+cmin\u2061(X)otherwisexmax={max\u2061(X)if xmax=None(1\u2212c)xmax+cmax\u2061(X)otherwise\\begin{array}{ll} x_\\text{min} = \\begin{cases} \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\ (1 - c) x_\\text{min} + c \\min(X) & \\text{otherwise} \\end{cases}\\\\ x_\\text{max} = \\begin{cases} \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\ (1 - c) x_\\text{max} + c \\max(X) & \\text{otherwise} \\end{cases}\\\\ \\end{array} \nwhere xmin/maxx_\\text{min/max}  is the running average min/max, XX  is is the incoming tensor, and cc  is the averaging_constant. The scale and zero point are then computed as in MinMaxObserver.  Note Only works with torch.per_tensor_affine quantization scheme.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  \n"}, {"name": "torch.quantization.MovingAveragePerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAveragePerChannelMinMaxObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01, ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \naveraging_constant \u2013 Averaging constant for min/max. \nch_axis \u2013 Channel axis \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  \n"}, {"name": "torch.quantization.NoopObserver", "path": "torch.quantization#torch.quantization.NoopObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.NoopObserver(dtype=torch.float16, custom_op_name='') [source]\n \nObserver that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float(). Primarily used for quantization to float16 which doesn\u2019t require determining ranges.  Parameters \n \ndtype \u2013 Quantized data type \ncustom_op_name \u2013 (temporary) specify this observer for an operator that doesn\u2019t require any observation (Can be used in Graph Mode Passes for special case ops).    \n"}, {"name": "torch.quantization.ObserverBase", "path": "torch.quantization#torch.quantization.ObserverBase", "type": "torch.quantization", "text": " \nclass torch.quantization.ObserverBase(dtype) [source]\n \nBase observer Module. Any observer implementation should derive from this class. Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.  Parameters \ndtype \u2013 Quantized data type    \nclassmethod with_args(**kwargs)  \nWrapper that allows creation of class factories. This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Example: >>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n>>> foo_instance1 = foo_builder()\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1) == id(foo_instance2)\nFalse\n \n \n"}, {"name": "torch.quantization.ObserverBase.with_args()", "path": "torch.quantization#torch.quantization.ObserverBase.with_args", "type": "torch.quantization", "text": " \nclassmethod with_args(**kwargs)  \nWrapper that allows creation of class factories. This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Example: >>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n>>> foo_instance1 = foo_builder()\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1) == id(foo_instance2)\nFalse\n \n"}, {"name": "torch.quantization.PerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.PerChannelMinMaxObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None) [source]\n \nObserver module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \nch_axis \u2013 Channel axis \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup.    The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  \n"}, {"name": "torch.quantization.prepare()", "path": "torch.quantization#torch.quantization.prepare", "type": "torch.quantization", "text": " \ntorch.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None) [source]\n \nPrepares a copy of the model for quantization calibration or quantization-aware training. Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute. The model will be attached with observer or fake quant modules, and qconfig will be propagated.  Parameters \n \nmodel \u2013 input model to be modified in-place \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nallow_list \u2013 list of quantizable modules \nobserver_non_leaf_module_list \u2013 list of non-leaf modules we want to add observer \nprepare_custom_config_dict \u2013 customization configuration dictionary for prepare function    # Example of prepare_custom_config_dict:\nprepare_custom_config_dict = {\n    # user will manually define the corresponding observed\n    # module class which has a from_float class method that converts\n    # float custom module to observed custom module\n    \"float_to_observed_custom_module_class\": {\n        CustomModule: ObservedCustomModule\n    }\n }\n \n"}, {"name": "torch.quantization.prepare_qat()", "path": "torch.quantization#torch.quantization.prepare_qat", "type": "torch.quantization", "text": " \ntorch.quantization.prepare_qat(model, mapping=None, inplace=False) [source]\n \nPrepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version. Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.  Parameters \n \nmodel \u2013 input model to be modified in-place \nmapping \u2013 dictionary that maps float modules to quantized modules to be replaced. \ninplace \u2013 carry out model transformations in-place, the original module is mutated    \n"}, {"name": "torch.quantization.propagate_qconfig_()", "path": "torch.quantization#torch.quantization.propagate_qconfig_", "type": "torch.quantization", "text": " \ntorch.quantization.propagate_qconfig_(module, qconfig_dict=None, allow_list=None) [source]\n \nPropagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module  Parameters \n \nmodule \u2013 input module \nqconfig_dict \u2013 dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)   Returns \nNone, module is modified inplace with qconfig attached   \n"}, {"name": "torch.quantization.QConfig", "path": "torch.quantization#torch.quantization.QConfig", "type": "torch.quantization", "text": " \nclass torch.quantization.QConfig [source]\n \nDescribes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively. Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial): my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8), weight=default_observer.with_args(dtype=torch.qint8)) \n"}, {"name": "torch.quantization.QConfigDynamic", "path": "torch.quantization#torch.quantization.QConfigDynamic", "type": "torch.quantization", "text": " \nclass torch.quantization.QConfigDynamic [source]\n \nDescribes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights. It\u2019s like QConfig, but for dynamic quantization. Note that QConfigDynamic needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial): my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8)) \n"}, {"name": "torch.quantization.quantize()", "path": "torch.quantization#torch.quantization.quantize", "type": "torch.quantization", "text": " \ntorch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False) [source]\n \nQuantize the input float model with post training static quantization. First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.  Parameters \n \nmodel \u2013 input float model \nrun_fn \u2013 a calibration function for calibrating the prepared model \nrun_args \u2013 positional arguments for run_fn\n \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nmapping \u2013 correspondence between original module types and quantized counterparts   Returns \nQuantized model.   \n"}, {"name": "torch.quantization.quantize_dynamic()", "path": "torch.quantization#torch.quantization.quantize_dynamic", "type": "torch.quantization", "text": " \ntorch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False) [source]\n \nConverts a float model to dynamic (i.e. weights-only) quantized model. Replaces specified modules with dynamic weight-only quantized versions and output the quantized model. For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants. Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.  Parameters \n \nmodel \u2013 input model \nqconfig_spec \u2013 \nEither:  A dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute). Entries in the dictionary need to be QConfigDynamic instances. A set of types and/or submodule names to apply dynamic quantization to, in which case the dtype argument is used to specify the bit-width   \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nmapping \u2013 maps type of a submodule to a type of corresponding dynamically quantized version with which the submodule needs to be replaced    \n"}, {"name": "torch.quantization.quantize_qat()", "path": "torch.quantization#torch.quantization.quantize_qat", "type": "torch.quantization", "text": " \ntorch.quantization.quantize_qat(model, run_fn, run_args, inplace=False) [source]\n \nDo quantization aware training and output a quantized model  Parameters \n \nmodel \u2013 input model \nrun_fn \u2013 a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop \nrun_args \u2013 positional arguments for run_fn\n   Returns \nQuantized model.   \n"}, {"name": "torch.quantization.QuantStub", "path": "torch.quantization#torch.quantization.QuantStub", "type": "torch.quantization", "text": " \nclass torch.quantization.QuantStub(qconfig=None) [source]\n \nQuantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.  Parameters \nqconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules   \n"}, {"name": "torch.quantization.QuantWrapper", "path": "torch.quantization#torch.quantization.QuantWrapper", "type": "torch.quantization", "text": " \nclass torch.quantization.QuantWrapper(module) [source]\n \nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules. This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub. \n"}, {"name": "torch.quantization.RecordingObserver", "path": "torch.quantization#torch.quantization.RecordingObserver", "type": "torch.quantization", "text": " \nclass torch.quantization.RecordingObserver(**kwargs) [source]\n \nThe module is mainly for debug and records the tensor values during runtime.  Parameters \n \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit    \n"}, {"name": "torch.quantization.swap_module()", "path": "torch.quantization#torch.quantization.swap_module", "type": "torch.quantization", "text": " \ntorch.quantization.swap_module(mod, mapping, custom_module_class_mapping) [source]\n \nSwaps the module if it has a quantized counterpart and it has an observer attached.  Parameters \n \nmod \u2013 input module \nmapping \u2013 a dictionary that maps from nn module to nnq module   Returns \nThe corresponding quantized module of mod   \n"}, {"name": "torch.quantize_per_channel()", "path": "generated/torch.quantize_per_channel#torch.quantize_per_channel", "type": "torch", "text": " \ntorch.quantize_per_channel(input, scales, zero_points, axis, dtype) \u2192 Tensor  \nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.  Parameters \n \ninput (Tensor) \u2013 float tensor to quantize \nscales (Tensor) \u2013 float 1D tensor of scales to use, size should match input.size(axis)\n \nzero_points (int) \u2013 integer 1D tensor of offset to use, size should match input.size(axis)\n \naxis (int) \u2013 dimension on which apply per-channel quantization \ndtype (torch.dtype) \u2013 the desired data type of returned tensor. Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32\n   Returns \nA newly quantized tensor  Return type \nTensor   Example: >>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)\n \n"}, {"name": "torch.quantize_per_tensor()", "path": "generated/torch.quantize_per_tensor#torch.quantize_per_tensor", "type": "torch", "text": " \ntorch.quantize_per_tensor(input, scale, zero_point, dtype) \u2192 Tensor  \nConverts a float tensor to a quantized tensor with given scale and zero point.  Parameters \n \ninput (Tensor) \u2013 float tensor to quantize \nscale (float) \u2013 scale to apply in quantization formula \nzero_point (int) \u2013 offset in integer value that maps to float zero \ndtype (torch.dtype) \u2013 the desired data type of returned tensor. Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32\n   Returns \nA newly quantized tensor  Return type \nTensor   Example: >>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)\n \n"}, {"name": "torch.quasirandom.SobolEngine", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine", "type": "torch", "text": " \nclass torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None) [source]\n \nThe torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences. Sobol sequences are an example of low discrepancy quasi-random sequences. This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 21201. It uses direction numbers from https://web.maths.unsw.edu.au/~fkuo/sobol/ obtained using the search criterion D(6) up to the dimension 21201. This is the recommended choice by the authors. References  Art B. Owen. Scrambling Sobol and Niederreiter-Xing points. Journal of Complexity, 14(4):466-489, December 1998. I. M. Sobol. The distribution of points in a cube and the accurate evaluation of integrals. Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.   Parameters \n \ndimension (Int) \u2013 The dimensionality of the sequence to be drawn \nscramble (bool, optional) \u2013 Setting this to True will produce scrambled Sobol sequences. Scrambling is capable of producing better Sobol sequences. Default: False. \nseed (Int, optional) \u2013 This is the seed for the scrambling. The seed of the random number generator is set to this, if specified. Otherwise, it uses a random seed. Default: None\n    Examples: >>> soboleng = torch.quasirandom.SobolEngine(dimension=5)\n>>> soboleng.draw(3)\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])\n  \ndraw(n=1, out=None, dtype=torch.float32) [source]\n \nFunction to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension) .  Parameters \n \nn (Int, optional) \u2013 The length of sequence of points to draw. Default: 1 \nout (Tensor, optional) \u2013 The output tensor \ndtype (torch.dtype, optional) \u2013 the desired data type of the returned tensor. Default: torch.float32\n    \n  \ndraw_base2(m, out=None, dtype=torch.float32) [source]\n \nFunction to draw a sequence of 2**m points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (2\u2217\u2217m,dimension)(2**m, dimension) .  Parameters \n \nm (Int) \u2013 The (base2) exponent of the number of points to draw. \nout (Tensor, optional) \u2013 The output tensor \ndtype (torch.dtype, optional) \u2013 the desired data type of the returned tensor. Default: torch.float32\n    \n  \nfast_forward(n) [source]\n \nFunction to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.  Parameters \nn (Int) \u2013 The number of steps to fast-forward by.   \n  \nreset() [source]\n \nFunction to reset the SobolEngine to base state. \n \n"}, {"name": "torch.quasirandom.SobolEngine.draw()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw", "type": "torch", "text": " \ndraw(n=1, out=None, dtype=torch.float32) [source]\n \nFunction to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension) .  Parameters \n \nn (Int, optional) \u2013 The length of sequence of points to draw. Default: 1 \nout (Tensor, optional) \u2013 The output tensor \ndtype (torch.dtype, optional) \u2013 the desired data type of the returned tensor. Default: torch.float32\n    \n"}, {"name": "torch.quasirandom.SobolEngine.draw_base2()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw_base2", "type": "torch", "text": " \ndraw_base2(m, out=None, dtype=torch.float32) [source]\n \nFunction to draw a sequence of 2**m points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (2\u2217\u2217m,dimension)(2**m, dimension) .  Parameters \n \nm (Int) \u2013 The (base2) exponent of the number of points to draw. \nout (Tensor, optional) \u2013 The output tensor \ndtype (torch.dtype, optional) \u2013 the desired data type of the returned tensor. Default: torch.float32\n    \n"}, {"name": "torch.quasirandom.SobolEngine.fast_forward()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.fast_forward", "type": "torch", "text": " \nfast_forward(n) [source]\n \nFunction to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.  Parameters \nn (Int) \u2013 The number of steps to fast-forward by.   \n"}, {"name": "torch.quasirandom.SobolEngine.reset()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.reset", "type": "torch", "text": " \nreset() [source]\n \nFunction to reset the SobolEngine to base state. \n"}, {"name": "torch.rad2deg()", "path": "generated/torch.rad2deg#torch.rad2deg", "type": "torch", "text": " \ntorch.rad2deg(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with each of the elements of input converted from angles in radians to degrees.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\n>>> torch.rad2deg(a)\ntensor([[ 180.0233, -180.0233],\n        [ 359.9894, -359.9894],\n        [  89.9544,  -89.9544]])\n \n"}, {"name": "torch.rand()", "path": "generated/torch.rand#torch.rand", "type": "torch", "text": " \ntorch.rand(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)  The shape of the tensor is defined by the variable argument size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n>>> torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])\n \n"}, {"name": "torch.randint()", "path": "generated/torch.randint#torch.randint", "type": "torch", "text": " \ntorch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive). The shape of the tensor is defined by the variable argument size.  Note With the global dtype default (torch.float32), this function returns a tensor with dtype torch.int64.   Parameters \n \nlow (int, optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. \nhigh (int) \u2013 One above the highest integer to be drawn from the distribution. \nsize (tuple) \u2013 a tuple defining the shape of the output tensor.   Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n\n>>> torch.randint(10, (2, 2))\ntensor([[0, 2],\n        [5, 5]])\n\n\n>>> torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n        [6, 7]])\n \n"}, {"name": "torch.randint_like()", "path": "generated/torch.randint_like#torch.randint_like", "type": "torch", "text": " \ntorch.randint_like(input, low=0, high, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).  Parameters \n \ninput (Tensor) \u2013 the size of input will determine size of the output tensor. \nlow (int, optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. \nhigh (int) \u2013 One above the highest integer to be drawn from the distribution.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n"}, {"name": "torch.randn()", "path": "generated/torch.randn#torch.randn", "type": "torch", "text": " \ntorch.randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).  outi\u223cN(0,1)\\text{out}_{i} \\sim \\mathcal{N}(0, 1)  \nThe shape of the tensor is defined by the variable argument size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n>>> torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n        [ 1.1719, -0.4709, -0.1996]])\n \n"}, {"name": "torch.randn_like()", "path": "generated/torch.randn_like#torch.randn_like", "type": "torch", "text": " \ntorch.randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like(input) is equivalent to torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters \ninput (Tensor) \u2013 the size of input will determine size of the output tensor.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n"}, {"name": "torch.random", "path": "random", "type": "torch.random", "text": "torch.random  \ntorch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices') [source]\n \nForks the RNG, so that when you return, the RNG is reset to the state that it was previously in.  Parameters \n \ndevices (iterable of CUDA IDs) \u2013 CUDA devices for which to fork the RNG. CPU RNG state is always forked. By default, fork_rng() operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed \nenabled (bool) \u2013 if False, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.    \n  \ntorch.random.get_rng_state() [source]\n \nReturns the random number generator state as a torch.ByteTensor. \n  \ntorch.random.initial_seed() [source]\n \nReturns the initial seed for generating random numbers as a Python long. \n  \ntorch.random.manual_seed(seed) [source]\n \nSets the seed for generating random numbers. Returns a torch.Generator object.  Parameters \nseed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.   \n  \ntorch.random.seed() [source]\n \nSets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. \n  \ntorch.random.set_rng_state(new_state) [source]\n \nSets the random number generator state.  Parameters \nnew_state (torch.ByteTensor) \u2013 The desired state   \n\n"}, {"name": "torch.random.fork_rng()", "path": "random#torch.random.fork_rng", "type": "torch.random", "text": " \ntorch.random.fork_rng(devices=None, enabled=True, _caller='fork_rng', _devices_kw='devices') [source]\n \nForks the RNG, so that when you return, the RNG is reset to the state that it was previously in.  Parameters \n \ndevices (iterable of CUDA IDs) \u2013 CUDA devices for which to fork the RNG. CPU RNG state is always forked. By default, fork_rng() operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed \nenabled (bool) \u2013 if False, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.    \n"}, {"name": "torch.random.get_rng_state()", "path": "random#torch.random.get_rng_state", "type": "torch.random", "text": " \ntorch.random.get_rng_state() [source]\n \nReturns the random number generator state as a torch.ByteTensor. \n"}, {"name": "torch.random.initial_seed()", "path": "random#torch.random.initial_seed", "type": "torch.random", "text": " \ntorch.random.initial_seed() [source]\n \nReturns the initial seed for generating random numbers as a Python long. \n"}, {"name": "torch.random.manual_seed()", "path": "random#torch.random.manual_seed", "type": "torch.random", "text": " \ntorch.random.manual_seed(seed) [source]\n \nSets the seed for generating random numbers. Returns a torch.Generator object.  Parameters \nseed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.   \n"}, {"name": "torch.random.seed()", "path": "random#torch.random.seed", "type": "torch.random", "text": " \ntorch.random.seed() [source]\n \nSets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. \n"}, {"name": "torch.random.set_rng_state()", "path": "random#torch.random.set_rng_state", "type": "torch.random", "text": " \ntorch.random.set_rng_state(new_state) [source]\n \nSets the random number generator state.  Parameters \nnew_state (torch.ByteTensor) \u2013 The desired state   \n"}, {"name": "torch.randperm()", "path": "generated/torch.randperm#torch.randperm", "type": "torch", "text": " \ntorch.randperm(n, *, generator=None, out=None, dtype=torch.int64, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) \u2192 Tensor  \nReturns a random permutation of integers from 0 to n - 1.  Parameters \nn (int) \u2013 the upper bound (exclusive)  Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: torch.int64. \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \npin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: >>> torch.randperm(4)\ntensor([2, 1, 0, 3])\n \n"}, {"name": "torch.rand_like()", "path": "generated/torch.rand_like#torch.rand_like", "type": "torch", "text": " \ntorch.rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) . torch.rand_like(input) is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters \ninput (Tensor) \u2013 the size of input will determine size of the output tensor.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n"}, {"name": "torch.range()", "path": "generated/torch.range#torch.range", "type": "torch", "text": " \ntorch.range(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1  with values from start to end with step step. Step is the gap between two values in the tensor.  outi+1=outi+step.\\text{out}_{i+1} = \\text{out}_i + \\text{step}.  \n Warning This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python\u2019s range builtin. Instead, use torch.arange(), which produces values in [start, end).   Parameters \n \nstart (float) \u2013 the starting value for the set of points. Default: 0. \nend (float) \u2013 the ending value for the set of points \nstep (float) \u2013 the gap between each pair of adjacent points. Default: 1.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64. \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\n \n"}, {"name": "torch.ravel()", "path": "generated/torch.ravel#torch.ravel", "type": "torch", "text": " \ntorch.ravel(input) \u2192 Tensor  \nReturn a contiguous flattened tensor. A copy is made only if needed.  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.ravel(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n \n"}, {"name": "torch.real()", "path": "generated/torch.real#torch.real", "type": "torch", "text": " \ntorch.real(input) \u2192 Tensor  \nReturns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.  Warning real() is only supported for tensors with complex dtypes.   Parameters \ninput (Tensor) \u2013 the input tensor.    Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n   \n"}, {"name": "torch.reciprocal()", "path": "generated/torch.reciprocal#torch.reciprocal", "type": "torch", "text": " \ntorch.reciprocal(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the reciprocal of the elements of input  Note Unlike NumPy\u2019s reciprocal, torch.reciprocal supports integral inputs. Integral inputs to reciprocal are automatically promoted to the default scalar type.   outi=1inputi\\text{out}_{i} = \\frac{1}{\\text{input}_{i}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n>>> torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])\n \n"}, {"name": "torch.remainder()", "path": "generated/torch.remainder#torch.remainder", "type": "torch", "text": " \ntorch.remainder(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape, type promotion, and integer and float inputs.  Note Complex inputs are not supported. In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers. See torch.fmod() for how division by zero is handled.   Parameters \n \ninput (Tensor) \u2013 the dividend \nother (Tensor or Scalar) \u2013 the divisor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n  See also torch.fmod(), which computes the element-wise remainder of division equivalently to the C library function fmod().  \n"}, {"name": "torch.renorm()", "path": "generated/torch.renorm#torch.renorm", "type": "torch", "text": " \ntorch.renorm(input, p, dim, maxnorm, *, out=None) \u2192 Tensor  \nReturns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm  Note If the norm of a row is lower than maxnorm, the row is unchanged   Parameters \n \ninput (Tensor) \u2013 the input tensor. \np (float) \u2013 the power for the norm computation \ndim (int) \u2013 the dimension to slice over to get the sub-tensors \nmaxnorm (float) \u2013 the maximum norm to keep each sub-tensor under   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.ones(3, 3)\n>>> x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n>>> x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n>>> x\ntensor([[ 1.,  1.,  1.],\n        [ 2.,  2.,  2.],\n        [ 3.,  3.,  3.]])\n>>> torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n        [ 1.6667,  1.6667,  1.6667],\n        [ 1.6667,  1.6667,  1.6667]])\n \n"}, {"name": "torch.repeat_interleave()", "path": "generated/torch.repeat_interleave#torch.repeat_interleave", "type": "torch", "text": " \ntorch.repeat_interleave(input, repeats, dim=None) \u2192 Tensor  \nRepeat elements of a tensor.  Warning This is different from torch.Tensor.repeat() but similar to numpy.repeat.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nrepeats (Tensor or int) \u2013 The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis. \ndim (int, optional) \u2013 The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.   Returns \nRepeated tensor which has the same shape as input, except along the given axis.  Return type \nTensor   Example: >>> x = torch.tensor([1, 2, 3])\n>>> x.repeat_interleave(2)\ntensor([1, 1, 2, 2, 3, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\n>>> torch.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]])\n>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]])\n  \ntorch.repeat_interleave(repeats) \u2192 Tensor \n If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be tensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times, 1 appears n2 times, 2 appears n3 times, etc. \n"}, {"name": "torch.reshape()", "path": "generated/torch.reshape#torch.reshape", "type": "torch", "text": " \ntorch.reshape(input, shape) \u2192 Tensor  \nReturns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior. See torch.Tensor.view() on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remaining dimensions and the number of elements in input.  Parameters \n \ninput (Tensor) \u2013 the tensor to be reshaped \nshape (tuple of python:ints) \u2013 the new shape    Example: >>> a = torch.arange(4.)\n>>> torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n>>> b = torch.tensor([[0, 1], [2, 3]])\n>>> torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])\n \n"}, {"name": "torch.result_type()", "path": "generated/torch.result_type#torch.result_type", "type": "torch", "text": " \ntorch.result_type(tensor1, tensor2) \u2192 dtype  \nReturns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors. See type promotion documentation for more information on the type promotion logic.  Parameters \n \ntensor1 (Tensor or Number) \u2013 an input tensor or number \ntensor2 (Tensor or Number) \u2013 an input tensor or number    Example: >>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\ntorch.float32\n>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\ntorch.uint8\n \n"}, {"name": "torch.roll()", "path": "generated/torch.roll#torch.roll", "type": "torch", "text": " \ntorch.roll(input, shifts, dims=None) \u2192 Tensor  \nRoll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nshifts (int or tuple of python:ints) \u2013 The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value \ndims (int or tuple of python:ints) \u2013 Axis along which to roll    Example: >>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n>>> x\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n>>> torch.roll(x, 1, 0)\ntensor([[7, 8],\n        [1, 2],\n        [3, 4],\n        [5, 6]])\n>>> torch.roll(x, -1, 0)\ntensor([[3, 4],\n        [5, 6],\n        [7, 8],\n        [1, 2]])\n>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\ntensor([[6, 5],\n        [8, 7],\n        [2, 1],\n        [4, 3]])\n \n"}, {"name": "torch.rot90()", "path": "generated/torch.rot90#torch.rot90", "type": "torch", "text": " \ntorch.rot90(input, k, dims) \u2192 Tensor  \nRotate a n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nk (int) \u2013 number of times to rotate \ndims (a list or tuple) \u2013 axis to rotate    Example: >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.rot90(x, 1, [0, 1])\ntensor([[1, 3],\n        [0, 2]])\n\n>>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]])\n>>> torch.rot90(x, 1, [1, 2])\ntensor([[[1, 3],\n         [0, 2]],\n\n        [[5, 7],\n         [4, 6]]])\n \n"}, {"name": "torch.round()", "path": "generated/torch.round#torch.round", "type": "torch", "text": " \ntorch.round(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with each of the elements of input rounded to the closest integer.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n>>> torch.round(a)\ntensor([ 1.,  1.,  1., -1.])\n \n"}, {"name": "torch.row_stack()", "path": "generated/torch.row_stack#torch.row_stack", "type": "torch", "text": " \ntorch.row_stack(tensors, *, out=None) \u2192 Tensor  \nAlias of torch.vstack(). \n"}, {"name": "torch.rsqrt()", "path": "generated/torch.rsqrt#torch.rsqrt", "type": "torch", "text": " \ntorch.rsqrt(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the reciprocal of the square-root of each of the elements of input.  outi=1inputi\\text{out}_{i} = \\frac{1}{\\sqrt{\\text{input}_{i}}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n>>> torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])\n \n"}, {"name": "torch.save()", "path": "generated/torch.save#torch.save", "type": "torch", "text": " \ntorch.save(obj, f, pickle_module=<module 'pickle' from '/home/matti/miniconda3/lib/python3.7/pickle.py'>, pickle_protocol=2, _use_new_zipfile_serialization=True) [source]\n \nSaves an object to a disk file. See also: saving-loading-tensors  Parameters \n \nobj \u2013 saved object \nf \u2013 a file-like object (has to implement write and flush) or a string or os.PathLike object containing a file name \npickle_module \u2013 module used for pickling metadata and objects \npickle_protocol \u2013 can be specified to override the default protocol     Note A common PyTorch convention is to save tensors using .pt file extension.   Note PyTorch preserves storage sharing across serialization. See preserve-storage-sharing for more details.   Note The 1.6 release of PyTorch switched torch.save to use a new zipfile-based file format. torch.load still retains the ability to load files in the old format. If for any reason you want torch.save to use the old format, pass the kwarg _use_new_zipfile_serialization=False.  Example >>> # Save to file\n>>> x = torch.tensor([0, 1, 2, 3, 4])\n>>> torch.save(x, 'tensor.pt')\n>>> # Save to io.BytesIO buffer\n>>> buffer = io.BytesIO()\n>>> torch.save(x, buffer)\n \n"}, {"name": "torch.scatter()", "path": "generated/torch.scatter#torch.scatter", "type": "torch", "text": " \ntorch.scatter(input, dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_() \n"}, {"name": "torch.scatter_add()", "path": "generated/torch.scatter_add#torch.scatter_add", "type": "torch", "text": " \ntorch.scatter_add(input, dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_add_() \n"}, {"name": "torch.searchsorted()", "path": "generated/torch.searchsorted#torch.searchsorted", "type": "torch", "text": " \ntorch.searchsorted(sorted_sequence, values, *, out_int32=False, right=False, out=None) \u2192 Tensor  \nFind the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, the order of the corresponding innermost dimension within sorted_sequence would be preserved. Return a new tensor with the same size as values. If right is False (default), then the left boundary of sorted_sequence is closed. More formally, the returned index satisfies the following rules:   \nsorted_sequence right returned index satisfies   \n1-D False sorted_sequence[i-1] < values[m][n]...[l][x] <= sorted_sequence[i]  \n1-D True sorted_sequence[i-1] <= values[m][n]...[l][x] < sorted_sequence[i]  \nN-D False sorted_sequence[m][n]...[l][i-1] < values[m][n]...[l][x] <= sorted_sequence[m][n]...[l][i]  \nN-D True sorted_sequence[m][n]...[l][i-1] <= values[m][n]...[l][x] < sorted_sequence[m][n]...[l][i]    Parameters \n \nsorted_sequence (Tensor) \u2013 N-D or 1-D tensor, containing monotonically increasing sequence on the innermost dimension. \nvalues (Tensor or Scalar) \u2013 N-D tensor or a Scalar containing the search value(s).   Keyword Arguments \n \nout_int32 (bool, optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise. Default value is False, i.e. default output data type is torch.int64. \nright (bool, optional) \u2013 if False, return the first suitable location that is found. If True, return the last such index. If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or the size of innermost dimension within sorted_sequence (one pass the last index of the innermost dimension). In other words, if False, gets the lower bound index for each value in values on the corresponding innermost dimension of the sorted_sequence. If True, gets the upper bound index instead. Default value is False. \nout (Tensor, optional) \u2013 the output tensor, must be the same size as values if provided.     Note If your use case is always 1-D sorted sequence, torch.bucketize() is preferred, because it has fewer dimension checks resulting in slightly better performance.  Example: >>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> sorted_sequence\ntensor([[ 1,  3,  5,  7,  9],\n        [ 2,  4,  6,  8, 10]])\n>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> values\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.searchsorted(sorted_sequence, values)\ntensor([[1, 3, 4],\n        [1, 2, 4]])\n>>> torch.searchsorted(sorted_sequence, values, right=True)\ntensor([[2, 3, 5],\n        [1, 3, 4]])\n\n>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])\n>>> sorted_sequence_1d\ntensor([1, 3, 5, 7, 9])\n>>> torch.searchsorted(sorted_sequence_1d, values)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n \n"}, {"name": "torch.seed()", "path": "generated/torch.seed#torch.seed", "type": "torch", "text": " \ntorch.seed() [source]\n \nSets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. \n"}, {"name": "torch.set_default_dtype()", "path": "generated/torch.set_default_dtype#torch.set_default_dtype", "type": "torch", "text": " \ntorch.set_default_dtype(d) [source]\n \nSets the default floating point dtype to d. This dtype is:  The inferred dtype for python floats in torch.tensor(). Used to infer dtype for python complex numbers. The default complex dtype is set to torch.complex128 if default floating point dtype is torch.float64, otherwise it\u2019s set to torch.complex64\n  The default floating point dtype is initially torch.float32.  Parameters \nd (torch.dtype) \u2013 the floating point dtype to make the default   Example >>> # initial default for floating point is torch.float32\n>>> torch.tensor([1.2, 3]).dtype\ntorch.float32\n>>> # initial default for floating point is torch.complex64\n>>> torch.tensor([1.2, 3j]).dtype\ntorch.complex64\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\ntorch.complex128\n \n"}, {"name": "torch.set_default_tensor_type()", "path": "generated/torch.set_default_tensor_type#torch.set_default_tensor_type", "type": "torch", "text": " \ntorch.set_default_tensor_type(t) [source]\n \nSets the default torch.Tensor type to floating point tensor type t. This type will also be used as default floating point type for type inference in torch.tensor(). The default floating point tensor type is initially torch.FloatTensor.  Parameters \nt (type or string) \u2013 the floating point tensor type or its name   Example: >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_tensor_type(torch.DoubleTensor)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n \n"}, {"name": "torch.set_flush_denormal()", "path": "generated/torch.set_flush_denormal#torch.set_flush_denormal", "type": "torch", "text": " \ntorch.set_flush_denormal(mode) \u2192 bool  \nDisables denormal floating numbers on CPU. Returns True if your system supports flushing denormal numbers and it successfully configures flush denormal mode. set_flush_denormal() is only supported on x86 architectures supporting SSE3.  Parameters \nmode (bool) \u2013 Controls whether to enable flush denormal mode or not   Example: >>> torch.set_flush_denormal(True)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n>>> torch.set_flush_denormal(False)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n       [ 1.0000], dtype=torch.float64)\n \n"}, {"name": "torch.set_grad_enabled", "path": "generated/torch.set_grad_enabled#torch.set_grad_enabled", "type": "torch", "text": " \nclass torch.set_grad_enabled(mode) [source]\n \nContext-manager that sets gradient calculation to on or off. set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation in other threads.  Parameters \nmode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.   Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n \n"}, {"name": "torch.set_num_interop_threads()", "path": "generated/torch.set_num_interop_threads#torch.set_num_interop_threads", "type": "torch", "text": " \ntorch.set_num_interop_threads(int)  \nSets the number of threads used for interop parallelism (e.g. in JIT interpreter) on CPU.  Warning Can only be called once and before any inter-op parallel work is started (e.g. JIT execution).  \n"}, {"name": "torch.set_num_threads()", "path": "generated/torch.set_num_threads#torch.set_num_threads", "type": "torch", "text": " \ntorch.set_num_threads(int)  \nSets the number of threads used for intraop parallelism on CPU.  Warning To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code.  \n"}, {"name": "torch.set_printoptions()", "path": "generated/torch.set_printoptions#torch.set_printoptions", "type": "torch", "text": " \ntorch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None) [source]\n \nSet options for printing. Items shamelessly taken from NumPy  Parameters \n \nprecision \u2013 Number of digits of precision for floating point output (default = 4). \nthreshold \u2013 Total number of array elements which trigger summarization rather than full repr (default = 1000). \nedgeitems \u2013 Number of array items in summary at beginning and end of each dimension (default = 3). \nlinewidth \u2013 The number of characters per line for the purpose of inserting line breaks (default = 80). Thresholded matrices will ignore this parameter. \nprofile \u2013 Sane defaults for pretty printing. Can override with any of the above options. (any one of default, short, full) \nsci_mode \u2013 Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by torch._tensor_str._Formatter. This value is automatically chosen by the framework.    \n"}, {"name": "torch.set_rng_state()", "path": "generated/torch.set_rng_state#torch.set_rng_state", "type": "torch", "text": " \ntorch.set_rng_state(new_state) [source]\n \nSets the random number generator state.  Parameters \nnew_state (torch.ByteTensor) \u2013 The desired state   \n"}, {"name": "torch.sgn()", "path": "generated/torch.sgn#torch.sgn", "type": "torch", "text": " \ntorch.sgn(input, *, out=None) \u2192 Tensor  \nFor complex tensors, this function returns a new tensor whose elemants have the same angle as that of the elements of input and absolute value 1. For a non-complex tensor, this function returns the signs of the elements of input (see torch.sign()). outi=0\\text{out}_{i} = 0 , if \u2223inputi\u2223==0|{\\text{{input}}_i}| == 0  outi=inputi\u2223inputi\u2223\\text{out}_{i} = \\frac{{\\text{{input}}_i}}{|{\\text{{input}}_i}|} , otherwise  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x=torch.tensor([3+4j, 7-24j, 0, 1+2j])\n>>> x.sgn()\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])\n \n"}, {"name": "torch.sigmoid()", "path": "generated/torch.sigmoid#torch.sigmoid", "type": "torch", "text": " \ntorch.sigmoid(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the sigmoid of the elements of input.  outi=11+e\u2212inputi\\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.sigmoid(a)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n \n"}, {"name": "torch.sign()", "path": "generated/torch.sign#torch.sign", "type": "torch", "text": " \ntorch.sign(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the signs of the elements of input.  outi=sgn\u2061(inputi)\\text{out}_{i} = \\operatorname{sgn}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n>>> torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])\n \n"}, {"name": "torch.signbit()", "path": "generated/torch.signbit#torch.signbit", "type": "torch", "text": " \ntorch.signbit(input, *, out=None) \u2192 Tensor  \nTests if each element of input has its sign bit set (is less than zero) or not.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> torch.signbit(a)\ntensor([ False, True,  False,  False])\n \n"}, {"name": "torch.sin()", "path": "generated/torch.sin#torch.sin", "type": "torch", "text": " \ntorch.sin(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the sine of the elements of input.  outi=sin\u2061(inputi)\\text{out}_{i} = \\sin(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n>>> torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\n \n"}, {"name": "torch.sinc()", "path": "generated/torch.sinc#torch.sinc", "type": "torch", "text": " \ntorch.sinc(input, *, out=None) \u2192 Tensor  \nComputes the normalized sinc of input.  outi={1,if inputi=0sin\u2061(\u03c0inputi)/(\u03c0inputi),otherwise\\text{out}_{i} = \\begin{cases} 1, & \\text{if}\\ \\text{input}_{i}=0 \\\\ \\sin(\\pi \\text{input}_{i}) / (\\pi \\text{input}_{i}), & \\text{otherwise} \\end{cases}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.sinc(a)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n \n"}, {"name": "torch.sinh()", "path": "generated/torch.sinh#torch.sinh", "type": "torch", "text": " \ntorch.sinh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the hyperbolic sine of the elements of input.  outi=sinh\u2061(inputi)\\text{out}_{i} = \\sinh(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n>>> torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])\n  Note When input is on the CPU, the implementation of torch.sinh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details.  \n"}, {"name": "torch.slogdet()", "path": "generated/torch.slogdet#torch.slogdet", "type": "torch", "text": " \ntorch.slogdet(input) -> (Tensor, Tensor)  \nCalculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.  Note torch.slogdet() is deprecated. Please use torch.linalg.slogdet() instead.   Note If input has zero determinant, this returns (0, -inf).   Note Backward through slogdet() internally uses SVD results when input is not invertible. In this case, double backward through slogdet() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.   Parameters \ninput (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.  Returns \nA namedtuple (sign, logabsdet) containing the sign of the determinant, and the log value of the absolute determinant.   Example: >>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.det(A)\ntensor(-0.7576)\n>>> torch.logdet(A)\ntensor(nan)\n>>> torch.slogdet(A)\ntorch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n \n"}, {"name": "torch.smm()", "path": "sparse#torch.smm", "type": "torch.sparse", "text": " \ntorch.smm(input, mat) \u2192 Tensor  \nPerforms a matrix multiplication of the sparse matrix input with the dense matrix mat.  Parameters \n \ninput (Tensor) \u2013 a sparse matrix to be matrix multiplied \nmat (Tensor) \u2013 a dense matrix to be matrix multiplied    \n"}, {"name": "torch.solve()", "path": "generated/torch.solve#torch.solve", "type": "torch", "text": " \ntorch.solve(input, A, *, out=None) -> (Tensor, Tensor)  \nThis function returns the solution to the system of linear equations represented by AX=BAX = B  and the LU factorization of A, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs solution, LU. Supports real-valued and complex-valued inputs.  Note Irrespective of the original strides, the returned matrices solution and LU will be transposed, i.e. with strides like B.contiguous().transpose(-1, -2).stride() and A.contiguous().transpose(-1, -2).stride() respectively.   Parameters \n \ninput (Tensor) \u2013 input matrix BB  of size (\u2217,m,k)(*, m, k)  , where \u2217*  is zero or more batch dimensions. \nA (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m) , where \u2217*  is zero or more batch dimensions.   Keyword Arguments \nout ((Tensor, Tensor), optional) \u2013 optional output tuple.   Example: >>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n>>> # Batched solver example\n>>> A = torch.randn(2, 3, 1, 4, 4)\n>>> B = torch.randn(2, 3, 1, 4, 6)\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)\n \n"}, {"name": "torch.sort()", "path": "generated/torch.sort#torch.sort", "type": "torch", "text": " \ntorch.sort(input, dim=-1, descending=False, *, out=None) -> (Tensor, LongTensor)  \nSorts the elements of the input tensor along a given dimension in ascending order by value. If dim is not given, the last dimension of the input is chosen. If descending is True then the elements are sorted in descending order by value. A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int, optional) \u2013 the dimension to sort along \ndescending (bool, optional) \u2013 controls the sorting order (ascending or descending)   Keyword Arguments \nout (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers   Example: >>> x = torch.randn(3, 4)\n>>> sorted, indices = torch.sort(x)\n>>> sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n>>> indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n>>> sorted, indices = torch.sort(x, 0)\n>>> sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n>>> indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n \n"}, {"name": "torch.sparse", "path": "sparse", "type": "torch.sparse", "text": "torch.sparse Introduction PyTorch provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type. By default, array elements are stored contiguously in memory leading to efficient implementations of various array processing algorithms that relay on the fast access to array elements. However, there exists an important class of multi-dimensional arrays, so-called sparse arrays, where the contiguous memory storage of array elements turns out to be suboptimal. Sparse arrays have a property of having a vast portion of elements being equal to zero which means that a lot of memory as well as processor resources can be spared if only the non-zero elements are stored or/and processed. Various sparse storage formats (such as COO, CSR/CSC, LIL, etc.) have been developed that are optimized for a particular structure of non-zero elements in sparse arrays as well as for specific operations on the arrays.  Note When talking about storing only non-zero elements of a sparse array, the usage of adjective \u201cnon-zero\u201d is not strict: one is allowed to store also zeros in the sparse array data structure. Hence, in the following, we use \u201cspecified elements\u201d for those array elements that are actually stored. In addition, the unspecified elements are typically assumed to have zero value, but not only, hence we use the term \u201cfill value\u201d to denote such elements.   Note Using a sparse storage format for storing sparse arrays can be advantageous only when the size and sparsity levels of arrays are high. Otherwise, for small-sized or low-sparsity arrays using the contiguous memory storage format is likely the most efficient approach.   Warning The PyTorch API of sparse tensors is in beta and may change in the near future.  Sparse COO tensors Currently, PyTorch implements the so-called Coordinate format, or COO format, as the default sparse storage format for storing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular,  the indices of specified elements are collected in indices tensor of size (ndim, nse) and with element type torch.int64, the corresponding values are collected in values tensor of size (nse,) and with an arbitrary integer or floating point number element type,  where ndim is the dimensionality of the tensor and nse is the number of specified elements.  Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant overhead from storing other tensor data). The memory consumption of a strided tensor is at least product(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least (2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor layout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using the default strided tensor layout. Notice the 200 fold memory saving from using the COO storage format.  Construction A sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function torch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write: >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n Note that the input i is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor: >>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n An empty sparse COO tensor can be constructed by specifying its size only: >>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n Hybrid sparse COO tensors Pytorch implements an extension of sparse tensors with scalar values to sparse tensors with (contiguous) tensor values. Such tensors are called hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional tensor so that we have:  the indices of specified elements are collected in indices tensor of size (sparse_dims, nse) and with element type torch.int64, the corresponding (tensor) values are collected in values tensor of size (nse, dense_dims) and with an arbitrary integer or floating point number element type.   Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid sparse tensor, where M and K are the numbers of sparse and dense dimensions, respectively, such that M + K == N holds.  Suppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\n >>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])\n In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following invariants:  \nM + K == len(s.shape) == s.ndim - dimensionality of a tensor is the sum of the number of sparse and dense dimensions, \ns.indices().shape == (M, nse) - sparse indices are stored explicitly, \ns.values().shape == (nse,) + s.shape[M : M + K] - the values of a hybrid tensor are K-dimensional tensors, \ns.values().layout == torch.strided - values are stored as strided tensors.   Note Dense dimensions always follow sparse dimensions, that is, mixing of dense and sparse dimensions is not supported.  Uncoalesced sparse COO tensors PyTorch sparse COO tensor format permits uncoalesced sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, 3 and 4, for the same index 1, that leads to an 1-D uncoalesced tensor: >>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)\n while the coalescing process will accumulate the multi-valued elements into a single value using summation: >>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)\n In general, the output of torch.Tensor.coalesce() method is a sparse tensor with the following properties:  the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, \ntorch.Tensor.is_coalesced() returns True.   Note For the most part, you shouldn\u2019t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by simply concatenating the indices and values tensors: >>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)\n If you repeatedly perform an operation that can produce duplicate entries (e.g., torch.Tensor.add()), you should occasionally coalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products.  Working with sparse COO tensors Let\u2019s consider the following example: >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n As mentioned above, a sparse COO tensor is a torch.Tensor instance and to distinguish it from the Tensor instances that use some other layout, on can use torch.Tensor.is_sparse or torch.Tensor.layout properties: >>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue\n The number of sparse and dense dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim(), respectively. For instance: >>> s.sparse_dim(), s.dense_dim()\n(2, 1)\n If s is a sparse COO tensor then its COO format data can be acquired using methods torch.Tensor.indices() and torch.Tensor.values().  Note Currently, one can acquire the COO format data only when the tensor instance is coalesced: >>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n For acquiring the COO format data of an uncoalesced tensor, use torch.Tensor._values() and torch.Tensor._indices(): >>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])\n  Constructing a new sparse COO tensor results a tensor that is not coalesced: >>> s.is_coalesced()\nFalse\n but one can construct a coalesced copy of a sparse COO tensor using the torch.Tensor.coalesce() method: >>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])\n When working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on an uncoalesced sparse tensor could be implemented by multiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not hold in general. Slicing (with positive step) of a sparse COO tensor is supported only for dense dimensions. Indexing is supported for both sparse and dense dimensions: >>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])\n In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, torch.sparse.softmax() computes the softmax with the assumption that the fill value is negative infinity. Supported Linear Algebra operations The following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here T[layout] denotes a tensor with a given layout. Similarly, M[layout] denotes a matrix (2-D PyTorch tensor), and V[layout] denotes a vector (1-D PyTorch tensor). In addition, f denotes a scalar (float or 0-D PyTorch tensor), * is element-wise multiplication, and @ is matrix multiplication.   \nPyTorch operation Sparse grad? Layout signature   \ntorch.mv() no M[sparse_coo] @ V[strided] -> V[strided]  \ntorch.matmul() no M[sparse_coo] @ M[strided] -> M[strided]  \ntorch.mm() no M[sparse_coo] @ M[strided] -> M[strided]  \ntorch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided]  \ntorch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo]  \ntorch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]  \ntorch.bmm() no T[sparse_coo] @ T[strided] -> T[strided]  \ntorch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]  \ntorch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]  \ntorch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]  \ntorch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]  \ntorch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]  \ntorch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]   where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports backward with respect to sparse matrix argument. All PyTorch operations, except torch.smm(), support backward with respect to strided matrix arguments.  Note Currently, PyTorch does not support matrix multiplication with the layout signature M[strided] @ M[sparse_coo]. However, applications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().   \nclass torch.Tensor  \nThe following methods are specific to sparse tensors:  \nis_sparse  \nIs True if the Tensor uses sparse storage layout, False otherwise. \n  \ndense_dim() \u2192 int  \nReturn the number of dense dimensions in a sparse tensor self.  Warning Throws an error if self is not a sparse tensor.  See also Tensor.sparse_dim() and hybrid tensors. \n  \nsparse_dim() \u2192 int  \nReturn the number of sparse dimensions in a sparse tensor self.  Warning Throws an error if self is not a sparse tensor.  See also Tensor.dense_dim() and hybrid tensors. \n  \nsparse_mask(mask) \u2192 Tensor  \nReturns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.  Note The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.   Parameters \nmask (Tensor) \u2013 a sparse tensor whose indices are used as a filter   Example: >>> nse = 5\n>>> dims = (5, 5, 2, 2)\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n>>> V = torch.randn(nse, dims[2], dims[3])\n>>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n>>> D = torch.randn(dims)\n>>> D.sparse_mask(S)\ntensor(indices=tensor([[0, 0, 0, 2],\n                       [0, 1, 4, 3]]),\n       values=tensor([[[ 1.6550,  0.2397],\n                       [-0.1611, -0.0779]],\n\n                      [[ 0.2326, -1.0558],\n                       [ 1.4711,  1.9678]],\n\n                      [[-0.5138, -0.0411],\n                       [ 1.9417,  0.5158]],\n\n                      [[ 0.0793,  0.0036],\n                       [-0.2569, -0.1055]]]),\n       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n \n  \nsparse_resize_(size, sparse_dim, dense_dim) \u2192 Tensor  \nResizes self sparse tensor to the desired size and the number of sparse and dense dimensions.  Note If the number of specified elements in self is zero, then size, sparse_dim, and dense_dim can be any size and positive integers such that len(size) == sparse_dim +\ndense_dim. If self specifies one or more elements, however, then each dimension in size must not be smaller than the corresponding dimension of self, sparse_dim must equal the number of sparse dimensions in self, and dense_dim must equal the number of dense dimensions in self.   Warning Throws an error if self is not a sparse tensor.   Parameters \n \nsize (torch.Size) \u2013 the desired size. If self is non-empty sparse tensor, the desired size cannot be smaller than the original size. \nsparse_dim (int) \u2013 the number of sparse dimensions \ndense_dim (int) \u2013 the number of dense dimensions    \n  \nsparse_resize_and_clear_(size, sparse_dim, dense_dim) \u2192 Tensor  \nRemoves all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions.  Parameters \n \nsize (torch.Size) \u2013 the desired size. \nsparse_dim (int) \u2013 the number of sparse dimensions \ndense_dim (int) \u2013 the number of dense dimensions    \n  \nto_dense() \u2192 Tensor  \nCreates a strided copy of self.  Warning Throws an error if self is a strided tensor.  Example: >>> s = torch.sparse_coo_tensor(\n...        torch.tensor([[1, 1],\n...                      [0, 2]]),\n...        torch.tensor([9, 10]),\n...        size=(3, 3))\n>>> s.to_dense()\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n \n  \nto_sparse(sparseDims) \u2192 Tensor  \nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.  Parameters \nsparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor   Example: >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n>>> d\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n>>> d.to_sparse()\ntensor(indices=tensor([[1, 1],\n                       [0, 2]]),\n       values=tensor([ 9, 10]),\n       size=(3, 3), nnz=2, layout=torch.sparse_coo)\n>>> d.to_sparse(1)\ntensor(indices=tensor([[1]]),\n       values=tensor([[ 9,  0, 10]]),\n       size=(3, 3), nnz=1, layout=torch.sparse_coo)\n \n  \ncoalesce() \u2192 Tensor  \nReturns a coalesced copy of self if self is an uncoalesced tensor. Returns self if self is a coalesced tensor.  Warning Throws an error if self is not a sparse COO tensor.  \n  \nis_coalesced() \u2192 bool  \nReturns True if self is a sparse COO tensor that is coalesced, False otherwise.  Warning Throws an error if self is not a sparse COO tensor.  See coalesce() and uncoalesced tensors. \n  \nindices() \u2192 Tensor  \nReturn the indices tensor of a sparse COO tensor.  Warning Throws an error if self is not a sparse COO tensor.  See also Tensor.values().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  \n  \nvalues() \u2192 Tensor  \nReturn the values tensor of a sparse COO tensor.  Warning Throws an error if self is not a sparse COO tensor.  See also Tensor.indices().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  \n \n The following torch.Tensor methods support sparse COO tensors: add() add_() addmm() addmm_() any() asin() asin_() arcsin() arcsin_() bmm() clone() deg2rad() deg2rad_() detach() detach_() dim() div() div_() floor_divide() floor_divide_() get_device() index_select() isnan() log1p() log1p_() mm() mul() mul_() mv() narrow_copy() neg() neg_() negative() negative_() numel() rad2deg() rad2deg_() resize_as_() size() pow() sqrt() square() smm() sspaddmm() sub() sub_() t() t_() transpose() transpose_() zero_() Sparse tensor functions  \ntorch.sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nConstructs a sparse tensor in COO(rdinate) format with specified values at the given indices.  Note This function returns an uncoalesced tensor.   Parameters \n \nindices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values. \nvalues (array_like) \u2013 Initial values for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. \nsize (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from values. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> i = torch.tensor([[0, 1, 1],\n...                   [2, 0, 2]])\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\n...                         dtype=torch.float64,\n...                         device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\n \n  \ntorch.sparse.sum(input, dim=None, dtype=None) [source]\n \nReturns the sum of each row of the sparse tensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a dense tensor instead of a sparse tensor. All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input. During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.  Parameters \n \ninput (Tensor) \u2013 the input sparse tensor \ndim (int or tuple of python:ints) \u2013 a dimension or a list of dimensions to reduce. Default: reduce over all dims. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: dtype of input.    Example: >>> nnz = 3\n>>> dims = [5, 5, 2, 3]\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n>>> V = torch.randn(nnz, dims[2], dims[3])\n>>> size = torch.Size(dims)\n>>> S = torch.sparse_coo_tensor(I, V, size)\n>>> S\ntensor(indices=tensor([[2, 0, 3],\n                       [2, 4, 1]]),\n       values=tensor([[[-0.6438, -1.6467,  1.4004],\n                       [ 0.3411,  0.0918, -0.2312]],\n\n                      [[ 0.5348,  0.0634, -2.0494],\n                       [-0.7125, -1.0646,  2.1844]],\n\n                      [[ 0.1276,  0.1874, -0.6334],\n                       [-1.9682, -0.5340,  0.7483]]]),\n       size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n\n# when sum over only part of sparse_dims, return a sparse tensor\n>>> torch.sparse.sum(S, [1, 3])\ntensor(indices=tensor([[0, 2, 3]]),\n       values=tensor([[-1.4512,  0.4073],\n                      [-0.8901,  0.2017],\n                      [-0.3183, -1.7539]]),\n       size=(5, 2), nnz=3, layout=torch.sparse_coo)\n\n# when sum over all sparse dim, return a dense tensor\n# with summed dims squeezed\n>>> torch.sparse.sum(S, [0, 1, 3])\ntensor([-2.6596, -1.1450])\n \n  \ntorch.sparse.addmm(mat, mat1, mat2, beta=1.0, alpha=1.0) [source]\n \nThis function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. mat1 need to have sparse_dim = 2. Note that the gradients of mat1 is a coalesced sparse tensor.  Parameters \n \nmat (Tensor) \u2013 a dense matrix to be added \nmat1 (Tensor) \u2013 a sparse matrix to be multiplied \nmat2 (Tensor) \u2013 a dense matrix to be multiplied \nbeta (Number, optional) \u2013 multiplier for mat (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2  (\u03b1\\alpha )    \n  \ntorch.sparse.mm(mat1, mat2) [source]\n \nPerforms a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2. Similar to torch.mm(), If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, out will be a (n\u00d7p)(n \\times p)  tensor. mat1 need to have sparse_dim = 2. This function also supports backward for both matrices. Note that the gradients of mat1 is a coalesced sparse tensor.  Parameters \n \nmat1 (SparseTensor) \u2013 the first sparse matrix to be multiplied \nmat2 (Tensor) \u2013 the second matrix to be multiplied, which could be sparse or dense     Shape:\n\nThe format of the output tensor of this function follows: - sparse x sparse -> sparse - sparse x dense -> dense   Example: >>> a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n>>> a\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n                       [0, 1, 2, 0, 1, 2]]),\n       values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),\n       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n\n>>> b = torch.randn(3, 2, requires_grad=True)\n>>> b\ntensor([[-0.6479,  0.7874],\n        [-1.2056,  0.5641],\n        [-1.1716, -0.9923]], requires_grad=True)\n\n>>> y = torch.sparse.mm(a, b)\n>>> y\ntensor([[-0.3323,  1.8723],\n        [-1.8951,  0.7904]], grad_fn=<SparseAddmmBackward>)\n>>> y.sum().backward()\n>>> a.grad\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n                       [0, 1, 2, 0, 1, 2]]),\n       values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),\n       size=(2, 3), nnz=6, layout=torch.sparse_coo)\n \n  \ntorch.sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nMatrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result. Note: This function is equivalent to torch.addmm(), except input and mat1 are sparse.  Parameters \n \ninput (Tensor) \u2013 a sparse matrix to be added \nmat1 (Tensor) \u2013 a sparse matrix to be matrix multiplied \nmat2 (Tensor) \u2013 a dense matrix to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for mat (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    \n  \ntorch.hspmm(mat1, mat2, *, out=None) \u2192 Tensor  \nPerforms a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The result is a (1 + 1)-dimensional hybrid COO matrix.  Parameters \n \nmat1 (Tensor) \u2013 the first sparse matrix to be matrix multiplied \nmat2 (Tensor) \u2013 the second strided matrix to be matrix multiplied   Keyword Arguments \n{out} \u2013    \n  \ntorch.smm(input, mat) \u2192 Tensor  \nPerforms a matrix multiplication of the sparse matrix input with the dense matrix mat.  Parameters \n \ninput (Tensor) \u2013 a sparse matrix to be matrix multiplied \nmat (Tensor) \u2013 a dense matrix to be matrix multiplied    \n  \ntorch.sparse.softmax(input, dim, dtype=None) [source]\n \nApplies a softmax function. Softmax is defined as: Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}  where i,ji, j  run over sparse tensor indices and unspecified entries are ignores. This is equivalent to defining unspecified entries as negative infinity so that exp(xk)=0exp(x_k) = 0  when the entry with index kk  has not specified. It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None    \n  \ntorch.sparse.log_softmax(input, dim, dtype=None) [source]\n \nApplies a softmax function followed by logarithm. See softmax for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None    \n Other functions The following torch functions support sparse COO tensors: cat() dstack() empty() empty_like() hstack() index_select() is_complex() is_floating_point() is_nonzero() is_same_size() is_signed() is_tensor() lobpcg() mm() native_norm() pca_lowrank() select() stack() svd_lowrank() unsqueeze() vstack() zeros() zeros_like()\n"}, {"name": "torch.sparse.addmm()", "path": "sparse#torch.sparse.addmm", "type": "torch.sparse", "text": " \ntorch.sparse.addmm(mat, mat1, mat2, beta=1.0, alpha=1.0) [source]\n \nThis function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. mat1 need to have sparse_dim = 2. Note that the gradients of mat1 is a coalesced sparse tensor.  Parameters \n \nmat (Tensor) \u2013 a dense matrix to be added \nmat1 (Tensor) \u2013 a sparse matrix to be multiplied \nmat2 (Tensor) \u2013 a dense matrix to be multiplied \nbeta (Number, optional) \u2013 multiplier for mat (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2  (\u03b1\\alpha )    \n"}, {"name": "torch.sparse.log_softmax()", "path": "sparse#torch.sparse.log_softmax", "type": "torch.sparse", "text": " \ntorch.sparse.log_softmax(input, dim, dtype=None) [source]\n \nApplies a softmax function followed by logarithm. See softmax for more details.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None    \n"}, {"name": "torch.sparse.mm()", "path": "sparse#torch.sparse.mm", "type": "torch.sparse", "text": " \ntorch.sparse.mm(mat1, mat2) [source]\n \nPerforms a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2. Similar to torch.mm(), If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, out will be a (n\u00d7p)(n \\times p)  tensor. mat1 need to have sparse_dim = 2. This function also supports backward for both matrices. Note that the gradients of mat1 is a coalesced sparse tensor.  Parameters \n \nmat1 (SparseTensor) \u2013 the first sparse matrix to be multiplied \nmat2 (Tensor) \u2013 the second matrix to be multiplied, which could be sparse or dense     Shape:\n\nThe format of the output tensor of this function follows: - sparse x sparse -> sparse - sparse x dense -> dense   Example: >>> a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n>>> a\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n                       [0, 1, 2, 0, 1, 2]]),\n       values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),\n       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n\n>>> b = torch.randn(3, 2, requires_grad=True)\n>>> b\ntensor([[-0.6479,  0.7874],\n        [-1.2056,  0.5641],\n        [-1.1716, -0.9923]], requires_grad=True)\n\n>>> y = torch.sparse.mm(a, b)\n>>> y\ntensor([[-0.3323,  1.8723],\n        [-1.8951,  0.7904]], grad_fn=<SparseAddmmBackward>)\n>>> y.sum().backward()\n>>> a.grad\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n                       [0, 1, 2, 0, 1, 2]]),\n       values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),\n       size=(2, 3), nnz=6, layout=torch.sparse_coo)\n \n"}, {"name": "torch.sparse.softmax()", "path": "sparse#torch.sparse.softmax", "type": "torch.sparse", "text": " \ntorch.sparse.softmax(input, dim, dtype=None) [source]\n \nApplies a softmax function. Softmax is defined as: Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}  where i,ji, j  run over sparse tensor indices and unspecified entries are ignores. This is equivalent to defining unspecified entries as negative infinity so that exp(xk)=0exp(x_k) = 0  when the entry with index kk  has not specified. It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.  Parameters \n \ninput (Tensor) \u2013 input \ndim (int) \u2013 A dimension along which softmax will be computed. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None    \n"}, {"name": "torch.sparse.sum()", "path": "sparse#torch.sparse.sum", "type": "torch.sparse", "text": " \ntorch.sparse.sum(input, dim=None, dtype=None) [source]\n \nReturns the sum of each row of the sparse tensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a dense tensor instead of a sparse tensor. All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input. During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.  Parameters \n \ninput (Tensor) \u2013 the input sparse tensor \ndim (int or tuple of python:ints) \u2013 a dimension or a list of dimensions to reduce. Default: reduce over all dims. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: dtype of input.    Example: >>> nnz = 3\n>>> dims = [5, 5, 2, 3]\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n>>> V = torch.randn(nnz, dims[2], dims[3])\n>>> size = torch.Size(dims)\n>>> S = torch.sparse_coo_tensor(I, V, size)\n>>> S\ntensor(indices=tensor([[2, 0, 3],\n                       [2, 4, 1]]),\n       values=tensor([[[-0.6438, -1.6467,  1.4004],\n                       [ 0.3411,  0.0918, -0.2312]],\n\n                      [[ 0.5348,  0.0634, -2.0494],\n                       [-0.7125, -1.0646,  2.1844]],\n\n                      [[ 0.1276,  0.1874, -0.6334],\n                       [-1.9682, -0.5340,  0.7483]]]),\n       size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n\n# when sum over only part of sparse_dims, return a sparse tensor\n>>> torch.sparse.sum(S, [1, 3])\ntensor(indices=tensor([[0, 2, 3]]),\n       values=tensor([[-1.4512,  0.4073],\n                      [-0.8901,  0.2017],\n                      [-0.3183, -1.7539]]),\n       size=(5, 2), nnz=3, layout=torch.sparse_coo)\n\n# when sum over all sparse dim, return a dense tensor\n# with summed dims squeezed\n>>> torch.sparse.sum(S, [0, 1, 3])\ntensor([-2.6596, -1.1450])\n \n"}, {"name": "torch.sparse_coo_tensor()", "path": "generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor", "type": "torch", "text": " \ntorch.sparse_coo_tensor(indices, values, size=None, *, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nConstructs a sparse tensor in COO(rdinate) format with specified values at the given indices.  Note This function returns an uncoalesced tensor.   Parameters \n \nindices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values. \nvalues (array_like) \u2013 Initial values for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. \nsize (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from values. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> i = torch.tensor([[0, 1, 1],\n...                   [2, 0, 2]])\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\n...                         dtype=torch.float64,\n...                         device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\n \n"}, {"name": "torch.split()", "path": "generated/torch.split#torch.split", "type": "torch", "text": " \ntorch.split(tensor, split_size_or_sections, dim=0) [source]\n \nSplits the tensor into chunks. Each chunk is a view of the original tensor. If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size. If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.  Parameters \n \ntensor (Tensor) \u2013 tensor to split. \nsplit_size_or_sections (int) or (list(int)) \u2013 size of a single chunk or list of sizes for each chunk \ndim (int) \u2013 dimension along which to split the tensor.     Example::\n\n>>> a = torch.arange(10).reshape(5,2)\n>>> a\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n>>> torch.split(a, 2)\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n>>> torch.split(a, [1,4])\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))\n   \n"}, {"name": "torch.sqrt()", "path": "generated/torch.sqrt#torch.sqrt", "type": "torch", "text": " \ntorch.sqrt(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the square-root of the elements of input.  outi=inputi\\text{out}_{i} = \\sqrt{\\text{input}_{i}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])\n \n"}, {"name": "torch.square()", "path": "generated/torch.square#torch.square", "type": "torch", "text": " \ntorch.square(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the square of the elements of input.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.square(a)\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])\n \n"}, {"name": "torch.squeeze()", "path": "generated/torch.squeeze#torch.squeeze", "type": "torch", "text": " \ntorch.squeeze(input, dim=None, *, out=None) \u2192 Tensor  \nReturns a tensor with all the dimensions of input of size 1 removed. For example, if input is of shape: (A\u00d71\u00d7B\u00d7C\u00d71\u00d7D)(A \\times 1 \\times B \\times C \\times 1 \\times D)  then the out tensor will be of shape: (A\u00d7B\u00d7C\u00d7D)(A \\times B \\times C \\times D) . When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B) , squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A\u00d7B)(A \\times B) .  Note The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.   Warning If the tensor has a batch dimension of size 1, then squeeze(input) will also remove the batch dimension, which can lead to unexpected errors.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int, optional) \u2013 if given, the input will be squeezed only in this dimension   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.zeros(2, 1, 2, 1, 2)\n>>> x.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x)\n>>> y.size()\ntorch.Size([2, 2, 2])\n>>> y = torch.squeeze(x, 0)\n>>> y.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x, 1)\n>>> y.size()\ntorch.Size([2, 2, 1, 2])\n \n"}, {"name": "torch.sspaddmm()", "path": "sparse#torch.sspaddmm", "type": "torch.sparse", "text": " \ntorch.sspaddmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nMatrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result. Note: This function is equivalent to torch.addmm(), except input and mat1 are sparse.  Parameters \n \ninput (Tensor) \u2013 a sparse matrix to be added \nmat1 (Tensor) \u2013 a sparse matrix to be matrix multiplied \nmat2 (Tensor) \u2013 a dense matrix to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for mat (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    \n"}, {"name": "torch.stack()", "path": "generated/torch.stack#torch.stack", "type": "torch", "text": " \ntorch.stack(tensors, dim=0, *, out=None) \u2192 Tensor  \nConcatenates a sequence of tensors along a new dimension. All tensors need to be of the same size.  Parameters \n \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate \ndim (int) \u2013 dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   \n"}, {"name": "torch.std()", "path": "generated/torch.std#torch.std", "type": "torch", "text": " \ntorch.std(input, unbiased=True) \u2192 Tensor  \nReturns the standard-deviation of all elements in the input tensor. If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std(a)\ntensor(0.5130)\n  \ntorch.std(input, dim, unbiased=True, keepdim=False, *, out=None) \u2192 Tensor \n Returns the standard-deviation of each row of the input tensor in the dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.2035,  1.2959,  1.8101, -0.4644],\n        [ 1.5027, -0.3270,  0.5905,  0.6538],\n        [-1.5745,  1.3330, -0.5596, -0.6548],\n        [ 0.1264, -0.5080,  1.6420,  0.1992]])\n>>> torch.std(a, dim=1)\ntensor([ 1.0311,  0.7477,  1.2204,  0.9087])\n \n"}, {"name": "torch.std_mean()", "path": "generated/torch.std_mean#torch.std_mean", "type": "torch", "text": " \ntorch.std_mean(input, unbiased=True) -> (Tensor, Tensor)  \nReturns the standard-deviation and mean of all elements in the input tensor. If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[0.3364, 0.3591, 0.9462]])\n>>> torch.std_mean(a)\n(tensor(0.3457), tensor(0.5472))\n  \ntorch.std_mean(input, dim, unbiased=True, keepdim=False) -> (Tensor, Tensor) \n Returns the standard-deviation and mean of each row of the input tensor in the dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.5648, -0.5984, -1.2676, -1.4471],\n        [ 0.9267,  1.0612,  1.1050, -0.6014],\n        [ 0.0154,  1.9301,  0.0125, -1.0904],\n        [-1.9711, -0.7748, -1.3840,  0.5067]])\n>>> torch.std_mean(a, 1)\n(tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))\n \n"}, {"name": "torch.stft()", "path": "generated/torch.stft#torch.stft", "type": "torch", "text": " \ntorch.stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=None, return_complex=None) [source]\n \nShort-time Fourier transform (STFT).  Warning From version 1.8.0, return_complex must always be given explicitly for real inputs and return_complex=False has been deprecated. Strongly prefer return_complex=True as in a future pytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real tensor with an extra last dimension for real and imaginary components.  The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following expression:  X[m,\u03c9]=\u2211k=0win_length-1window[k] input[m\u00d7hop_length+k]exp\u2061(\u2212j2\u03c0\u22c5\u03c9kwin_length),X[m, \\omega] = \\sum_{k = 0}^{\\text{win\\_length-1}}% \\text{window}[k]\\ \\text{input}[m \\times \\text{hop\\_length} + k]\\ % \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{win\\_length}}\\right),  \nwhere mm  is the index of the sliding window, and \u03c9\\omega  is the frequency that 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft} . When onesided is the default value True,  \ninput must be either a 1-D time sequence or a 2-D batch of time sequences. If hop_length is None (default), it is treated as equal to floor(n_fft / 4). If win_length is None (default), it is treated as equal to n_fft. \nwindow can be a 1-D tensor of size win_length, e.g., from torch.hann_window(). If window is None (default), it is treated as if having 11  everywhere in the window. If win_length<n_fft\\text{win\\_length} < \\text{n\\_fft} , window will be padded on both sides to length n_fft before being applied. If center is True (default), input will be padded on both sides so that the tt -th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length} . Otherwise, the tt -th frame begins at time t\u00d7hop_lengtht \\times \\text{hop\\_length} . \npad_mode determines the padding method used on input when center is True. See torch.nn.functional.pad() for all available options. Default is \"reflect\". If onesided is True (default for real input), only values for \u03c9\\omega  in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor \\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right]  are returned because the real-to-complex Fourier transform satisfies the conjugate symmetry, i.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^* . Note if the input or window tensors are complex, then onesided output is not possible. If normalized is True (default is False), the function returns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5} . If return_complex is True (default if input is complex), the return is a input.dim() + 1 dimensional complex tensor. If False, the output is a input.dim() + 2 dimensional real tensor where the last dimension represents the real and imaginary components.  Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)  if return_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N \\times T \\times 2) . Where \u2217*  is the optional batch size of input, NN  is the number of frequencies where STFT is applied and TT  is the total number of frames used.  Warning This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nn_fft (int) \u2013 size of Fourier transform \nhop_length (int, optional) \u2013 the distance between neighboring sliding window frames. Default: None (treated as equal to floor(n_fft / 4)) \nwin_length (int, optional) \u2013 the size of window frame and STFT filter. Default: None (treated as equal to n_fft) \nwindow (Tensor, optional) \u2013 the optional window function. Default: None (treated as window of all 11  s) \ncenter (bool, optional) \u2013 whether to pad input on both sides so that the tt -th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length} . Default: True\n \npad_mode (string, optional) \u2013 controls the padding method used when center is True. Default: \"reflect\"\n \nnormalized (bool, optional) \u2013 controls whether to return the normalized STFT results Default: False\n \nonesided (bool, optional) \u2013 controls whether to return half of results to avoid redundancy for real inputs. Default: True for real input and window, False otherwise. \nreturn_complex (bool, optional) \u2013 whether to return a complex tensor, or a real tensor with an extra last dimension for the real and imaginary components.   Returns \nA tensor containing the STFT result with shape described above  Return type \nTensor   \n"}, {"name": "torch.Storage", "path": "storage", "type": "torch.Storage", "text": "torch.Storage A torch.Storage is a contiguous, one-dimensional array of a single data type. Every torch.Tensor has a corresponding storage of the same data type.  \nclass torch.FloatStorage(*args, **kwargs) [source]\n \n \nbfloat16()  \nCasts this storage to bfloat16 type \n  \nbool()  \nCasts this storage to bool type \n  \nbyte()  \nCasts this storage to byte type \n  \nchar()  \nCasts this storage to char type \n  \nclone()  \nReturns a copy of this storage \n  \ncomplex_double()  \nCasts this storage to complex double type \n  \ncomplex_float()  \nCasts this storage to complex float type \n  \ncopy_() \n  \ncpu()  \nReturns a CPU copy of this storage if it\u2019s not already on the CPU \n  \ncuda(device=None, non_blocking=False, **kwargs)  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (int) \u2013 The destination GPU id. Defaults to the current device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.    \n  \ndata_ptr() \n  \ndevice \n  \ndouble()  \nCasts this storage to double type \n  \ndtype \n  \nelement_size() \n  \nfill_() \n  \nfloat()  \nCasts this storage to float type \n  \nstatic from_buffer() \n  \nstatic from_file(filename, shared=False, size=0) \u2192 Storage  \nIf shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters \n \nfilename (str) \u2013 file name to map \nshared (bool) \u2013 whether to share memory \nsize (int) \u2013 number of elements in the storage    \n  \nget_device() \n  \nhalf()  \nCasts this storage to half type \n  \nint()  \nCasts this storage to int type \n  \nis_cuda: bool = False \n  \nis_pinned() \n  \nis_shared() \n  \nis_sparse: bool = False \n  \nlong()  \nCasts this storage to long type \n  \nnew() \n  \npin_memory()  \nCopies the storage to pinned memory, if it\u2019s not already pinned. \n  \nresize_() \n  \nshare_memory_()  \nMoves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self \n  \nshort()  \nCasts this storage to short type \n  \nsize() \n  \ntolist()  \nReturns a list containing the elements of this storage \n  \ntype(dtype=None, non_blocking=False, **kwargs)  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n \n\n"}, {"name": "torch.sub()", "path": "generated/torch.sub#torch.sub", "type": "torch", "text": " \ntorch.sub(input, other, *, alpha=1, out=None) \u2192 Tensor  \nSubtracts other, scaled by alpha, from input.  outi=inputi\u2212alpha\u00d7otheri\\text{{out}}_i = \\text{{input}}_i - \\text{{alpha}} \\times \\text{{other}}_i  \nSupports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor or Scalar) \u2013 the tensor or scalar to subtract from input\n   Keyword Arguments \n \nalpha (Scalar) \u2013 the scalar multiplier for other\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.tensor((1, 2))\n>>> b = torch.tensor((0, 1))\n>>> torch.sub(a, b, alpha=2)\ntensor([1, 0])\n \n"}, {"name": "torch.subtract()", "path": "generated/torch.subtract#torch.subtract", "type": "torch", "text": " \ntorch.subtract(input, other, *, alpha=1, out=None) \u2192 Tensor  \nAlias for torch.sub(). \n"}, {"name": "torch.sum()", "path": "generated/torch.sum#torch.sum", "type": "torch", "text": " \ntorch.sum(input, *, dtype=None) \u2192 Tensor  \nReturns the sum of all elements in the input tensor.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.   Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n>>> torch.sum(a)\ntensor(-0.5475)\n  \ntorch.sum(input, dim, keepdim=False, *, dtype=None) \u2192 Tensor \n Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n        [-0.2993,  0.9138,  0.9337, -1.6864],\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n>>> torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n>>> torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])\n \n"}, {"name": "torch.svd()", "path": "generated/torch.svd#torch.svd", "type": "torch", "text": " \ntorch.svd(input, some=True, compute_uv=True, *, out=None) -> (Tensor, Tensor, Tensor)  \nComputes the singular value decomposition of either a matrix or batch of matrices input. The singular value decomposition is represented as a namedtuple (U,S,V), such that input = U diag(S) V\u1d34, where V\u1d34 is the transpose of V for the real-valued inputs, or the conjugate transpose of V for the complex-valued inputs. If input is a batch of tensors, then U, S, and V are also batched with the same batch dimensions as input. If some is True (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n, m) orthonormal columns. If compute_uv is False, the returned U and V will be zero-filled matrices of shape (m \u00d7 m) and (n \u00d7 n) respectively, and the same device as input. The some argument has no effect when compute_uv is False. Supports input of float, double, cfloat and cdouble data types. The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.  Warning torch.svd() is deprecated. Please use torch.linalg.svd() instead, which is similar to NumPy\u2019s numpy.linalg.svd.   Note Differences with torch.linalg.svd():  \nsome is the opposite of torch.linalg.svd()\u2019s full_matricies. Note that default value for both is True, so the default behavior is effectively the opposite. \ntorch.svd() returns V, whereas torch.linalg.svd() returns V\u1d34. If compute_uv=False, torch.svd() returns zero-filled tensors for U and Vh, whereas torch.linalg.svd() returns empty tensors.    Note The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.   Note The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the cuSOLVER routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and uses the MAGMA routine gesdd on earlier versions of CUDA.   Note The returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride().   Note Gradients computed using U and V may be unstable if input is not full rank or has non-unique singular values.   Note When some = False, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.   Note The S tensor can only be used to compute gradients if compute_uv is True.   Note With the complex-valued input the backward operation works correctly only for gauge invariant loss functions. Please look at Gauge problem in AD for more details.   Note Since U and V of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi}  while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different U and V tensors.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of (m \u00d7 n) matrices. \nsome (bool, optional) \u2013 controls whether to compute the reduced or full decomposition, and consequently the shape of returned U and V. Defaults to True. \ncompute_uv (bool, optional) \u2013 option whether to compute U and V or not. Defaults to True.   Keyword Arguments \nout (tuple, optional) \u2013 the output tuple of tensors   Example: >>> a = torch.randn(5, 3)\n>>> a\ntensor([[ 0.2364, -0.7752,  0.6372],\n        [ 1.7201,  0.7394, -0.0504],\n        [-0.3371, -1.0584,  0.5296],\n        [ 0.3550, -0.4022,  1.5569],\n        [ 0.2445, -0.0158,  1.1414]])\n>>> u, s, v = torch.svd(a)\n>>> u\ntensor([[ 0.4027,  0.0287,  0.5434],\n        [-0.1946,  0.8833,  0.3679],\n        [ 0.4296, -0.2890,  0.5261],\n        [ 0.6604,  0.2717, -0.2618],\n        [ 0.4234,  0.2481, -0.4733]])\n>>> s\ntensor([2.3289, 2.0315, 0.7806])\n>>> v\ntensor([[-0.0199,  0.8766,  0.4809],\n        [-0.5080,  0.4054, -0.7600],\n        [ 0.8611,  0.2594, -0.4373]])\n>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(8.6531e-07)\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, v = torch.svd(a_big)\n>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\ntensor(2.6503e-06)\n \n"}, {"name": "torch.svd_lowrank()", "path": "generated/torch.svd_lowrank#torch.svd_lowrank", "type": "torch", "text": " \ntorch.svd_lowrank(A, q=6, niter=2, M=None) [source]\n \nReturn the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix AA  such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T . In case MM  is given, then SVD is computed for the matrix A\u2212MA - M .  Note The implementation is based on the Algorithm 5.1 from Halko et al, 2009.   Note To obtain repeatable results, reset the seed for the pseudorandom number generator   Note The input is assumed to be a low-rank matrix.   Note In general, use the full-rank SVD implementation torch.svd for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that torch.svd cannot handle.   Args::\n\nA (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)  q (int, optional): a slightly overestimated rank of A.  niter (int, optional): the number of subspace iterations to\n\nconduct; niter must be a nonnegative integer, and defaults to 2  M (Tensor, optional): the input tensor\u2019s mean of size\n\n(\u2217,1,n)(*, 1, n) .    References::\n\n Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, arXiv:0909.4061 [math.NA; math.PR], 2009 (available at arXiv).    \n"}, {"name": "torch.swapaxes()", "path": "generated/torch.swapaxes#torch.swapaxes", "type": "torch", "text": " \ntorch.swapaxes(input, axis0, axis1) \u2192 Tensor  \nAlias for torch.transpose(). This function is equivalent to NumPy\u2019s swapaxes function. Examples: >>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])\n \n"}, {"name": "torch.swapdims()", "path": "generated/torch.swapdims#torch.swapdims", "type": "torch", "text": " \ntorch.swapdims(input, dim0, dim1) \u2192 Tensor  \nAlias for torch.transpose(). This function is equivalent to NumPy\u2019s swapaxes function. Examples: >>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])\n \n"}, {"name": "torch.symeig()", "path": "generated/torch.symeig#torch.symeig", "type": "torch", "text": " \ntorch.symeig(input, eigenvectors=False, upper=True, *, out=None) -> (Tensor, Tensor)  \nThis function returns eigenvalues and eigenvectors of a real symmetric matrix input or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) of input such that input=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^T . The boolean argument eigenvectors defines computation of both eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True, both eigenvalues and eigenvectors are computed. Since the input matrix input is supposed to be symmetric, only the upper triangular portion is used by default. If upper is False, then lower triangular portion is used.  Note The eigenvalues are returned in ascending order. If input is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.   Note Irrespective of the original strides, the returned matrix V will be transposed, i.e. with strides V.contiguous().transpose(-1, -2).stride().   Warning Extra care needs to be taken when backward through outputs. Such operation is only stable when all eigenvalues are distinct and becomes less stable the smaller min\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|  is.   Parameters \n \ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices. \neigenvectors (bool, optional) \u2013 controls whether eigenvectors have to be computed \nupper (boolean, optional) \u2013 controls whether to consider upper-triangular or lower-triangular region   Keyword Arguments \nout (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)  Returns \nA namedtuple (eigenvalues, eigenvectors) containing  \neigenvalues (Tensor): Shape (\u2217,m)(*, m) . The eigenvalues in ascending order. \neigenvectors (Tensor): Shape (\u2217,m,m)(*, m, m) . If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor contains the orthonormal eigenvectors of the input.   Return type \n(Tensor, Tensor)   Examples: >>> a = torch.randn(5, 5)\n>>> a = a + a.t()  # To make a symmetric\n>>> a\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\n>>> e, v = torch.symeig(a, eigenvectors=True)\n>>> e\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\n>>> v\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\n>>> a_big = torch.randn(5, 2, 2)\n>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\n>>> e, v = a_big.symeig(eigenvectors=True)\n>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\nTrue\n \n"}, {"name": "torch.t()", "path": "generated/torch.t#torch.t", "type": "torch", "text": " \ntorch.t(input) \u2192 Tensor  \nExpects input to be <= 2-D tensor and transposes dimensions 0 and 1. 0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).  Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> x = torch.randn(())\n>>> x\ntensor(0.1995)\n>>> torch.t(x)\ntensor(0.1995)\n>>> x = torch.randn(3)\n>>> x\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> torch.t(x)\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.4875,  0.9158, -0.5872],\n        [ 0.3938, -0.6929,  0.6932]])\n>>> torch.t(x)\ntensor([[ 0.4875,  0.3938],\n        [ 0.9158, -0.6929],\n        [-0.5872,  0.6932]])\n \n"}, {"name": "torch.take()", "path": "generated/torch.take#torch.take", "type": "torch", "text": " \ntorch.take(input, index) \u2192 Tensor  \nReturns a new tensor with the elements of input at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nindices (LongTensor) \u2013 the indices into tensor    Example: >>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])\n \n"}, {"name": "torch.tan()", "path": "generated/torch.tan#torch.tan", "type": "torch", "text": " \ntorch.tan(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the tangent of the elements of input.  outi=tan\u2061(inputi)\\text{out}_{i} = \\tan(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n>>> torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])\n \n"}, {"name": "torch.tanh()", "path": "generated/torch.tanh#torch.tanh", "type": "torch", "text": " \ntorch.tanh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the hyperbolic tangent of the elements of input.  outi=tanh\u2061(inputi)\\text{out}_{i} = \\tanh(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n>>> torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])\n \n"}, {"name": "torch.Tensor", "path": "tensors", "type": "torch.Tensor", "text": "torch.Tensor A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows:   \nData type dtype CPU tensor GPU tensor   \n32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor  \n64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor  \n16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor  \n16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor  \n32-bit complex torch.complex32    \n64-bit complex torch.complex64    \n128-bit complex torch.complex128 or torch.cdouble    \n8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor  \n8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor  \n16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor  \n32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor  \n64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor  \nBoolean torch.bool torch.BoolTensor torch.cuda.BoolTensor    \n1  \nSometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.  \n2  \nSometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as float32   torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor: >>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n  Warning torch.tensor() always copies data. If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach() to avoid a copy. If you have a numpy array and want to avoid a copy, use torch.as_tensor().  A tensor of specific data type can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor creation op: >>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n The contents of a tensor can be accessed and modified using Python\u2019s indexing and slicing notation: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n Use torch.Tensor.item() to get a Python number from a tensor containing a single value: >>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n A tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n Each tensor has an associated torch.Storage, which holds its data. The tensor class also provides multi-dimensional, strided view of a storage and defines numeric operations on it.  Note For more information on tensor views, see Tensor Views.   Note For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor, see Tensor Attributes.   Note Methods which mutate a tensor are marked with an underscore suffix. For example, torch.FloatTensor.abs_() computes the absolute value in-place and returns the modified tensor, while torch.FloatTensor.abs() computes the result in a new tensor.   Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using to() method on the tensor.   Warning Current implementation of torch.Tensor introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.   \nclass torch.Tensor  \nThere are a few main ways to create a tensor, depending on your use case.  To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation ops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor, use torch.*_like tensor creation ops (see Creation Ops). To create a tensor with similar type but different size as another tensor, use tensor.new_* creation ops.   \nnew_tensor(data, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Warning new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().   Warning When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.   Parameters \n \ndata (array_like) \u2013 The returned Tensor copies data. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.ones((2,), dtype=torch.int8)\n>>> data = [[0, 1], [2, 3]]\n>>> tensor.new_tensor(data)\ntensor([[ 0,  1],\n        [ 2,  3]], dtype=torch.int8)\n \n  \nnew_full(size, fill_value, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \nfill_value (scalar) \u2013 the number to fill the output tensor with. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.ones((2,), dtype=torch.float64)\n>>> tensor.new_full((3, 4), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n \n  \nnew_empty(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.ones(())\n>>> tensor.new_empty((2, 3))\ntensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n \n  \nnew_ones(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \nsize (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.tensor((), dtype=torch.int32)\n>>> tensor.new_ones((2, 3))\ntensor([[ 1,  1,  1],\n        [ 1,  1,  1]], dtype=torch.int32)\n \n  \nnew_zeros(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \nsize (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.tensor((), dtype=torch.float64)\n>>> tensor.new_zeros((2, 3))\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]], dtype=torch.float64)\n \n  \nis_cuda  \nIs True if the Tensor is stored on the GPU, False otherwise. \n  \nis_quantized  \nIs True if the Tensor is quantized, False otherwise. \n  \nis_meta  \nIs True if the Tensor is a meta tensor, False otherwise. Meta tensors are like normal tensors, but they carry no data. \n  \ndevice  \nIs the torch.device where this Tensor is. \n  \ngrad  \nThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it. \n  \nndim  \nAlias for dim() \n  \nT  \nIs this Tensor with its dimensions reversed. If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0). \n  \nreal  \nReturns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.  Warning real() is only supported for tensors with complex dtypes.   Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n   \n  \nimag  \nReturns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.  Warning imag() is only supported for tensors with complex dtypes.   Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n   \n  \nabs() \u2192 Tensor  \nSee torch.abs() \n  \nabs_() \u2192 Tensor  \nIn-place version of abs() \n  \nabsolute() \u2192 Tensor  \nAlias for abs() \n  \nabsolute_() \u2192 Tensor  \nIn-place version of absolute() Alias for abs_() \n  \nacos() \u2192 Tensor  \nSee torch.acos() \n  \nacos_() \u2192 Tensor  \nIn-place version of acos() \n  \narccos() \u2192 Tensor  \nSee torch.arccos() \n  \narccos_() \u2192 Tensor  \nIn-place version of arccos() \n  \nadd(other, *, alpha=1) \u2192 Tensor  \nAdd a scalar or tensor to self tensor. If both alpha and other are specified, each element of other is scaled by alpha before being used. When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor See torch.add() \n  \nadd_(other, *, alpha=1) \u2192 Tensor  \nIn-place version of add() \n  \naddbmm(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addbmm() \n  \naddbmm_(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addbmm() \n  \naddcdiv(tensor1, tensor2, *, value=1) \u2192 Tensor  \nSee torch.addcdiv() \n  \naddcdiv_(tensor1, tensor2, *, value=1) \u2192 Tensor  \nIn-place version of addcdiv() \n  \naddcmul(tensor1, tensor2, *, value=1) \u2192 Tensor  \nSee torch.addcmul() \n  \naddcmul_(tensor1, tensor2, *, value=1) \u2192 Tensor  \nIn-place version of addcmul() \n  \naddmm(mat1, mat2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addmm() \n  \naddmm_(mat1, mat2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addmm() \n  \nsspaddmm(mat1, mat2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.sspaddmm() \n  \naddmv(mat, vec, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addmv() \n  \naddmv_(mat, vec, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addmv() \n  \naddr(vec1, vec2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addr() \n  \naddr_(vec1, vec2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addr() \n  \nallclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor  \nSee torch.allclose() \n  \namax(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.amax() \n  \namin(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.amin() \n  \nangle() \u2192 Tensor  \nSee torch.angle() \n  \napply_(callable) \u2192 Tensor  \nApplies the function callable to each element in the tensor, replacing each element with the value returned by callable.  Note This function only works with CPU tensors and should not be used in code sections that require high performance.  \n  \nargmax(dim=None, keepdim=False) \u2192 LongTensor  \nSee torch.argmax() \n  \nargmin(dim=None, keepdim=False) \u2192 LongTensor  \nSee torch.argmin() \n  \nargsort(dim=-1, descending=False) \u2192 LongTensor  \nSee torch.argsort() \n  \nasin() \u2192 Tensor  \nSee torch.asin() \n  \nasin_() \u2192 Tensor  \nIn-place version of asin() \n  \narcsin() \u2192 Tensor  \nSee torch.arcsin() \n  \narcsin_() \u2192 Tensor  \nIn-place version of arcsin() \n  \nas_strided(size, stride, storage_offset=0) \u2192 Tensor  \nSee torch.as_strided() \n  \natan() \u2192 Tensor  \nSee torch.atan() \n  \natan_() \u2192 Tensor  \nIn-place version of atan() \n  \narctan() \u2192 Tensor  \nSee torch.arctan() \n  \narctan_() \u2192 Tensor  \nIn-place version of arctan() \n  \natan2(other) \u2192 Tensor  \nSee torch.atan2() \n  \natan2_(other) \u2192 Tensor  \nIn-place version of atan2() \n  \nall(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.all() \n  \nany(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.any() \n  \nbackward(gradient=None, retain_graph=None, create_graph=False, inputs=None) [source]\n \nComputes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ngradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n  \nbaddbmm(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.baddbmm() \n  \nbaddbmm_(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of baddbmm() \n  \nbernoulli(*, generator=None) \u2192 Tensor  \nReturns a result tensor where each result[i]\\texttt{result[i]}  is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . self must have floating point dtype, and the result will have the same dtype. See torch.bernoulli() \n  \nbernoulli_()  \n \nbernoulli_(p=0.5, *, generator=None) \u2192 Tensor  \nFills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}) . self can have integral dtype. \n  \nbernoulli_(p_tensor, *, generator=None) \u2192 Tensor  \np_tensor should be a tensor containing probabilities to be used for drawing the binary random number. The ith\\text{i}^{th}  element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]}) . self can have integral dtype, but p_tensor must have floating point dtype. \n See also bernoulli() and torch.bernoulli() \n  \nbfloat16(memory_format=torch.preserve_format) \u2192 Tensor  \nself.bfloat16() is equivalent to self.to(torch.bfloat16). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nbincount(weights=None, minlength=0) \u2192 Tensor  \nSee torch.bincount() \n  \nbitwise_not() \u2192 Tensor  \nSee torch.bitwise_not() \n  \nbitwise_not_() \u2192 Tensor  \nIn-place version of bitwise_not() \n  \nbitwise_and() \u2192 Tensor  \nSee torch.bitwise_and() \n  \nbitwise_and_() \u2192 Tensor  \nIn-place version of bitwise_and() \n  \nbitwise_or() \u2192 Tensor  \nSee torch.bitwise_or() \n  \nbitwise_or_() \u2192 Tensor  \nIn-place version of bitwise_or() \n  \nbitwise_xor() \u2192 Tensor  \nSee torch.bitwise_xor() \n  \nbitwise_xor_() \u2192 Tensor  \nIn-place version of bitwise_xor() \n  \nbmm(batch2) \u2192 Tensor  \nSee torch.bmm() \n  \nbool(memory_format=torch.preserve_format) \u2192 Tensor  \nself.bool() is equivalent to self.to(torch.bool). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nbyte(memory_format=torch.preserve_format) \u2192 Tensor  \nself.byte() is equivalent to self.to(torch.uint8). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nbroadcast_to(shape) \u2192 Tensor  \nSee torch.broadcast_to(). \n  \ncauchy_(median=0, sigma=1, *, generator=None) \u2192 Tensor  \nFills the tensor with numbers drawn from the Cauchy distribution:  f(x)=1\u03c0\u03c3(x\u2212median)2+\u03c32f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2} \n\n  \nceil() \u2192 Tensor  \nSee torch.ceil() \n  \nceil_() \u2192 Tensor  \nIn-place version of ceil() \n  \nchar(memory_format=torch.preserve_format) \u2192 Tensor  \nself.char() is equivalent to self.to(torch.int8). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \ncholesky(upper=False) \u2192 Tensor  \nSee torch.cholesky() \n  \ncholesky_inverse(upper=False) \u2192 Tensor  \nSee torch.cholesky_inverse() \n  \ncholesky_solve(input2, upper=False) \u2192 Tensor  \nSee torch.cholesky_solve() \n  \nchunk(chunks, dim=0) \u2192 List of Tensors  \nSee torch.chunk() \n  \nclamp(min, max) \u2192 Tensor  \nSee torch.clamp() \n  \nclamp_(min, max) \u2192 Tensor  \nIn-place version of clamp() \n  \nclip(min, max) \u2192 Tensor  \nAlias for clamp(). \n  \nclip_(min, max) \u2192 Tensor  \nAlias for clamp_(). \n  \nclone(*, memory_format=torch.preserve_format) \u2192 Tensor  \nSee torch.clone() \n  \ncontiguous(memory_format=torch.contiguous_format) \u2192 Tensor  \nReturns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format.   \n  \ncopy_(src, non_blocking=False) \u2192 Tensor  \nCopies the elements from src into self tensor and returns self. The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device.  Parameters \n \nsrc (Tensor) \u2013 the source tensor to copy from \nnon_blocking (bool) \u2013 if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.    \n  \nconj() \u2192 Tensor  \nSee torch.conj() \n  \ncopysign(other) \u2192 Tensor  \nSee torch.copysign() \n  \ncopysign_(other) \u2192 Tensor  \nIn-place version of copysign() \n  \ncos() \u2192 Tensor  \nSee torch.cos() \n  \ncos_() \u2192 Tensor  \nIn-place version of cos() \n  \ncosh() \u2192 Tensor  \nSee torch.cosh() \n  \ncosh_() \u2192 Tensor  \nIn-place version of cosh() \n  \ncount_nonzero(dim=None) \u2192 Tensor  \nSee torch.count_nonzero() \n  \nacosh() \u2192 Tensor  \nSee torch.acosh() \n  \nacosh_() \u2192 Tensor  \nIn-place version of acosh() \n  \narccosh()  \nacosh() -> Tensor See torch.arccosh() \n  \narccosh_()  \nacosh_() -> Tensor In-place version of arccosh() \n  \ncpu(memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of this object in CPU memory. If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \ncross(other, dim=-1) \u2192 Tensor  \nSee torch.cross() \n  \ncuda(device=None, non_blocking=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (torch.device) \u2013 The destination GPU device. Defaults to the current CUDA device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n  \nlogcumsumexp(dim) \u2192 Tensor  \nSee torch.logcumsumexp() \n  \ncummax(dim) -> (Tensor, Tensor)  \nSee torch.cummax() \n  \ncummin(dim) -> (Tensor, Tensor)  \nSee torch.cummin() \n  \ncumprod(dim, dtype=None) \u2192 Tensor  \nSee torch.cumprod() \n  \ncumprod_(dim, dtype=None) \u2192 Tensor  \nIn-place version of cumprod() \n  \ncumsum(dim, dtype=None) \u2192 Tensor  \nSee torch.cumsum() \n  \ncumsum_(dim, dtype=None) \u2192 Tensor  \nIn-place version of cumsum() \n  \ndata_ptr() \u2192 int  \nReturns the address of the first element of self tensor. \n  \ndeg2rad() \u2192 Tensor  \nSee torch.deg2rad() \n  \ndequantize() \u2192 Tensor  \nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor. \n  \ndet() \u2192 Tensor  \nSee torch.det() \n  \ndense_dim() \u2192 int  \nReturn the number of dense dimensions in a sparse tensor self.  Warning Throws an error if self is not a sparse tensor.  See also Tensor.sparse_dim() and hybrid tensors. \n  \ndetach()  \nReturns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.  \n  \ndetach_()  \nDetaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place. \n  \ndiag(diagonal=0) \u2192 Tensor  \nSee torch.diag() \n  \ndiag_embed(offset=0, dim1=-2, dim2=-1) \u2192 Tensor  \nSee torch.diag_embed() \n  \ndiagflat(offset=0) \u2192 Tensor  \nSee torch.diagflat() \n  \ndiagonal(offset=0, dim1=0, dim2=1) \u2192 Tensor  \nSee torch.diagonal() \n  \nfill_diagonal_(fill_value, wrap=False) \u2192 Tensor  \nFill the main diagonal of a tensor that has at least 2-dimensions. When dims>2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.  Parameters \n \nfill_value (Scalar) \u2013 the fill value \nwrap (bool) \u2013 the diagonal \u2018wrapped\u2019 after N columns for tall matrices.    Example: >>> a = torch.zeros(3, 3)\n>>> a.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n>>> b = torch.zeros(7, 3)\n>>> b.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n>>> c = torch.zeros(7, 3)\n>>> c.fill_diagonal_(5, wrap=True)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n \n  \nfmax(other) \u2192 Tensor  \nSee torch.fmax() \n  \nfmin(other) \u2192 Tensor  \nSee torch.fmin() \n  \ndiff(n=1, dim=-1, prepend=None, append=None) \u2192 Tensor  \nSee torch.diff() \n  \ndigamma() \u2192 Tensor  \nSee torch.digamma() \n  \ndigamma_() \u2192 Tensor  \nIn-place version of digamma() \n  \ndim() \u2192 int  \nReturns the number of dimensions of self tensor. \n  \ndist(other, p=2) \u2192 Tensor  \nSee torch.dist() \n  \ndiv(value, *, rounding_mode=None) \u2192 Tensor  \nSee torch.div() \n  \ndiv_(value, *, rounding_mode=None) \u2192 Tensor  \nIn-place version of div() \n  \ndivide(value, *, rounding_mode=None) \u2192 Tensor  \nSee torch.divide() \n  \ndivide_(value, *, rounding_mode=None) \u2192 Tensor  \nIn-place version of divide() \n  \ndot(other) \u2192 Tensor  \nSee torch.dot() \n  \ndouble(memory_format=torch.preserve_format) \u2192 Tensor  \nself.double() is equivalent to self.to(torch.float64). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \neig(eigenvectors=False) -> (Tensor, Tensor)  \nSee torch.eig() \n  \nelement_size() \u2192 int  \nReturns the size in bytes of an individual element. Example: >>> torch.tensor([]).element_size()\n4\n>>> torch.tensor([], dtype=torch.uint8).element_size()\n1\n \n  \neq(other) \u2192 Tensor  \nSee torch.eq() \n  \neq_(other) \u2192 Tensor  \nIn-place version of eq() \n  \nequal(other) \u2192 bool  \nSee torch.equal() \n  \nerf() \u2192 Tensor  \nSee torch.erf() \n  \nerf_() \u2192 Tensor  \nIn-place version of erf() \n  \nerfc() \u2192 Tensor  \nSee torch.erfc() \n  \nerfc_() \u2192 Tensor  \nIn-place version of erfc() \n  \nerfinv() \u2192 Tensor  \nSee torch.erfinv() \n  \nerfinv_() \u2192 Tensor  \nIn-place version of erfinv() \n  \nexp() \u2192 Tensor  \nSee torch.exp() \n  \nexp_() \u2192 Tensor  \nIn-place version of exp() \n  \nexpm1() \u2192 Tensor  \nSee torch.expm1() \n  \nexpm1_() \u2192 Tensor  \nIn-place version of expm1() \n  \nexpand(*sizes) \u2192 Tensor  \nReturns a new view of the self tensor with singleton dimensions expanded to a larger size. Passing -1 as the size for a dimension means not changing the size of that dimension. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.  Parameters \n*sizes (torch.Size or int...) \u2013 the desired expanded size    Warning More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: >>> x = torch.tensor([[1], [2], [3]])\n>>> x.size()\ntorch.Size([3, 1])\n>>> x.expand(3, 4)\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n>>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n \n  \nexpand_as(other) \u2192 Tensor  \nExpand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()). Please see expand() for more information about expand.  Parameters \nother (torch.Tensor) \u2013 The result tensor has the same size as other.   \n  \nexponential_(lambd=1, *, generator=None) \u2192 Tensor  \nFills self tensor with elements drawn from the exponential distribution:  f(x)=\u03bbe\u2212\u03bbxf(x) = \\lambda e^{-\\lambda x} \n\n  \nfix() \u2192 Tensor  \nSee torch.fix(). \n  \nfix_() \u2192 Tensor  \nIn-place version of fix() \n  \nfill_(value) \u2192 Tensor  \nFills self tensor with the specified value. \n  \nflatten(input, start_dim=0, end_dim=-1) \u2192 Tensor  \nsee torch.flatten() \n  \nflip(dims) \u2192 Tensor  \nSee torch.flip() \n  \nfliplr() \u2192 Tensor  \nSee torch.fliplr() \n  \nflipud() \u2192 Tensor  \nSee torch.flipud() \n  \nfloat(memory_format=torch.preserve_format) \u2192 Tensor  \nself.float() is equivalent to self.to(torch.float32). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nfloat_power(exponent) \u2192 Tensor  \nSee torch.float_power() \n  \nfloat_power_(exponent) \u2192 Tensor  \nIn-place version of float_power() \n  \nfloor() \u2192 Tensor  \nSee torch.floor() \n  \nfloor_() \u2192 Tensor  \nIn-place version of floor() \n  \nfloor_divide(value) \u2192 Tensor  \nSee torch.floor_divide() \n  \nfloor_divide_(value) \u2192 Tensor  \nIn-place version of floor_divide() \n  \nfmod(divisor) \u2192 Tensor  \nSee torch.fmod() \n  \nfmod_(divisor) \u2192 Tensor  \nIn-place version of fmod() \n  \nfrac() \u2192 Tensor  \nSee torch.frac() \n  \nfrac_() \u2192 Tensor  \nIn-place version of frac() \n  \ngather(dim, index) \u2192 Tensor  \nSee torch.gather() \n  \ngcd(other) \u2192 Tensor  \nSee torch.gcd() \n  \ngcd_(other) \u2192 Tensor  \nIn-place version of gcd() \n  \nge(other) \u2192 Tensor  \nSee torch.ge(). \n  \nge_(other) \u2192 Tensor  \nIn-place version of ge(). \n  \ngreater_equal(other) \u2192 Tensor  \nSee torch.greater_equal(). \n  \ngreater_equal_(other) \u2192 Tensor  \nIn-place version of greater_equal(). \n  \ngeometric_(p, *, generator=None) \u2192 Tensor  \nFills self tensor with elements drawn from the geometric distribution:  f(X=k)=pk\u22121(1\u2212p)f(X=k) = p^{k - 1} (1 - p) \n\n  \ngeqrf() -> (Tensor, Tensor)  \nSee torch.geqrf() \n  \nger(vec2) \u2192 Tensor  \nSee torch.ger() \n  \nget_device() -> Device ordinal (Integer)  \nFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown. Example: >>> x = torch.randn(3, 4, 5, device='cuda:0')\n>>> x.get_device()\n0\n>>> x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor\n \n  \ngt(other) \u2192 Tensor  \nSee torch.gt(). \n  \ngt_(other) \u2192 Tensor  \nIn-place version of gt(). \n  \ngreater(other) \u2192 Tensor  \nSee torch.greater(). \n  \ngreater_(other) \u2192 Tensor  \nIn-place version of greater(). \n  \nhalf(memory_format=torch.preserve_format) \u2192 Tensor  \nself.half() is equivalent to self.to(torch.float16). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nhardshrink(lambd=0.5) \u2192 Tensor  \nSee torch.nn.functional.hardshrink() \n  \nheaviside(values) \u2192 Tensor  \nSee torch.heaviside() \n  \nhistc(bins=100, min=0, max=0) \u2192 Tensor  \nSee torch.histc() \n  \nhypot(other) \u2192 Tensor  \nSee torch.hypot() \n  \nhypot_(other) \u2192 Tensor  \nIn-place version of hypot() \n  \ni0() \u2192 Tensor  \nSee torch.i0() \n  \ni0_() \u2192 Tensor  \nIn-place version of i0() \n  \nigamma(other) \u2192 Tensor  \nSee torch.igamma() \n  \nigamma_(other) \u2192 Tensor  \nIn-place version of igamma() \n  \nigammac(other) \u2192 Tensor  \nSee torch.igammac() \n  \nigammac_(other) \u2192 Tensor  \nIn-place version of igammac() \n  \nindex_add_(dim, index, tensor) \u2192 Tensor  \nAccumulate the elements of tensor into the self tensor by adding to the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is added to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Note This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ndim (int) \u2013 dimension along which to index \nindex (IntTensor or LongTensor) \u2013 indices of tensor to select from \ntensor (Tensor) \u2013 the tensor containing values to add    Example: >>> x = torch.ones(5, 3)\n>>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 4, 2])\n>>> x.index_add_(0, index, t)\ntensor([[  2.,   3.,   4.],\n        [  1.,   1.,   1.],\n        [  8.,   9.,  10.],\n        [  1.,   1.,   1.],\n        [  5.,   6.,   7.]])\n \n  \nindex_add(tensor1, dim, index, tensor2) \u2192 Tensor  \nOut-of-place version of torch.Tensor.index_add_(). tensor1 corresponds to self in torch.Tensor.index_add_(). \n  \nindex_copy_(dim, index, tensor) \u2192 Tensor  \nCopies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Note If index contains duplicate entries, multiple elements from tensor will be copied to the same index of self. The result is nondeterministic since it depends on which copy occurs last.   Parameters \n \ndim (int) \u2013 dimension along which to index \nindex (LongTensor) \u2013 indices of tensor to select from \ntensor (Tensor) \u2013 the tensor containing values to copy    Example: >>> x = torch.zeros(5, 3)\n>>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 4, 2])\n>>> x.index_copy_(0, index, t)\ntensor([[ 1.,  2.,  3.],\n        [ 0.,  0.,  0.],\n        [ 7.,  8.,  9.],\n        [ 0.,  0.,  0.],\n        [ 4.,  5.,  6.]])\n \n  \nindex_copy(tensor1, dim, index, tensor2) \u2192 Tensor  \nOut-of-place version of torch.Tensor.index_copy_(). tensor1 corresponds to self in torch.Tensor.index_copy_(). \n  \nindex_fill_(dim, index, val) \u2192 Tensor  \nFills the elements of the self tensor with value val by selecting the indices in the order given in index.  Parameters \n \ndim (int) \u2013 dimension along which to index \nindex (LongTensor) \u2013 indices of self tensor to fill in \nval (float) \u2013 the value to fill with     Example::\n\n>>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 2])\n>>> x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n        [-1.,  5., -1.],\n        [-1.,  8., -1.]])\n   \n  \nindex_fill(tensor1, dim, index, value) \u2192 Tensor  \nOut-of-place version of torch.Tensor.index_fill_(). tensor1 corresponds to self in torch.Tensor.index_fill_(). \n  \nindex_put_(indices, values, accumulate=False) \u2192 Tensor  \nPuts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, values) is equivalent to tensor[indices] = values. Returns self. If accumulate is True, the elements in values are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters \n \nindices (tuple of LongTensor) \u2013 tensors used to index into self. \nvalues (Tensor) \u2013 tensor of same dtype as self. \naccumulate (bool) \u2013 whether to accumulate into self    \n  \nindex_put(tensor1, indices, values, accumulate=False) \u2192 Tensor  \nOut-place version of index_put_(). tensor1 corresponds to self in torch.Tensor.index_put_(). \n  \nindex_select(dim, index) \u2192 Tensor  \nSee torch.index_select() \n  \nindices() \u2192 Tensor  \nReturn the indices tensor of a sparse COO tensor.  Warning Throws an error if self is not a sparse COO tensor.  See also Tensor.values().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  \n  \ninner(other) \u2192 Tensor  \nSee torch.inner(). \n  \nint(memory_format=torch.preserve_format) \u2192 Tensor  \nself.int() is equivalent to self.to(torch.int32). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nint_repr() \u2192 Tensor  \nGiven a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor. \n  \ninverse() \u2192 Tensor  \nSee torch.inverse() \n  \nisclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor  \nSee torch.isclose() \n  \nisfinite() \u2192 Tensor  \nSee torch.isfinite() \n  \nisinf() \u2192 Tensor  \nSee torch.isinf() \n  \nisposinf() \u2192 Tensor  \nSee torch.isposinf() \n  \nisneginf() \u2192 Tensor  \nSee torch.isneginf() \n  \nisnan() \u2192 Tensor  \nSee torch.isnan() \n  \nis_contiguous(memory_format=torch.contiguous_format) \u2192 bool  \nReturns True if self tensor is contiguous in memory in the order specified by memory format.  Parameters \nmemory_format (torch.memory_format, optional) \u2013 Specifies memory allocation order. Default: torch.contiguous_format.   \n  \nis_complex() \u2192 bool  \nReturns True if the data type of self is a complex data type. \n  \nis_floating_point() \u2192 bool  \nReturns True if the data type of self is a floating point data type. \n  \nis_leaf  \nAll Tensors that have requires_grad which is False will be leaf Tensors by convention. For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Example: >>> a = torch.rand(10, requires_grad=True)\n>>> a.is_leaf\nTrue\n>>> b = torch.rand(10, requires_grad=True).cuda()\n>>> b.is_leaf\nFalse\n# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n>>> c = torch.rand(10, requires_grad=True) + 2\n>>> c.is_leaf\nFalse\n# c was created by the addition operation\n>>> d = torch.rand(10).cuda()\n>>> d.is_leaf\nTrue\n# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n>>> e = torch.rand(10).cuda().requires_grad_()\n>>> e.is_leaf\nTrue\n# e requires gradients and has no operations creating it\n>>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n>>> f.is_leaf\nTrue\n# f requires grad, has no operation creating it\n \n  \nis_pinned()  \nReturns true if this tensor resides in pinned memory. \n  \nis_set_to(tensor) \u2192 bool  \nReturns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride). \n  \nis_shared() [source]\n \nChecks if tensor is in shared memory. This is always True for CUDA tensors. \n  \nis_signed() \u2192 bool  \nReturns True if the data type of self is a signed data type. \n  \nis_sparse  \nIs True if the Tensor uses sparse storage layout, False otherwise. \n  \nistft(n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False) [source]\n \nSee torch.istft() \n  \nisreal() \u2192 Tensor  \nSee torch.isreal() \n  \nitem() \u2192 number  \nReturns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist(). This operation is not differentiable. Example: >>> x = torch.tensor([1.0])\n>>> x.item()\n1.0\n \n  \nkthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.kthvalue() \n  \nlcm(other) \u2192 Tensor  \nSee torch.lcm() \n  \nlcm_(other) \u2192 Tensor  \nIn-place version of lcm() \n  \nldexp(other) \u2192 Tensor  \nSee torch.ldexp() \n  \nldexp_(other) \u2192 Tensor  \nIn-place version of ldexp() \n  \nle(other) \u2192 Tensor  \nSee torch.le(). \n  \nle_(other) \u2192 Tensor  \nIn-place version of le(). \n  \nless_equal(other) \u2192 Tensor  \nSee torch.less_equal(). \n  \nless_equal_(other) \u2192 Tensor  \nIn-place version of less_equal(). \n  \nlerp(end, weight) \u2192 Tensor  \nSee torch.lerp() \n  \nlerp_(end, weight) \u2192 Tensor  \nIn-place version of lerp() \n  \nlgamma() \u2192 Tensor  \nSee torch.lgamma() \n  \nlgamma_() \u2192 Tensor  \nIn-place version of lgamma() \n  \nlog() \u2192 Tensor  \nSee torch.log() \n  \nlog_() \u2192 Tensor  \nIn-place version of log() \n  \nlogdet() \u2192 Tensor  \nSee torch.logdet() \n  \nlog10() \u2192 Tensor  \nSee torch.log10() \n  \nlog10_() \u2192 Tensor  \nIn-place version of log10() \n  \nlog1p() \u2192 Tensor  \nSee torch.log1p() \n  \nlog1p_() \u2192 Tensor  \nIn-place version of log1p() \n  \nlog2() \u2192 Tensor  \nSee torch.log2() \n  \nlog2_() \u2192 Tensor  \nIn-place version of log2() \n  \nlog_normal_(mean=1, std=2, *, generator=None)  \nFills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu  and standard deviation \u03c3\\sigma . Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:  f(x)=1x\u03c32\u03c0e\u2212(ln\u2061x\u2212\u03bc)22\u03c32f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \n\n  \nlogaddexp(other) \u2192 Tensor  \nSee torch.logaddexp() \n  \nlogaddexp2(other) \u2192 Tensor  \nSee torch.logaddexp2() \n  \nlogsumexp(dim, keepdim=False) \u2192 Tensor  \nSee torch.logsumexp() \n  \nlogical_and() \u2192 Tensor  \nSee torch.logical_and() \n  \nlogical_and_() \u2192 Tensor  \nIn-place version of logical_and() \n  \nlogical_not() \u2192 Tensor  \nSee torch.logical_not() \n  \nlogical_not_() \u2192 Tensor  \nIn-place version of logical_not() \n  \nlogical_or() \u2192 Tensor  \nSee torch.logical_or() \n  \nlogical_or_() \u2192 Tensor  \nIn-place version of logical_or() \n  \nlogical_xor() \u2192 Tensor  \nSee torch.logical_xor() \n  \nlogical_xor_() \u2192 Tensor  \nIn-place version of logical_xor() \n  \nlogit() \u2192 Tensor  \nSee torch.logit() \n  \nlogit_() \u2192 Tensor  \nIn-place version of logit() \n  \nlong(memory_format=torch.preserve_format) \u2192 Tensor  \nself.long() is equivalent to self.to(torch.int64). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nlstsq(A) -> (Tensor, Tensor)  \nSee torch.lstsq() \n  \nlt(other) \u2192 Tensor  \nSee torch.lt(). \n  \nlt_(other) \u2192 Tensor  \nIn-place version of lt(). \n  \nless()  \nlt(other) -> Tensor See torch.less(). \n  \nless_(other) \u2192 Tensor  \nIn-place version of less(). \n  \nlu(pivot=True, get_infos=False) [source]\n \nSee torch.lu() \n  \nlu_solve(LU_data, LU_pivots) \u2192 Tensor  \nSee torch.lu_solve() \n  \nas_subclass(cls) \u2192 Tensor  \nMakes a cls instance with the same data pointer as self. Changes in the output mirror changes in self, and the output stays attached to the autograd graph. cls must be a subclass of Tensor. \n  \nmap_(tensor, callable)  \nApplies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable. The callable should have the signature: def callable(a, b) -> number\n \n  \nmasked_scatter_(mask, source)  \nCopies elements from source into self tensor at positions where the mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask  Parameters \n \nmask (BoolTensor) \u2013 the boolean mask \nsource (Tensor) \u2013 the tensor to copy from     Note The mask operates on the self tensor, not on the given source tensor.  \n  \nmasked_scatter(mask, tensor) \u2192 Tensor  \nOut-of-place version of torch.Tensor.masked_scatter_() \n  \nmasked_fill_(mask, value)  \nFills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor.  Parameters \n \nmask (BoolTensor) \u2013 the boolean mask \nvalue (float) \u2013 the value to fill in with    \n  \nmasked_fill(mask, value) \u2192 Tensor  \nOut-of-place version of torch.Tensor.masked_fill_() \n  \nmasked_select(mask) \u2192 Tensor  \nSee torch.masked_select() \n  \nmatmul(tensor2) \u2192 Tensor  \nSee torch.matmul() \n  \nmatrix_power(n) \u2192 Tensor  \nSee torch.matrix_power() \n  \nmatrix_exp() \u2192 Tensor  \nSee torch.matrix_exp() \n  \nmax(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.max() \n  \nmaximum(other) \u2192 Tensor  \nSee torch.maximum() \n  \nmean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.mean() \n  \nmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.median() \n  \nnanmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.nanmedian() \n  \nmin(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.min() \n  \nminimum(other) \u2192 Tensor  \nSee torch.minimum() \n  \nmm(mat2) \u2192 Tensor  \nSee torch.mm() \n  \nsmm(mat) \u2192 Tensor  \nSee torch.smm() \n  \nmode(dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.mode() \n  \nmovedim(source, destination) \u2192 Tensor  \nSee torch.movedim() \n  \nmoveaxis(source, destination) \u2192 Tensor  \nSee torch.moveaxis() \n  \nmsort() \u2192 Tensor  \nSee torch.msort() \n  \nmul(value) \u2192 Tensor  \nSee torch.mul(). \n  \nmul_(value) \u2192 Tensor  \nIn-place version of mul(). \n  \nmultiply(value) \u2192 Tensor  \nSee torch.multiply(). \n  \nmultiply_(value) \u2192 Tensor  \nIn-place version of multiply(). \n  \nmultinomial(num_samples, replacement=False, *, generator=None) \u2192 Tensor  \nSee torch.multinomial() \n  \nmv(vec) \u2192 Tensor  \nSee torch.mv() \n  \nmvlgamma(p) \u2192 Tensor  \nSee torch.mvlgamma() \n  \nmvlgamma_(p) \u2192 Tensor  \nIn-place version of mvlgamma() \n  \nnansum(dim=None, keepdim=False, dtype=None) \u2192 Tensor  \nSee torch.nansum() \n  \nnarrow(dimension, start, length) \u2192 Tensor  \nSee torch.narrow() Example: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> x.narrow(0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> x.narrow(1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n \n  \nnarrow_copy(dimension, start, length) \u2192 Tensor  \nSame as Tensor.narrow() except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling `narrow_copy with `dimemsion > self.sparse_dim()` will return a copy with the relevant dense dimension narrowed, and `self.shape` updated accordingly. \n  \nndimension() \u2192 int  \nAlias for dim() \n  \nnan_to_num(nan=0.0, posinf=None, neginf=None) \u2192 Tensor  \nSee torch.nan_to_num(). \n  \nnan_to_num_(nan=0.0, posinf=None, neginf=None) \u2192 Tensor  \nIn-place version of nan_to_num(). \n  \nne(other) \u2192 Tensor  \nSee torch.ne(). \n  \nne_(other) \u2192 Tensor  \nIn-place version of ne(). \n  \nnot_equal(other) \u2192 Tensor  \nSee torch.not_equal(). \n  \nnot_equal_(other) \u2192 Tensor  \nIn-place version of not_equal(). \n  \nneg() \u2192 Tensor  \nSee torch.neg() \n  \nneg_() \u2192 Tensor  \nIn-place version of neg() \n  \nnegative() \u2192 Tensor  \nSee torch.negative() \n  \nnegative_() \u2192 Tensor  \nIn-place version of negative() \n  \nnelement() \u2192 int  \nAlias for numel() \n  \nnextafter(other) \u2192 Tensor  \nSee torch.nextafter() \n  \nnextafter_(other) \u2192 Tensor  \nIn-place version of nextafter() \n  \nnonzero() \u2192 LongTensor  \nSee torch.nonzero() \n  \nnorm(p='fro', dim=None, keepdim=False, dtype=None) [source]\n \nSee torch.norm() \n  \nnormal_(mean=0, std=1, *, generator=None) \u2192 Tensor  \nFills self tensor with elements samples from the normal distribution parameterized by mean and std. \n  \nnumel() \u2192 int  \nSee torch.numel() \n  \nnumpy() \u2192 numpy.ndarray  \nReturns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage. Changes to self tensor will be reflected in the ndarray and vice versa. \n  \norgqr(input2) \u2192 Tensor  \nSee torch.orgqr() \n  \normqr(input2, input3, left=True, transpose=False) \u2192 Tensor  \nSee torch.ormqr() \n  \nouter(vec2) \u2192 Tensor  \nSee torch.outer(). \n  \npermute(*dims) \u2192 Tensor  \nReturns a view of the original tensor with its dimensions permuted.  Parameters \n*dims (int...) \u2013 The desired ordering of dimensions   Example >>> x = torch.randn(2, 3, 5)\n>>> x.size()\ntorch.Size([2, 3, 5])\n>>> x.permute(2, 0, 1).size()\ntorch.Size([5, 2, 3])\n \n  \npin_memory() \u2192 Tensor  \nCopies the tensor to pinned memory, if it\u2019s not already pinned. \n  \npinverse() \u2192 Tensor  \nSee torch.pinverse() \n  \npolygamma(n) \u2192 Tensor  \nSee torch.polygamma() \n  \npolygamma_(n) \u2192 Tensor  \nIn-place version of polygamma() \n  \npow(exponent) \u2192 Tensor  \nSee torch.pow() \n  \npow_(exponent) \u2192 Tensor  \nIn-place version of pow() \n  \nprod(dim=None, keepdim=False, dtype=None) \u2192 Tensor  \nSee torch.prod() \n  \nput_(indices, tensor, accumulate=False) \u2192 Tensor  \nCopies the elements from tensor into the positions specified by indices. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor. If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters \n \nindices (LongTensor) \u2013 the indices into self \ntensor (Tensor) \u2013 the tensor containing values to copy from \naccumulate (bool) \u2013 whether to accumulate into self    Example: >>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\ntensor([[  4,   9,   5],\n        [ 10,   7,   8]])\n \n  \nqr(some=True) -> (Tensor, Tensor)  \nSee torch.qr() \n  \nqscheme() \u2192 torch.qscheme  \nReturns the quantization scheme of a given QTensor. \n  \nquantile(q, dim=None, keepdim=False) \u2192 Tensor  \nSee torch.quantile() \n  \nnanquantile(q, dim=None, keepdim=False) \u2192 Tensor  \nSee torch.nanquantile() \n  \nq_scale() \u2192 float  \nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer(). \n  \nq_zero_point() \u2192 int  \nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer(). \n  \nq_per_channel_scales() \u2192 Tensor  \nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor. \n  \nq_per_channel_zero_points() \u2192 Tensor  \nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor. \n  \nq_per_channel_axis() \u2192 int  \nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied. \n  \nrad2deg() \u2192 Tensor  \nSee torch.rad2deg() \n  \nrandom_(from=0, to=None, *, generator=None) \u2192 Tensor  \nFills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53]. \n  \nravel(input) \u2192 Tensor  \nsee torch.ravel() \n  \nreciprocal() \u2192 Tensor  \nSee torch.reciprocal() \n  \nreciprocal_() \u2192 Tensor  \nIn-place version of reciprocal() \n  \nrecord_stream(stream)  \nEnsures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.  Note The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.  \n  \nregister_hook(hook) [source]\n \nRegisters a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -> Tensor or None\n The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n>>> v.backward(torch.tensor([1., 2., 3.]))\n>>> v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n>>> h.remove()  # removes the hook\n \n  \nremainder(divisor) \u2192 Tensor  \nSee torch.remainder() \n  \nremainder_(divisor) \u2192 Tensor  \nIn-place version of remainder() \n  \nrenorm(p, dim, maxnorm) \u2192 Tensor  \nSee torch.renorm() \n  \nrenorm_(p, dim, maxnorm) \u2192 Tensor  \nIn-place version of renorm() \n  \nrepeat(*sizes) \u2192 Tensor  \nRepeats this tensor along the specified dimensions. Unlike expand(), this function copies the tensor\u2019s data.  Warning repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().   Parameters \nsizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along each dimension   Example: >>> x = torch.tensor([1, 2, 3])\n>>> x.repeat(4, 2)\ntensor([[ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3]])\n>>> x.repeat(4, 2, 1).size()\ntorch.Size([4, 2, 3])\n \n  \nrepeat_interleave(repeats, dim=None) \u2192 Tensor  \nSee torch.repeat_interleave(). \n  \nrequires_grad  \nIs True if gradients need to be computed for this Tensor, False otherwise.  Note The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.  \n  \nrequires_grad_(requires_grad=True) \u2192 Tensor  \nChange if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor. requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.  Parameters \nrequires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.   Example: >>> # Let's say we want to preprocess some saved weights and use\n>>> # the result as new weights.\n>>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n>>> loaded_weights = torch.tensor(saved_weights)\n>>> weights = preprocess(loaded_weights)  # some function\n>>> weights\ntensor([-0.5503,  0.4926, -2.1158, -0.8303])\n\n>>> # Now, start to record operations done to weights\n>>> weights.requires_grad_()\n>>> out = weights.pow(2).sum()\n>>> out.backward()\n>>> weights.grad\ntensor([-1.1007,  0.9853, -4.2316, -1.6606])\n \n  \nreshape(*shape) \u2192 Tensor  \nReturns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. See torch.reshape()  Parameters \nshape (tuple of python:ints or int...) \u2013 the desired shape   \n  \nreshape_as(other) \u2192 Tensor  \nReturns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. Please see reshape() for more information about reshape.  Parameters \nother (torch.Tensor) \u2013 The result tensor has the same shape as other.   \n  \nresize_(*sizes, memory_format=torch.contiguous_format) \u2192 Tensor  \nResizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.  Warning This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().   Parameters \n \nsizes (torch.Size or int...) \u2013 the desired size \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches sizes.    Example: >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n>>> x.resize_(2, 2)\ntensor([[ 1,  2],\n        [ 3,  4]])\n \n  \nresize_as_(tensor, memory_format=torch.contiguous_format) \u2192 Tensor  \nResizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches tensor.size().   \n  \nretain_grad() [source]\n \nEnables .grad attribute for non-leaf Tensors. \n  \nroll(shifts, dims) \u2192 Tensor  \nSee torch.roll() \n  \nrot90(k, dims) \u2192 Tensor  \nSee torch.rot90() \n  \nround() \u2192 Tensor  \nSee torch.round() \n  \nround_() \u2192 Tensor  \nIn-place version of round() \n  \nrsqrt() \u2192 Tensor  \nSee torch.rsqrt() \n  \nrsqrt_() \u2192 Tensor  \nIn-place version of rsqrt() \n  \nscatter(dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_() \n  \nscatter_(dim, index, src, reduce=None) \u2192 Tensor  \nWrites all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n This is the reverse operation of the manner described in gather(). self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast. Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.  Warning When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!   Note The backward pass is implemented only for src.shape == index.shape.  Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indicies specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. Given a 3-D tensor and reduction using the multiplication operation, self is updated as: self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n Reducing with the addition operation is the same as using scatter_add_().  Parameters \n \ndim (int) \u2013 the axis along which to index \nindex (LongTensor) \u2013 the indices of elements to scatter, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged. \nsrc (Tensor or float) \u2013 the source element(s) to scatter. \nreduce (str, optional) \u2013 reduction operation to apply, can be either 'add' or 'multiply'.    Example: >>> src = torch.arange(1, 11).reshape((2, 5))\n>>> src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n>>> index = torch.tensor([[0, 1, 2, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])\n \n  \nscatter_add_(dim, index, src) \u2192 Tensor  \nAdds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in src, it is added to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.  Note This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.   Note The backward pass is implemented only for src.shape == index.shape.   Parameters \n \ndim (int) \u2013 the axis along which to index \nindex (LongTensor) \u2013 the indices of elements to scatter and add, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged. \nsrc (Tensor) \u2013 the source elements to scatter and add    Example: >>> src = torch.ones((2, 5))\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])\n \n  \nscatter_add(dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_add_() \n  \nselect(dim, index) \u2192 Tensor  \nSlices the self tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.  Parameters \n \ndim (int) \u2013 the dimension to slice \nindex (int) \u2013 the index to select with     Note select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index].  \n  \nset_(source=None, storage_offset=0, size=None, stride=None) \u2192 Tensor  \nSets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other. If source is a Storage, the method sets the underlying storage, offset, size, and stride.  Parameters \n \nsource (Tensor or Storage) \u2013 the tensor or storage to use \nstorage_offset (int, optional) \u2013 the offset in the storage \nsize (torch.Size, optional) \u2013 the desired size. Defaults to the size of the source. \nstride (tuple, optional) \u2013 the desired stride. Defaults to C-contiguous strides.    \n  \nshare_memory_() [source]\n \nMoves the underlying storage to shared memory. This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized. \n  \nshort(memory_format=torch.preserve_format) \u2192 Tensor  \nself.short() is equivalent to self.to(torch.int16). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nsigmoid() \u2192 Tensor  \nSee torch.sigmoid() \n  \nsigmoid_() \u2192 Tensor  \nIn-place version of sigmoid() \n  \nsign() \u2192 Tensor  \nSee torch.sign() \n  \nsign_() \u2192 Tensor  \nIn-place version of sign() \n  \nsignbit() \u2192 Tensor  \nSee torch.signbit() \n  \nsgn() \u2192 Tensor  \nSee torch.sgn() \n  \nsgn_() \u2192 Tensor  \nIn-place version of sgn() \n  \nsin() \u2192 Tensor  \nSee torch.sin() \n  \nsin_() \u2192 Tensor  \nIn-place version of sin() \n  \nsinc() \u2192 Tensor  \nSee torch.sinc() \n  \nsinc_() \u2192 Tensor  \nIn-place version of sinc() \n  \nsinh() \u2192 Tensor  \nSee torch.sinh() \n  \nsinh_() \u2192 Tensor  \nIn-place version of sinh() \n  \nasinh() \u2192 Tensor  \nSee torch.asinh() \n  \nasinh_() \u2192 Tensor  \nIn-place version of asinh() \n  \narcsinh() \u2192 Tensor  \nSee torch.arcsinh() \n  \narcsinh_() \u2192 Tensor  \nIn-place version of arcsinh() \n  \nsize() \u2192 torch.Size  \nReturns the size of the self tensor. The returned value is a subclass of tuple. Example: >>> torch.empty(3, 4, 5).size()\ntorch.Size([3, 4, 5])\n \n  \nslogdet() -> (Tensor, Tensor)  \nSee torch.slogdet() \n  \nsolve(A) \u2192 Tensor, Tensor  \nSee torch.solve() \n  \nsort(dim=-1, descending=False) -> (Tensor, LongTensor)  \nSee torch.sort() \n  \nsplit(split_size, dim=0) [source]\n \nSee torch.split() \n  \nsparse_mask(mask) \u2192 Tensor  \nReturns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.  Note The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.   Parameters \nmask (Tensor) \u2013 a sparse tensor whose indices are used as a filter   Example: >>> nse = 5\n>>> dims = (5, 5, 2, 2)\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n>>> V = torch.randn(nse, dims[2], dims[3])\n>>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n>>> D = torch.randn(dims)\n>>> D.sparse_mask(S)\ntensor(indices=tensor([[0, 0, 0, 2],\n                       [0, 1, 4, 3]]),\n       values=tensor([[[ 1.6550,  0.2397],\n                       [-0.1611, -0.0779]],\n\n                      [[ 0.2326, -1.0558],\n                       [ 1.4711,  1.9678]],\n\n                      [[-0.5138, -0.0411],\n                       [ 1.9417,  0.5158]],\n\n                      [[ 0.0793,  0.0036],\n                       [-0.2569, -0.1055]]]),\n       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n \n  \nsparse_dim() \u2192 int  \nReturn the number of sparse dimensions in a sparse tensor self.  Warning Throws an error if self is not a sparse tensor.  See also Tensor.dense_dim() and hybrid tensors. \n  \nsqrt() \u2192 Tensor  \nSee torch.sqrt() \n  \nsqrt_() \u2192 Tensor  \nIn-place version of sqrt() \n  \nsquare() \u2192 Tensor  \nSee torch.square() \n  \nsquare_() \u2192 Tensor  \nIn-place version of square() \n  \nsqueeze(dim=None) \u2192 Tensor  \nSee torch.squeeze() \n  \nsqueeze_(dim=None) \u2192 Tensor  \nIn-place version of squeeze() \n  \nstd(dim=None, unbiased=True, keepdim=False) \u2192 Tensor  \nSee torch.std() \n  \nstft(n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=None, return_complex=None) [source]\n \nSee torch.stft()  Warning This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.  \n  \nstorage() \u2192 torch.Storage  \nReturns the underlying storage. \n  \nstorage_offset() \u2192 int  \nReturns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes). Example: >>> x = torch.tensor([1, 2, 3, 4, 5])\n>>> x.storage_offset()\n0\n>>> x[3:].storage_offset()\n3\n \n  \nstorage_type() \u2192 type  \nReturns the type of the underlying storage. \n  \nstride(dim) \u2192 tuple or int  \nReturns the stride of self tensor. Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.  Parameters \ndim (int, optional) \u2013 the desired dimension in which stride is required   Example: >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n>>> x.stride(0)\n5\n>>> x.stride(-1)\n1\n \n  \nsub(other, *, alpha=1) \u2192 Tensor  \nSee torch.sub(). \n  \nsub_(other, *, alpha=1) \u2192 Tensor  \nIn-place version of sub() \n  \nsubtract(other, *, alpha=1) \u2192 Tensor  \nSee torch.subtract(). \n  \nsubtract_(other, *, alpha=1) \u2192 Tensor  \nIn-place version of subtract(). \n  \nsum(dim=None, keepdim=False, dtype=None) \u2192 Tensor  \nSee torch.sum() \n  \nsum_to_size(*size) \u2192 Tensor  \nSum this tensor to size. size must be broadcastable to this tensor size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor.   \n  \nsvd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor)  \nSee torch.svd() \n  \nswapaxes(axis0, axis1) \u2192 Tensor  \nSee torch.swapaxes() \n  \nswapdims(dim0, dim1) \u2192 Tensor  \nSee torch.swapdims() \n  \nsymeig(eigenvectors=False, upper=True) -> (Tensor, Tensor)  \nSee torch.symeig() \n  \nt() \u2192 Tensor  \nSee torch.t() \n  \nt_() \u2192 Tensor  \nIn-place version of t() \n  \ntensor_split(indices_or_sections, dim=0) \u2192 List of Tensors  \nSee torch.tensor_split() \n  \ntile(*reps) \u2192 Tensor  \nSee torch.tile() \n  \nto(*args, **kwargs) \u2192 Tensor  \nPerforms Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).  Note If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.  Here are the ways to call to:  \nto(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a Tensor with the specified dtype  Args:\n\nmemory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nto(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.  Args:\n\nmemory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nto(other, non_blocking=False, copy=False) \u2192 Tensor  \nReturns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion. \n Example: >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n>>> tensor.to(torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64)\n\n>>> cuda0 = torch.device('cuda:0')\n>>> tensor.to(cuda0)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], device='cuda:0')\n\n>>> tensor.to(cuda0, dtype=torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n>>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n>>> tensor.to(other, non_blocking=True)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n \n  \nto_mkldnn() \u2192 Tensor  \nReturns a copy of the tensor in torch.mkldnn layout. \n  \ntake(indices) \u2192 Tensor  \nSee torch.take() \n  \ntan() \u2192 Tensor  \nSee torch.tan() \n  \ntan_() \u2192 Tensor  \nIn-place version of tan() \n  \ntanh() \u2192 Tensor  \nSee torch.tanh() \n  \ntanh_() \u2192 Tensor  \nIn-place version of tanh() \n  \natanh() \u2192 Tensor  \nSee torch.atanh() \n  \natanh_(other) \u2192 Tensor  \nIn-place version of atanh() \n  \narctanh() \u2192 Tensor  \nSee torch.arctanh() \n  \narctanh_(other) \u2192 Tensor  \nIn-place version of arctanh() \n  \ntolist() \u2192 list or number  \nReturns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary. This operation is not differentiable. Examples: >>> a = torch.randn(2, 2)\n>>> a.tolist()\n[[0.012766935862600803, 0.5415473580360413],\n [-0.08909505605697632, 0.7729271650314331]]\n>>> a[0,0].tolist()\n0.012766935862600803\n \n  \ntopk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)  \nSee torch.topk() \n  \nto_sparse(sparseDims) \u2192 Tensor  \nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.  Parameters \nsparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor   Example: >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n>>> d\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n>>> d.to_sparse()\ntensor(indices=tensor([[1, 1],\n                       [0, 2]]),\n       values=tensor([ 9, 10]),\n       size=(3, 3), nnz=2, layout=torch.sparse_coo)\n>>> d.to_sparse(1)\ntensor(indices=tensor([[1]]),\n       values=tensor([[ 9,  0, 10]]),\n       size=(3, 3), nnz=1, layout=torch.sparse_coo)\n \n  \ntrace() \u2192 Tensor  \nSee torch.trace() \n  \ntranspose(dim0, dim1) \u2192 Tensor  \nSee torch.transpose() \n  \ntranspose_(dim0, dim1) \u2192 Tensor  \nIn-place version of transpose() \n  \ntriangular_solve(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)  \nSee torch.triangular_solve() \n  \ntril(k=0) \u2192 Tensor  \nSee torch.tril() \n  \ntril_(k=0) \u2192 Tensor  \nIn-place version of tril() \n  \ntriu(k=0) \u2192 Tensor  \nSee torch.triu() \n  \ntriu_(k=0) \u2192 Tensor  \nIn-place version of triu() \n  \ntrue_divide(value) \u2192 Tensor  \nSee torch.true_divide() \n  \ntrue_divide_(value) \u2192 Tensor  \nIn-place version of true_divide_() \n  \ntrunc() \u2192 Tensor  \nSee torch.trunc() \n  \ntrunc_() \u2192 Tensor  \nIn-place version of trunc() \n  \ntype(dtype=None, non_blocking=False, **kwargs) \u2192 str or Tensor  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n  \ntype_as(tensor) \u2192 Tensor  \nReturns this tensor cast to the type of the given tensor. This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())  Parameters \ntensor (Tensor) \u2013 the tensor which has the desired type   \n  \nunbind(dim=0) \u2192 seq  \nSee torch.unbind() \n  \nunfold(dimension, size, step) \u2192 Tensor  \nReturns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension. Step between two slices is given by step. If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1. An additional dimension of size size is appended in the returned tensor.  Parameters \n \ndimension (int) \u2013 dimension in which unfolding happens \nsize (int) \u2013 the size of each slice that is unfolded \nstep (int) \u2013 the step between each slice    Example: >>> x = torch.arange(1., 8)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n>>> x.unfold(0, 2, 1)\ntensor([[ 1.,  2.],\n        [ 2.,  3.],\n        [ 3.,  4.],\n        [ 4.,  5.],\n        [ 5.,  6.],\n        [ 6.,  7.]])\n>>> x.unfold(0, 2, 2)\ntensor([[ 1.,  2.],\n        [ 3.,  4.],\n        [ 5.,  6.]])\n \n  \nuniform_(from=0, to=1) \u2192 Tensor  \nFills self tensor with numbers sampled from the continuous uniform distribution:  P(x)=1to\u2212fromP(x) = \\dfrac{1}{\\text{to} - \\text{from}}  \n\n  \nunique(sorted=True, return_inverse=False, return_counts=False, dim=None) [source]\n \nReturns the unique elements of the input tensor. See torch.unique() \n  \nunique_consecutive(return_inverse=False, return_counts=False, dim=None) [source]\n \nEliminates all but the first element from every consecutive group of equivalent elements. See torch.unique_consecutive() \n  \nunsqueeze(dim) \u2192 Tensor  \nSee torch.unsqueeze() \n  \nunsqueeze_(dim) \u2192 Tensor  \nIn-place version of unsqueeze() \n  \nvalues() \u2192 Tensor  \nReturn the values tensor of a sparse COO tensor.  Warning Throws an error if self is not a sparse COO tensor.  See also Tensor.indices().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  \n  \nvar(dim=None, unbiased=True, keepdim=False) \u2192 Tensor  \nSee torch.var() \n  \nvdot(other) \u2192 Tensor  \nSee torch.vdot() \n  \nview(*shape) \u2192 Tensor  \nReturns a new tensor with the same data as the self tensor but of a different shape. The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k  that satisfy the following contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots, d+k-1 ,  stride[i]=stride[i+1]\u00d7size[i+1]\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1] \nOtherwise, it will not be possible to view self tensor as shape without copying it (e.g., via contiguous()). When it is unclear whether a view() can be performed, it is advisable to use reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.  Parameters \nshape (torch.Size or int...) \u2013 the desired size   Example: >>> x = torch.randn(4, 4)\n>>> x.size()\ntorch.Size([4, 4])\n>>> y = x.view(16)\n>>> y.size()\ntorch.Size([16])\n>>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n>>> z.size()\ntorch.Size([2, 8])\n\n>>> a = torch.randn(1, 2, 3, 4)\n>>> a.size()\ntorch.Size([1, 2, 3, 4])\n>>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n>>> b.size()\ntorch.Size([1, 3, 2, 4])\n>>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n>>> c.size()\ntorch.Size([1, 3, 2, 4])\n>>> torch.equal(b, c)\nFalse\n  \nview(dtype) \u2192 Tensor \n Returns a new tensor with the same data as the self tensor but of a different dtype. dtype must have the same number of bytes per element as self\u2019s dtype.  Warning This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.   Parameters \ndtype (torch.dtype) \u2013 the desired dtype   Example: >>> x = torch.randn(4, 4)\n>>> x\ntensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n        [-0.1520,  0.7472,  0.5617, -0.8649],\n        [-2.4724, -0.0334, -0.2976, -0.8499],\n        [-0.2109,  1.9913, -0.9607, -0.6123]])\n>>> x.dtype\ntorch.float32\n\n>>> y = x.view(torch.int32)\n>>> y\ntensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n        [-1105482831,  1061112040,  1057999968, -1084397505],\n        [-1071760287, -1123489973, -1097310419, -1084649136],\n        [-1101533110,  1073668768, -1082790149, -1088634448]],\n    dtype=torch.int32)\n>>> y[0, 0] = 1000000000\n>>> x\ntensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n        [-0.1520,  0.7472,  0.5617, -0.8649],\n        [-2.4724, -0.0334, -0.2976, -0.8499],\n        [-0.2109,  1.9913, -0.9607, -0.6123]])\n\n>>> x.view(torch.int16)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.\n \n  \nview_as(other) \u2192 Tensor  \nView this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()). Please see view() for more information about view.  Parameters \nother (torch.Tensor) \u2013 The result tensor has the same size as other.   \n  \nwhere(condition, y) \u2192 Tensor  \nself.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where() \n  \nxlogy(other) \u2192 Tensor  \nSee torch.xlogy() \n  \nxlogy_(other) \u2192 Tensor  \nIn-place version of xlogy() \n  \nzero_() \u2192 Tensor  \nFills self tensor with zeros. \n \n\n"}, {"name": "torch.tensor()", "path": "generated/torch.tensor#torch.tensor", "type": "torch", "text": " \ntorch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) \u2192 Tensor  \nConstructs a tensor with data.  Warning torch.tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a NumPy ndarray and want to avoid a copy, use torch.as_tensor().   Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach() and torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.   Parameters \ndata (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from data. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \npin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n        [ 2.2000,  3.1000],\n        [ 4.9000,  5.2000]])\n\n>>> torch.tensor([0, 1])  # Type inference on data\ntensor([ 0,  1])\n\n>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n...              dtype=torch.float64,\n...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\ntensor(3.1416)\n\n>>> torch.tensor([])  # Create an empty tensor (of size (0,))\ntensor([])\n \n"}, {"name": "torch.Tensor.abs()", "path": "tensors#torch.Tensor.abs", "type": "torch.Tensor", "text": " \nabs() \u2192 Tensor  \nSee torch.abs() \n"}, {"name": "torch.Tensor.absolute()", "path": "tensors#torch.Tensor.absolute", "type": "torch.Tensor", "text": " \nabsolute() \u2192 Tensor  \nAlias for abs() \n"}, {"name": "torch.Tensor.absolute_()", "path": "tensors#torch.Tensor.absolute_", "type": "torch.Tensor", "text": " \nabsolute_() \u2192 Tensor  \nIn-place version of absolute() Alias for abs_() \n"}, {"name": "torch.Tensor.abs_()", "path": "tensors#torch.Tensor.abs_", "type": "torch.Tensor", "text": " \nabs_() \u2192 Tensor  \nIn-place version of abs() \n"}, {"name": "torch.Tensor.acos()", "path": "tensors#torch.Tensor.acos", "type": "torch.Tensor", "text": " \nacos() \u2192 Tensor  \nSee torch.acos() \n"}, {"name": "torch.Tensor.acosh()", "path": "tensors#torch.Tensor.acosh", "type": "torch.Tensor", "text": " \nacosh() \u2192 Tensor  \nSee torch.acosh() \n"}, {"name": "torch.Tensor.acosh_()", "path": "tensors#torch.Tensor.acosh_", "type": "torch.Tensor", "text": " \nacosh_() \u2192 Tensor  \nIn-place version of acosh() \n"}, {"name": "torch.Tensor.acos_()", "path": "tensors#torch.Tensor.acos_", "type": "torch.Tensor", "text": " \nacos_() \u2192 Tensor  \nIn-place version of acos() \n"}, {"name": "torch.Tensor.add()", "path": "tensors#torch.Tensor.add", "type": "torch.Tensor", "text": " \nadd(other, *, alpha=1) \u2192 Tensor  \nAdd a scalar or tensor to self tensor. If both alpha and other are specified, each element of other is scaled by alpha before being used. When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor See torch.add() \n"}, {"name": "torch.Tensor.addbmm()", "path": "tensors#torch.Tensor.addbmm", "type": "torch.Tensor", "text": " \naddbmm(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addbmm() \n"}, {"name": "torch.Tensor.addbmm_()", "path": "tensors#torch.Tensor.addbmm_", "type": "torch.Tensor", "text": " \naddbmm_(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addbmm() \n"}, {"name": "torch.Tensor.addcdiv()", "path": "tensors#torch.Tensor.addcdiv", "type": "torch.Tensor", "text": " \naddcdiv(tensor1, tensor2, *, value=1) \u2192 Tensor  \nSee torch.addcdiv() \n"}, {"name": "torch.Tensor.addcdiv_()", "path": "tensors#torch.Tensor.addcdiv_", "type": "torch.Tensor", "text": " \naddcdiv_(tensor1, tensor2, *, value=1) \u2192 Tensor  \nIn-place version of addcdiv() \n"}, {"name": "torch.Tensor.addcmul()", "path": "tensors#torch.Tensor.addcmul", "type": "torch.Tensor", "text": " \naddcmul(tensor1, tensor2, *, value=1) \u2192 Tensor  \nSee torch.addcmul() \n"}, {"name": "torch.Tensor.addcmul_()", "path": "tensors#torch.Tensor.addcmul_", "type": "torch.Tensor", "text": " \naddcmul_(tensor1, tensor2, *, value=1) \u2192 Tensor  \nIn-place version of addcmul() \n"}, {"name": "torch.Tensor.addmm()", "path": "tensors#torch.Tensor.addmm", "type": "torch.Tensor", "text": " \naddmm(mat1, mat2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addmm() \n"}, {"name": "torch.Tensor.addmm_()", "path": "tensors#torch.Tensor.addmm_", "type": "torch.Tensor", "text": " \naddmm_(mat1, mat2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addmm() \n"}, {"name": "torch.Tensor.addmv()", "path": "tensors#torch.Tensor.addmv", "type": "torch.Tensor", "text": " \naddmv(mat, vec, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addmv() \n"}, {"name": "torch.Tensor.addmv_()", "path": "tensors#torch.Tensor.addmv_", "type": "torch.Tensor", "text": " \naddmv_(mat, vec, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addmv() \n"}, {"name": "torch.Tensor.addr()", "path": "tensors#torch.Tensor.addr", "type": "torch.Tensor", "text": " \naddr(vec1, vec2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.addr() \n"}, {"name": "torch.Tensor.addr_()", "path": "tensors#torch.Tensor.addr_", "type": "torch.Tensor", "text": " \naddr_(vec1, vec2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of addr() \n"}, {"name": "torch.Tensor.add_()", "path": "tensors#torch.Tensor.add_", "type": "torch.Tensor", "text": " \nadd_(other, *, alpha=1) \u2192 Tensor  \nIn-place version of add() \n"}, {"name": "torch.Tensor.align_as()", "path": "named_tensor#torch.Tensor.align_as", "type": "Named Tensors", "text": " \nalign_as(other) \u2192 Tensor  \nPermutes the dimensions of the self tensor to match the dimension order in the other tensor, adding size-one dims for any new names. This operation is useful for explicit broadcasting by names (see examples). All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor. All dimension names of self must be present in other.names. other may contain named dimensions that are not in self.names; the output tensor has a size-one dimension for each of those new names. To align a tensor to a specific order, use align_to(). Examples: # Example 1: Applying a mask\n>>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n>>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n>>> imgs.masked_fill_(mask.align_as(imgs), 0)\n\n\n# Example 2: Applying a per-channel-scale\n>>> def scale_channels(input, scale):\n>>>    scale = scale.refine_names('C')\n>>>    return input * scale.align_as(input)\n\n>>> num_channels = 3\n>>> scale = torch.randn(num_channels, names=('C',))\n>>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n>>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n\n# scale_channels is agnostic to the dimension order of the input\n>>> scale_channels(imgs, scale)\n>>> scale_channels(more_imgs, scale)\n>>> scale_channels(videos, scale)\n  Warning The named tensor API is experimental and subject to change.  \n"}, {"name": "torch.Tensor.align_to()", "path": "named_tensor#torch.Tensor.align_to", "type": "Named Tensors", "text": " \nalign_to(*names) [source]\n \nPermutes the dimensions of the self tensor to match the order specified in names, adding size-one dims for any new names. All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor. All dimension names of self must be present in names. names may contain additional names that are not in self.names; the output tensor has a size-one dimension for each of those new names. names may contain up to one Ellipsis (...). The Ellipsis is expanded to be equal to all dimension names of self that are not mentioned in names, in the order that they appear in self. Python 2 does not support Ellipsis but one may use a string literal instead ('...').  Parameters \nnames (iterable of str) \u2013 The desired dimension ordering of the output tensor. May contain up to one Ellipsis that is expanded to all unmentioned dim names of self.   Examples: >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F and E dims to the front while keeping the rest in order\n>>> named_tensor.align_to('F', 'E', ...)\n  Warning The named tensor API is experimental and subject to change.  \n"}, {"name": "torch.Tensor.all()", "path": "tensors#torch.Tensor.all", "type": "torch.Tensor", "text": " \nall(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.all() \n"}, {"name": "torch.Tensor.allclose()", "path": "tensors#torch.Tensor.allclose", "type": "torch.Tensor", "text": " \nallclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor  \nSee torch.allclose() \n"}, {"name": "torch.Tensor.amax()", "path": "tensors#torch.Tensor.amax", "type": "torch.Tensor", "text": " \namax(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.amax() \n"}, {"name": "torch.Tensor.amin()", "path": "tensors#torch.Tensor.amin", "type": "torch.Tensor", "text": " \namin(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.amin() \n"}, {"name": "torch.Tensor.angle()", "path": "tensors#torch.Tensor.angle", "type": "torch.Tensor", "text": " \nangle() \u2192 Tensor  \nSee torch.angle() \n"}, {"name": "torch.Tensor.any()", "path": "tensors#torch.Tensor.any", "type": "torch.Tensor", "text": " \nany(dim=None, keepdim=False) \u2192 Tensor  \nSee torch.any() \n"}, {"name": "torch.Tensor.apply_()", "path": "tensors#torch.Tensor.apply_", "type": "torch.Tensor", "text": " \napply_(callable) \u2192 Tensor  \nApplies the function callable to each element in the tensor, replacing each element with the value returned by callable.  Note This function only works with CPU tensors and should not be used in code sections that require high performance.  \n"}, {"name": "torch.Tensor.arccos()", "path": "tensors#torch.Tensor.arccos", "type": "torch.Tensor", "text": " \narccos() \u2192 Tensor  \nSee torch.arccos() \n"}, {"name": "torch.Tensor.arccosh()", "path": "tensors#torch.Tensor.arccosh", "type": "torch.Tensor", "text": " \narccosh()  \nacosh() -> Tensor See torch.arccosh() \n"}, {"name": "torch.Tensor.arccosh_()", "path": "tensors#torch.Tensor.arccosh_", "type": "torch.Tensor", "text": " \narccosh_()  \nacosh_() -> Tensor In-place version of arccosh() \n"}, {"name": "torch.Tensor.arccos_()", "path": "tensors#torch.Tensor.arccos_", "type": "torch.Tensor", "text": " \narccos_() \u2192 Tensor  \nIn-place version of arccos() \n"}, {"name": "torch.Tensor.arcsin()", "path": "tensors#torch.Tensor.arcsin", "type": "torch.Tensor", "text": " \narcsin() \u2192 Tensor  \nSee torch.arcsin() \n"}, {"name": "torch.Tensor.arcsinh()", "path": "tensors#torch.Tensor.arcsinh", "type": "torch.Tensor", "text": " \narcsinh() \u2192 Tensor  \nSee torch.arcsinh() \n"}, {"name": "torch.Tensor.arcsinh_()", "path": "tensors#torch.Tensor.arcsinh_", "type": "torch.Tensor", "text": " \narcsinh_() \u2192 Tensor  \nIn-place version of arcsinh() \n"}, {"name": "torch.Tensor.arcsin_()", "path": "tensors#torch.Tensor.arcsin_", "type": "torch.Tensor", "text": " \narcsin_() \u2192 Tensor  \nIn-place version of arcsin() \n"}, {"name": "torch.Tensor.arctan()", "path": "tensors#torch.Tensor.arctan", "type": "torch.Tensor", "text": " \narctan() \u2192 Tensor  \nSee torch.arctan() \n"}, {"name": "torch.Tensor.arctanh()", "path": "tensors#torch.Tensor.arctanh", "type": "torch.Tensor", "text": " \narctanh() \u2192 Tensor  \nSee torch.arctanh() \n"}, {"name": "torch.Tensor.arctanh_()", "path": "tensors#torch.Tensor.arctanh_", "type": "torch.Tensor", "text": " \narctanh_(other) \u2192 Tensor  \nIn-place version of arctanh() \n"}, {"name": "torch.Tensor.arctan_()", "path": "tensors#torch.Tensor.arctan_", "type": "torch.Tensor", "text": " \narctan_() \u2192 Tensor  \nIn-place version of arctan() \n"}, {"name": "torch.Tensor.argmax()", "path": "tensors#torch.Tensor.argmax", "type": "torch.Tensor", "text": " \nargmax(dim=None, keepdim=False) \u2192 LongTensor  \nSee torch.argmax() \n"}, {"name": "torch.Tensor.argmin()", "path": "tensors#torch.Tensor.argmin", "type": "torch.Tensor", "text": " \nargmin(dim=None, keepdim=False) \u2192 LongTensor  \nSee torch.argmin() \n"}, {"name": "torch.Tensor.argsort()", "path": "tensors#torch.Tensor.argsort", "type": "torch.Tensor", "text": " \nargsort(dim=-1, descending=False) \u2192 LongTensor  \nSee torch.argsort() \n"}, {"name": "torch.Tensor.asin()", "path": "tensors#torch.Tensor.asin", "type": "torch.Tensor", "text": " \nasin() \u2192 Tensor  \nSee torch.asin() \n"}, {"name": "torch.Tensor.asinh()", "path": "tensors#torch.Tensor.asinh", "type": "torch.Tensor", "text": " \nasinh() \u2192 Tensor  \nSee torch.asinh() \n"}, {"name": "torch.Tensor.asinh_()", "path": "tensors#torch.Tensor.asinh_", "type": "torch.Tensor", "text": " \nasinh_() \u2192 Tensor  \nIn-place version of asinh() \n"}, {"name": "torch.Tensor.asin_()", "path": "tensors#torch.Tensor.asin_", "type": "torch.Tensor", "text": " \nasin_() \u2192 Tensor  \nIn-place version of asin() \n"}, {"name": "torch.Tensor.as_strided()", "path": "tensors#torch.Tensor.as_strided", "type": "torch.Tensor", "text": " \nas_strided(size, stride, storage_offset=0) \u2192 Tensor  \nSee torch.as_strided() \n"}, {"name": "torch.Tensor.as_subclass()", "path": "tensors#torch.Tensor.as_subclass", "type": "torch.Tensor", "text": " \nas_subclass(cls) \u2192 Tensor  \nMakes a cls instance with the same data pointer as self. Changes in the output mirror changes in self, and the output stays attached to the autograd graph. cls must be a subclass of Tensor. \n"}, {"name": "torch.Tensor.atan()", "path": "tensors#torch.Tensor.atan", "type": "torch.Tensor", "text": " \natan() \u2192 Tensor  \nSee torch.atan() \n"}, {"name": "torch.Tensor.atan2()", "path": "tensors#torch.Tensor.atan2", "type": "torch.Tensor", "text": " \natan2(other) \u2192 Tensor  \nSee torch.atan2() \n"}, {"name": "torch.Tensor.atan2_()", "path": "tensors#torch.Tensor.atan2_", "type": "torch.Tensor", "text": " \natan2_(other) \u2192 Tensor  \nIn-place version of atan2() \n"}, {"name": "torch.Tensor.atanh()", "path": "tensors#torch.Tensor.atanh", "type": "torch.Tensor", "text": " \natanh() \u2192 Tensor  \nSee torch.atanh() \n"}, {"name": "torch.Tensor.atanh_()", "path": "tensors#torch.Tensor.atanh_", "type": "torch.Tensor", "text": " \natanh_(other) \u2192 Tensor  \nIn-place version of atanh() \n"}, {"name": "torch.Tensor.atan_()", "path": "tensors#torch.Tensor.atan_", "type": "torch.Tensor", "text": " \natan_() \u2192 Tensor  \nIn-place version of atan() \n"}, {"name": "torch.Tensor.backward()", "path": "autograd#torch.Tensor.backward", "type": "torch.autograd", "text": " \nbackward(gradient=None, retain_graph=None, create_graph=False, inputs=None) [source]\n \nComputes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ngradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n"}, {"name": "torch.Tensor.baddbmm()", "path": "tensors#torch.Tensor.baddbmm", "type": "torch.Tensor", "text": " \nbaddbmm(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nSee torch.baddbmm() \n"}, {"name": "torch.Tensor.baddbmm_()", "path": "tensors#torch.Tensor.baddbmm_", "type": "torch.Tensor", "text": " \nbaddbmm_(batch1, batch2, *, beta=1, alpha=1) \u2192 Tensor  \nIn-place version of baddbmm() \n"}, {"name": "torch.Tensor.bernoulli()", "path": "tensors#torch.Tensor.bernoulli", "type": "torch.Tensor", "text": " \nbernoulli(*, generator=None) \u2192 Tensor  \nReturns a result tensor where each result[i]\\texttt{result[i]}  is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . self must have floating point dtype, and the result will have the same dtype. See torch.bernoulli() \n"}, {"name": "torch.Tensor.bernoulli_()", "path": "tensors#torch.Tensor.bernoulli_", "type": "torch.Tensor", "text": " \nbernoulli_()  \n \nbernoulli_(p=0.5, *, generator=None) \u2192 Tensor  \nFills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}) . self can have integral dtype. \n  \nbernoulli_(p_tensor, *, generator=None) \u2192 Tensor  \np_tensor should be a tensor containing probabilities to be used for drawing the binary random number. The ith\\text{i}^{th}  element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]}) . self can have integral dtype, but p_tensor must have floating point dtype. \n See also bernoulli() and torch.bernoulli() \n"}, {"name": "torch.Tensor.bfloat16()", "path": "tensors#torch.Tensor.bfloat16", "type": "torch.Tensor", "text": " \nbfloat16(memory_format=torch.preserve_format) \u2192 Tensor  \nself.bfloat16() is equivalent to self.to(torch.bfloat16). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.bincount()", "path": "tensors#torch.Tensor.bincount", "type": "torch.Tensor", "text": " \nbincount(weights=None, minlength=0) \u2192 Tensor  \nSee torch.bincount() \n"}, {"name": "torch.Tensor.bitwise_and()", "path": "tensors#torch.Tensor.bitwise_and", "type": "torch.Tensor", "text": " \nbitwise_and() \u2192 Tensor  \nSee torch.bitwise_and() \n"}, {"name": "torch.Tensor.bitwise_and_()", "path": "tensors#torch.Tensor.bitwise_and_", "type": "torch.Tensor", "text": " \nbitwise_and_() \u2192 Tensor  \nIn-place version of bitwise_and() \n"}, {"name": "torch.Tensor.bitwise_not()", "path": "tensors#torch.Tensor.bitwise_not", "type": "torch.Tensor", "text": " \nbitwise_not() \u2192 Tensor  \nSee torch.bitwise_not() \n"}, {"name": "torch.Tensor.bitwise_not_()", "path": "tensors#torch.Tensor.bitwise_not_", "type": "torch.Tensor", "text": " \nbitwise_not_() \u2192 Tensor  \nIn-place version of bitwise_not() \n"}, {"name": "torch.Tensor.bitwise_or()", "path": "tensors#torch.Tensor.bitwise_or", "type": "torch.Tensor", "text": " \nbitwise_or() \u2192 Tensor  \nSee torch.bitwise_or() \n"}, {"name": "torch.Tensor.bitwise_or_()", "path": "tensors#torch.Tensor.bitwise_or_", "type": "torch.Tensor", "text": " \nbitwise_or_() \u2192 Tensor  \nIn-place version of bitwise_or() \n"}, {"name": "torch.Tensor.bitwise_xor()", "path": "tensors#torch.Tensor.bitwise_xor", "type": "torch.Tensor", "text": " \nbitwise_xor() \u2192 Tensor  \nSee torch.bitwise_xor() \n"}, {"name": "torch.Tensor.bitwise_xor_()", "path": "tensors#torch.Tensor.bitwise_xor_", "type": "torch.Tensor", "text": " \nbitwise_xor_() \u2192 Tensor  \nIn-place version of bitwise_xor() \n"}, {"name": "torch.Tensor.bmm()", "path": "tensors#torch.Tensor.bmm", "type": "torch.Tensor", "text": " \nbmm(batch2) \u2192 Tensor  \nSee torch.bmm() \n"}, {"name": "torch.Tensor.bool()", "path": "tensors#torch.Tensor.bool", "type": "torch.Tensor", "text": " \nbool(memory_format=torch.preserve_format) \u2192 Tensor  \nself.bool() is equivalent to self.to(torch.bool). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.broadcast_to()", "path": "tensors#torch.Tensor.broadcast_to", "type": "torch.Tensor", "text": " \nbroadcast_to(shape) \u2192 Tensor  \nSee torch.broadcast_to(). \n"}, {"name": "torch.Tensor.byte()", "path": "tensors#torch.Tensor.byte", "type": "torch.Tensor", "text": " \nbyte(memory_format=torch.preserve_format) \u2192 Tensor  \nself.byte() is equivalent to self.to(torch.uint8). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.cauchy_()", "path": "tensors#torch.Tensor.cauchy_", "type": "torch.Tensor", "text": " \ncauchy_(median=0, sigma=1, *, generator=None) \u2192 Tensor  \nFills the tensor with numbers drawn from the Cauchy distribution:  f(x)=1\u03c0\u03c3(x\u2212median)2+\u03c32f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2} \n\n"}, {"name": "torch.Tensor.ceil()", "path": "tensors#torch.Tensor.ceil", "type": "torch.Tensor", "text": " \nceil() \u2192 Tensor  \nSee torch.ceil() \n"}, {"name": "torch.Tensor.ceil_()", "path": "tensors#torch.Tensor.ceil_", "type": "torch.Tensor", "text": " \nceil_() \u2192 Tensor  \nIn-place version of ceil() \n"}, {"name": "torch.Tensor.char()", "path": "tensors#torch.Tensor.char", "type": "torch.Tensor", "text": " \nchar(memory_format=torch.preserve_format) \u2192 Tensor  \nself.char() is equivalent to self.to(torch.int8). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.cholesky()", "path": "tensors#torch.Tensor.cholesky", "type": "torch.Tensor", "text": " \ncholesky(upper=False) \u2192 Tensor  \nSee torch.cholesky() \n"}, {"name": "torch.Tensor.cholesky_inverse()", "path": "tensors#torch.Tensor.cholesky_inverse", "type": "torch.Tensor", "text": " \ncholesky_inverse(upper=False) \u2192 Tensor  \nSee torch.cholesky_inverse() \n"}, {"name": "torch.Tensor.cholesky_solve()", "path": "tensors#torch.Tensor.cholesky_solve", "type": "torch.Tensor", "text": " \ncholesky_solve(input2, upper=False) \u2192 Tensor  \nSee torch.cholesky_solve() \n"}, {"name": "torch.Tensor.chunk()", "path": "tensors#torch.Tensor.chunk", "type": "torch.Tensor", "text": " \nchunk(chunks, dim=0) \u2192 List of Tensors  \nSee torch.chunk() \n"}, {"name": "torch.Tensor.clamp()", "path": "tensors#torch.Tensor.clamp", "type": "torch.Tensor", "text": " \nclamp(min, max) \u2192 Tensor  \nSee torch.clamp() \n"}, {"name": "torch.Tensor.clamp_()", "path": "tensors#torch.Tensor.clamp_", "type": "torch.Tensor", "text": " \nclamp_(min, max) \u2192 Tensor  \nIn-place version of clamp() \n"}, {"name": "torch.Tensor.clip()", "path": "tensors#torch.Tensor.clip", "type": "torch.Tensor", "text": " \nclip(min, max) \u2192 Tensor  \nAlias for clamp(). \n"}, {"name": "torch.Tensor.clip_()", "path": "tensors#torch.Tensor.clip_", "type": "torch.Tensor", "text": " \nclip_(min, max) \u2192 Tensor  \nAlias for clamp_(). \n"}, {"name": "torch.Tensor.clone()", "path": "tensors#torch.Tensor.clone", "type": "torch.Tensor", "text": " \nclone(*, memory_format=torch.preserve_format) \u2192 Tensor  \nSee torch.clone() \n"}, {"name": "torch.Tensor.coalesce()", "path": "sparse#torch.Tensor.coalesce", "type": "torch.sparse", "text": " \ncoalesce() \u2192 Tensor  \nReturns a coalesced copy of self if self is an uncoalesced tensor. Returns self if self is a coalesced tensor.  Warning Throws an error if self is not a sparse COO tensor.  \n"}, {"name": "torch.Tensor.conj()", "path": "tensors#torch.Tensor.conj", "type": "torch.Tensor", "text": " \nconj() \u2192 Tensor  \nSee torch.conj() \n"}, {"name": "torch.Tensor.contiguous()", "path": "tensors#torch.Tensor.contiguous", "type": "torch.Tensor", "text": " \ncontiguous(memory_format=torch.contiguous_format) \u2192 Tensor  \nReturns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format.   \n"}, {"name": "torch.Tensor.copysign()", "path": "tensors#torch.Tensor.copysign", "type": "torch.Tensor", "text": " \ncopysign(other) \u2192 Tensor  \nSee torch.copysign() \n"}, {"name": "torch.Tensor.copysign_()", "path": "tensors#torch.Tensor.copysign_", "type": "torch.Tensor", "text": " \ncopysign_(other) \u2192 Tensor  \nIn-place version of copysign() \n"}, {"name": "torch.Tensor.copy_()", "path": "tensors#torch.Tensor.copy_", "type": "torch.Tensor", "text": " \ncopy_(src, non_blocking=False) \u2192 Tensor  \nCopies the elements from src into self tensor and returns self. The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device.  Parameters \n \nsrc (Tensor) \u2013 the source tensor to copy from \nnon_blocking (bool) \u2013 if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.    \n"}, {"name": "torch.Tensor.cos()", "path": "tensors#torch.Tensor.cos", "type": "torch.Tensor", "text": " \ncos() \u2192 Tensor  \nSee torch.cos() \n"}, {"name": "torch.Tensor.cosh()", "path": "tensors#torch.Tensor.cosh", "type": "torch.Tensor", "text": " \ncosh() \u2192 Tensor  \nSee torch.cosh() \n"}, {"name": "torch.Tensor.cosh_()", "path": "tensors#torch.Tensor.cosh_", "type": "torch.Tensor", "text": " \ncosh_() \u2192 Tensor  \nIn-place version of cosh() \n"}, {"name": "torch.Tensor.cos_()", "path": "tensors#torch.Tensor.cos_", "type": "torch.Tensor", "text": " \ncos_() \u2192 Tensor  \nIn-place version of cos() \n"}, {"name": "torch.Tensor.count_nonzero()", "path": "tensors#torch.Tensor.count_nonzero", "type": "torch.Tensor", "text": " \ncount_nonzero(dim=None) \u2192 Tensor  \nSee torch.count_nonzero() \n"}, {"name": "torch.Tensor.cpu()", "path": "tensors#torch.Tensor.cpu", "type": "torch.Tensor", "text": " \ncpu(memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of this object in CPU memory. If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.cross()", "path": "tensors#torch.Tensor.cross", "type": "torch.Tensor", "text": " \ncross(other, dim=-1) \u2192 Tensor  \nSee torch.cross() \n"}, {"name": "torch.Tensor.cuda()", "path": "tensors#torch.Tensor.cuda", "type": "torch.Tensor", "text": " \ncuda(device=None, non_blocking=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (torch.device) \u2013 The destination GPU device. Defaults to the current CUDA device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n"}, {"name": "torch.Tensor.cummax()", "path": "tensors#torch.Tensor.cummax", "type": "torch.Tensor", "text": " \ncummax(dim) -> (Tensor, Tensor)  \nSee torch.cummax() \n"}, {"name": "torch.Tensor.cummin()", "path": "tensors#torch.Tensor.cummin", "type": "torch.Tensor", "text": " \ncummin(dim) -> (Tensor, Tensor)  \nSee torch.cummin() \n"}, {"name": "torch.Tensor.cumprod()", "path": "tensors#torch.Tensor.cumprod", "type": "torch.Tensor", "text": " \ncumprod(dim, dtype=None) \u2192 Tensor  \nSee torch.cumprod() \n"}, {"name": "torch.Tensor.cumprod_()", "path": "tensors#torch.Tensor.cumprod_", "type": "torch.Tensor", "text": " \ncumprod_(dim, dtype=None) \u2192 Tensor  \nIn-place version of cumprod() \n"}, {"name": "torch.Tensor.cumsum()", "path": "tensors#torch.Tensor.cumsum", "type": "torch.Tensor", "text": " \ncumsum(dim, dtype=None) \u2192 Tensor  \nSee torch.cumsum() \n"}, {"name": "torch.Tensor.cumsum_()", "path": "tensors#torch.Tensor.cumsum_", "type": "torch.Tensor", "text": " \ncumsum_(dim, dtype=None) \u2192 Tensor  \nIn-place version of cumsum() \n"}, {"name": "torch.Tensor.data_ptr()", "path": "tensors#torch.Tensor.data_ptr", "type": "torch.Tensor", "text": " \ndata_ptr() \u2192 int  \nReturns the address of the first element of self tensor. \n"}, {"name": "torch.Tensor.deg2rad()", "path": "tensors#torch.Tensor.deg2rad", "type": "torch.Tensor", "text": " \ndeg2rad() \u2192 Tensor  \nSee torch.deg2rad() \n"}, {"name": "torch.Tensor.dense_dim()", "path": "sparse#torch.Tensor.dense_dim", "type": "torch.sparse", "text": " \ndense_dim() \u2192 int  \nReturn the number of dense dimensions in a sparse tensor self.  Warning Throws an error if self is not a sparse tensor.  See also Tensor.sparse_dim() and hybrid tensors. \n"}, {"name": "torch.Tensor.dequantize()", "path": "tensors#torch.Tensor.dequantize", "type": "torch.Tensor", "text": " \ndequantize() \u2192 Tensor  \nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor. \n"}, {"name": "torch.Tensor.det()", "path": "tensors#torch.Tensor.det", "type": "torch.Tensor", "text": " \ndet() \u2192 Tensor  \nSee torch.det() \n"}, {"name": "torch.Tensor.detach()", "path": "autograd#torch.Tensor.detach", "type": "torch.autograd", "text": " \ndetach()  \nReturns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.  \n"}, {"name": "torch.Tensor.detach_()", "path": "autograd#torch.Tensor.detach_", "type": "torch.autograd", "text": " \ndetach_()  \nDetaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place. \n"}, {"name": "torch.Tensor.device", "path": "tensors#torch.Tensor.device", "type": "torch.Tensor", "text": " \ndevice  \nIs the torch.device where this Tensor is. \n"}, {"name": "torch.Tensor.diag()", "path": "tensors#torch.Tensor.diag", "type": "torch.Tensor", "text": " \ndiag(diagonal=0) \u2192 Tensor  \nSee torch.diag() \n"}, {"name": "torch.Tensor.diagflat()", "path": "tensors#torch.Tensor.diagflat", "type": "torch.Tensor", "text": " \ndiagflat(offset=0) \u2192 Tensor  \nSee torch.diagflat() \n"}, {"name": "torch.Tensor.diagonal()", "path": "tensors#torch.Tensor.diagonal", "type": "torch.Tensor", "text": " \ndiagonal(offset=0, dim1=0, dim2=1) \u2192 Tensor  \nSee torch.diagonal() \n"}, {"name": "torch.Tensor.diag_embed()", "path": "tensors#torch.Tensor.diag_embed", "type": "torch.Tensor", "text": " \ndiag_embed(offset=0, dim1=-2, dim2=-1) \u2192 Tensor  \nSee torch.diag_embed() \n"}, {"name": "torch.Tensor.diff()", "path": "tensors#torch.Tensor.diff", "type": "torch.Tensor", "text": " \ndiff(n=1, dim=-1, prepend=None, append=None) \u2192 Tensor  \nSee torch.diff() \n"}, {"name": "torch.Tensor.digamma()", "path": "tensors#torch.Tensor.digamma", "type": "torch.Tensor", "text": " \ndigamma() \u2192 Tensor  \nSee torch.digamma() \n"}, {"name": "torch.Tensor.digamma_()", "path": "tensors#torch.Tensor.digamma_", "type": "torch.Tensor", "text": " \ndigamma_() \u2192 Tensor  \nIn-place version of digamma() \n"}, {"name": "torch.Tensor.dim()", "path": "tensors#torch.Tensor.dim", "type": "torch.Tensor", "text": " \ndim() \u2192 int  \nReturns the number of dimensions of self tensor. \n"}, {"name": "torch.Tensor.dist()", "path": "tensors#torch.Tensor.dist", "type": "torch.Tensor", "text": " \ndist(other, p=2) \u2192 Tensor  \nSee torch.dist() \n"}, {"name": "torch.Tensor.div()", "path": "tensors#torch.Tensor.div", "type": "torch.Tensor", "text": " \ndiv(value, *, rounding_mode=None) \u2192 Tensor  \nSee torch.div() \n"}, {"name": "torch.Tensor.divide()", "path": "tensors#torch.Tensor.divide", "type": "torch.Tensor", "text": " \ndivide(value, *, rounding_mode=None) \u2192 Tensor  \nSee torch.divide() \n"}, {"name": "torch.Tensor.divide_()", "path": "tensors#torch.Tensor.divide_", "type": "torch.Tensor", "text": " \ndivide_(value, *, rounding_mode=None) \u2192 Tensor  \nIn-place version of divide() \n"}, {"name": "torch.Tensor.div_()", "path": "tensors#torch.Tensor.div_", "type": "torch.Tensor", "text": " \ndiv_(value, *, rounding_mode=None) \u2192 Tensor  \nIn-place version of div() \n"}, {"name": "torch.Tensor.dot()", "path": "tensors#torch.Tensor.dot", "type": "torch.Tensor", "text": " \ndot(other) \u2192 Tensor  \nSee torch.dot() \n"}, {"name": "torch.Tensor.double()", "path": "tensors#torch.Tensor.double", "type": "torch.Tensor", "text": " \ndouble(memory_format=torch.preserve_format) \u2192 Tensor  \nself.double() is equivalent to self.to(torch.float64). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.eig()", "path": "tensors#torch.Tensor.eig", "type": "torch.Tensor", "text": " \neig(eigenvectors=False) -> (Tensor, Tensor)  \nSee torch.eig() \n"}, {"name": "torch.Tensor.element_size()", "path": "tensors#torch.Tensor.element_size", "type": "torch.Tensor", "text": " \nelement_size() \u2192 int  \nReturns the size in bytes of an individual element. Example: >>> torch.tensor([]).element_size()\n4\n>>> torch.tensor([], dtype=torch.uint8).element_size()\n1\n \n"}, {"name": "torch.Tensor.eq()", "path": "tensors#torch.Tensor.eq", "type": "torch.Tensor", "text": " \neq(other) \u2192 Tensor  \nSee torch.eq() \n"}, {"name": "torch.Tensor.equal()", "path": "tensors#torch.Tensor.equal", "type": "torch.Tensor", "text": " \nequal(other) \u2192 bool  \nSee torch.equal() \n"}, {"name": "torch.Tensor.eq_()", "path": "tensors#torch.Tensor.eq_", "type": "torch.Tensor", "text": " \neq_(other) \u2192 Tensor  \nIn-place version of eq() \n"}, {"name": "torch.Tensor.erf()", "path": "tensors#torch.Tensor.erf", "type": "torch.Tensor", "text": " \nerf() \u2192 Tensor  \nSee torch.erf() \n"}, {"name": "torch.Tensor.erfc()", "path": "tensors#torch.Tensor.erfc", "type": "torch.Tensor", "text": " \nerfc() \u2192 Tensor  \nSee torch.erfc() \n"}, {"name": "torch.Tensor.erfc_()", "path": "tensors#torch.Tensor.erfc_", "type": "torch.Tensor", "text": " \nerfc_() \u2192 Tensor  \nIn-place version of erfc() \n"}, {"name": "torch.Tensor.erfinv()", "path": "tensors#torch.Tensor.erfinv", "type": "torch.Tensor", "text": " \nerfinv() \u2192 Tensor  \nSee torch.erfinv() \n"}, {"name": "torch.Tensor.erfinv_()", "path": "tensors#torch.Tensor.erfinv_", "type": "torch.Tensor", "text": " \nerfinv_() \u2192 Tensor  \nIn-place version of erfinv() \n"}, {"name": "torch.Tensor.erf_()", "path": "tensors#torch.Tensor.erf_", "type": "torch.Tensor", "text": " \nerf_() \u2192 Tensor  \nIn-place version of erf() \n"}, {"name": "torch.Tensor.exp()", "path": "tensors#torch.Tensor.exp", "type": "torch.Tensor", "text": " \nexp() \u2192 Tensor  \nSee torch.exp() \n"}, {"name": "torch.Tensor.expand()", "path": "tensors#torch.Tensor.expand", "type": "torch.Tensor", "text": " \nexpand(*sizes) \u2192 Tensor  \nReturns a new view of the self tensor with singleton dimensions expanded to a larger size. Passing -1 as the size for a dimension means not changing the size of that dimension. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.  Parameters \n*sizes (torch.Size or int...) \u2013 the desired expanded size    Warning More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: >>> x = torch.tensor([[1], [2], [3]])\n>>> x.size()\ntorch.Size([3, 1])\n>>> x.expand(3, 4)\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n>>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n \n"}, {"name": "torch.Tensor.expand_as()", "path": "tensors#torch.Tensor.expand_as", "type": "torch.Tensor", "text": " \nexpand_as(other) \u2192 Tensor  \nExpand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()). Please see expand() for more information about expand.  Parameters \nother (torch.Tensor) \u2013 The result tensor has the same size as other.   \n"}, {"name": "torch.Tensor.expm1()", "path": "tensors#torch.Tensor.expm1", "type": "torch.Tensor", "text": " \nexpm1() \u2192 Tensor  \nSee torch.expm1() \n"}, {"name": "torch.Tensor.expm1_()", "path": "tensors#torch.Tensor.expm1_", "type": "torch.Tensor", "text": " \nexpm1_() \u2192 Tensor  \nIn-place version of expm1() \n"}, {"name": "torch.Tensor.exponential_()", "path": "tensors#torch.Tensor.exponential_", "type": "torch.Tensor", "text": " \nexponential_(lambd=1, *, generator=None) \u2192 Tensor  \nFills self tensor with elements drawn from the exponential distribution:  f(x)=\u03bbe\u2212\u03bbxf(x) = \\lambda e^{-\\lambda x} \n\n"}, {"name": "torch.Tensor.exp_()", "path": "tensors#torch.Tensor.exp_", "type": "torch.Tensor", "text": " \nexp_() \u2192 Tensor  \nIn-place version of exp() \n"}, {"name": "torch.Tensor.fill_()", "path": "tensors#torch.Tensor.fill_", "type": "torch.Tensor", "text": " \nfill_(value) \u2192 Tensor  \nFills self tensor with the specified value. \n"}, {"name": "torch.Tensor.fill_diagonal_()", "path": "tensors#torch.Tensor.fill_diagonal_", "type": "torch.Tensor", "text": " \nfill_diagonal_(fill_value, wrap=False) \u2192 Tensor  \nFill the main diagonal of a tensor that has at least 2-dimensions. When dims>2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.  Parameters \n \nfill_value (Scalar) \u2013 the fill value \nwrap (bool) \u2013 the diagonal \u2018wrapped\u2019 after N columns for tall matrices.    Example: >>> a = torch.zeros(3, 3)\n>>> a.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n>>> b = torch.zeros(7, 3)\n>>> b.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n>>> c = torch.zeros(7, 3)\n>>> c.fill_diagonal_(5, wrap=True)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n \n"}, {"name": "torch.Tensor.fix()", "path": "tensors#torch.Tensor.fix", "type": "torch.Tensor", "text": " \nfix() \u2192 Tensor  \nSee torch.fix(). \n"}, {"name": "torch.Tensor.fix_()", "path": "tensors#torch.Tensor.fix_", "type": "torch.Tensor", "text": " \nfix_() \u2192 Tensor  \nIn-place version of fix() \n"}, {"name": "torch.Tensor.flatten()", "path": "tensors#torch.Tensor.flatten", "type": "torch.Tensor", "text": " \nflatten(input, start_dim=0, end_dim=-1) \u2192 Tensor  \nsee torch.flatten() \n"}, {"name": "torch.Tensor.flip()", "path": "tensors#torch.Tensor.flip", "type": "torch.Tensor", "text": " \nflip(dims) \u2192 Tensor  \nSee torch.flip() \n"}, {"name": "torch.Tensor.fliplr()", "path": "tensors#torch.Tensor.fliplr", "type": "torch.Tensor", "text": " \nfliplr() \u2192 Tensor  \nSee torch.fliplr() \n"}, {"name": "torch.Tensor.flipud()", "path": "tensors#torch.Tensor.flipud", "type": "torch.Tensor", "text": " \nflipud() \u2192 Tensor  \nSee torch.flipud() \n"}, {"name": "torch.Tensor.float()", "path": "tensors#torch.Tensor.float", "type": "torch.Tensor", "text": " \nfloat(memory_format=torch.preserve_format) \u2192 Tensor  \nself.float() is equivalent to self.to(torch.float32). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.float_power()", "path": "tensors#torch.Tensor.float_power", "type": "torch.Tensor", "text": " \nfloat_power(exponent) \u2192 Tensor  \nSee torch.float_power() \n"}, {"name": "torch.Tensor.float_power_()", "path": "tensors#torch.Tensor.float_power_", "type": "torch.Tensor", "text": " \nfloat_power_(exponent) \u2192 Tensor  \nIn-place version of float_power() \n"}, {"name": "torch.Tensor.floor()", "path": "tensors#torch.Tensor.floor", "type": "torch.Tensor", "text": " \nfloor() \u2192 Tensor  \nSee torch.floor() \n"}, {"name": "torch.Tensor.floor_()", "path": "tensors#torch.Tensor.floor_", "type": "torch.Tensor", "text": " \nfloor_() \u2192 Tensor  \nIn-place version of floor() \n"}, {"name": "torch.Tensor.floor_divide()", "path": "tensors#torch.Tensor.floor_divide", "type": "torch.Tensor", "text": " \nfloor_divide(value) \u2192 Tensor  \nSee torch.floor_divide() \n"}, {"name": "torch.Tensor.floor_divide_()", "path": "tensors#torch.Tensor.floor_divide_", "type": "torch.Tensor", "text": " \nfloor_divide_(value) \u2192 Tensor  \nIn-place version of floor_divide() \n"}, {"name": "torch.Tensor.fmax()", "path": "tensors#torch.Tensor.fmax", "type": "torch.Tensor", "text": " \nfmax(other) \u2192 Tensor  \nSee torch.fmax() \n"}, {"name": "torch.Tensor.fmin()", "path": "tensors#torch.Tensor.fmin", "type": "torch.Tensor", "text": " \nfmin(other) \u2192 Tensor  \nSee torch.fmin() \n"}, {"name": "torch.Tensor.fmod()", "path": "tensors#torch.Tensor.fmod", "type": "torch.Tensor", "text": " \nfmod(divisor) \u2192 Tensor  \nSee torch.fmod() \n"}, {"name": "torch.Tensor.fmod_()", "path": "tensors#torch.Tensor.fmod_", "type": "torch.Tensor", "text": " \nfmod_(divisor) \u2192 Tensor  \nIn-place version of fmod() \n"}, {"name": "torch.Tensor.frac()", "path": "tensors#torch.Tensor.frac", "type": "torch.Tensor", "text": " \nfrac() \u2192 Tensor  \nSee torch.frac() \n"}, {"name": "torch.Tensor.frac_()", "path": "tensors#torch.Tensor.frac_", "type": "torch.Tensor", "text": " \nfrac_() \u2192 Tensor  \nIn-place version of frac() \n"}, {"name": "torch.Tensor.gather()", "path": "tensors#torch.Tensor.gather", "type": "torch.Tensor", "text": " \ngather(dim, index) \u2192 Tensor  \nSee torch.gather() \n"}, {"name": "torch.Tensor.gcd()", "path": "tensors#torch.Tensor.gcd", "type": "torch.Tensor", "text": " \ngcd(other) \u2192 Tensor  \nSee torch.gcd() \n"}, {"name": "torch.Tensor.gcd_()", "path": "tensors#torch.Tensor.gcd_", "type": "torch.Tensor", "text": " \ngcd_(other) \u2192 Tensor  \nIn-place version of gcd() \n"}, {"name": "torch.Tensor.ge()", "path": "tensors#torch.Tensor.ge", "type": "torch.Tensor", "text": " \nge(other) \u2192 Tensor  \nSee torch.ge(). \n"}, {"name": "torch.Tensor.geometric_()", "path": "tensors#torch.Tensor.geometric_", "type": "torch.Tensor", "text": " \ngeometric_(p, *, generator=None) \u2192 Tensor  \nFills self tensor with elements drawn from the geometric distribution:  f(X=k)=pk\u22121(1\u2212p)f(X=k) = p^{k - 1} (1 - p) \n\n"}, {"name": "torch.Tensor.geqrf()", "path": "tensors#torch.Tensor.geqrf", "type": "torch.Tensor", "text": " \ngeqrf() -> (Tensor, Tensor)  \nSee torch.geqrf() \n"}, {"name": "torch.Tensor.ger()", "path": "tensors#torch.Tensor.ger", "type": "torch.Tensor", "text": " \nger(vec2) \u2192 Tensor  \nSee torch.ger() \n"}, {"name": "torch.Tensor.get_device()", "path": "tensors#torch.Tensor.get_device", "type": "torch.Tensor", "text": " \nget_device() -> Device ordinal (Integer)  \nFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown. Example: >>> x = torch.randn(3, 4, 5, device='cuda:0')\n>>> x.get_device()\n0\n>>> x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor\n \n"}, {"name": "torch.Tensor.ge_()", "path": "tensors#torch.Tensor.ge_", "type": "torch.Tensor", "text": " \nge_(other) \u2192 Tensor  \nIn-place version of ge(). \n"}, {"name": "torch.Tensor.grad", "path": "autograd#torch.Tensor.grad", "type": "torch.autograd", "text": " \ngrad  \nThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it. \n"}, {"name": "torch.Tensor.greater()", "path": "tensors#torch.Tensor.greater", "type": "torch.Tensor", "text": " \ngreater(other) \u2192 Tensor  \nSee torch.greater(). \n"}, {"name": "torch.Tensor.greater_()", "path": "tensors#torch.Tensor.greater_", "type": "torch.Tensor", "text": " \ngreater_(other) \u2192 Tensor  \nIn-place version of greater(). \n"}, {"name": "torch.Tensor.greater_equal()", "path": "tensors#torch.Tensor.greater_equal", "type": "torch.Tensor", "text": " \ngreater_equal(other) \u2192 Tensor  \nSee torch.greater_equal(). \n"}, {"name": "torch.Tensor.greater_equal_()", "path": "tensors#torch.Tensor.greater_equal_", "type": "torch.Tensor", "text": " \ngreater_equal_(other) \u2192 Tensor  \nIn-place version of greater_equal(). \n"}, {"name": "torch.Tensor.gt()", "path": "tensors#torch.Tensor.gt", "type": "torch.Tensor", "text": " \ngt(other) \u2192 Tensor  \nSee torch.gt(). \n"}, {"name": "torch.Tensor.gt_()", "path": "tensors#torch.Tensor.gt_", "type": "torch.Tensor", "text": " \ngt_(other) \u2192 Tensor  \nIn-place version of gt(). \n"}, {"name": "torch.Tensor.half()", "path": "tensors#torch.Tensor.half", "type": "torch.Tensor", "text": " \nhalf(memory_format=torch.preserve_format) \u2192 Tensor  \nself.half() is equivalent to self.to(torch.float16). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.hardshrink()", "path": "tensors#torch.Tensor.hardshrink", "type": "torch.Tensor", "text": " \nhardshrink(lambd=0.5) \u2192 Tensor  \nSee torch.nn.functional.hardshrink() \n"}, {"name": "torch.Tensor.heaviside()", "path": "tensors#torch.Tensor.heaviside", "type": "torch.Tensor", "text": " \nheaviside(values) \u2192 Tensor  \nSee torch.heaviside() \n"}, {"name": "torch.Tensor.histc()", "path": "tensors#torch.Tensor.histc", "type": "torch.Tensor", "text": " \nhistc(bins=100, min=0, max=0) \u2192 Tensor  \nSee torch.histc() \n"}, {"name": "torch.Tensor.hypot()", "path": "tensors#torch.Tensor.hypot", "type": "torch.Tensor", "text": " \nhypot(other) \u2192 Tensor  \nSee torch.hypot() \n"}, {"name": "torch.Tensor.hypot_()", "path": "tensors#torch.Tensor.hypot_", "type": "torch.Tensor", "text": " \nhypot_(other) \u2192 Tensor  \nIn-place version of hypot() \n"}, {"name": "torch.Tensor.i0()", "path": "tensors#torch.Tensor.i0", "type": "torch.Tensor", "text": " \ni0() \u2192 Tensor  \nSee torch.i0() \n"}, {"name": "torch.Tensor.i0_()", "path": "tensors#torch.Tensor.i0_", "type": "torch.Tensor", "text": " \ni0_() \u2192 Tensor  \nIn-place version of i0() \n"}, {"name": "torch.Tensor.igamma()", "path": "tensors#torch.Tensor.igamma", "type": "torch.Tensor", "text": " \nigamma(other) \u2192 Tensor  \nSee torch.igamma() \n"}, {"name": "torch.Tensor.igammac()", "path": "tensors#torch.Tensor.igammac", "type": "torch.Tensor", "text": " \nigammac(other) \u2192 Tensor  \nSee torch.igammac() \n"}, {"name": "torch.Tensor.igammac_()", "path": "tensors#torch.Tensor.igammac_", "type": "torch.Tensor", "text": " \nigammac_(other) \u2192 Tensor  \nIn-place version of igammac() \n"}, {"name": "torch.Tensor.igamma_()", "path": "tensors#torch.Tensor.igamma_", "type": "torch.Tensor", "text": " \nigamma_(other) \u2192 Tensor  \nIn-place version of igamma() \n"}, {"name": "torch.Tensor.imag", "path": "tensors#torch.Tensor.imag", "type": "torch.Tensor", "text": " \nimag  \nReturns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.  Warning imag() is only supported for tensors with complex dtypes.   Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n   \n"}, {"name": "torch.Tensor.index_add()", "path": "tensors#torch.Tensor.index_add", "type": "torch.Tensor", "text": " \nindex_add(tensor1, dim, index, tensor2) \u2192 Tensor  \nOut-of-place version of torch.Tensor.index_add_(). tensor1 corresponds to self in torch.Tensor.index_add_(). \n"}, {"name": "torch.Tensor.index_add_()", "path": "tensors#torch.Tensor.index_add_", "type": "torch.Tensor", "text": " \nindex_add_(dim, index, tensor) \u2192 Tensor  \nAccumulate the elements of tensor into the self tensor by adding to the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is added to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Note This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ndim (int) \u2013 dimension along which to index \nindex (IntTensor or LongTensor) \u2013 indices of tensor to select from \ntensor (Tensor) \u2013 the tensor containing values to add    Example: >>> x = torch.ones(5, 3)\n>>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 4, 2])\n>>> x.index_add_(0, index, t)\ntensor([[  2.,   3.,   4.],\n        [  1.,   1.,   1.],\n        [  8.,   9.,  10.],\n        [  1.,   1.,   1.],\n        [  5.,   6.,   7.]])\n \n"}, {"name": "torch.Tensor.index_copy()", "path": "tensors#torch.Tensor.index_copy", "type": "torch.Tensor", "text": " \nindex_copy(tensor1, dim, index, tensor2) \u2192 Tensor  \nOut-of-place version of torch.Tensor.index_copy_(). tensor1 corresponds to self in torch.Tensor.index_copy_(). \n"}, {"name": "torch.Tensor.index_copy_()", "path": "tensors#torch.Tensor.index_copy_", "type": "torch.Tensor", "text": " \nindex_copy_(dim, index, tensor) \u2192 Tensor  \nCopies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Note If index contains duplicate entries, multiple elements from tensor will be copied to the same index of self. The result is nondeterministic since it depends on which copy occurs last.   Parameters \n \ndim (int) \u2013 dimension along which to index \nindex (LongTensor) \u2013 indices of tensor to select from \ntensor (Tensor) \u2013 the tensor containing values to copy    Example: >>> x = torch.zeros(5, 3)\n>>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 4, 2])\n>>> x.index_copy_(0, index, t)\ntensor([[ 1.,  2.,  3.],\n        [ 0.,  0.,  0.],\n        [ 7.,  8.,  9.],\n        [ 0.,  0.,  0.],\n        [ 4.,  5.,  6.]])\n \n"}, {"name": "torch.Tensor.index_fill()", "path": "tensors#torch.Tensor.index_fill", "type": "torch.Tensor", "text": " \nindex_fill(tensor1, dim, index, value) \u2192 Tensor  \nOut-of-place version of torch.Tensor.index_fill_(). tensor1 corresponds to self in torch.Tensor.index_fill_(). \n"}, {"name": "torch.Tensor.index_fill_()", "path": "tensors#torch.Tensor.index_fill_", "type": "torch.Tensor", "text": " \nindex_fill_(dim, index, val) \u2192 Tensor  \nFills the elements of the self tensor with value val by selecting the indices in the order given in index.  Parameters \n \ndim (int) \u2013 dimension along which to index \nindex (LongTensor) \u2013 indices of self tensor to fill in \nval (float) \u2013 the value to fill with     Example::\n\n>>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n>>> index = torch.tensor([0, 2])\n>>> x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n        [-1.,  5., -1.],\n        [-1.,  8., -1.]])\n   \n"}, {"name": "torch.Tensor.index_put()", "path": "tensors#torch.Tensor.index_put", "type": "torch.Tensor", "text": " \nindex_put(tensor1, indices, values, accumulate=False) \u2192 Tensor  \nOut-place version of index_put_(). tensor1 corresponds to self in torch.Tensor.index_put_(). \n"}, {"name": "torch.Tensor.index_put_()", "path": "tensors#torch.Tensor.index_put_", "type": "torch.Tensor", "text": " \nindex_put_(indices, values, accumulate=False) \u2192 Tensor  \nPuts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, values) is equivalent to tensor[indices] = values. Returns self. If accumulate is True, the elements in values are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters \n \nindices (tuple of LongTensor) \u2013 tensors used to index into self. \nvalues (Tensor) \u2013 tensor of same dtype as self. \naccumulate (bool) \u2013 whether to accumulate into self    \n"}, {"name": "torch.Tensor.index_select()", "path": "tensors#torch.Tensor.index_select", "type": "torch.Tensor", "text": " \nindex_select(dim, index) \u2192 Tensor  \nSee torch.index_select() \n"}, {"name": "torch.Tensor.indices()", "path": "sparse#torch.Tensor.indices", "type": "torch.sparse", "text": " \nindices() \u2192 Tensor  \nReturn the indices tensor of a sparse COO tensor.  Warning Throws an error if self is not a sparse COO tensor.  See also Tensor.values().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  \n"}, {"name": "torch.Tensor.inner()", "path": "tensors#torch.Tensor.inner", "type": "torch.Tensor", "text": " \ninner(other) \u2192 Tensor  \nSee torch.inner(). \n"}, {"name": "torch.Tensor.int()", "path": "tensors#torch.Tensor.int", "type": "torch.Tensor", "text": " \nint(memory_format=torch.preserve_format) \u2192 Tensor  \nself.int() is equivalent to self.to(torch.int32). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.int_repr()", "path": "tensors#torch.Tensor.int_repr", "type": "torch.Tensor", "text": " \nint_repr() \u2192 Tensor  \nGiven a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor. \n"}, {"name": "torch.Tensor.inverse()", "path": "tensors#torch.Tensor.inverse", "type": "torch.Tensor", "text": " \ninverse() \u2192 Tensor  \nSee torch.inverse() \n"}, {"name": "torch.Tensor.isclose()", "path": "tensors#torch.Tensor.isclose", "type": "torch.Tensor", "text": " \nisclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor  \nSee torch.isclose() \n"}, {"name": "torch.Tensor.isfinite()", "path": "tensors#torch.Tensor.isfinite", "type": "torch.Tensor", "text": " \nisfinite() \u2192 Tensor  \nSee torch.isfinite() \n"}, {"name": "torch.Tensor.isinf()", "path": "tensors#torch.Tensor.isinf", "type": "torch.Tensor", "text": " \nisinf() \u2192 Tensor  \nSee torch.isinf() \n"}, {"name": "torch.Tensor.isnan()", "path": "tensors#torch.Tensor.isnan", "type": "torch.Tensor", "text": " \nisnan() \u2192 Tensor  \nSee torch.isnan() \n"}, {"name": "torch.Tensor.isneginf()", "path": "tensors#torch.Tensor.isneginf", "type": "torch.Tensor", "text": " \nisneginf() \u2192 Tensor  \nSee torch.isneginf() \n"}, {"name": "torch.Tensor.isposinf()", "path": "tensors#torch.Tensor.isposinf", "type": "torch.Tensor", "text": " \nisposinf() \u2192 Tensor  \nSee torch.isposinf() \n"}, {"name": "torch.Tensor.isreal()", "path": "tensors#torch.Tensor.isreal", "type": "torch.Tensor", "text": " \nisreal() \u2192 Tensor  \nSee torch.isreal() \n"}, {"name": "torch.Tensor.istft()", "path": "tensors#torch.Tensor.istft", "type": "torch.Tensor", "text": " \nistft(n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False) [source]\n \nSee torch.istft() \n"}, {"name": "torch.Tensor.is_coalesced()", "path": "sparse#torch.Tensor.is_coalesced", "type": "torch.sparse", "text": " \nis_coalesced() \u2192 bool  \nReturns True if self is a sparse COO tensor that is coalesced, False otherwise.  Warning Throws an error if self is not a sparse COO tensor.  See coalesce() and uncoalesced tensors. \n"}, {"name": "torch.Tensor.is_complex()", "path": "tensors#torch.Tensor.is_complex", "type": "torch.Tensor", "text": " \nis_complex() \u2192 bool  \nReturns True if the data type of self is a complex data type. \n"}, {"name": "torch.Tensor.is_contiguous()", "path": "tensors#torch.Tensor.is_contiguous", "type": "torch.Tensor", "text": " \nis_contiguous(memory_format=torch.contiguous_format) \u2192 bool  \nReturns True if self tensor is contiguous in memory in the order specified by memory format.  Parameters \nmemory_format (torch.memory_format, optional) \u2013 Specifies memory allocation order. Default: torch.contiguous_format.   \n"}, {"name": "torch.Tensor.is_cuda", "path": "tensors#torch.Tensor.is_cuda", "type": "torch.Tensor", "text": " \nis_cuda  \nIs True if the Tensor is stored on the GPU, False otherwise. \n"}, {"name": "torch.Tensor.is_floating_point()", "path": "tensors#torch.Tensor.is_floating_point", "type": "torch.Tensor", "text": " \nis_floating_point() \u2192 bool  \nReturns True if the data type of self is a floating point data type. \n"}, {"name": "torch.Tensor.is_leaf", "path": "autograd#torch.Tensor.is_leaf", "type": "torch.autograd", "text": " \nis_leaf  \nAll Tensors that have requires_grad which is False will be leaf Tensors by convention. For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Example: >>> a = torch.rand(10, requires_grad=True)\n>>> a.is_leaf\nTrue\n>>> b = torch.rand(10, requires_grad=True).cuda()\n>>> b.is_leaf\nFalse\n# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n>>> c = torch.rand(10, requires_grad=True) + 2\n>>> c.is_leaf\nFalse\n# c was created by the addition operation\n>>> d = torch.rand(10).cuda()\n>>> d.is_leaf\nTrue\n# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n>>> e = torch.rand(10).cuda().requires_grad_()\n>>> e.is_leaf\nTrue\n# e requires gradients and has no operations creating it\n>>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n>>> f.is_leaf\nTrue\n# f requires grad, has no operation creating it\n \n"}, {"name": "torch.Tensor.is_meta", "path": "tensors#torch.Tensor.is_meta", "type": "torch.Tensor", "text": " \nis_meta  \nIs True if the Tensor is a meta tensor, False otherwise. Meta tensors are like normal tensors, but they carry no data. \n"}, {"name": "torch.Tensor.is_pinned()", "path": "tensors#torch.Tensor.is_pinned", "type": "torch.Tensor", "text": " \nis_pinned()  \nReturns true if this tensor resides in pinned memory. \n"}, {"name": "torch.Tensor.is_quantized", "path": "tensors#torch.Tensor.is_quantized", "type": "torch.Tensor", "text": " \nis_quantized  \nIs True if the Tensor is quantized, False otherwise. \n"}, {"name": "torch.Tensor.is_set_to()", "path": "tensors#torch.Tensor.is_set_to", "type": "torch.Tensor", "text": " \nis_set_to(tensor) \u2192 bool  \nReturns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride). \n"}, {"name": "torch.Tensor.is_shared()", "path": "tensors#torch.Tensor.is_shared", "type": "torch.Tensor", "text": " \nis_shared() [source]\n \nChecks if tensor is in shared memory. This is always True for CUDA tensors. \n"}, {"name": "torch.Tensor.is_signed()", "path": "tensors#torch.Tensor.is_signed", "type": "torch.Tensor", "text": " \nis_signed() \u2192 bool  \nReturns True if the data type of self is a signed data type. \n"}, {"name": "torch.Tensor.is_sparse", "path": "sparse#torch.Tensor.is_sparse", "type": "torch.sparse", "text": " \nis_sparse  \nIs True if the Tensor uses sparse storage layout, False otherwise. \n"}, {"name": "torch.Tensor.item()", "path": "tensors#torch.Tensor.item", "type": "torch.Tensor", "text": " \nitem() \u2192 number  \nReturns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist(). This operation is not differentiable. Example: >>> x = torch.tensor([1.0])\n>>> x.item()\n1.0\n \n"}, {"name": "torch.Tensor.kthvalue()", "path": "tensors#torch.Tensor.kthvalue", "type": "torch.Tensor", "text": " \nkthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.kthvalue() \n"}, {"name": "torch.Tensor.lcm()", "path": "tensors#torch.Tensor.lcm", "type": "torch.Tensor", "text": " \nlcm(other) \u2192 Tensor  \nSee torch.lcm() \n"}, {"name": "torch.Tensor.lcm_()", "path": "tensors#torch.Tensor.lcm_", "type": "torch.Tensor", "text": " \nlcm_(other) \u2192 Tensor  \nIn-place version of lcm() \n"}, {"name": "torch.Tensor.ldexp()", "path": "tensors#torch.Tensor.ldexp", "type": "torch.Tensor", "text": " \nldexp(other) \u2192 Tensor  \nSee torch.ldexp() \n"}, {"name": "torch.Tensor.ldexp_()", "path": "tensors#torch.Tensor.ldexp_", "type": "torch.Tensor", "text": " \nldexp_(other) \u2192 Tensor  \nIn-place version of ldexp() \n"}, {"name": "torch.Tensor.le()", "path": "tensors#torch.Tensor.le", "type": "torch.Tensor", "text": " \nle(other) \u2192 Tensor  \nSee torch.le(). \n"}, {"name": "torch.Tensor.lerp()", "path": "tensors#torch.Tensor.lerp", "type": "torch.Tensor", "text": " \nlerp(end, weight) \u2192 Tensor  \nSee torch.lerp() \n"}, {"name": "torch.Tensor.lerp_()", "path": "tensors#torch.Tensor.lerp_", "type": "torch.Tensor", "text": " \nlerp_(end, weight) \u2192 Tensor  \nIn-place version of lerp() \n"}, {"name": "torch.Tensor.less()", "path": "tensors#torch.Tensor.less", "type": "torch.Tensor", "text": " \nless()  \nlt(other) -> Tensor See torch.less(). \n"}, {"name": "torch.Tensor.less_()", "path": "tensors#torch.Tensor.less_", "type": "torch.Tensor", "text": " \nless_(other) \u2192 Tensor  \nIn-place version of less(). \n"}, {"name": "torch.Tensor.less_equal()", "path": "tensors#torch.Tensor.less_equal", "type": "torch.Tensor", "text": " \nless_equal(other) \u2192 Tensor  \nSee torch.less_equal(). \n"}, {"name": "torch.Tensor.less_equal_()", "path": "tensors#torch.Tensor.less_equal_", "type": "torch.Tensor", "text": " \nless_equal_(other) \u2192 Tensor  \nIn-place version of less_equal(). \n"}, {"name": "torch.Tensor.le_()", "path": "tensors#torch.Tensor.le_", "type": "torch.Tensor", "text": " \nle_(other) \u2192 Tensor  \nIn-place version of le(). \n"}, {"name": "torch.Tensor.lgamma()", "path": "tensors#torch.Tensor.lgamma", "type": "torch.Tensor", "text": " \nlgamma() \u2192 Tensor  \nSee torch.lgamma() \n"}, {"name": "torch.Tensor.lgamma_()", "path": "tensors#torch.Tensor.lgamma_", "type": "torch.Tensor", "text": " \nlgamma_() \u2192 Tensor  \nIn-place version of lgamma() \n"}, {"name": "torch.Tensor.log()", "path": "tensors#torch.Tensor.log", "type": "torch.Tensor", "text": " \nlog() \u2192 Tensor  \nSee torch.log() \n"}, {"name": "torch.Tensor.log10()", "path": "tensors#torch.Tensor.log10", "type": "torch.Tensor", "text": " \nlog10() \u2192 Tensor  \nSee torch.log10() \n"}, {"name": "torch.Tensor.log10_()", "path": "tensors#torch.Tensor.log10_", "type": "torch.Tensor", "text": " \nlog10_() \u2192 Tensor  \nIn-place version of log10() \n"}, {"name": "torch.Tensor.log1p()", "path": "tensors#torch.Tensor.log1p", "type": "torch.Tensor", "text": " \nlog1p() \u2192 Tensor  \nSee torch.log1p() \n"}, {"name": "torch.Tensor.log1p_()", "path": "tensors#torch.Tensor.log1p_", "type": "torch.Tensor", "text": " \nlog1p_() \u2192 Tensor  \nIn-place version of log1p() \n"}, {"name": "torch.Tensor.log2()", "path": "tensors#torch.Tensor.log2", "type": "torch.Tensor", "text": " \nlog2() \u2192 Tensor  \nSee torch.log2() \n"}, {"name": "torch.Tensor.log2_()", "path": "tensors#torch.Tensor.log2_", "type": "torch.Tensor", "text": " \nlog2_() \u2192 Tensor  \nIn-place version of log2() \n"}, {"name": "torch.Tensor.logaddexp()", "path": "tensors#torch.Tensor.logaddexp", "type": "torch.Tensor", "text": " \nlogaddexp(other) \u2192 Tensor  \nSee torch.logaddexp() \n"}, {"name": "torch.Tensor.logaddexp2()", "path": "tensors#torch.Tensor.logaddexp2", "type": "torch.Tensor", "text": " \nlogaddexp2(other) \u2192 Tensor  \nSee torch.logaddexp2() \n"}, {"name": "torch.Tensor.logcumsumexp()", "path": "tensors#torch.Tensor.logcumsumexp", "type": "torch.Tensor", "text": " \nlogcumsumexp(dim) \u2192 Tensor  \nSee torch.logcumsumexp() \n"}, {"name": "torch.Tensor.logdet()", "path": "tensors#torch.Tensor.logdet", "type": "torch.Tensor", "text": " \nlogdet() \u2192 Tensor  \nSee torch.logdet() \n"}, {"name": "torch.Tensor.logical_and()", "path": "tensors#torch.Tensor.logical_and", "type": "torch.Tensor", "text": " \nlogical_and() \u2192 Tensor  \nSee torch.logical_and() \n"}, {"name": "torch.Tensor.logical_and_()", "path": "tensors#torch.Tensor.logical_and_", "type": "torch.Tensor", "text": " \nlogical_and_() \u2192 Tensor  \nIn-place version of logical_and() \n"}, {"name": "torch.Tensor.logical_not()", "path": "tensors#torch.Tensor.logical_not", "type": "torch.Tensor", "text": " \nlogical_not() \u2192 Tensor  \nSee torch.logical_not() \n"}, {"name": "torch.Tensor.logical_not_()", "path": "tensors#torch.Tensor.logical_not_", "type": "torch.Tensor", "text": " \nlogical_not_() \u2192 Tensor  \nIn-place version of logical_not() \n"}, {"name": "torch.Tensor.logical_or()", "path": "tensors#torch.Tensor.logical_or", "type": "torch.Tensor", "text": " \nlogical_or() \u2192 Tensor  \nSee torch.logical_or() \n"}, {"name": "torch.Tensor.logical_or_()", "path": "tensors#torch.Tensor.logical_or_", "type": "torch.Tensor", "text": " \nlogical_or_() \u2192 Tensor  \nIn-place version of logical_or() \n"}, {"name": "torch.Tensor.logical_xor()", "path": "tensors#torch.Tensor.logical_xor", "type": "torch.Tensor", "text": " \nlogical_xor() \u2192 Tensor  \nSee torch.logical_xor() \n"}, {"name": "torch.Tensor.logical_xor_()", "path": "tensors#torch.Tensor.logical_xor_", "type": "torch.Tensor", "text": " \nlogical_xor_() \u2192 Tensor  \nIn-place version of logical_xor() \n"}, {"name": "torch.Tensor.logit()", "path": "tensors#torch.Tensor.logit", "type": "torch.Tensor", "text": " \nlogit() \u2192 Tensor  \nSee torch.logit() \n"}, {"name": "torch.Tensor.logit_()", "path": "tensors#torch.Tensor.logit_", "type": "torch.Tensor", "text": " \nlogit_() \u2192 Tensor  \nIn-place version of logit() \n"}, {"name": "torch.Tensor.logsumexp()", "path": "tensors#torch.Tensor.logsumexp", "type": "torch.Tensor", "text": " \nlogsumexp(dim, keepdim=False) \u2192 Tensor  \nSee torch.logsumexp() \n"}, {"name": "torch.Tensor.log_()", "path": "tensors#torch.Tensor.log_", "type": "torch.Tensor", "text": " \nlog_() \u2192 Tensor  \nIn-place version of log() \n"}, {"name": "torch.Tensor.log_normal_()", "path": "tensors#torch.Tensor.log_normal_", "type": "torch.Tensor", "text": " \nlog_normal_(mean=1, std=2, *, generator=None)  \nFills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu  and standard deviation \u03c3\\sigma . Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:  f(x)=1x\u03c32\u03c0e\u2212(ln\u2061x\u2212\u03bc)22\u03c32f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}} \n\n"}, {"name": "torch.Tensor.long()", "path": "tensors#torch.Tensor.long", "type": "torch.Tensor", "text": " \nlong(memory_format=torch.preserve_format) \u2192 Tensor  \nself.long() is equivalent to self.to(torch.int64). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.lstsq()", "path": "tensors#torch.Tensor.lstsq", "type": "torch.Tensor", "text": " \nlstsq(A) -> (Tensor, Tensor)  \nSee torch.lstsq() \n"}, {"name": "torch.Tensor.lt()", "path": "tensors#torch.Tensor.lt", "type": "torch.Tensor", "text": " \nlt(other) \u2192 Tensor  \nSee torch.lt(). \n"}, {"name": "torch.Tensor.lt_()", "path": "tensors#torch.Tensor.lt_", "type": "torch.Tensor", "text": " \nlt_(other) \u2192 Tensor  \nIn-place version of lt(). \n"}, {"name": "torch.Tensor.lu()", "path": "tensors#torch.Tensor.lu", "type": "torch.Tensor", "text": " \nlu(pivot=True, get_infos=False) [source]\n \nSee torch.lu() \n"}, {"name": "torch.Tensor.lu_solve()", "path": "tensors#torch.Tensor.lu_solve", "type": "torch.Tensor", "text": " \nlu_solve(LU_data, LU_pivots) \u2192 Tensor  \nSee torch.lu_solve() \n"}, {"name": "torch.Tensor.map_()", "path": "tensors#torch.Tensor.map_", "type": "torch.Tensor", "text": " \nmap_(tensor, callable)  \nApplies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable. The callable should have the signature: def callable(a, b) -> number\n \n"}, {"name": "torch.Tensor.masked_fill()", "path": "tensors#torch.Tensor.masked_fill", "type": "torch.Tensor", "text": " \nmasked_fill(mask, value) \u2192 Tensor  \nOut-of-place version of torch.Tensor.masked_fill_() \n"}, {"name": "torch.Tensor.masked_fill_()", "path": "tensors#torch.Tensor.masked_fill_", "type": "torch.Tensor", "text": " \nmasked_fill_(mask, value)  \nFills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor.  Parameters \n \nmask (BoolTensor) \u2013 the boolean mask \nvalue (float) \u2013 the value to fill in with    \n"}, {"name": "torch.Tensor.masked_scatter()", "path": "tensors#torch.Tensor.masked_scatter", "type": "torch.Tensor", "text": " \nmasked_scatter(mask, tensor) \u2192 Tensor  \nOut-of-place version of torch.Tensor.masked_scatter_() \n"}, {"name": "torch.Tensor.masked_scatter_()", "path": "tensors#torch.Tensor.masked_scatter_", "type": "torch.Tensor", "text": " \nmasked_scatter_(mask, source)  \nCopies elements from source into self tensor at positions where the mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask  Parameters \n \nmask (BoolTensor) \u2013 the boolean mask \nsource (Tensor) \u2013 the tensor to copy from     Note The mask operates on the self tensor, not on the given source tensor.  \n"}, {"name": "torch.Tensor.masked_select()", "path": "tensors#torch.Tensor.masked_select", "type": "torch.Tensor", "text": " \nmasked_select(mask) \u2192 Tensor  \nSee torch.masked_select() \n"}, {"name": "torch.Tensor.matmul()", "path": "tensors#torch.Tensor.matmul", "type": "torch.Tensor", "text": " \nmatmul(tensor2) \u2192 Tensor  \nSee torch.matmul() \n"}, {"name": "torch.Tensor.matrix_exp()", "path": "tensors#torch.Tensor.matrix_exp", "type": "torch.Tensor", "text": " \nmatrix_exp() \u2192 Tensor  \nSee torch.matrix_exp() \n"}, {"name": "torch.Tensor.matrix_power()", "path": "tensors#torch.Tensor.matrix_power", "type": "torch.Tensor", "text": " \nmatrix_power(n) \u2192 Tensor  \nSee torch.matrix_power() \n"}, {"name": "torch.Tensor.max()", "path": "tensors#torch.Tensor.max", "type": "torch.Tensor", "text": " \nmax(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.max() \n"}, {"name": "torch.Tensor.maximum()", "path": "tensors#torch.Tensor.maximum", "type": "torch.Tensor", "text": " \nmaximum(other) \u2192 Tensor  \nSee torch.maximum() \n"}, {"name": "torch.Tensor.mean()", "path": "tensors#torch.Tensor.mean", "type": "torch.Tensor", "text": " \nmean(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.mean() \n"}, {"name": "torch.Tensor.median()", "path": "tensors#torch.Tensor.median", "type": "torch.Tensor", "text": " \nmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.median() \n"}, {"name": "torch.Tensor.min()", "path": "tensors#torch.Tensor.min", "type": "torch.Tensor", "text": " \nmin(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)  \nSee torch.min() \n"}, {"name": "torch.Tensor.minimum()", "path": "tensors#torch.Tensor.minimum", "type": "torch.Tensor", "text": " \nminimum(other) \u2192 Tensor  \nSee torch.minimum() \n"}, {"name": "torch.Tensor.mm()", "path": "tensors#torch.Tensor.mm", "type": "torch.Tensor", "text": " \nmm(mat2) \u2192 Tensor  \nSee torch.mm() \n"}, {"name": "torch.Tensor.mode()", "path": "tensors#torch.Tensor.mode", "type": "torch.Tensor", "text": " \nmode(dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.mode() \n"}, {"name": "torch.Tensor.moveaxis()", "path": "tensors#torch.Tensor.moveaxis", "type": "torch.Tensor", "text": " \nmoveaxis(source, destination) \u2192 Tensor  \nSee torch.moveaxis() \n"}, {"name": "torch.Tensor.movedim()", "path": "tensors#torch.Tensor.movedim", "type": "torch.Tensor", "text": " \nmovedim(source, destination) \u2192 Tensor  \nSee torch.movedim() \n"}, {"name": "torch.Tensor.msort()", "path": "tensors#torch.Tensor.msort", "type": "torch.Tensor", "text": " \nmsort() \u2192 Tensor  \nSee torch.msort() \n"}, {"name": "torch.Tensor.mul()", "path": "tensors#torch.Tensor.mul", "type": "torch.Tensor", "text": " \nmul(value) \u2192 Tensor  \nSee torch.mul(). \n"}, {"name": "torch.Tensor.multinomial()", "path": "tensors#torch.Tensor.multinomial", "type": "torch.Tensor", "text": " \nmultinomial(num_samples, replacement=False, *, generator=None) \u2192 Tensor  \nSee torch.multinomial() \n"}, {"name": "torch.Tensor.multiply()", "path": "tensors#torch.Tensor.multiply", "type": "torch.Tensor", "text": " \nmultiply(value) \u2192 Tensor  \nSee torch.multiply(). \n"}, {"name": "torch.Tensor.multiply_()", "path": "tensors#torch.Tensor.multiply_", "type": "torch.Tensor", "text": " \nmultiply_(value) \u2192 Tensor  \nIn-place version of multiply(). \n"}, {"name": "torch.Tensor.mul_()", "path": "tensors#torch.Tensor.mul_", "type": "torch.Tensor", "text": " \nmul_(value) \u2192 Tensor  \nIn-place version of mul(). \n"}, {"name": "torch.Tensor.mv()", "path": "tensors#torch.Tensor.mv", "type": "torch.Tensor", "text": " \nmv(vec) \u2192 Tensor  \nSee torch.mv() \n"}, {"name": "torch.Tensor.mvlgamma()", "path": "tensors#torch.Tensor.mvlgamma", "type": "torch.Tensor", "text": " \nmvlgamma(p) \u2192 Tensor  \nSee torch.mvlgamma() \n"}, {"name": "torch.Tensor.mvlgamma_()", "path": "tensors#torch.Tensor.mvlgamma_", "type": "torch.Tensor", "text": " \nmvlgamma_(p) \u2192 Tensor  \nIn-place version of mvlgamma() \n"}, {"name": "torch.Tensor.names", "path": "named_tensor#torch.Tensor.names", "type": "Named Tensors", "text": " \nnames  \nStores names for each of this tensor\u2019s dimensions. names[idx] corresponds to the name of tensor dimension idx. Names are either a string if the dimension is named or None if the dimension is unnamed. Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore). Tensors may not have two named dimensions with the same name.  Warning The named tensor API is experimental and subject to change.  \n"}, {"name": "torch.Tensor.nanmedian()", "path": "tensors#torch.Tensor.nanmedian", "type": "torch.Tensor", "text": " \nnanmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)  \nSee torch.nanmedian() \n"}, {"name": "torch.Tensor.nanquantile()", "path": "tensors#torch.Tensor.nanquantile", "type": "torch.Tensor", "text": " \nnanquantile(q, dim=None, keepdim=False) \u2192 Tensor  \nSee torch.nanquantile() \n"}, {"name": "torch.Tensor.nansum()", "path": "tensors#torch.Tensor.nansum", "type": "torch.Tensor", "text": " \nnansum(dim=None, keepdim=False, dtype=None) \u2192 Tensor  \nSee torch.nansum() \n"}, {"name": "torch.Tensor.nan_to_num()", "path": "tensors#torch.Tensor.nan_to_num", "type": "torch.Tensor", "text": " \nnan_to_num(nan=0.0, posinf=None, neginf=None) \u2192 Tensor  \nSee torch.nan_to_num(). \n"}, {"name": "torch.Tensor.nan_to_num_()", "path": "tensors#torch.Tensor.nan_to_num_", "type": "torch.Tensor", "text": " \nnan_to_num_(nan=0.0, posinf=None, neginf=None) \u2192 Tensor  \nIn-place version of nan_to_num(). \n"}, {"name": "torch.Tensor.narrow()", "path": "tensors#torch.Tensor.narrow", "type": "torch.Tensor", "text": " \nnarrow(dimension, start, length) \u2192 Tensor  \nSee torch.narrow() Example: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> x.narrow(0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> x.narrow(1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n \n"}, {"name": "torch.Tensor.narrow_copy()", "path": "tensors#torch.Tensor.narrow_copy", "type": "torch.Tensor", "text": " \nnarrow_copy(dimension, start, length) \u2192 Tensor  \nSame as Tensor.narrow() except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling `narrow_copy with `dimemsion > self.sparse_dim()` will return a copy with the relevant dense dimension narrowed, and `self.shape` updated accordingly. \n"}, {"name": "torch.Tensor.ndim", "path": "tensors#torch.Tensor.ndim", "type": "torch.Tensor", "text": " \nndim  \nAlias for dim() \n"}, {"name": "torch.Tensor.ndimension()", "path": "tensors#torch.Tensor.ndimension", "type": "torch.Tensor", "text": " \nndimension() \u2192 int  \nAlias for dim() \n"}, {"name": "torch.Tensor.ne()", "path": "tensors#torch.Tensor.ne", "type": "torch.Tensor", "text": " \nne(other) \u2192 Tensor  \nSee torch.ne(). \n"}, {"name": "torch.Tensor.neg()", "path": "tensors#torch.Tensor.neg", "type": "torch.Tensor", "text": " \nneg() \u2192 Tensor  \nSee torch.neg() \n"}, {"name": "torch.Tensor.negative()", "path": "tensors#torch.Tensor.negative", "type": "torch.Tensor", "text": " \nnegative() \u2192 Tensor  \nSee torch.negative() \n"}, {"name": "torch.Tensor.negative_()", "path": "tensors#torch.Tensor.negative_", "type": "torch.Tensor", "text": " \nnegative_() \u2192 Tensor  \nIn-place version of negative() \n"}, {"name": "torch.Tensor.neg_()", "path": "tensors#torch.Tensor.neg_", "type": "torch.Tensor", "text": " \nneg_() \u2192 Tensor  \nIn-place version of neg() \n"}, {"name": "torch.Tensor.nelement()", "path": "tensors#torch.Tensor.nelement", "type": "torch.Tensor", "text": " \nnelement() \u2192 int  \nAlias for numel() \n"}, {"name": "torch.Tensor.new_empty()", "path": "tensors#torch.Tensor.new_empty", "type": "torch.Tensor", "text": " \nnew_empty(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.ones(())\n>>> tensor.new_empty((2, 3))\ntensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n \n"}, {"name": "torch.Tensor.new_full()", "path": "tensors#torch.Tensor.new_full", "type": "torch.Tensor", "text": " \nnew_full(size, fill_value, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \nfill_value (scalar) \u2013 the number to fill the output tensor with. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.ones((2,), dtype=torch.float64)\n>>> tensor.new_full((3, 4), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n \n"}, {"name": "torch.Tensor.new_ones()", "path": "tensors#torch.Tensor.new_ones", "type": "torch.Tensor", "text": " \nnew_ones(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \nsize (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.tensor((), dtype=torch.int32)\n>>> tensor.new_ones((2, 3))\ntensor([[ 1,  1,  1],\n        [ 1,  1,  1]], dtype=torch.int32)\n \n"}, {"name": "torch.Tensor.new_tensor()", "path": "tensors#torch.Tensor.new_tensor", "type": "torch.Tensor", "text": " \nnew_tensor(data, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Warning new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().   Warning When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.   Parameters \n \ndata (array_like) \u2013 The returned Tensor copies data. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.ones((2,), dtype=torch.int8)\n>>> data = [[0, 1], [2, 3]]\n>>> tensor.new_tensor(data)\ntensor([[ 0,  1],\n        [ 2,  3]], dtype=torch.int8)\n \n"}, {"name": "torch.Tensor.new_zeros()", "path": "tensors#torch.Tensor.new_zeros", "type": "torch.Tensor", "text": " \nnew_zeros(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor  \nReturns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters \n \nsize (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> tensor = torch.tensor((), dtype=torch.float64)\n>>> tensor.new_zeros((2, 3))\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]], dtype=torch.float64)\n \n"}, {"name": "torch.Tensor.nextafter()", "path": "tensors#torch.Tensor.nextafter", "type": "torch.Tensor", "text": " \nnextafter(other) \u2192 Tensor  \nSee torch.nextafter() \n"}, {"name": "torch.Tensor.nextafter_()", "path": "tensors#torch.Tensor.nextafter_", "type": "torch.Tensor", "text": " \nnextafter_(other) \u2192 Tensor  \nIn-place version of nextafter() \n"}, {"name": "torch.Tensor.ne_()", "path": "tensors#torch.Tensor.ne_", "type": "torch.Tensor", "text": " \nne_(other) \u2192 Tensor  \nIn-place version of ne(). \n"}, {"name": "torch.Tensor.nonzero()", "path": "tensors#torch.Tensor.nonzero", "type": "torch.Tensor", "text": " \nnonzero() \u2192 LongTensor  \nSee torch.nonzero() \n"}, {"name": "torch.Tensor.norm()", "path": "tensors#torch.Tensor.norm", "type": "torch.Tensor", "text": " \nnorm(p='fro', dim=None, keepdim=False, dtype=None) [source]\n \nSee torch.norm() \n"}, {"name": "torch.Tensor.normal_()", "path": "tensors#torch.Tensor.normal_", "type": "torch.Tensor", "text": " \nnormal_(mean=0, std=1, *, generator=None) \u2192 Tensor  \nFills self tensor with elements samples from the normal distribution parameterized by mean and std. \n"}, {"name": "torch.Tensor.not_equal()", "path": "tensors#torch.Tensor.not_equal", "type": "torch.Tensor", "text": " \nnot_equal(other) \u2192 Tensor  \nSee torch.not_equal(). \n"}, {"name": "torch.Tensor.not_equal_()", "path": "tensors#torch.Tensor.not_equal_", "type": "torch.Tensor", "text": " \nnot_equal_(other) \u2192 Tensor  \nIn-place version of not_equal(). \n"}, {"name": "torch.Tensor.numel()", "path": "tensors#torch.Tensor.numel", "type": "torch.Tensor", "text": " \nnumel() \u2192 int  \nSee torch.numel() \n"}, {"name": "torch.Tensor.numpy()", "path": "tensors#torch.Tensor.numpy", "type": "torch.Tensor", "text": " \nnumpy() \u2192 numpy.ndarray  \nReturns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage. Changes to self tensor will be reflected in the ndarray and vice versa. \n"}, {"name": "torch.Tensor.orgqr()", "path": "tensors#torch.Tensor.orgqr", "type": "torch.Tensor", "text": " \norgqr(input2) \u2192 Tensor  \nSee torch.orgqr() \n"}, {"name": "torch.Tensor.ormqr()", "path": "tensors#torch.Tensor.ormqr", "type": "torch.Tensor", "text": " \normqr(input2, input3, left=True, transpose=False) \u2192 Tensor  \nSee torch.ormqr() \n"}, {"name": "torch.Tensor.outer()", "path": "tensors#torch.Tensor.outer", "type": "torch.Tensor", "text": " \nouter(vec2) \u2192 Tensor  \nSee torch.outer(). \n"}, {"name": "torch.Tensor.permute()", "path": "tensors#torch.Tensor.permute", "type": "torch.Tensor", "text": " \npermute(*dims) \u2192 Tensor  \nReturns a view of the original tensor with its dimensions permuted.  Parameters \n*dims (int...) \u2013 The desired ordering of dimensions   Example >>> x = torch.randn(2, 3, 5)\n>>> x.size()\ntorch.Size([2, 3, 5])\n>>> x.permute(2, 0, 1).size()\ntorch.Size([5, 2, 3])\n \n"}, {"name": "torch.Tensor.pinverse()", "path": "tensors#torch.Tensor.pinverse", "type": "torch.Tensor", "text": " \npinverse() \u2192 Tensor  \nSee torch.pinverse() \n"}, {"name": "torch.Tensor.pin_memory()", "path": "tensors#torch.Tensor.pin_memory", "type": "torch.Tensor", "text": " \npin_memory() \u2192 Tensor  \nCopies the tensor to pinned memory, if it\u2019s not already pinned. \n"}, {"name": "torch.Tensor.polygamma()", "path": "tensors#torch.Tensor.polygamma", "type": "torch.Tensor", "text": " \npolygamma(n) \u2192 Tensor  \nSee torch.polygamma() \n"}, {"name": "torch.Tensor.polygamma_()", "path": "tensors#torch.Tensor.polygamma_", "type": "torch.Tensor", "text": " \npolygamma_(n) \u2192 Tensor  \nIn-place version of polygamma() \n"}, {"name": "torch.Tensor.pow()", "path": "tensors#torch.Tensor.pow", "type": "torch.Tensor", "text": " \npow(exponent) \u2192 Tensor  \nSee torch.pow() \n"}, {"name": "torch.Tensor.pow_()", "path": "tensors#torch.Tensor.pow_", "type": "torch.Tensor", "text": " \npow_(exponent) \u2192 Tensor  \nIn-place version of pow() \n"}, {"name": "torch.Tensor.prod()", "path": "tensors#torch.Tensor.prod", "type": "torch.Tensor", "text": " \nprod(dim=None, keepdim=False, dtype=None) \u2192 Tensor  \nSee torch.prod() \n"}, {"name": "torch.Tensor.put_()", "path": "tensors#torch.Tensor.put_", "type": "torch.Tensor", "text": " \nput_(indices, tensor, accumulate=False) \u2192 Tensor  \nCopies the elements from tensor into the positions specified by indices. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor. If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters \n \nindices (LongTensor) \u2013 the indices into self \ntensor (Tensor) \u2013 the tensor containing values to copy from \naccumulate (bool) \u2013 whether to accumulate into self    Example: >>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\ntensor([[  4,   9,   5],\n        [ 10,   7,   8]])\n \n"}, {"name": "torch.Tensor.qr()", "path": "tensors#torch.Tensor.qr", "type": "torch.Tensor", "text": " \nqr(some=True) -> (Tensor, Tensor)  \nSee torch.qr() \n"}, {"name": "torch.Tensor.qscheme()", "path": "tensors#torch.Tensor.qscheme", "type": "torch.Tensor", "text": " \nqscheme() \u2192 torch.qscheme  \nReturns the quantization scheme of a given QTensor. \n"}, {"name": "torch.Tensor.quantile()", "path": "tensors#torch.Tensor.quantile", "type": "torch.Tensor", "text": " \nquantile(q, dim=None, keepdim=False) \u2192 Tensor  \nSee torch.quantile() \n"}, {"name": "torch.Tensor.q_per_channel_axis()", "path": "tensors#torch.Tensor.q_per_channel_axis", "type": "torch.Tensor", "text": " \nq_per_channel_axis() \u2192 int  \nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied. \n"}, {"name": "torch.Tensor.q_per_channel_scales()", "path": "tensors#torch.Tensor.q_per_channel_scales", "type": "torch.Tensor", "text": " \nq_per_channel_scales() \u2192 Tensor  \nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor. \n"}, {"name": "torch.Tensor.q_per_channel_zero_points()", "path": "tensors#torch.Tensor.q_per_channel_zero_points", "type": "torch.Tensor", "text": " \nq_per_channel_zero_points() \u2192 Tensor  \nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor. \n"}, {"name": "torch.Tensor.q_scale()", "path": "tensors#torch.Tensor.q_scale", "type": "torch.Tensor", "text": " \nq_scale() \u2192 float  \nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer(). \n"}, {"name": "torch.Tensor.q_zero_point()", "path": "tensors#torch.Tensor.q_zero_point", "type": "torch.Tensor", "text": " \nq_zero_point() \u2192 int  \nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer(). \n"}, {"name": "torch.Tensor.rad2deg()", "path": "tensors#torch.Tensor.rad2deg", "type": "torch.Tensor", "text": " \nrad2deg() \u2192 Tensor  \nSee torch.rad2deg() \n"}, {"name": "torch.Tensor.random_()", "path": "tensors#torch.Tensor.random_", "type": "torch.Tensor", "text": " \nrandom_(from=0, to=None, *, generator=None) \u2192 Tensor  \nFills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53]. \n"}, {"name": "torch.Tensor.ravel()", "path": "tensors#torch.Tensor.ravel", "type": "torch.Tensor", "text": " \nravel(input) \u2192 Tensor  \nsee torch.ravel() \n"}, {"name": "torch.Tensor.real", "path": "tensors#torch.Tensor.real", "type": "torch.Tensor", "text": " \nreal  \nReturns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.  Warning real() is only supported for tensors with complex dtypes.   Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n   \n"}, {"name": "torch.Tensor.reciprocal()", "path": "tensors#torch.Tensor.reciprocal", "type": "torch.Tensor", "text": " \nreciprocal() \u2192 Tensor  \nSee torch.reciprocal() \n"}, {"name": "torch.Tensor.reciprocal_()", "path": "tensors#torch.Tensor.reciprocal_", "type": "torch.Tensor", "text": " \nreciprocal_() \u2192 Tensor  \nIn-place version of reciprocal() \n"}, {"name": "torch.Tensor.record_stream()", "path": "tensors#torch.Tensor.record_stream", "type": "torch.Tensor", "text": " \nrecord_stream(stream)  \nEnsures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.  Note The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.  \n"}, {"name": "torch.Tensor.refine_names()", "path": "named_tensor#torch.Tensor.refine_names", "type": "Named Tensors", "text": " \nrefine_names(*names) [source]\n \nRefines the dimension names of self according to names. Refining is a special case of renaming that \u201clifts\u201d unnamed dimensions. A None dim can be refined to have any name; a named dim can only be refined to have the same name. Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors. names may contain up to one Ellipsis (...). The Ellipsis is expanded greedily; it is expanded in-place to fill names to the same length as self.dim() using names from the corresponding indices of self.names. Python 2 does not support Ellipsis but one may use a string literal instead ('...').  Parameters \nnames (iterable of str) \u2013 The desired names of the output tensor. May contain up to one Ellipsis.   Examples: >>> imgs = torch.randn(32, 3, 128, 128)\n>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n>>> named_imgs.names\n('N', 'C', 'H', 'W')\n\n>>> tensor = torch.randn(2, 3, 5, 7, 11)\n>>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n>>> tensor.names\n('A', None, None, 'B', 'C')\n  Warning The named tensor API is experimental and subject to change.  \n"}, {"name": "torch.Tensor.register_hook()", "path": "autograd#torch.Tensor.register_hook", "type": "torch.autograd", "text": " \nregister_hook(hook) [source]\n \nRegisters a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -> Tensor or None\n The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n>>> v.backward(torch.tensor([1., 2., 3.]))\n>>> v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n>>> h.remove()  # removes the hook\n \n"}, {"name": "torch.Tensor.remainder()", "path": "tensors#torch.Tensor.remainder", "type": "torch.Tensor", "text": " \nremainder(divisor) \u2192 Tensor  \nSee torch.remainder() \n"}, {"name": "torch.Tensor.remainder_()", "path": "tensors#torch.Tensor.remainder_", "type": "torch.Tensor", "text": " \nremainder_(divisor) \u2192 Tensor  \nIn-place version of remainder() \n"}, {"name": "torch.Tensor.rename()", "path": "named_tensor#torch.Tensor.rename", "type": "Named Tensors", "text": " \nrename(*names, **rename_map) [source]\n \nRenames dimension names of self. There are two main usages: self.rename(**rename_map) returns a view on tensor that has dims renamed as specified in the mapping rename_map. self.rename(*names) returns a view on tensor, renaming all dimensions positionally using names. Use self.rename(None) to drop names on a tensor. One cannot specify both positional args names and keyword args rename_map. Examples: >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n>>> renamed_imgs = imgs.rename(N='batch', C='channels')\n>>> renamed_imgs.names\n('batch', 'channels', 'H', 'W')\n\n>>> renamed_imgs = imgs.rename(None)\n>>> renamed_imgs.names\n(None,)\n\n>>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n>>> renamed_imgs.names\n('batch', 'channel', 'height', 'width')\n  Warning The named tensor API is experimental and subject to change.  \n"}, {"name": "torch.Tensor.rename_()", "path": "named_tensor#torch.Tensor.rename_", "type": "Named Tensors", "text": " \nrename_(*names, **rename_map) [source]\n \nIn-place version of rename(). \n"}, {"name": "torch.Tensor.renorm()", "path": "tensors#torch.Tensor.renorm", "type": "torch.Tensor", "text": " \nrenorm(p, dim, maxnorm) \u2192 Tensor  \nSee torch.renorm() \n"}, {"name": "torch.Tensor.renorm_()", "path": "tensors#torch.Tensor.renorm_", "type": "torch.Tensor", "text": " \nrenorm_(p, dim, maxnorm) \u2192 Tensor  \nIn-place version of renorm() \n"}, {"name": "torch.Tensor.repeat()", "path": "tensors#torch.Tensor.repeat", "type": "torch.Tensor", "text": " \nrepeat(*sizes) \u2192 Tensor  \nRepeats this tensor along the specified dimensions. Unlike expand(), this function copies the tensor\u2019s data.  Warning repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().   Parameters \nsizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along each dimension   Example: >>> x = torch.tensor([1, 2, 3])\n>>> x.repeat(4, 2)\ntensor([[ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3]])\n>>> x.repeat(4, 2, 1).size()\ntorch.Size([4, 2, 3])\n \n"}, {"name": "torch.Tensor.repeat_interleave()", "path": "tensors#torch.Tensor.repeat_interleave", "type": "torch.Tensor", "text": " \nrepeat_interleave(repeats, dim=None) \u2192 Tensor  \nSee torch.repeat_interleave(). \n"}, {"name": "torch.Tensor.requires_grad", "path": "autograd#torch.Tensor.requires_grad", "type": "torch.autograd", "text": " \nrequires_grad  \nIs True if gradients need to be computed for this Tensor, False otherwise.  Note The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.  \n"}, {"name": "torch.Tensor.requires_grad_()", "path": "tensors#torch.Tensor.requires_grad_", "type": "torch.Tensor", "text": " \nrequires_grad_(requires_grad=True) \u2192 Tensor  \nChange if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor. requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.  Parameters \nrequires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.   Example: >>> # Let's say we want to preprocess some saved weights and use\n>>> # the result as new weights.\n>>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n>>> loaded_weights = torch.tensor(saved_weights)\n>>> weights = preprocess(loaded_weights)  # some function\n>>> weights\ntensor([-0.5503,  0.4926, -2.1158, -0.8303])\n\n>>> # Now, start to record operations done to weights\n>>> weights.requires_grad_()\n>>> out = weights.pow(2).sum()\n>>> out.backward()\n>>> weights.grad\ntensor([-1.1007,  0.9853, -4.2316, -1.6606])\n \n"}, {"name": "torch.Tensor.reshape()", "path": "tensors#torch.Tensor.reshape", "type": "torch.Tensor", "text": " \nreshape(*shape) \u2192 Tensor  \nReturns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. See torch.reshape()  Parameters \nshape (tuple of python:ints or int...) \u2013 the desired shape   \n"}, {"name": "torch.Tensor.reshape_as()", "path": "tensors#torch.Tensor.reshape_as", "type": "torch.Tensor", "text": " \nreshape_as(other) \u2192 Tensor  \nReturns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. Please see reshape() for more information about reshape.  Parameters \nother (torch.Tensor) \u2013 The result tensor has the same shape as other.   \n"}, {"name": "torch.Tensor.resize_()", "path": "tensors#torch.Tensor.resize_", "type": "torch.Tensor", "text": " \nresize_(*sizes, memory_format=torch.contiguous_format) \u2192 Tensor  \nResizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.  Warning This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().   Parameters \n \nsizes (torch.Size or int...) \u2013 the desired size \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches sizes.    Example: >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n>>> x.resize_(2, 2)\ntensor([[ 1,  2],\n        [ 3,  4]])\n \n"}, {"name": "torch.Tensor.resize_as_()", "path": "tensors#torch.Tensor.resize_as_", "type": "torch.Tensor", "text": " \nresize_as_(tensor, memory_format=torch.contiguous_format) \u2192 Tensor  \nResizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches tensor.size().   \n"}, {"name": "torch.Tensor.retain_grad()", "path": "autograd#torch.Tensor.retain_grad", "type": "torch.autograd", "text": " \nretain_grad() [source]\n \nEnables .grad attribute for non-leaf Tensors. \n"}, {"name": "torch.Tensor.roll()", "path": "tensors#torch.Tensor.roll", "type": "torch.Tensor", "text": " \nroll(shifts, dims) \u2192 Tensor  \nSee torch.roll() \n"}, {"name": "torch.Tensor.rot90()", "path": "tensors#torch.Tensor.rot90", "type": "torch.Tensor", "text": " \nrot90(k, dims) \u2192 Tensor  \nSee torch.rot90() \n"}, {"name": "torch.Tensor.round()", "path": "tensors#torch.Tensor.round", "type": "torch.Tensor", "text": " \nround() \u2192 Tensor  \nSee torch.round() \n"}, {"name": "torch.Tensor.round_()", "path": "tensors#torch.Tensor.round_", "type": "torch.Tensor", "text": " \nround_() \u2192 Tensor  \nIn-place version of round() \n"}, {"name": "torch.Tensor.rsqrt()", "path": "tensors#torch.Tensor.rsqrt", "type": "torch.Tensor", "text": " \nrsqrt() \u2192 Tensor  \nSee torch.rsqrt() \n"}, {"name": "torch.Tensor.rsqrt_()", "path": "tensors#torch.Tensor.rsqrt_", "type": "torch.Tensor", "text": " \nrsqrt_() \u2192 Tensor  \nIn-place version of rsqrt() \n"}, {"name": "torch.Tensor.scatter()", "path": "tensors#torch.Tensor.scatter", "type": "torch.Tensor", "text": " \nscatter(dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_() \n"}, {"name": "torch.Tensor.scatter_()", "path": "tensors#torch.Tensor.scatter_", "type": "torch.Tensor", "text": " \nscatter_(dim, index, src, reduce=None) \u2192 Tensor  \nWrites all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n This is the reverse operation of the manner described in gather(). self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast. Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.  Warning When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!   Note The backward pass is implemented only for src.shape == index.shape.  Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indicies specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. Given a 3-D tensor and reduction using the multiplication operation, self is updated as: self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n Reducing with the addition operation is the same as using scatter_add_().  Parameters \n \ndim (int) \u2013 the axis along which to index \nindex (LongTensor) \u2013 the indices of elements to scatter, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged. \nsrc (Tensor or float) \u2013 the source element(s) to scatter. \nreduce (str, optional) \u2013 reduction operation to apply, can be either 'add' or 'multiply'.    Example: >>> src = torch.arange(1, 11).reshape((2, 5))\n>>> src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n>>> index = torch.tensor([[0, 1, 2, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])\n \n"}, {"name": "torch.Tensor.scatter_add()", "path": "tensors#torch.Tensor.scatter_add", "type": "torch.Tensor", "text": " \nscatter_add(dim, index, src) \u2192 Tensor  \nOut-of-place version of torch.Tensor.scatter_add_() \n"}, {"name": "torch.Tensor.scatter_add_()", "path": "tensors#torch.Tensor.scatter_add_", "type": "torch.Tensor", "text": " \nscatter_add_(dim, index, src) \u2192 Tensor  \nAdds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in src, it is added to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.  Note This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.   Note The backward pass is implemented only for src.shape == index.shape.   Parameters \n \ndim (int) \u2013 the axis along which to index \nindex (LongTensor) \u2013 the indices of elements to scatter and add, can be either empty or of the same dimensionality as src. When empty, the operation returns self unchanged. \nsrc (Tensor) \u2013 the source elements to scatter and add    Example: >>> src = torch.ones((2, 5))\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])\n \n"}, {"name": "torch.Tensor.select()", "path": "tensors#torch.Tensor.select", "type": "torch.Tensor", "text": " \nselect(dim, index) \u2192 Tensor  \nSlices the self tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.  Parameters \n \ndim (int) \u2013 the dimension to slice \nindex (int) \u2013 the index to select with     Note select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index].  \n"}, {"name": "torch.Tensor.set_()", "path": "tensors#torch.Tensor.set_", "type": "torch.Tensor", "text": " \nset_(source=None, storage_offset=0, size=None, stride=None) \u2192 Tensor  \nSets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other. If source is a Storage, the method sets the underlying storage, offset, size, and stride.  Parameters \n \nsource (Tensor or Storage) \u2013 the tensor or storage to use \nstorage_offset (int, optional) \u2013 the offset in the storage \nsize (torch.Size, optional) \u2013 the desired size. Defaults to the size of the source. \nstride (tuple, optional) \u2013 the desired stride. Defaults to C-contiguous strides.    \n"}, {"name": "torch.Tensor.sgn()", "path": "tensors#torch.Tensor.sgn", "type": "torch.Tensor", "text": " \nsgn() \u2192 Tensor  \nSee torch.sgn() \n"}, {"name": "torch.Tensor.sgn_()", "path": "tensors#torch.Tensor.sgn_", "type": "torch.Tensor", "text": " \nsgn_() \u2192 Tensor  \nIn-place version of sgn() \n"}, {"name": "torch.Tensor.share_memory_()", "path": "tensors#torch.Tensor.share_memory_", "type": "torch.Tensor", "text": " \nshare_memory_() [source]\n \nMoves the underlying storage to shared memory. This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized. \n"}, {"name": "torch.Tensor.short()", "path": "tensors#torch.Tensor.short", "type": "torch.Tensor", "text": " \nshort(memory_format=torch.preserve_format) \u2192 Tensor  \nself.short() is equivalent to self.to(torch.int16). See to().  Parameters \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.Tensor.sigmoid()", "path": "tensors#torch.Tensor.sigmoid", "type": "torch.Tensor", "text": " \nsigmoid() \u2192 Tensor  \nSee torch.sigmoid() \n"}, {"name": "torch.Tensor.sigmoid_()", "path": "tensors#torch.Tensor.sigmoid_", "type": "torch.Tensor", "text": " \nsigmoid_() \u2192 Tensor  \nIn-place version of sigmoid() \n"}, {"name": "torch.Tensor.sign()", "path": "tensors#torch.Tensor.sign", "type": "torch.Tensor", "text": " \nsign() \u2192 Tensor  \nSee torch.sign() \n"}, {"name": "torch.Tensor.signbit()", "path": "tensors#torch.Tensor.signbit", "type": "torch.Tensor", "text": " \nsignbit() \u2192 Tensor  \nSee torch.signbit() \n"}, {"name": "torch.Tensor.sign_()", "path": "tensors#torch.Tensor.sign_", "type": "torch.Tensor", "text": " \nsign_() \u2192 Tensor  \nIn-place version of sign() \n"}, {"name": "torch.Tensor.sin()", "path": "tensors#torch.Tensor.sin", "type": "torch.Tensor", "text": " \nsin() \u2192 Tensor  \nSee torch.sin() \n"}, {"name": "torch.Tensor.sinc()", "path": "tensors#torch.Tensor.sinc", "type": "torch.Tensor", "text": " \nsinc() \u2192 Tensor  \nSee torch.sinc() \n"}, {"name": "torch.Tensor.sinc_()", "path": "tensors#torch.Tensor.sinc_", "type": "torch.Tensor", "text": " \nsinc_() \u2192 Tensor  \nIn-place version of sinc() \n"}, {"name": "torch.Tensor.sinh()", "path": "tensors#torch.Tensor.sinh", "type": "torch.Tensor", "text": " \nsinh() \u2192 Tensor  \nSee torch.sinh() \n"}, {"name": "torch.Tensor.sinh_()", "path": "tensors#torch.Tensor.sinh_", "type": "torch.Tensor", "text": " \nsinh_() \u2192 Tensor  \nIn-place version of sinh() \n"}, {"name": "torch.Tensor.sin_()", "path": "tensors#torch.Tensor.sin_", "type": "torch.Tensor", "text": " \nsin_() \u2192 Tensor  \nIn-place version of sin() \n"}, {"name": "torch.Tensor.size()", "path": "tensors#torch.Tensor.size", "type": "torch.Tensor", "text": " \nsize() \u2192 torch.Size  \nReturns the size of the self tensor. The returned value is a subclass of tuple. Example: >>> torch.empty(3, 4, 5).size()\ntorch.Size([3, 4, 5])\n \n"}, {"name": "torch.Tensor.slogdet()", "path": "tensors#torch.Tensor.slogdet", "type": "torch.Tensor", "text": " \nslogdet() -> (Tensor, Tensor)  \nSee torch.slogdet() \n"}, {"name": "torch.Tensor.solve()", "path": "tensors#torch.Tensor.solve", "type": "torch.Tensor", "text": " \nsolve(A) \u2192 Tensor, Tensor  \nSee torch.solve() \n"}, {"name": "torch.Tensor.sort()", "path": "tensors#torch.Tensor.sort", "type": "torch.Tensor", "text": " \nsort(dim=-1, descending=False) -> (Tensor, LongTensor)  \nSee torch.sort() \n"}, {"name": "torch.Tensor.sparse_dim()", "path": "sparse#torch.Tensor.sparse_dim", "type": "torch.sparse", "text": " \nsparse_dim() \u2192 int  \nReturn the number of sparse dimensions in a sparse tensor self.  Warning Throws an error if self is not a sparse tensor.  See also Tensor.dense_dim() and hybrid tensors. \n"}, {"name": "torch.Tensor.sparse_mask()", "path": "sparse#torch.Tensor.sparse_mask", "type": "torch.sparse", "text": " \nsparse_mask(mask) \u2192 Tensor  \nReturns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.  Note The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.   Parameters \nmask (Tensor) \u2013 a sparse tensor whose indices are used as a filter   Example: >>> nse = 5\n>>> dims = (5, 5, 2, 2)\n>>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n>>> V = torch.randn(nse, dims[2], dims[3])\n>>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n>>> D = torch.randn(dims)\n>>> D.sparse_mask(S)\ntensor(indices=tensor([[0, 0, 0, 2],\n                       [0, 1, 4, 3]]),\n       values=tensor([[[ 1.6550,  0.2397],\n                       [-0.1611, -0.0779]],\n\n                      [[ 0.2326, -1.0558],\n                       [ 1.4711,  1.9678]],\n\n                      [[-0.5138, -0.0411],\n                       [ 1.9417,  0.5158]],\n\n                      [[ 0.0793,  0.0036],\n                       [-0.2569, -0.1055]]]),\n       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n \n"}, {"name": "torch.Tensor.sparse_resize_()", "path": "sparse#torch.Tensor.sparse_resize_", "type": "torch.sparse", "text": " \nsparse_resize_(size, sparse_dim, dense_dim) \u2192 Tensor  \nResizes self sparse tensor to the desired size and the number of sparse and dense dimensions.  Note If the number of specified elements in self is zero, then size, sparse_dim, and dense_dim can be any size and positive integers such that len(size) == sparse_dim +\ndense_dim. If self specifies one or more elements, however, then each dimension in size must not be smaller than the corresponding dimension of self, sparse_dim must equal the number of sparse dimensions in self, and dense_dim must equal the number of dense dimensions in self.   Warning Throws an error if self is not a sparse tensor.   Parameters \n \nsize (torch.Size) \u2013 the desired size. If self is non-empty sparse tensor, the desired size cannot be smaller than the original size. \nsparse_dim (int) \u2013 the number of sparse dimensions \ndense_dim (int) \u2013 the number of dense dimensions    \n"}, {"name": "torch.Tensor.sparse_resize_and_clear_()", "path": "sparse#torch.Tensor.sparse_resize_and_clear_", "type": "torch.sparse", "text": " \nsparse_resize_and_clear_(size, sparse_dim, dense_dim) \u2192 Tensor  \nRemoves all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions.  Parameters \n \nsize (torch.Size) \u2013 the desired size. \nsparse_dim (int) \u2013 the number of sparse dimensions \ndense_dim (int) \u2013 the number of dense dimensions    \n"}, {"name": "torch.Tensor.split()", "path": "tensors#torch.Tensor.split", "type": "torch.Tensor", "text": " \nsplit(split_size, dim=0) [source]\n \nSee torch.split() \n"}, {"name": "torch.Tensor.sqrt()", "path": "tensors#torch.Tensor.sqrt", "type": "torch.Tensor", "text": " \nsqrt() \u2192 Tensor  \nSee torch.sqrt() \n"}, {"name": "torch.Tensor.sqrt_()", "path": "tensors#torch.Tensor.sqrt_", "type": "torch.Tensor", "text": " \nsqrt_() \u2192 Tensor  \nIn-place version of sqrt() \n"}, {"name": "torch.Tensor.square()", "path": "tensors#torch.Tensor.square", "type": "torch.Tensor", "text": " \nsquare() \u2192 Tensor  \nSee torch.square() \n"}, {"name": "torch.Tensor.square_()", "path": "tensors#torch.Tensor.square_", "type": "torch.Tensor", "text": " \nsquare_() \u2192 Tensor  \nIn-place version of square() \n"}, {"name": "torch.Tensor.squeeze()", "path": "tensors#torch.Tensor.squeeze", "type": "torch.Tensor", "text": " \nsqueeze(dim=None) \u2192 Tensor  \nSee torch.squeeze() \n"}, {"name": "torch.Tensor.squeeze_()", "path": "tensors#torch.Tensor.squeeze_", "type": "torch.Tensor", "text": " \nsqueeze_(dim=None) \u2192 Tensor  \nIn-place version of squeeze() \n"}, {"name": "torch.Tensor.std()", "path": "tensors#torch.Tensor.std", "type": "torch.Tensor", "text": " \nstd(dim=None, unbiased=True, keepdim=False) \u2192 Tensor  \nSee torch.std() \n"}, {"name": "torch.Tensor.stft()", "path": "tensors#torch.Tensor.stft", "type": "torch.Tensor", "text": " \nstft(n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=None, return_complex=None) [source]\n \nSee torch.stft()  Warning This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.  \n"}, {"name": "torch.Tensor.storage()", "path": "tensors#torch.Tensor.storage", "type": "torch.Tensor", "text": " \nstorage() \u2192 torch.Storage  \nReturns the underlying storage. \n"}, {"name": "torch.Tensor.storage_offset()", "path": "tensors#torch.Tensor.storage_offset", "type": "torch.Tensor", "text": " \nstorage_offset() \u2192 int  \nReturns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes). Example: >>> x = torch.tensor([1, 2, 3, 4, 5])\n>>> x.storage_offset()\n0\n>>> x[3:].storage_offset()\n3\n \n"}, {"name": "torch.Tensor.storage_type()", "path": "tensors#torch.Tensor.storage_type", "type": "torch.Tensor", "text": " \nstorage_type() \u2192 type  \nReturns the type of the underlying storage. \n"}, {"name": "torch.Tensor.stride()", "path": "tensors#torch.Tensor.stride", "type": "torch.Tensor", "text": " \nstride(dim) \u2192 tuple or int  \nReturns the stride of self tensor. Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.  Parameters \ndim (int, optional) \u2013 the desired dimension in which stride is required   Example: >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n>>> x.stride(0)\n5\n>>> x.stride(-1)\n1\n \n"}, {"name": "torch.Tensor.sub()", "path": "tensors#torch.Tensor.sub", "type": "torch.Tensor", "text": " \nsub(other, *, alpha=1) \u2192 Tensor  \nSee torch.sub(). \n"}, {"name": "torch.Tensor.subtract()", "path": "tensors#torch.Tensor.subtract", "type": "torch.Tensor", "text": " \nsubtract(other, *, alpha=1) \u2192 Tensor  \nSee torch.subtract(). \n"}, {"name": "torch.Tensor.subtract_()", "path": "tensors#torch.Tensor.subtract_", "type": "torch.Tensor", "text": " \nsubtract_(other, *, alpha=1) \u2192 Tensor  \nIn-place version of subtract(). \n"}, {"name": "torch.Tensor.sub_()", "path": "tensors#torch.Tensor.sub_", "type": "torch.Tensor", "text": " \nsub_(other, *, alpha=1) \u2192 Tensor  \nIn-place version of sub() \n"}, {"name": "torch.Tensor.sum()", "path": "tensors#torch.Tensor.sum", "type": "torch.Tensor", "text": " \nsum(dim=None, keepdim=False, dtype=None) \u2192 Tensor  \nSee torch.sum() \n"}, {"name": "torch.Tensor.sum_to_size()", "path": "tensors#torch.Tensor.sum_to_size", "type": "torch.Tensor", "text": " \nsum_to_size(*size) \u2192 Tensor  \nSum this tensor to size. size must be broadcastable to this tensor size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor.   \n"}, {"name": "torch.Tensor.svd()", "path": "tensors#torch.Tensor.svd", "type": "torch.Tensor", "text": " \nsvd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor)  \nSee torch.svd() \n"}, {"name": "torch.Tensor.swapaxes()", "path": "tensors#torch.Tensor.swapaxes", "type": "torch.Tensor", "text": " \nswapaxes(axis0, axis1) \u2192 Tensor  \nSee torch.swapaxes() \n"}, {"name": "torch.Tensor.swapdims()", "path": "tensors#torch.Tensor.swapdims", "type": "torch.Tensor", "text": " \nswapdims(dim0, dim1) \u2192 Tensor  \nSee torch.swapdims() \n"}, {"name": "torch.Tensor.symeig()", "path": "tensors#torch.Tensor.symeig", "type": "torch.Tensor", "text": " \nsymeig(eigenvectors=False, upper=True) -> (Tensor, Tensor)  \nSee torch.symeig() \n"}, {"name": "torch.Tensor.T", "path": "tensors#torch.Tensor.T", "type": "torch.Tensor", "text": " \nT  \nIs this Tensor with its dimensions reversed. If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0). \n"}, {"name": "torch.Tensor.t()", "path": "tensors#torch.Tensor.t", "type": "torch.Tensor", "text": " \nt() \u2192 Tensor  \nSee torch.t() \n"}, {"name": "torch.Tensor.take()", "path": "tensors#torch.Tensor.take", "type": "torch.Tensor", "text": " \ntake(indices) \u2192 Tensor  \nSee torch.take() \n"}, {"name": "torch.Tensor.tan()", "path": "tensors#torch.Tensor.tan", "type": "torch.Tensor", "text": " \ntan() \u2192 Tensor  \nSee torch.tan() \n"}, {"name": "torch.Tensor.tanh()", "path": "tensors#torch.Tensor.tanh", "type": "torch.Tensor", "text": " \ntanh() \u2192 Tensor  \nSee torch.tanh() \n"}, {"name": "torch.Tensor.tanh_()", "path": "tensors#torch.Tensor.tanh_", "type": "torch.Tensor", "text": " \ntanh_() \u2192 Tensor  \nIn-place version of tanh() \n"}, {"name": "torch.Tensor.tan_()", "path": "tensors#torch.Tensor.tan_", "type": "torch.Tensor", "text": " \ntan_() \u2192 Tensor  \nIn-place version of tan() \n"}, {"name": "torch.Tensor.tensor_split()", "path": "tensors#torch.Tensor.tensor_split", "type": "torch.Tensor", "text": " \ntensor_split(indices_or_sections, dim=0) \u2192 List of Tensors  \nSee torch.tensor_split() \n"}, {"name": "torch.Tensor.tile()", "path": "tensors#torch.Tensor.tile", "type": "torch.Tensor", "text": " \ntile(*reps) \u2192 Tensor  \nSee torch.tile() \n"}, {"name": "torch.Tensor.to()", "path": "tensors#torch.Tensor.to", "type": "torch.Tensor", "text": " \nto(*args, **kwargs) \u2192 Tensor  \nPerforms Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).  Note If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.  Here are the ways to call to:  \nto(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a Tensor with the specified dtype  Args:\n\nmemory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nto(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.  Args:\n\nmemory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.   \n  \nto(other, non_blocking=False, copy=False) \u2192 Tensor  \nReturns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion. \n Example: >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n>>> tensor.to(torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64)\n\n>>> cuda0 = torch.device('cuda:0')\n>>> tensor.to(cuda0)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], device='cuda:0')\n\n>>> tensor.to(cuda0, dtype=torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n>>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n>>> tensor.to(other, non_blocking=True)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n \n"}, {"name": "torch.Tensor.tolist()", "path": "tensors#torch.Tensor.tolist", "type": "torch.Tensor", "text": " \ntolist() \u2192 list or number  \nReturns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary. This operation is not differentiable. Examples: >>> a = torch.randn(2, 2)\n>>> a.tolist()\n[[0.012766935862600803, 0.5415473580360413],\n [-0.08909505605697632, 0.7729271650314331]]\n>>> a[0,0].tolist()\n0.012766935862600803\n \n"}, {"name": "torch.Tensor.topk()", "path": "tensors#torch.Tensor.topk", "type": "torch.Tensor", "text": " \ntopk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)  \nSee torch.topk() \n"}, {"name": "torch.Tensor.to_dense()", "path": "sparse#torch.Tensor.to_dense", "type": "torch.sparse", "text": " \nto_dense() \u2192 Tensor  \nCreates a strided copy of self.  Warning Throws an error if self is a strided tensor.  Example: >>> s = torch.sparse_coo_tensor(\n...        torch.tensor([[1, 1],\n...                      [0, 2]]),\n...        torch.tensor([9, 10]),\n...        size=(3, 3))\n>>> s.to_dense()\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n \n"}, {"name": "torch.Tensor.to_mkldnn()", "path": "tensors#torch.Tensor.to_mkldnn", "type": "torch.Tensor", "text": " \nto_mkldnn() \u2192 Tensor  \nReturns a copy of the tensor in torch.mkldnn layout. \n"}, {"name": "torch.Tensor.to_sparse()", "path": "sparse#torch.Tensor.to_sparse", "type": "torch.sparse", "text": " \nto_sparse(sparseDims) \u2192 Tensor  \nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.  Parameters \nsparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor   Example: >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n>>> d\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n>>> d.to_sparse()\ntensor(indices=tensor([[1, 1],\n                       [0, 2]]),\n       values=tensor([ 9, 10]),\n       size=(3, 3), nnz=2, layout=torch.sparse_coo)\n>>> d.to_sparse(1)\ntensor(indices=tensor([[1]]),\n       values=tensor([[ 9,  0, 10]]),\n       size=(3, 3), nnz=1, layout=torch.sparse_coo)\n \n"}, {"name": "torch.Tensor.trace()", "path": "tensors#torch.Tensor.trace", "type": "torch.Tensor", "text": " \ntrace() \u2192 Tensor  \nSee torch.trace() \n"}, {"name": "torch.Tensor.transpose()", "path": "tensors#torch.Tensor.transpose", "type": "torch.Tensor", "text": " \ntranspose(dim0, dim1) \u2192 Tensor  \nSee torch.transpose() \n"}, {"name": "torch.Tensor.transpose_()", "path": "tensors#torch.Tensor.transpose_", "type": "torch.Tensor", "text": " \ntranspose_(dim0, dim1) \u2192 Tensor  \nIn-place version of transpose() \n"}, {"name": "torch.Tensor.triangular_solve()", "path": "tensors#torch.Tensor.triangular_solve", "type": "torch.Tensor", "text": " \ntriangular_solve(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)  \nSee torch.triangular_solve() \n"}, {"name": "torch.Tensor.tril()", "path": "tensors#torch.Tensor.tril", "type": "torch.Tensor", "text": " \ntril(k=0) \u2192 Tensor  \nSee torch.tril() \n"}, {"name": "torch.Tensor.tril_()", "path": "tensors#torch.Tensor.tril_", "type": "torch.Tensor", "text": " \ntril_(k=0) \u2192 Tensor  \nIn-place version of tril() \n"}, {"name": "torch.Tensor.triu()", "path": "tensors#torch.Tensor.triu", "type": "torch.Tensor", "text": " \ntriu(k=0) \u2192 Tensor  \nSee torch.triu() \n"}, {"name": "torch.Tensor.triu_()", "path": "tensors#torch.Tensor.triu_", "type": "torch.Tensor", "text": " \ntriu_(k=0) \u2192 Tensor  \nIn-place version of triu() \n"}, {"name": "torch.Tensor.true_divide()", "path": "tensors#torch.Tensor.true_divide", "type": "torch.Tensor", "text": " \ntrue_divide(value) \u2192 Tensor  \nSee torch.true_divide() \n"}, {"name": "torch.Tensor.true_divide_()", "path": "tensors#torch.Tensor.true_divide_", "type": "torch.Tensor", "text": " \ntrue_divide_(value) \u2192 Tensor  \nIn-place version of true_divide_() \n"}, {"name": "torch.Tensor.trunc()", "path": "tensors#torch.Tensor.trunc", "type": "torch.Tensor", "text": " \ntrunc() \u2192 Tensor  \nSee torch.trunc() \n"}, {"name": "torch.Tensor.trunc_()", "path": "tensors#torch.Tensor.trunc_", "type": "torch.Tensor", "text": " \ntrunc_() \u2192 Tensor  \nIn-place version of trunc() \n"}, {"name": "torch.Tensor.type()", "path": "tensors#torch.Tensor.type", "type": "torch.Tensor", "text": " \ntype(dtype=None, non_blocking=False, **kwargs) \u2192 str or Tensor  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n"}, {"name": "torch.Tensor.type_as()", "path": "tensors#torch.Tensor.type_as", "type": "torch.Tensor", "text": " \ntype_as(tensor) \u2192 Tensor  \nReturns this tensor cast to the type of the given tensor. This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())  Parameters \ntensor (Tensor) \u2013 the tensor which has the desired type   \n"}, {"name": "torch.Tensor.t_()", "path": "tensors#torch.Tensor.t_", "type": "torch.Tensor", "text": " \nt_() \u2192 Tensor  \nIn-place version of t() \n"}, {"name": "torch.Tensor.unbind()", "path": "tensors#torch.Tensor.unbind", "type": "torch.Tensor", "text": " \nunbind(dim=0) \u2192 seq  \nSee torch.unbind() \n"}, {"name": "torch.Tensor.unflatten()", "path": "named_tensor#torch.Tensor.unflatten", "type": "Named Tensors", "text": " \nunflatten(dim, sizes) [source]\n \nExpands the dimension dim of the self tensor over multiple dimensions of sizes given by sizes.  \nsizes is the new shape of the unflattened dimension and it can be a Tuple[int] as well as torch.Size if self is a Tensor, or namedshape (Tuple[(name: str, size: int)]) if self is a NamedTensor. The total number of elements in sizes must match the number of elements in the original dim being unflattened.   Parameters \n \ndim (Union[int, str]) \u2013 Dimension to unflatten \nsizes (Union[Tuple[int] or torch.Size, Tuple[Tuple[str, int]]]) \u2013 New shape of the unflattened dimension    Examples >>> torch.randn(3, 4, 1).unflatten(1, (2, 2)).shape\ntorch.Size([3, 2, 2, 1])\n>>> torch.randn(2, 4, names=('A', 'B')).unflatten('B', (('B1', 2), ('B2', 2)))\ntensor([[[-1.1772,  0.0180],\n        [ 0.2412,  0.1431]],\n [[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(\u2018A\u2019, \u2018B1\u2019, \u2018B2\u2019))  Warning The named tensor API is experimental and subject to change.  \n"}, {"name": "torch.Tensor.unfold()", "path": "tensors#torch.Tensor.unfold", "type": "torch.Tensor", "text": " \nunfold(dimension, size, step) \u2192 Tensor  \nReturns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension. Step between two slices is given by step. If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1. An additional dimension of size size is appended in the returned tensor.  Parameters \n \ndimension (int) \u2013 dimension in which unfolding happens \nsize (int) \u2013 the size of each slice that is unfolded \nstep (int) \u2013 the step between each slice    Example: >>> x = torch.arange(1., 8)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n>>> x.unfold(0, 2, 1)\ntensor([[ 1.,  2.],\n        [ 2.,  3.],\n        [ 3.,  4.],\n        [ 4.,  5.],\n        [ 5.,  6.],\n        [ 6.,  7.]])\n>>> x.unfold(0, 2, 2)\ntensor([[ 1.,  2.],\n        [ 3.,  4.],\n        [ 5.,  6.]])\n \n"}, {"name": "torch.Tensor.uniform_()", "path": "tensors#torch.Tensor.uniform_", "type": "torch.Tensor", "text": " \nuniform_(from=0, to=1) \u2192 Tensor  \nFills self tensor with numbers sampled from the continuous uniform distribution:  P(x)=1to\u2212fromP(x) = \\dfrac{1}{\\text{to} - \\text{from}}  \n\n"}, {"name": "torch.Tensor.unique()", "path": "tensors#torch.Tensor.unique", "type": "torch.Tensor", "text": " \nunique(sorted=True, return_inverse=False, return_counts=False, dim=None) [source]\n \nReturns the unique elements of the input tensor. See torch.unique() \n"}, {"name": "torch.Tensor.unique_consecutive()", "path": "tensors#torch.Tensor.unique_consecutive", "type": "torch.Tensor", "text": " \nunique_consecutive(return_inverse=False, return_counts=False, dim=None) [source]\n \nEliminates all but the first element from every consecutive group of equivalent elements. See torch.unique_consecutive() \n"}, {"name": "torch.Tensor.unsqueeze()", "path": "tensors#torch.Tensor.unsqueeze", "type": "torch.Tensor", "text": " \nunsqueeze(dim) \u2192 Tensor  \nSee torch.unsqueeze() \n"}, {"name": "torch.Tensor.unsqueeze_()", "path": "tensors#torch.Tensor.unsqueeze_", "type": "torch.Tensor", "text": " \nunsqueeze_(dim) \u2192 Tensor  \nIn-place version of unsqueeze() \n"}, {"name": "torch.Tensor.values()", "path": "sparse#torch.Tensor.values", "type": "torch.sparse", "text": " \nvalues() \u2192 Tensor  \nReturn the values tensor of a sparse COO tensor.  Warning Throws an error if self is not a sparse COO tensor.  See also Tensor.indices().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  \n"}, {"name": "torch.Tensor.var()", "path": "tensors#torch.Tensor.var", "type": "torch.Tensor", "text": " \nvar(dim=None, unbiased=True, keepdim=False) \u2192 Tensor  \nSee torch.var() \n"}, {"name": "torch.Tensor.vdot()", "path": "tensors#torch.Tensor.vdot", "type": "torch.Tensor", "text": " \nvdot(other) \u2192 Tensor  \nSee torch.vdot() \n"}, {"name": "torch.Tensor.view()", "path": "tensors#torch.Tensor.view", "type": "torch.Tensor", "text": " \nview(*shape) \u2192 Tensor  \nReturns a new tensor with the same data as the self tensor but of a different shape. The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k  that satisfy the following contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots, d+k-1 ,  stride[i]=stride[i+1]\u00d7size[i+1]\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1] \nOtherwise, it will not be possible to view self tensor as shape without copying it (e.g., via contiguous()). When it is unclear whether a view() can be performed, it is advisable to use reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.  Parameters \nshape (torch.Size or int...) \u2013 the desired size   Example: >>> x = torch.randn(4, 4)\n>>> x.size()\ntorch.Size([4, 4])\n>>> y = x.view(16)\n>>> y.size()\ntorch.Size([16])\n>>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n>>> z.size()\ntorch.Size([2, 8])\n\n>>> a = torch.randn(1, 2, 3, 4)\n>>> a.size()\ntorch.Size([1, 2, 3, 4])\n>>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n>>> b.size()\ntorch.Size([1, 3, 2, 4])\n>>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n>>> c.size()\ntorch.Size([1, 3, 2, 4])\n>>> torch.equal(b, c)\nFalse\n  \nview(dtype) \u2192 Tensor \n Returns a new tensor with the same data as the self tensor but of a different dtype. dtype must have the same number of bytes per element as self\u2019s dtype.  Warning This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.   Parameters \ndtype (torch.dtype) \u2013 the desired dtype   Example: >>> x = torch.randn(4, 4)\n>>> x\ntensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n        [-0.1520,  0.7472,  0.5617, -0.8649],\n        [-2.4724, -0.0334, -0.2976, -0.8499],\n        [-0.2109,  1.9913, -0.9607, -0.6123]])\n>>> x.dtype\ntorch.float32\n\n>>> y = x.view(torch.int32)\n>>> y\ntensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n        [-1105482831,  1061112040,  1057999968, -1084397505],\n        [-1071760287, -1123489973, -1097310419, -1084649136],\n        [-1101533110,  1073668768, -1082790149, -1088634448]],\n    dtype=torch.int32)\n>>> y[0, 0] = 1000000000\n>>> x\ntensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n        [-0.1520,  0.7472,  0.5617, -0.8649],\n        [-2.4724, -0.0334, -0.2976, -0.8499],\n        [-0.2109,  1.9913, -0.9607, -0.6123]])\n\n>>> x.view(torch.int16)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.\n \n"}, {"name": "torch.Tensor.view_as()", "path": "tensors#torch.Tensor.view_as", "type": "torch.Tensor", "text": " \nview_as(other) \u2192 Tensor  \nView this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()). Please see view() for more information about view.  Parameters \nother (torch.Tensor) \u2013 The result tensor has the same size as other.   \n"}, {"name": "torch.Tensor.where()", "path": "tensors#torch.Tensor.where", "type": "torch.Tensor", "text": " \nwhere(condition, y) \u2192 Tensor  \nself.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where() \n"}, {"name": "torch.Tensor.xlogy()", "path": "tensors#torch.Tensor.xlogy", "type": "torch.Tensor", "text": " \nxlogy(other) \u2192 Tensor  \nSee torch.xlogy() \n"}, {"name": "torch.Tensor.xlogy_()", "path": "tensors#torch.Tensor.xlogy_", "type": "torch.Tensor", "text": " \nxlogy_(other) \u2192 Tensor  \nIn-place version of xlogy() \n"}, {"name": "torch.Tensor.zero_()", "path": "tensors#torch.Tensor.zero_", "type": "torch.Tensor", "text": " \nzero_() \u2192 Tensor  \nFills self tensor with zeros. \n"}, {"name": "torch.tensordot()", "path": "generated/torch.tensordot#torch.tensordot", "type": "torch", "text": " \ntorch.tensordot(a, b, dims=2, out=None) [source]\n \nReturns a contraction of a and b over multiple dimensions. tensordot implements a generalized matrix product.  Parameters \n \na (Tensor) \u2013 Left tensor to contract \nb (Tensor) \u2013 Right tensor to contract \ndims (int or Tuple[List[int]] containing two lists) \u2013 number of dimensions to contract or explicit lists of dimensions for a and b respectively    When called with a non-negative integer argument dims = dd , and the number of dimensions of a and b is mm  and nn , respectively, tensordot() computes  ri0,...,im\u2212d,id,...,in=\u2211k0,...,kd\u22121ai0,...,im\u2212d,k0,...,kd\u22121\u00d7bk0,...,kd\u22121,id,...,in.r_{i_0,...,i_{m-d}, i_d,...,i_n} = \\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}.  \nWhen called with dims of the list form, the given dimensions will be contracted in place of the last dd  of a and the first dd  of bb . The sizes in these dimensions must match, but tensordot() will deal with broadcasted dimensions. Examples: >>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[  7.7193,  -2.4867, -10.3204],\n        [  1.5513, -14.4737,  -6.5113],\n        [ -0.2850,   4.2573,  -3.5997]])\n \n"}, {"name": "torch.tensor_split()", "path": "generated/torch.tensor_split#torch.tensor_split", "type": "torch", "text": " \ntorch.tensor_split(input, indices_or_sections, dim=0) \u2192 List of Tensors  \nSplits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections. This function is based on NumPy\u2019s numpy.array_split().  Parameters \n \ninput (Tensor) \u2013 the tensor to split \nindices_or_sections (Tensor, int or list or tuple of python:ints) \u2013 \nIf indices_or_sections is an integer n or a zero dimensional long tensor with value n, input is split into n sections along dimension dim. If input is divisible by n along dimension dim, each section will be of equal size, input.size(dim) / n. If input is not divisible by n, the sizes of the first int(input.size(dim) % n) sections will have size int(input.size(dim) / n) + 1, and the rest will have size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long tensor, then input is split along dimension dim at each of the indices in the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0 would result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional long tensor on the CPU.  \ndim (int, optional) \u2013 dimension along which to split the tensor. Default: 0\n     Example::\n\n>>> x = torch.arange(8)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))\n >>> x = torch.arange(7)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))\n>>> torch.tensor_split(x, (1, 6))\n(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))\n >>> x = torch.arange(14).reshape(2, 7)\n>>> x\ntensor([[ 0,  1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12, 13]])\n>>> torch.tensor_split(x, 3, dim=1)\n(tensor([[0, 1, 2],\n        [7, 8, 9]]),\n tensor([[ 3,  4],\n        [10, 11]]),\n tensor([[ 5,  6],\n        [12, 13]]))\n>>> torch.tensor_split(x, (1, 6), dim=1)\n(tensor([[0],\n        [7]]),\n tensor([[ 1,  2,  3,  4,  5],\n        [ 8,  9, 10, 11, 12]]),\n tensor([[ 6],\n        [13]]))\n   \n"}, {"name": "torch.tile()", "path": "generated/torch.tile#torch.tile", "type": "torch", "text": " \ntorch.tile(input, reps) \u2192 Tensor  \nConstructs a tensor by repeating the elements of input. The reps argument specifies the number of repetitions in each dimension. If reps specifies fewer dimensions than input has, then ones are prepended to reps until all dimensions are specified. For example, if input has shape (8, 6, 4, 2) and reps is (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has as many dimensions as reps specifies. For example, if input has shape (4, 2) and reps is (3, 3, 2, 2), then input is treated as if it had the shape (1, 1, 4, 2).  Note This function is similar to NumPy\u2019s tile function.   Parameters \n \ninput (Tensor) \u2013 the tensor whose elements to repeat. \nreps (tuple) \u2013 the number of repetitions per dimension.    Example: >>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])\n \n"}, {"name": "torch.topk()", "path": "generated/torch.topk#torch.topk", "type": "torch", "text": " \ntorch.topk(input, k, dim=None, largest=True, sorted=True, *, out=None) -> (Tensor, LongTensor)  \nReturns the k largest elements of the given input tensor along a given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned k elements are themselves sorted  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nk (int) \u2013 the k in \u201ctop-k\u201d \ndim (int, optional) \u2013 the dimension to sort along \nlargest (bool, optional) \u2013 controls whether to return largest or smallest elements \nsorted (bool, optional) \u2013 controls whether to return the elements in sorted order   Keyword Arguments \nout (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers   Example: >>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.topk(x, 3)\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))\n \n"}, {"name": "torch.torch.default_generator", "path": "torch#torch.torch.default_generator", "type": "torch", "text": " \ntorch.default_generator Returns the default CPU torch.Generator \n"}, {"name": "torch.torch.device", "path": "tensor_attributes#torch.torch.device", "type": "Tensor Attributes", "text": " \nclass torch.device \n"}, {"name": "torch.torch.dtype", "path": "tensor_attributes#torch.torch.dtype", "type": "Tensor Attributes", "text": " \nclass torch.dtype \n"}, {"name": "torch.torch.finfo", "path": "type_info#torch.torch.finfo", "type": "Type Info", "text": " \nclass torch.finfo \n"}, {"name": "torch.torch.iinfo", "path": "type_info#torch.torch.iinfo", "type": "Type Info", "text": " \nclass torch.iinfo \n"}, {"name": "torch.torch.layout", "path": "tensor_attributes#torch.torch.layout", "type": "Tensor Attributes", "text": " \nclass torch.layout \n"}, {"name": "torch.torch.memory_format", "path": "tensor_attributes#torch.torch.memory_format", "type": "Tensor Attributes", "text": " \nclass torch.memory_format \n"}, {"name": "torch.trace()", "path": "generated/torch.trace#torch.trace", "type": "torch", "text": " \ntorch.trace(input) \u2192 Tensor  \nReturns the sum of the elements of the diagonal of the input 2-D matrix. Example: >>> x = torch.arange(1., 10.).view(3, 3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n>>> torch.trace(x)\ntensor(15.)\n \n"}, {"name": "torch.transpose()", "path": "generated/torch.transpose#torch.transpose", "type": "torch", "text": " \ntorch.transpose(input, dim0, dim1) \u2192 Tensor  \nReturns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped. The resulting out tensor shares its underlying storage with the input tensor, so changing the content of one would change the content of the other.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim0 (int) \u2013 the first dimension to be transposed \ndim1 (int) \u2013 the second dimension to be transposed    Example: >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])\n \n"}, {"name": "torch.trapz()", "path": "generated/torch.trapz#torch.trapz", "type": "torch", "text": " \ntorch.trapz(y, x, *, dim=-1) \u2192 Tensor  \nEstimate \u222bydx\\int y\\,dx  along dim, using the trapezoid rule.  Parameters \n \ny (Tensor) \u2013 The values of the function to integrate \nx (Tensor) \u2013 The points at which the function y is sampled. If x is not in ascending order, intervals on which it is decreasing contribute negatively to the estimated integral (i.e., the convention \u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f  is followed). \ndim (int) \u2013 The dimension along which to integrate. By default, use the last dimension.   Returns \nA Tensor with the same shape as the input, except with dim removed. Each element of the returned tensor represents the estimated integral \u222bydx\\int y\\,dx  along dim.   Example: >>> y = torch.randn((2, 3))\n>>> y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n>>> torch.trapz(y, x)\ntensor([-1.2220,  0.9683])\n  \ntorch.trapz(y, *, dx=1, dim=-1) \u2192 Tensor \n As above, but the sample points are spaced uniformly at a distance of dx.  Parameters \ny (Tensor) \u2013 The values of the function to integrate  Keyword Arguments \n \ndx (float) \u2013 The distance between points at which y is sampled. \ndim (int) \u2013 The dimension along which to integrate. By default, use the last dimension.   Returns \nA Tensor with the same shape as the input, except with dim removed. Each element of the returned tensor represents the estimated integral \u222bydx\\int y\\,dx  along dim.   \n"}, {"name": "torch.triangular_solve()", "path": "generated/torch.triangular_solve#torch.triangular_solve", "type": "torch", "text": " \ntorch.triangular_solve(input, A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)  \nSolves a system of equations with a triangular coefficient matrix AA  and multiple right-hand sides bb . In particular, solves AX=bAX = b  and assumes AA  is upper-triangular with the default keyword arguments. torch.triangular_solve(b, A) can take in 2D inputs b, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs X Supports real-valued and complex-valued inputs.  Parameters \n \ninput (Tensor) \u2013 multiple right-hand sides of size (\u2217,m,k)(*, m, k)  where \u2217*  is zero of more batch dimensions (bb ) \nA (Tensor) \u2013 the input triangular coefficient matrix of size (\u2217,m,m)(*, m, m)  where \u2217*  is zero or more batch dimensions \nupper (bool, optional) \u2013 whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations. Default: True. \ntranspose (bool, optional) \u2013 whether AA  should be transposed before being sent into the solver. Default: False. \nunitriangular (bool, optional) \u2013 whether AA  is unit triangular. If True, the diagonal elements of AA  are assumed to be 1 and not referenced from AA . Default: False.   Returns \nA namedtuple (solution, cloned_coefficient) where cloned_coefficient is a clone of AA  and solution is the solution XX  to AX=bAX = b  (or whatever variant of the system of equations, depending on the keyword arguments.)   Examples: >>> A = torch.randn(2, 2).triu()\n>>> A\ntensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]])\n>>> b = torch.randn(2, 3)\n>>> b\ntensor([[-0.0210,  2.3513, -1.5492],\n        [ 1.5429,  0.7403, -1.0243]])\n>>> torch.triangular_solve(b, A)\ntorch.return_types.triangular_solve(\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\n        [ 1.9320,  0.9270, -1.2826]]),\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]]))\n \n"}, {"name": "torch.tril()", "path": "generated/torch.tril#torch.tril", "type": "torch", "text": " \ntorch.tril(input, diagonal=0, *, out=None) \u2192 Tensor  \nReturns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0. The lower triangular part of the matrix is defined as the elements on and below the diagonal. The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndiagonal (int, optional) \u2013 the diagonal to consider   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0813, -0.8619,  0.7105],\n        [ 0.0935,  0.1380,  2.2112],\n        [-0.3409, -0.9828,  0.0289]])\n>>> torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n        [ 0.0935,  0.1380,  0.0000],\n        [-0.3409, -0.9828,  0.0289]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n>>> torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n>>> torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n \n"}, {"name": "torch.tril_indices()", "path": "generated/torch.tril_indices#torch.tril_indices", "type": "torch", "text": " \ntorch.tril_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) \u2192 Tensor  \nReturns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns. The lower triangular part of the matrix is defined as the elements on and below the diagonal. The argument offset controls which diagonal to consider. If offset = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.  Note When running on CUDA, row * col must be less than 2592^{59}  to prevent overflow during calculation.   Parameters \n \nrow (int) \u2013 number of rows in the 2-D matrix. \ncol (int) \u2013 number of columns in the 2-D matrix. \noffset (int) \u2013 diagonal offset from the main diagonal. Default: if not provided, 0.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, torch.long. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nlayout (torch.layout, optional) \u2013 currently only support torch.strided.     Example::\n\n>>> a = torch.tril_indices(3, 3)\n>>> a\ntensor([[0, 1, 1, 2, 2, 2],\n        [0, 0, 1, 0, 1, 2]])\n >>> a = torch.tril_indices(4, 3, -1)\n>>> a\ntensor([[1, 2, 2, 3, 3, 3],\n        [0, 0, 1, 0, 1, 2]])\n >>> a = torch.tril_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\n   \n"}, {"name": "torch.triu()", "path": "generated/torch.triu#torch.triu", "type": "torch", "text": " \ntorch.triu(input, diagonal=0, *, out=None) \u2192 Tensor  \nReturns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0. The upper triangular part of the matrix is defined as the elements on and above the diagonal. The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndiagonal (int, optional) \u2013 the diagonal to consider   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.3480, -0.5211, -0.4573]])\n>>> torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.0000, -1.0680,  0.6602],\n        [ 0.0000,  0.0000, -0.4573]])\n>>> torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n        [ 0.0000,  0.0000,  0.6602],\n        [ 0.0000,  0.0000,  0.0000]])\n>>> torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.0000, -0.5211, -0.4573]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=1)\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=-1)\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\n \n"}, {"name": "torch.triu_indices()", "path": "generated/torch.triu_indices#torch.triu_indices", "type": "torch", "text": " \ntorch.triu_indices(row, col, offset=0, *, dtype=torch.long, device='cpu', layout=torch.strided) \u2192 Tensor  \nReturns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and above the diagonal. The argument offset controls which diagonal to consider. If offset = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.  Note When running on CUDA, row * col must be less than 2592^{59}  to prevent overflow during calculation.   Parameters \n \nrow (int) \u2013 number of rows in the 2-D matrix. \ncol (int) \u2013 number of columns in the 2-D matrix. \noffset (int) \u2013 diagonal offset from the main diagonal. Default: if not provided, 0.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, torch.long. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nlayout (torch.layout, optional) \u2013 currently only support torch.strided.     Example::\n\n>>> a = torch.triu_indices(3, 3)\n>>> a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n >>> a = torch.triu_indices(4, 3, -1)\n>>> a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n >>> a = torch.triu_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1],\n        [1, 2, 2]])\n   \n"}, {"name": "torch.true_divide()", "path": "generated/torch.true_divide#torch.true_divide", "type": "torch", "text": " \ntorch.true_divide(dividend, divisor, *, out) \u2192 Tensor  \nAlias for torch.div() with rounding_mode=None. \n"}, {"name": "torch.trunc()", "path": "generated/torch.trunc#torch.trunc", "type": "torch", "text": " \ntorch.trunc(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the truncated integer values of the elements of input.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])\n \n"}, {"name": "torch.unbind()", "path": "generated/torch.unbind#torch.unbind", "type": "torch", "text": " \ntorch.unbind(input, dim=0) \u2192 seq  \nRemoves a tensor dimension. Returns a tuple of all slices along a given dimension, already without it.  Parameters \n \ninput (Tensor) \u2013 the tensor to unbind \ndim (int) \u2013 dimension to remove    Example: >>> torch.unbind(torch.tensor([[1, 2, 3],\n>>>                            [4, 5, 6],\n>>>                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n \n"}, {"name": "torch.unique()", "path": "generated/torch.unique#torch.unique", "type": "torch", "text": " \ntorch.unique(*args, **kwargs)  \nReturns the unique elements of the input tensor.  Note This function is different from torch.unique_consecutive() in the sense that this function also eliminates non-consecutive duplicate values.   Note Currently in the CUDA implementation and the CPU implementation when dim is specified, torch.unique always sort the tensor at the beginning regardless of the sort argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use torch.unique_consecutive() which avoids the sorting.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nsorted (bool) \u2013 Whether to sort the unique elements in ascending order before returning as output. \nreturn_inverse (bool) \u2013 Whether to also return the indices for where elements in the original input ended up in the returned unique list. \nreturn_counts (bool) \u2013 Whether to also return the counts for each unique element. \ndim (int) \u2013 the dimension to apply unique. If None, the unique of the flattened input is returned. default: None\n   Returns \nA tensor or a tuple of tensors containing  \noutput (Tensor): the output list of unique scalar elements. \ninverse_indices (Tensor): (optional) if return_inverse is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor. \ncounts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.   Return type \n(Tensor, Tensor (optional), Tensor (optional))   Example: >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n>>> output\ntensor([ 2,  3,  1])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([ 0,  2,  1,  2])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([[ 0,  2],\n        [ 1,  2]])\n \n"}, {"name": "torch.unique_consecutive()", "path": "generated/torch.unique_consecutive#torch.unique_consecutive", "type": "torch", "text": " \ntorch.unique_consecutive(*args, **kwargs)  \nEliminates all but the first element from every consecutive group of equivalent elements.  Note This function is different from torch.unique() in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to std::unique in C++.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nreturn_inverse (bool) \u2013 Whether to also return the indices for where elements in the original input ended up in the returned unique list. \nreturn_counts (bool) \u2013 Whether to also return the counts for each unique element. \ndim (int) \u2013 the dimension to apply unique. If None, the unique of the flattened input is returned. default: None\n   Returns \nA tensor or a tuple of tensors containing  \noutput (Tensor): the output list of unique scalar elements. \ninverse_indices (Tensor): (optional) if return_inverse is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor. \ncounts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.   Return type \n(Tensor, Tensor (optional), Tensor (optional))   Example: >>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n>>> output = torch.unique_consecutive(x)\n>>> output\ntensor([1, 2, 3, 1, 2])\n\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> counts\ntensor([2, 2, 1, 2, 1])\n \n"}, {"name": "torch.unsqueeze()", "path": "generated/torch.unsqueeze#torch.unsqueeze", "type": "torch", "text": " \ntorch.unsqueeze(input, dim) \u2192 Tensor  \nReturns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor. A dim value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at dim = dim + input.dim() + 1.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the index at which to insert the singleton dimension    Example: >>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n \n"}, {"name": "torch.use_deterministic_algorithms()", "path": "generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms", "type": "torch", "text": " \ntorch.use_deterministic_algorithms(d) [source]\n \nSets whether PyTorch operations must use \u201cdeterministic\u201d algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When True, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a :class:RuntimeError when called.  Warning This feature is in beta, and its design and implementation may change in the future.  The following normally-nondeterministic operations will act deterministically when d=True:  \ntorch.nn.Conv1d when called on CUDA tensor \ntorch.nn.Conv2d when called on CUDA tensor \ntorch.nn.Conv3d when called on CUDA tensor \ntorch.nn.ConvTranspose1d when called on CUDA tensor \ntorch.nn.ConvTranspose2d when called on CUDA tensor \ntorch.nn.ConvTranspose3d when called on CUDA tensor \ntorch.bmm() when called on sparse-dense CUDA tensors \ntorch.__getitem__() backward when self is a CPU tensor and indices is a list of tensors \ntorch.index_put() with accumulate=True when called on a CPU tensor  The following normally-nondeterministic operations will throw a RuntimeError when d=True:  \ntorch.nn.AvgPool3d when called on a CUDA tensor that requires grad \ntorch.nn.AdaptiveAvgPool2d when called on a CUDA tensor that requires grad \ntorch.nn.AdaptiveAvgPool3d when called on a CUDA tensor that requires grad \ntorch.nn.MaxPool3d when called on a CUDA tensor that requires grad \ntorch.nn.AdaptiveMaxPool2d when called on a CUDA tensor that requires grad \ntorch.nn.FractionalMaxPool2d when called on a CUDA tensor that requires grad \ntorch.nn.FractionalMaxPool3d when called on a CUDA tensor that requires grad \ntorch.nn.functional.interpolate() when called on a CUDA tensor that requires grad and one of the following modes is used:  linear bilinear bicubic trilinear   \ntorch.nn.ReflectionPad1d when called on a CUDA tensor that requires grad \ntorch.nn.ReflectionPad2d when called on a CUDA tensor that requires grad \ntorch.nn.ReplicationPad1d when called on a CUDA tensor that requires grad \ntorch.nn.ReplicationPad2d when called on a CUDA tensor that requires grad \ntorch.nn.ReplicationPad3d when called on a CUDA tensor that requires grad \ntorch.nn.NLLLoss when called on a CUDA tensor that requires grad \ntorch.nn.CTCLoss when called on a CUDA tensor that requires grad \ntorch.nn.EmbeddingBag when called on a CUDA tensor that requires grad \ntorch.scatter_add_() when called on a CUDA tensor \ntorch.index_add_() when called on a CUDA tensor torch.index_copy() \ntorch.index_select() when called on a CUDA tensor that requires grad \ntorch.repeat_interleave() when called on a CUDA tensor that requires grad \ntorch.histc() when called on a CUDA tensor \ntorch.bincount() when called on a CUDA tensor \ntorch.kthvalue() with called on a CUDA tensor \ntorch.median() with indices output when called on a CUDA tensor  A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more details: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility If one of these environment variable configurations is not set, a RuntimeError will be raised from these operations when called with CUDA tensors:  torch.mm() torch.mv() torch.bmm()  Note that deterministic operations tend to have worse performance than non-deterministic operations.  Parameters \nd (bool) \u2013 If True, force operations to be deterministic. If False, allow non-deterministic operations.   \n"}, {"name": "torch.utils.benchmark", "path": "benchmark_utils", "type": "torch.utils.benchmark", "text": "Benchmark Utils - torch.utils.benchmark  \nclass torch.utils.benchmark.Timer(stmt='pass', setup='pass', timer=<function timer>, globals=None, label=None, sub_label=None, description=None, env=None, num_threads=1, language=<Language.PYTHON: 0>) [source]\n \nHelper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based on timeit.Timer (and in fact uses timeit.Timer internally), but with several key differences:  \n Runtime aware:\n\nTimer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.    \n Focus on replicates:\n\nWhen measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the timeit API by conceptually merging timeit.Timer.repeat and timeit.Timer.autorange. (Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not desired.    \n Optional metadata:\n\nWhen defining a Timer, one can optionally specify label, sub_label, description, and env. (Defined later) These fields are included in the representation of result object and by the Compare class to group and display results for comparison.    \n Instruction counts\n\nIn addition to wall times, Timer can run a statement under Callgrind and report instructions executed.     Directly analogous to timeit.Timer constructor arguments: stmt, setup, timer, globals PyTorch Timer specific constructor arguments: label, sub_label, description, env, num_threads  Parameters \n \nstmt \u2013 Code snippet to be run in a loop and timed. \nsetup \u2013 Optional setup code. Used to define variables used in stmt\n \ntimer \u2013 Callable which returns the current time. If PyTorch was built without CUDA or there is no GPU present, this defaults to timeit.default_timer; otherwise it will synchronize CUDA before measuring the time. \nglobals \u2013 A dict which defines the global variables when stmt is being executed. This is the other method for providing variables which stmt needs. \nlabel \u2013 String which summarizes stmt. For instance, if stmt is \u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d one might set label to \u201cReLU(x + 1)\u201d to improve readability. \nsub_label \u2013 \nProvide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using Compare.  \ndescription \u2013 \nString to distinguish measurements with identical label and sub_label. The principal use of description is to signal to Compare the columns of data. For instance one might set it based on the input size to create a table of the form:                         | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\n using Compare. It is also included when printing a Measurement.  \nenv \u2013 This tag indicates that otherwise identical tasks were run in different environments, and are therefore not equivilent, for instance when A/B testing a change to a kernel. Compare will treat Measurements with different env specification as distinct when merging replicate runs. \nnum_threads \u2013 The size of the PyTorch threadpool when executing stmt. Single threaded performace is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency, so the default is set to one. This is in contrast to the default PyTorch threadpool size which tries to utilize all cores.     \nblocked_autorange(callback=None, min_run_time=0.2) [source]\n \nMeasure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: `setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:  A small block size results in more replicates and generally better statistics. A large block size better amortizes the cost of timer invocation, and results in a less biased measurement. This is important because CUDA syncronization time is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement.  blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.  Returns \nA Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)   \n  \ncollect_callgrind(number=100, collect_baseline=True) [source]\n \nCollect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements. In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed. Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly. By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.  Returns \nA CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.   \n  \ntimeit(number=1000000) [source]\n \nMirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit \n \n  \nclass torch.utils.benchmark.Measurement(number_per_run, raw_times, task_spec, metadata=None) [source]\n \nThe result of a Timer measurement. This class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers.  \nstatic merge(measurements) [source]\n \nConvenience method for merging replicates. Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates) \n  \nproperty significant_figures  \nApproximate significant figure estimate. This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t. The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare. \n \n  \nclass torch.utils.benchmark.CallgrindStats(task_spec, number_per_run, built_with_debug_symbols, baseline_inclusive_stats, baseline_exclusive_stats, stmt_inclusive_stats, stmt_exclusive_stats) [source]\n \nTop level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is obtained by calling CallgrindStats.stats(\u2026). Several convenience methods are provided as well; the most significant is CallgrindStats.as_standardized().  \nas_standardized() [source]\n \nStrip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling: 23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing. \n  \ncounts(*, denoise=False) [source]\n \nReturns the total number of instructions executed. See FunctionCounts.denoise() for an explation of the denoise arg. \n  \ndelta(other, inclusive=False, subtract_baselines=True) [source]\n \nDiff two sets of counts. One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. The subtract_baselines argument allows one to disable baseline correction, though in most cases it shouldn\u2019t matter as the baselines are expected to more or less cancel out. \n  \nstats(inclusive=False) [source]\n \nReturns detailed function counts. Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples. inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details) \n \n  \nclass torch.utils.benchmark.FunctionCounts(_data, inclusive, _linewidth=None) [source]\n \nContainer for manipulating Callgrind results.  It supports:\n\n Addition and subtraction to combine or diff results. Tuple-like indexing. A denoise function which strips CPython calls which are known to be non-deterministic and quite noisy. Two higher order methods (filter and transform) for custom manipulation.     \ndenoise() [source]\n \nRemove known noisy instructions. Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception. \n  \nfilter(filter_fn) [source]\n \nKeep only the elements where filter_fn applied to function name returns True. \n  \ntransform(map_fn) [source]\n \nApply map_fn to all of the function names. This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc. \n \n\n"}, {"name": "torch.utils.benchmark.CallgrindStats", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats", "type": "torch.utils.benchmark", "text": " \nclass torch.utils.benchmark.CallgrindStats(task_spec, number_per_run, built_with_debug_symbols, baseline_inclusive_stats, baseline_exclusive_stats, stmt_inclusive_stats, stmt_exclusive_stats) [source]\n \nTop level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is obtained by calling CallgrindStats.stats(\u2026). Several convenience methods are provided as well; the most significant is CallgrindStats.as_standardized().  \nas_standardized() [source]\n \nStrip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling: 23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing. \n  \ncounts(*, denoise=False) [source]\n \nReturns the total number of instructions executed. See FunctionCounts.denoise() for an explation of the denoise arg. \n  \ndelta(other, inclusive=False, subtract_baselines=True) [source]\n \nDiff two sets of counts. One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. The subtract_baselines argument allows one to disable baseline correction, though in most cases it shouldn\u2019t matter as the baselines are expected to more or less cancel out. \n  \nstats(inclusive=False) [source]\n \nReturns detailed function counts. Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples. inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details) \n \n"}, {"name": "torch.utils.benchmark.CallgrindStats.as_standardized()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.as_standardized", "type": "torch.utils.benchmark", "text": " \nas_standardized() [source]\n \nStrip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling: 23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing. \n"}, {"name": "torch.utils.benchmark.CallgrindStats.counts()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.counts", "type": "torch.utils.benchmark", "text": " \ncounts(*, denoise=False) [source]\n \nReturns the total number of instructions executed. See FunctionCounts.denoise() for an explation of the denoise arg. \n"}, {"name": "torch.utils.benchmark.CallgrindStats.delta()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.delta", "type": "torch.utils.benchmark", "text": " \ndelta(other, inclusive=False, subtract_baselines=True) [source]\n \nDiff two sets of counts. One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. The subtract_baselines argument allows one to disable baseline correction, though in most cases it shouldn\u2019t matter as the baselines are expected to more or less cancel out. \n"}, {"name": "torch.utils.benchmark.CallgrindStats.stats()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.stats", "type": "torch.utils.benchmark", "text": " \nstats(inclusive=False) [source]\n \nReturns detailed function counts. Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples. inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details) \n"}, {"name": "torch.utils.benchmark.FunctionCounts", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts", "type": "torch.utils.benchmark", "text": " \nclass torch.utils.benchmark.FunctionCounts(_data, inclusive, _linewidth=None) [source]\n \nContainer for manipulating Callgrind results.  It supports:\n\n Addition and subtraction to combine or diff results. Tuple-like indexing. A denoise function which strips CPython calls which are known to be non-deterministic and quite noisy. Two higher order methods (filter and transform) for custom manipulation.     \ndenoise() [source]\n \nRemove known noisy instructions. Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception. \n  \nfilter(filter_fn) [source]\n \nKeep only the elements where filter_fn applied to function name returns True. \n  \ntransform(map_fn) [source]\n \nApply map_fn to all of the function names. This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc. \n \n"}, {"name": "torch.utils.benchmark.FunctionCounts.denoise()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.denoise", "type": "torch.utils.benchmark", "text": " \ndenoise() [source]\n \nRemove known noisy instructions. Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception. \n"}, {"name": "torch.utils.benchmark.FunctionCounts.filter()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.filter", "type": "torch.utils.benchmark", "text": " \nfilter(filter_fn) [source]\n \nKeep only the elements where filter_fn applied to function name returns True. \n"}, {"name": "torch.utils.benchmark.FunctionCounts.transform()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.transform", "type": "torch.utils.benchmark", "text": " \ntransform(map_fn) [source]\n \nApply map_fn to all of the function names. This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc. \n"}, {"name": "torch.utils.benchmark.Measurement", "path": "benchmark_utils#torch.utils.benchmark.Measurement", "type": "torch.utils.benchmark", "text": " \nclass torch.utils.benchmark.Measurement(number_per_run, raw_times, task_spec, metadata=None) [source]\n \nThe result of a Timer measurement. This class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers.  \nstatic merge(measurements) [source]\n \nConvenience method for merging replicates. Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates) \n  \nproperty significant_figures  \nApproximate significant figure estimate. This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t. The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare. \n \n"}, {"name": "torch.utils.benchmark.Measurement.merge()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.merge", "type": "torch.utils.benchmark", "text": " \nstatic merge(measurements) [source]\n \nConvenience method for merging replicates. Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates) \n"}, {"name": "torch.utils.benchmark.Measurement.significant_figures()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.significant_figures", "type": "torch.utils.benchmark", "text": " \nproperty significant_figures  \nApproximate significant figure estimate. This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t. The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare. \n"}, {"name": "torch.utils.benchmark.Timer", "path": "benchmark_utils#torch.utils.benchmark.Timer", "type": "torch.utils.benchmark", "text": " \nclass torch.utils.benchmark.Timer(stmt='pass', setup='pass', timer=<function timer>, globals=None, label=None, sub_label=None, description=None, env=None, num_threads=1, language=<Language.PYTHON: 0>) [source]\n \nHelper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based on timeit.Timer (and in fact uses timeit.Timer internally), but with several key differences:  \n Runtime aware:\n\nTimer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.    \n Focus on replicates:\n\nWhen measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the timeit API by conceptually merging timeit.Timer.repeat and timeit.Timer.autorange. (Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not desired.    \n Optional metadata:\n\nWhen defining a Timer, one can optionally specify label, sub_label, description, and env. (Defined later) These fields are included in the representation of result object and by the Compare class to group and display results for comparison.    \n Instruction counts\n\nIn addition to wall times, Timer can run a statement under Callgrind and report instructions executed.     Directly analogous to timeit.Timer constructor arguments: stmt, setup, timer, globals PyTorch Timer specific constructor arguments: label, sub_label, description, env, num_threads  Parameters \n \nstmt \u2013 Code snippet to be run in a loop and timed. \nsetup \u2013 Optional setup code. Used to define variables used in stmt\n \ntimer \u2013 Callable which returns the current time. If PyTorch was built without CUDA or there is no GPU present, this defaults to timeit.default_timer; otherwise it will synchronize CUDA before measuring the time. \nglobals \u2013 A dict which defines the global variables when stmt is being executed. This is the other method for providing variables which stmt needs. \nlabel \u2013 String which summarizes stmt. For instance, if stmt is \u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d one might set label to \u201cReLU(x + 1)\u201d to improve readability. \nsub_label \u2013 \nProvide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using Compare.  \ndescription \u2013 \nString to distinguish measurements with identical label and sub_label. The principal use of description is to signal to Compare the columns of data. For instance one might set it based on the input size to create a table of the form:                         | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\n using Compare. It is also included when printing a Measurement.  \nenv \u2013 This tag indicates that otherwise identical tasks were run in different environments, and are therefore not equivilent, for instance when A/B testing a change to a kernel. Compare will treat Measurements with different env specification as distinct when merging replicate runs. \nnum_threads \u2013 The size of the PyTorch threadpool when executing stmt. Single threaded performace is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency, so the default is set to one. This is in contrast to the default PyTorch threadpool size which tries to utilize all cores.     \nblocked_autorange(callback=None, min_run_time=0.2) [source]\n \nMeasure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: `setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:  A small block size results in more replicates and generally better statistics. A large block size better amortizes the cost of timer invocation, and results in a less biased measurement. This is important because CUDA syncronization time is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement.  blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.  Returns \nA Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)   \n  \ncollect_callgrind(number=100, collect_baseline=True) [source]\n \nCollect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements. In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed. Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly. By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.  Returns \nA CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.   \n  \ntimeit(number=1000000) [source]\n \nMirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit \n \n"}, {"name": "torch.utils.benchmark.Timer.blocked_autorange()", "path": "benchmark_utils#torch.utils.benchmark.Timer.blocked_autorange", "type": "torch.utils.benchmark", "text": " \nblocked_autorange(callback=None, min_run_time=0.2) [source]\n \nMeasure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: `setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:  A small block size results in more replicates and generally better statistics. A large block size better amortizes the cost of timer invocation, and results in a less biased measurement. This is important because CUDA syncronization time is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement.  blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.  Returns \nA Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)   \n"}, {"name": "torch.utils.benchmark.Timer.collect_callgrind()", "path": "benchmark_utils#torch.utils.benchmark.Timer.collect_callgrind", "type": "torch.utils.benchmark", "text": " \ncollect_callgrind(number=100, collect_baseline=True) [source]\n \nCollect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements. In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed. Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly. By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.  Returns \nA CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.   \n"}, {"name": "torch.utils.benchmark.Timer.timeit()", "path": "benchmark_utils#torch.utils.benchmark.Timer.timeit", "type": "torch.utils.benchmark", "text": " \ntimeit(number=1000000) [source]\n \nMirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit \n"}, {"name": "torch.utils.bottleneck", "path": "bottleneck", "type": "torch.utils.bottleneck", "text": "torch.utils.bottleneck torch.utils.bottleneck is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch\u2019s autograd profiler. Run it on the command line with python -m torch.utils.bottleneck /path/to/source/script.py [args]\n where [args] are any number of arguments to script.py, or run python -m torch.utils.bottleneck -h for more usage instructions.  Warning Because your script will be profiled, please ensure that it exits in a finite amount of time.   Warning Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.   Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (\u201cCPU total time is much greater than CUDA total time\u201d). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler. Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline.   Warning If you are profiling CUDA code, the first profiler that bottleneck runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.  For more complicated uses of the profilers (like in a multi-GPU case), please see https://docs.python.org/3/library/profile.html or torch.autograd.profiler.profile() for more information.\n"}, {"name": "torch.utils.checkpoint", "path": "checkpoint", "type": "torch.utils.checkpoint", "text": "torch.utils.checkpoint  Note Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply preserve_rng_state=False to checkpoint or checkpoint_sequential to omit stashing and restoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the run_fn. However, the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself. Therefore, if you move Tensors to a new device (\u201cnew\u201d meaning not belonging to the set of [current device + devices of Tensor arguments]) within run_fn, deterministic output compared to non-checkpointed passes is never guaranteed.   \ntorch.utils.checkpoint.checkpoint(function, *args, **kwargs) [source]\n \nCheckpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.  Warning Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().   Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected.   Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.   Parameters \n \nfunction \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden\n \npreserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. \nargs \u2013 tuple containing inputs to the function\n   Returns \nOutput of running function on *args   \n  \ntorch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs) [source]\n \nA helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass. See checkpoint() on how checkpointing works.  Warning Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().   Parameters \n \nfunctions \u2013 A torch.nn.Sequential or the list of modules or functions (comprising the model) to run sequentially. \nsegments \u2013 Number of chunks to create in the model \ninput \u2013 A Tensor that is input to functions\n \npreserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint.   Returns \nOutput of running functions sequentially on *inputs   Example >>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n \n\n"}, {"name": "torch.utils.checkpoint.checkpoint()", "path": "checkpoint#torch.utils.checkpoint.checkpoint", "type": "torch.utils.checkpoint", "text": " \ntorch.utils.checkpoint.checkpoint(function, *args, **kwargs) [source]\n \nCheckpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.  Warning Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().   Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected.   Warning If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.   Parameters \n \nfunction \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden\n \npreserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. \nargs \u2013 tuple containing inputs to the function\n   Returns \nOutput of running function on *args   \n"}, {"name": "torch.utils.checkpoint.checkpoint_sequential()", "path": "checkpoint#torch.utils.checkpoint.checkpoint_sequential", "type": "torch.utils.checkpoint", "text": " \ntorch.utils.checkpoint.checkpoint_sequential(functions, segments, input, **kwargs) [source]\n \nA helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass. See checkpoint() on how checkpointing works.  Warning Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().   Parameters \n \nfunctions \u2013 A torch.nn.Sequential or the list of modules or functions (comprising the model) to run sequentially. \nsegments \u2013 Number of chunks to create in the model \ninput \u2013 A Tensor that is input to functions\n \npreserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint.   Returns \nOutput of running functions sequentially on *inputs   Example >>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n \n"}, {"name": "torch.utils.cpp_extension", "path": "cpp_extension", "type": "torch.utils.cpp_extension", "text": "torch.utils.cpp_extension  \ntorch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs) [source]\n \nCreates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension constructor. Example >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n \n  \ntorch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs) [source]\n \nCreates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library. All arguments are forwarded to the setuptools.Extension constructor. Example >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. \n  \ntorch.utils.cpp_extension.BuildExtension(*args, **kwargs) [source]\n \nA custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available.  Note By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.  \n  \ntorch.utils.cpp_extension.load(name, sources, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, is_standalone=False, keep_intermediates=True) [source]\n \nLoads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use. By default, the directory to which the build file is emitted and the resulting library compiled to is <tmp>/torch_extensions/<name>, where <tmp> is the temporary folder on the current platform and <name> the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces <tmp>/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly. To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.  Parameters \n \nname \u2013 The name of the extension to build. This MUST be the same as the name of the pybind11 module! \nsources \u2013 A list of relative or absolute paths to C++ source files. \nextra_cflags \u2013 optional list of compiler flags to forward to the build. \nextra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc when building CUDA sources. \nextra_ldflags \u2013 optional list of linker flags to forward to the build. \nextra_include_paths \u2013 optional list of include directories to forward to the build. \nbuild_directory \u2013 optional path to use as build workspace. \nverbose \u2013 If True, turns on verbose logging of load steps. \nwith_cuda \u2013 Determines whether CUDA headers and libraries are added to the build. If set to None (default), this value is automatically determined based on the existence of .cu or .cuh in sources. Set it to True` to force CUDA headers and libraries to be included. \nis_python_module \u2013 If True (default), imports the produced shared library as a Python module. If False, behavior depends on is_standalone. \nis_standalone \u2013 If False (default) loads the constructed extension into the process as a plain dynamic library. If True, build a standalone executable.   Returns \nReturns the loaded PyTorch extension as a Python module.  \nIf is_python_module is False and is_standalone is False: \n\nReturns nothing. (The shared library is loaded into the process as a side effect.)  \nIf is_standalone is True. \n\nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)    Return type \nIf is_python_module is True   Example >>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)\n \n  \ntorch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True) [source]\n \nLoads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load(). See the tests for good examples of using this function. Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include\n<torch/extension.h>. Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring. The sources in cuda_sources are concatenated into a separate .cu file and prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions). See load() for a description of arguments omitted below.  Parameters \n \ncpp_sources \u2013 A string, or list of strings, containing C++ source code. \ncuda_sources \u2013 A string, or list of strings, containing CUDA source code. \nfunctions \u2013 A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names). \nwith_cuda \u2013 Determines whether CUDA headers and libraries are added to the build. If set to None (default), this value is automatically determined based on whether cuda_sources is provided. Set it to True to force CUDA headers and libraries to be included. \nwith_pytorch_error_handling \u2013 Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function foo is called via an intermediary _safe_foo function. This redirection might cause issues in obscure cases of cpp. This flag should be set to False when this redirect causes issues.    Example >>> from torch.utils.cpp_extension import load_inline\n>>> source = \\'\\'\\'\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\\'\\'\\'\n>>> module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])\n  Note By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.  \n  \ntorch.utils.cpp_extension.include_paths(cuda=False) [source]\n \nGet the include paths required to build a C++ or CUDA extension.  Parameters \ncuda \u2013 If True, includes CUDA-specific include paths.  Returns \nA list of include path strings.   \n  \ntorch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) [source]\n \nVerifies that the given compiler is ABI-compatible with PyTorch.  Parameters \ncompiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.  Returns \nFalse if the compiler is (likely) ABI-incompatible with PyTorch, else True.   \n  \ntorch.utils.cpp_extension.verify_ninja_availability() [source]\n \nRaises RuntimeError if ninja build system is not available on the system, does nothing otherwise. \n  \ntorch.utils.cpp_extension.is_ninja_available() [source]\n \nReturns True if the ninja build system is available on the system, False otherwise. \n\n"}, {"name": "torch.utils.cpp_extension.BuildExtension()", "path": "cpp_extension#torch.utils.cpp_extension.BuildExtension", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.BuildExtension(*args, **kwargs) [source]\n \nA custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available.  Note By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.  \n"}, {"name": "torch.utils.cpp_extension.check_compiler_abi_compatibility()", "path": "cpp_extension#torch.utils.cpp_extension.check_compiler_abi_compatibility", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.check_compiler_abi_compatibility(compiler) [source]\n \nVerifies that the given compiler is ABI-compatible with PyTorch.  Parameters \ncompiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.  Returns \nFalse if the compiler is (likely) ABI-incompatible with PyTorch, else True.   \n"}, {"name": "torch.utils.cpp_extension.CppExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CppExtension", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs) [source]\n \nCreates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension constructor. Example >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n \n"}, {"name": "torch.utils.cpp_extension.CUDAExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CUDAExtension", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs) [source]\n \nCreates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library. All arguments are forwarded to the setuptools.Extension constructor. Example >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch. \n"}, {"name": "torch.utils.cpp_extension.include_paths()", "path": "cpp_extension#torch.utils.cpp_extension.include_paths", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.include_paths(cuda=False) [source]\n \nGet the include paths required to build a C++ or CUDA extension.  Parameters \ncuda \u2013 If True, includes CUDA-specific include paths.  Returns \nA list of include path strings.   \n"}, {"name": "torch.utils.cpp_extension.is_ninja_available()", "path": "cpp_extension#torch.utils.cpp_extension.is_ninja_available", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.is_ninja_available() [source]\n \nReturns True if the ninja build system is available on the system, False otherwise. \n"}, {"name": "torch.utils.cpp_extension.load()", "path": "cpp_extension#torch.utils.cpp_extension.load", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.load(name, sources, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, is_standalone=False, keep_intermediates=True) [source]\n \nLoads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use. By default, the directory to which the build file is emitted and the resulting library compiled to is <tmp>/torch_extensions/<name>, where <tmp> is the temporary folder on the current platform and <name> the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces <tmp>/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly. To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.  Parameters \n \nname \u2013 The name of the extension to build. This MUST be the same as the name of the pybind11 module! \nsources \u2013 A list of relative or absolute paths to C++ source files. \nextra_cflags \u2013 optional list of compiler flags to forward to the build. \nextra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc when building CUDA sources. \nextra_ldflags \u2013 optional list of linker flags to forward to the build. \nextra_include_paths \u2013 optional list of include directories to forward to the build. \nbuild_directory \u2013 optional path to use as build workspace. \nverbose \u2013 If True, turns on verbose logging of load steps. \nwith_cuda \u2013 Determines whether CUDA headers and libraries are added to the build. If set to None (default), this value is automatically determined based on the existence of .cu or .cuh in sources. Set it to True` to force CUDA headers and libraries to be included. \nis_python_module \u2013 If True (default), imports the produced shared library as a Python module. If False, behavior depends on is_standalone. \nis_standalone \u2013 If False (default) loads the constructed extension into the process as a plain dynamic library. If True, build a standalone executable.   Returns \nReturns the loaded PyTorch extension as a Python module.  \nIf is_python_module is False and is_standalone is False: \n\nReturns nothing. (The shared library is loaded into the process as a side effect.)  \nIf is_standalone is True. \n\nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)    Return type \nIf is_python_module is True   Example >>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)\n \n"}, {"name": "torch.utils.cpp_extension.load_inline()", "path": "cpp_extension#torch.utils.cpp_extension.load_inline", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True) [source]\n \nLoads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load(). See the tests for good examples of using this function. Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include\n<torch/extension.h>. Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring. The sources in cuda_sources are concatenated into a separate .cu file and prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions). See load() for a description of arguments omitted below.  Parameters \n \ncpp_sources \u2013 A string, or list of strings, containing C++ source code. \ncuda_sources \u2013 A string, or list of strings, containing CUDA source code. \nfunctions \u2013 A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names). \nwith_cuda \u2013 Determines whether CUDA headers and libraries are added to the build. If set to None (default), this value is automatically determined based on whether cuda_sources is provided. Set it to True to force CUDA headers and libraries to be included. \nwith_pytorch_error_handling \u2013 Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function foo is called via an intermediary _safe_foo function. This redirection might cause issues in obscure cases of cpp. This flag should be set to False when this redirect causes issues.    Example >>> from torch.utils.cpp_extension import load_inline\n>>> source = \\'\\'\\'\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\\'\\'\\'\n>>> module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])\n  Note By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.  \n"}, {"name": "torch.utils.cpp_extension.verify_ninja_availability()", "path": "cpp_extension#torch.utils.cpp_extension.verify_ninja_availability", "type": "torch.utils.cpp_extension", "text": " \ntorch.utils.cpp_extension.verify_ninja_availability() [source]\n \nRaises RuntimeError if ninja build system is not available on the system, does nothing otherwise. \n"}, {"name": "torch.utils.data", "path": "data", "type": "torch.utils.data", "text": "torch.utils.data At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for  \nmap-style and iterable-style datasets, \ncustomizing data loading order, \nautomatic batching, \nsingle- and multi-process data loading, \nautomatic memory pinning.  These options are configured by the constructor arguments of a DataLoader, which has signature: DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n The sections below describe in details the effects and usages of these options. Dataset Types The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:  \nmap-style datasets, \niterable-style datasets.  Map-style datasets A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples. For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk. See Dataset for more details. Iterable-style datasets An iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data. For example, such a dataset, when called iter(dataset), could return a stream of data reading from a database, a remote server, or even logs generated in real time. See IterableDataset for more details.  Note When using an IterableDataset with multi-process data loading. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See IterableDataset documentations for how to achieve this.  Data Loading Order and Sampler For iterable-style datasets, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time). The rest of this section concerns the case with map-style datasets. torch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD. A sequential or shuffled sampler will be automatically constructed based on the shuffle argument to a DataLoader. Alternatively, users may use the sampler argument to specify a custom Sampler object that at each time yields the next index/key to fetch. A custom Sampler that yields a list of batch indices at a time can be passed as the batch_sampler argument. Automatic batching can also be enabled via batch_size and drop_last arguments. See the next section for more details on this.  Note Neither sampler nor batch_sampler is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.  Loading Batched and Non-Batched Data DataLoader supports automatically collating individual fetched data samples into batches via arguments batch_size, drop_last, and batch_sampler. Automatic batching (default) This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first). When batch_size (default 1) is not None, the data loader yields batched samples instead of individual samples. batch_size and drop_last arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify batch_sampler, which yields a list of keys at a time.  Note The batch_size and drop_last arguments essentially are used to construct a batch_sampler from sampler. For map-style datasets, the sampler is either provided by user or constructed based on the shuffle argument. For iterable-style datasets, the sampler is a dummy infinite one. See this section on more details on samplers.   Note When fetching from iterable-style datasets with multi-processing, the drop_last argument drops the last non-full batch of each worker\u2019s dataset replica.  After fetching a list of samples using the indices from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches. In this case, loading from a map-style dataset is roughly equivalent with: for indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n and loading from an iterable-style dataset is roughly equivalent with: dataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n A custom collate_fn can be used to customize collation, e.g., padding sequential data to max length of a batch. See this section on more about collate_fn. Disable automatic batching In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it\u2019s likely better to not use automatic batching (where collate_fn is used to collate the samples), but let the data loader directly return each member of the dataset object. When both batch_size and batch_sampler are None (default value for batch_sampler is already None), automatic batching is disabled. Each sample obtained from the dataset is processed with the function passed as the collate_fn argument. When automatic batching is disabled, the default collate_fn simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched. In this case, loading from a map-style dataset is roughly equivalent with: for index in sampler:\n    yield collate_fn(dataset[index])\n and loading from an iterable-style dataset is roughly equivalent with: for data in iter(dataset):\n    yield collate_fn(data)\n See this section on more about collate_fn. Working with collate_fn\n The use of collate_fn is slightly different when automatic batching is enabled or disabled. When automatic batching is disabled, collate_fn is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default collate_fn simply converts NumPy arrays in PyTorch tensors. When automatic batching is enabled, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes behavior of the default collate_fn in this case. For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple (image, class_index), the default collate_fn collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default collate_fn has the following properties:  It always prepends a new dimension as the batch dimension. It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors. It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for list s, tuple s, namedtuple s, etc.  Users may use customized collate_fn to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types. Single- and Multi-process Data Loading A DataLoader uses single-process data loading by default. Within a Python process, the Global Interpreter Lock (GIL) prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument num_workers to a positive integer. Single-process data loading (default) In this mode, data fetching is done in the same process a DataLoader is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging. Multi-process data loading Setting the argument num_workers as a positive integer will turn on multi-process data loading with the specified number of loader worker processes. In this mode, each time an iterator of a DataLoader is created (e.g., when you call enumerate(dataloader)), num_workers worker processes are created. At this point, the dataset, collate_fn, and worker_init_fn are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including collate_fn) runs in the worker process. torch.utils.data.get_worker_info() returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns None in main process. Users may use this function in dataset code and/or worker_init_fn to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset. For map-style datasets, the main process generates the indices using sampler and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load. For iterable-style datasets, since each worker process gets a replica of the dataset object, naive multi-process loading will often result in duplicated data. Using torch.utils.data.get_worker_info() and/or worker_init_fn, users may configure each replica independently. (See IterableDataset documentations for how to achieve this. ) For similar reasons, in multi-process loading, the drop_last argument drops the last non-full batch of each worker\u2019s iterable-style dataset replica. Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.  Warning It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see CUDA in multiprocessing). Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True), which enables fast data transfer to CUDA-enabled GPUs.  Platform-specific behaviors Since workers rely on Python multiprocessing, worker launch behavior is different on Windows compared to Unix.  On Unix, fork() is the default multiprocessing start method. Using fork(), child workers typically can access the dataset and Python argument functions directly through the cloned address space. On Windows, spawn() is the default multiprocessing start method. Using spawn(), another interpreter is launched which runs your main script, followed by the internal worker function that receives the dataset, collate_fn and other arguments through pickle serialization.  This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:  Wrap most of you main script\u2019s code within if __name__ == '__main__': block, to make sure it doesn\u2019t run again (most likely generating error) when each worker process is launched. You can place your dataset and DataLoader instance creation logic here, as it doesn\u2019t need to be re-executed in workers. Make sure that any custom collate_fn, worker_init_fn or dataset code is declared as top level definitions, outside of the __main__ check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not bytecode.)  Randomness in multi-process data loading By default, each worker will have its PyTorch seed set to base_seed + worker_id, where base_seed is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily). However, seeds for other libraries may be duplicated upon initializing workers (e.g., NumPy), causing each worker to return identical random numbers. (See this section in FAQ.). In worker_init_fn, you may access the PyTorch seed set for each worker with either torch.utils.data.get_worker_info().seed or torch.initial_seed(), and use it to seed other libraries before data loading. Memory Pinning Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally. For data loading, passing pin_memory=True to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs. The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a collate_fn that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a pin_memory() method on your custom type(s). See the example below. Example: class SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n  \nclass torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=2, persistent_workers=False) [source]\n \nData loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. See torch.utils.data documentation page for more details.  Parameters \n \ndataset (Dataset) \u2013 dataset from which to load the data. \nbatch_size (int, optional) \u2013 how many samples per batch to load (default: 1). \nshuffle (bool, optional) \u2013 set to True to have the data reshuffled at every epoch (default: False). \nsampler (Sampler or Iterable, optional) \u2013 defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified. \nbatch_sampler (Sampler or Iterable, optional) \u2013 like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last. \nnum_workers (int, optional) \u2013 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) \ncollate_fn (callable, optional) \u2013 merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. \npin_memory (bool, optional) \u2013 If True, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below. \ndrop_last (bool, optional) \u2013 set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False) \ntimeout (numeric, optional) \u2013 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0) \nworker_init_fn (callable, optional) \u2013 If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None) \nprefetch_factor (int, optional, keyword-only arg) \u2013 Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. (default: 2) \npersistent_workers (bool, optional) \u2013 If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: False)     Warning If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.   Warning len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user dataset code in correctly handling multi-process loading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when drop_last is set. Unfortunately, PyTorch can not detect such cases in general. See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.   Warning See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions.  \n  \nclass torch.utils.data.Dataset [source]\n \nAn abstract class representing a Dataset. All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader.  Note DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.  \n  \nclass torch.utils.data.IterableDataset [source]\n \nAn iterable Dataset. All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream. All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset. When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers > 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in __iter__(): >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]\n Example 2: splitting workload across all workers using worker_init_fn: >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         return iter(range(self.start, self.end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n>>>\n>>> # Directly doing multi-process loading yields duplicate data\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 3, 4, 4, 5, 5, 6, 6]\n\n>>> # Define a `worker_init_fn` that configures each dataset copy differently\n>>> def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n...\n\n>>> # Mult-process loading with the custom `worker_init_fn`\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n[3, 4, 5, 6]\n \n  \nclass torch.utils.data.TensorDataset(*tensors) [source]\n \nDataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.  Parameters \n*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.   \n  \nclass torch.utils.data.ConcatDataset(datasets) [source]\n \nDataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets.  Parameters \ndatasets (sequence) \u2013 List of datasets to be concatenated   \n  \nclass torch.utils.data.ChainDataset(datasets) [source]\n \nDataset for chainning multiple IterableDataset s. This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.  Parameters \ndatasets (iterable of IterableDataset) \u2013 datasets to be chained together   \n  \nclass torch.utils.data.BufferedShuffleDataset(dataset, buffer_size) [source]\n \nDataset shuffled from the original dataset. This class is useful to shuffle an existing instance of an IterableDataset. The buffer with buffer_size is filled with the items from the dataset first. Then, each item will be yielded from the buffer by reservoir sampling via iterator. buffer_size is required to be larger than 0. For buffer_size == 1, the dataset is not shuffled. In order to fully shuffle the whole dataset, buffer_size is required to be greater than or equal to the size of dataset. When it is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. And, the method to set up a random seed is different based on num_workers. For single-process mode (num_workers == 0), the random seed is required to be set before the DataLoader in the main process. >>> ds = BufferedShuffleDataset(dataset)\n>>> random.seed(...)\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n For multi-process mode (num_workers > 0), the random seed is set by a callable function in each worker. >>> ds = BufferedShuffleDataset(dataset)\n>>> def init_fn(worker_id):\n...     random.seed(...)\n>>> print(list(torch.utils.data.DataLoader(ds, ..., num_workers=n, worker_init_fn=init_fn)))\n  Parameters \n \ndataset (IterableDataset) \u2013 The original IterableDataset. \nbuffer_size (int) \u2013 The buffer size for shuffling.    \n  \nclass torch.utils.data.Subset(dataset, indices) [source]\n \nSubset of a dataset at specified indices.  Parameters \n \ndataset (Dataset) \u2013 The whole Dataset \nindices (sequence) \u2013 Indices in the whole set selected for subset    \n  \ntorch.utils.data.get_worker_info() [source]\n \nReturns the information about the current DataLoader iterator worker process. When called in a worker, this returns an object guaranteed to have the following attributes:  \nid: the current worker id. \nnum_workers: the total number of workers. \nseed: the random seed set for the current worker. This value is determined by main process RNG and the worker id. See DataLoader\u2019s documentation for more details. \ndataset: the copy of the dataset object in this process. Note that this will be a different object in a different process than the one in the main process.  When called in the main process, this returns None.  Note When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code (e.g., NumPy).  \n  \ntorch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>) [source]\n \nRandomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.: >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n  Parameters \n \ndataset (Dataset) \u2013 Dataset to be split \nlengths (sequence) \u2013 lengths of splits to be produced \ngenerator (Generator) \u2013 Generator used for the random permutation.    \n  \nclass torch.utils.data.Sampler(data_source) [source]\n \nBase class for all Samplers. Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices of dataset elements, and a __len__() method that returns the length of the returned iterators.  Note The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader.  \n  \nclass torch.utils.data.SequentialSampler(data_source) [source]\n \nSamples elements sequentially, always in the same order.  Parameters \ndata_source (Dataset) \u2013 dataset to sample from   \n  \nclass torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None) [source]\n \nSamples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw.  Parameters \n \ndata_source (Dataset) \u2013 dataset to sample from \nreplacement (bool) \u2013 samples are drawn on-demand with replacement if True, default=``False`` \nnum_samples (int) \u2013 number of samples to draw, default=`len(dataset)`. This argument is supposed to be specified only when replacement is True. \ngenerator (Generator) \u2013 Generator used in sampling.    \n  \nclass torch.utils.data.SubsetRandomSampler(indices, generator=None) [source]\n \nSamples elements randomly from a given list of indices, without replacement.  Parameters \n \nindices (sequence) \u2013 a sequence of indices \ngenerator (Generator) \u2013 Generator used in sampling.    \n  \nclass torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True, generator=None) [source]\n \nSamples elements from [0,..,len(weights)-1] with given probabilities (weights).  Parameters \n \nweights (sequence) \u2013 a sequence of weights, not necessary summing up to one \nnum_samples (int) \u2013 number of samples to draw \nreplacement (bool) \u2013 if True, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row. \ngenerator (Generator) \u2013 Generator used in sampling.    Example >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[4, 4, 1, 4, 5]\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]\n \n  \nclass torch.utils.data.BatchSampler(sampler, batch_size, drop_last) [source]\n \nWraps another sampler to yield a mini-batch of indices.  Parameters \n \nsampler (Sampler or Iterable) \u2013 Base sampler. Can be any iterable object \nbatch_size (int) \u2013 Size of mini-batch. \ndrop_last (bool) \u2013 If True, the sampler will drop the last batch if its size would be less than batch_size\n    Example >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n \n  \nclass torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None, shuffle=True, seed=0, drop_last=False) [source]\n \nSampler that restricts data loading to a subset of the dataset. It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.  Note Dataset is assumed to be of constant size.   Parameters \n \ndataset \u2013 Dataset used for sampling. \nnum_replicas (int, optional) \u2013 Number of processes participating in distributed training. By default, world_size is retrieved from the current distributed group. \nrank (int, optional) \u2013 Rank of the current process within num_replicas. By default, rank is retrieved from the current distributed group. \nshuffle (bool, optional) \u2013 If True (default), sampler will shuffle the indices. \nseed (int, optional) \u2013 random seed used to shuffle the sampler if shuffle=True. This number should be identical across all processes in the distributed group. Default: 0. \ndrop_last (bool, optional) \u2013 if True, then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If False, the sampler will add extra indices to make the data evenly divisible across the replicas. Default: False.     Warning In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.  Example: >>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)\n \n\n"}, {"name": "torch.utils.data.BatchSampler", "path": "data#torch.utils.data.BatchSampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.BatchSampler(sampler, batch_size, drop_last) [source]\n \nWraps another sampler to yield a mini-batch of indices.  Parameters \n \nsampler (Sampler or Iterable) \u2013 Base sampler. Can be any iterable object \nbatch_size (int) \u2013 Size of mini-batch. \ndrop_last (bool) \u2013 If True, the sampler will drop the last batch if its size would be less than batch_size\n    Example >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n \n"}, {"name": "torch.utils.data.BufferedShuffleDataset", "path": "data#torch.utils.data.BufferedShuffleDataset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.BufferedShuffleDataset(dataset, buffer_size) [source]\n \nDataset shuffled from the original dataset. This class is useful to shuffle an existing instance of an IterableDataset. The buffer with buffer_size is filled with the items from the dataset first. Then, each item will be yielded from the buffer by reservoir sampling via iterator. buffer_size is required to be larger than 0. For buffer_size == 1, the dataset is not shuffled. In order to fully shuffle the whole dataset, buffer_size is required to be greater than or equal to the size of dataset. When it is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. And, the method to set up a random seed is different based on num_workers. For single-process mode (num_workers == 0), the random seed is required to be set before the DataLoader in the main process. >>> ds = BufferedShuffleDataset(dataset)\n>>> random.seed(...)\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n For multi-process mode (num_workers > 0), the random seed is set by a callable function in each worker. >>> ds = BufferedShuffleDataset(dataset)\n>>> def init_fn(worker_id):\n...     random.seed(...)\n>>> print(list(torch.utils.data.DataLoader(ds, ..., num_workers=n, worker_init_fn=init_fn)))\n  Parameters \n \ndataset (IterableDataset) \u2013 The original IterableDataset. \nbuffer_size (int) \u2013 The buffer size for shuffling.    \n"}, {"name": "torch.utils.data.ChainDataset", "path": "data#torch.utils.data.ChainDataset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.ChainDataset(datasets) [source]\n \nDataset for chainning multiple IterableDataset s. This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.  Parameters \ndatasets (iterable of IterableDataset) \u2013 datasets to be chained together   \n"}, {"name": "torch.utils.data.ConcatDataset", "path": "data#torch.utils.data.ConcatDataset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.ConcatDataset(datasets) [source]\n \nDataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets.  Parameters \ndatasets (sequence) \u2013 List of datasets to be concatenated   \n"}, {"name": "torch.utils.data.DataLoader", "path": "data#torch.utils.data.DataLoader", "type": "torch.utils.data", "text": " \nclass torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=2, persistent_workers=False) [source]\n \nData loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. See torch.utils.data documentation page for more details.  Parameters \n \ndataset (Dataset) \u2013 dataset from which to load the data. \nbatch_size (int, optional) \u2013 how many samples per batch to load (default: 1). \nshuffle (bool, optional) \u2013 set to True to have the data reshuffled at every epoch (default: False). \nsampler (Sampler or Iterable, optional) \u2013 defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified. \nbatch_sampler (Sampler or Iterable, optional) \u2013 like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last. \nnum_workers (int, optional) \u2013 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) \ncollate_fn (callable, optional) \u2013 merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. \npin_memory (bool, optional) \u2013 If True, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below. \ndrop_last (bool, optional) \u2013 set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False) \ntimeout (numeric, optional) \u2013 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0) \nworker_init_fn (callable, optional) \u2013 If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None) \nprefetch_factor (int, optional, keyword-only arg) \u2013 Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. (default: 2) \npersistent_workers (bool, optional) \u2013 If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: False)     Warning If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.   Warning len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user dataset code in correctly handling multi-process loading to avoid duplicate data. However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when drop_last is set. Unfortunately, PyTorch can not detect such cases in general. See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.   Warning See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions.  \n"}, {"name": "torch.utils.data.Dataset", "path": "data#torch.utils.data.Dataset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.Dataset [source]\n \nAn abstract class representing a Dataset. All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader.  Note DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.  \n"}, {"name": "torch.utils.data.distributed.DistributedSampler", "path": "data#torch.utils.data.distributed.DistributedSampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None, shuffle=True, seed=0, drop_last=False) [source]\n \nSampler that restricts data loading to a subset of the dataset. It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.  Note Dataset is assumed to be of constant size.   Parameters \n \ndataset \u2013 Dataset used for sampling. \nnum_replicas (int, optional) \u2013 Number of processes participating in distributed training. By default, world_size is retrieved from the current distributed group. \nrank (int, optional) \u2013 Rank of the current process within num_replicas. By default, rank is retrieved from the current distributed group. \nshuffle (bool, optional) \u2013 If True (default), sampler will shuffle the indices. \nseed (int, optional) \u2013 random seed used to shuffle the sampler if shuffle=True. This number should be identical across all processes in the distributed group. Default: 0. \ndrop_last (bool, optional) \u2013 if True, then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If False, the sampler will add extra indices to make the data evenly divisible across the replicas. Default: False.     Warning In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.  Example: >>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)\n \n"}, {"name": "torch.utils.data.get_worker_info()", "path": "data#torch.utils.data.get_worker_info", "type": "torch.utils.data", "text": " \ntorch.utils.data.get_worker_info() [source]\n \nReturns the information about the current DataLoader iterator worker process. When called in a worker, this returns an object guaranteed to have the following attributes:  \nid: the current worker id. \nnum_workers: the total number of workers. \nseed: the random seed set for the current worker. This value is determined by main process RNG and the worker id. See DataLoader\u2019s documentation for more details. \ndataset: the copy of the dataset object in this process. Note that this will be a different object in a different process than the one in the main process.  When called in the main process, this returns None.  Note When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code (e.g., NumPy).  \n"}, {"name": "torch.utils.data.IterableDataset", "path": "data#torch.utils.data.IterableDataset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.IterableDataset [source]\n \nAn iterable Dataset. All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream. All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset. When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers > 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in __iter__(): >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]\n Example 2: splitting workload across all workers using worker_init_fn: >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         return iter(range(self.start, self.end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n>>>\n>>> # Directly doing multi-process loading yields duplicate data\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 3, 4, 4, 5, 5, 6, 6]\n\n>>> # Define a `worker_init_fn` that configures each dataset copy differently\n>>> def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n...\n\n>>> # Mult-process loading with the custom `worker_init_fn`\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn)))\n[3, 4, 5, 6]\n \n"}, {"name": "torch.utils.data.RandomSampler", "path": "data#torch.utils.data.RandomSampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None) [source]\n \nSamples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw.  Parameters \n \ndata_source (Dataset) \u2013 dataset to sample from \nreplacement (bool) \u2013 samples are drawn on-demand with replacement if True, default=``False`` \nnum_samples (int) \u2013 number of samples to draw, default=`len(dataset)`. This argument is supposed to be specified only when replacement is True. \ngenerator (Generator) \u2013 Generator used in sampling.    \n"}, {"name": "torch.utils.data.random_split()", "path": "data#torch.utils.data.random_split", "type": "torch.utils.data", "text": " \ntorch.utils.data.random_split(dataset, lengths, generator=<torch._C.Generator object>) [source]\n \nRandomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.: >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n  Parameters \n \ndataset (Dataset) \u2013 Dataset to be split \nlengths (sequence) \u2013 lengths of splits to be produced \ngenerator (Generator) \u2013 Generator used for the random permutation.    \n"}, {"name": "torch.utils.data.Sampler", "path": "data#torch.utils.data.Sampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.Sampler(data_source) [source]\n \nBase class for all Samplers. Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices of dataset elements, and a __len__() method that returns the length of the returned iterators.  Note The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader.  \n"}, {"name": "torch.utils.data.SequentialSampler", "path": "data#torch.utils.data.SequentialSampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.SequentialSampler(data_source) [source]\n \nSamples elements sequentially, always in the same order.  Parameters \ndata_source (Dataset) \u2013 dataset to sample from   \n"}, {"name": "torch.utils.data.Subset", "path": "data#torch.utils.data.Subset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.Subset(dataset, indices) [source]\n \nSubset of a dataset at specified indices.  Parameters \n \ndataset (Dataset) \u2013 The whole Dataset \nindices (sequence) \u2013 Indices in the whole set selected for subset    \n"}, {"name": "torch.utils.data.SubsetRandomSampler", "path": "data#torch.utils.data.SubsetRandomSampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.SubsetRandomSampler(indices, generator=None) [source]\n \nSamples elements randomly from a given list of indices, without replacement.  Parameters \n \nindices (sequence) \u2013 a sequence of indices \ngenerator (Generator) \u2013 Generator used in sampling.    \n"}, {"name": "torch.utils.data.TensorDataset", "path": "data#torch.utils.data.TensorDataset", "type": "torch.utils.data", "text": " \nclass torch.utils.data.TensorDataset(*tensors) [source]\n \nDataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.  Parameters \n*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.   \n"}, {"name": "torch.utils.data.WeightedRandomSampler", "path": "data#torch.utils.data.WeightedRandomSampler", "type": "torch.utils.data", "text": " \nclass torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True, generator=None) [source]\n \nSamples elements from [0,..,len(weights)-1] with given probabilities (weights).  Parameters \n \nweights (sequence) \u2013 a sequence of weights, not necessary summing up to one \nnum_samples (int) \u2013 number of samples to draw \nreplacement (bool) \u2013 if True, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row. \ngenerator (Generator) \u2013 Generator used in sampling.    Example >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[4, 4, 1, 4, 5]\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]\n \n"}, {"name": "torch.utils.dlpack", "path": "dlpack", "type": "torch.utils.dlpack", "text": "torch.utils.dlpack  \ntorch.utils.dlpack.from_dlpack(dlpack) \u2192 Tensor  \nDecodes a DLPack to a tensor.  Parameters \ndlpack \u2013 a PyCapsule object with the dltensor   The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once. \n  \ntorch.utils.dlpack.to_dlpack(tensor) \u2192 PyCapsule  \nReturns a DLPack representing the tensor.  Parameters \ntensor \u2013 a tensor to be exported   The dlpack shares the tensors memory. Note that each dlpack can only be consumed once. \n\n"}, {"name": "torch.utils.dlpack.from_dlpack()", "path": "dlpack#torch.utils.dlpack.from_dlpack", "type": "torch.utils.dlpack", "text": " \ntorch.utils.dlpack.from_dlpack(dlpack) \u2192 Tensor  \nDecodes a DLPack to a tensor.  Parameters \ndlpack \u2013 a PyCapsule object with the dltensor   The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once. \n"}, {"name": "torch.utils.dlpack.to_dlpack()", "path": "dlpack#torch.utils.dlpack.to_dlpack", "type": "torch.utils.dlpack", "text": " \ntorch.utils.dlpack.to_dlpack(tensor) \u2192 PyCapsule  \nReturns a DLPack representing the tensor.  Parameters \ntensor \u2013 a tensor to be exported   The dlpack shares the tensors memory. Note that each dlpack can only be consumed once. \n"}, {"name": "torch.utils.mobile_optimizer", "path": "mobile_optimizer", "type": "torch.utils.mobile_optimizer", "text": "torch.utils.mobile_optimizer  Warning This API is in beta and may change in the near future.  Torch mobile supports torch.mobile_optimizer.optimize_for_mobile utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blocklisting optimization set and a preserved method list  \nBy default, if optimization blocklist is None or empty, optimize_for_mobile will run the following optimizations: \n\n \nConv2D + BatchNorm fusion (blocklisting option MobileOptimizerType::CONV_BN_FUSION): This optimization pass folds Conv2d-BatchNorm2d into Conv2d in forward method of this module and all its submodules. The weight and bias of the Conv2d are correspondingly updated. \nInsert and Fold prepacked ops (blocklisting option MobileOptimizerType::INSERT_FOLD_PREPACK_OPS): This optimization pass rewrites the graph to replace 2D convolutions and linear ops with their prepacked counterparts. Prepacked ops are stateful ops in that, they require some state to be created, such as weight prepacking and use this state, i.e. prepacked weights, during op execution. XNNPACK is one such backend that provides prepacked ops, with kernels optimized for mobile platforms (such as ARM CPUs). Prepacking of weight enables efficient memory access and thus faster kernel execution. At the moment optimize_for_mobile pass rewrites the graph to replace Conv2D/Linear with 1) op that pre-packs weight for XNNPACK conv2d/linear ops and 2) op that takes pre-packed weight and activation as input and generates output activations. Since 1 needs to be done only once, we fold the weight pre-packing such that it is done only once at model load time. This pass of the optimize_for_mobile does 1 and 2 and then folds, i.e. removes, weight pre-packing ops. \nReLU/Hardtanh fusion: XNNPACK ops support fusion of clamping. That is clamping of output activation is done as part of the kernel, including for 2D convolution and linear op kernels. Thus clamping effectively comes for free. Thus any op that can be expressed as clamping op, such as ReLU or hardtanh, can be fused with previous Conv2D or linear op in XNNPACK. This pass rewrites graph by finding ReLU/hardtanh ops that follow XNNPACK Conv2D/linear ops, written by the previous pass, and fuses them together. \nDropout removal (blocklisting option MobileOptimizerType::REMOVE_DROPOUT): This optimization pass removes dropout and dropout_ nodes from this module when training is false. \nConv packed params hoisting (blocklisting option MobileOptimizerType::HOIST_CONV_PACKED_PARAMS): This optimization pass moves convolution packed params to the root module, so that the convolution structs can be deleted. This decreases model size without impacting numerics.    optimize_for_mobile will also invoke freeze_module pass which only preserves forward method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method.  \ntorch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist=None, preserved_methods=None, backend='CPU') [source]\n \n Parameters \n \nscript_module \u2013 An instance of torch script module with type of ScriptModule. \noptimization_blocklist \u2013 A set with type of MobileOptimizerType. When set is not passed, optimization method will run all the optimizer pass; otherwise, optimizer method will run the optimization pass that is not included inside optimization_blocklist. \nperserved_methods \u2013 A list of methods that needed to be preserved when freeze_module pass is invoked \nbackend \u2013 Device type to use for running the result model (\u2018CPU\u2019(default), \u2018Vulkan\u2019 or \u2018Metal\u2019).   Returns \nA new optimized torch script module   \n\n"}, {"name": "torch.utils.mobile_optimizer.optimize_for_mobile()", "path": "mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile", "type": "torch.utils.mobile_optimizer", "text": " \ntorch.utils.mobile_optimizer.optimize_for_mobile(script_module, optimization_blocklist=None, preserved_methods=None, backend='CPU') [source]\n \n Parameters \n \nscript_module \u2013 An instance of torch script module with type of ScriptModule. \noptimization_blocklist \u2013 A set with type of MobileOptimizerType. When set is not passed, optimization method will run all the optimizer pass; otherwise, optimizer method will run the optimization pass that is not included inside optimization_blocklist. \nperserved_methods \u2013 A list of methods that needed to be preserved when freeze_module pass is invoked \nbackend \u2013 Device type to use for running the result model (\u2018CPU\u2019(default), \u2018Vulkan\u2019 or \u2018Metal\u2019).   Returns \nA new optimized torch script module   \n"}, {"name": "torch.utils.model_zoo", "path": "model_zoo", "type": "torch.utils.model_zoo", "text": "torch.utils.model_zoo Moved to torch.hub.  \ntorch.utils.model_zoo.load_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)  \nLoads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().  Parameters \n \nurl (string) \u2013 URL of the object to download \nmodel_dir (string, optional) \u2013 directory in which to save the object \nmap_location (optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr. Default: True \ncheck_hash (bool, optional) \u2013 If True, the filename part of the URL should follow the naming convention filename-<sha256>.ext where <sha256> is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False \nfile_name (string, optional) \u2013 name for the downloaded file. Filename from url will be used if not set.    Example >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n \n\n"}, {"name": "torch.utils.model_zoo.load_url()", "path": "model_zoo#torch.utils.model_zoo.load_url", "type": "torch.utils.model_zoo", "text": " \ntorch.utils.model_zoo.load_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)  \nLoads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().  Parameters \n \nurl (string) \u2013 URL of the object to download \nmodel_dir (string, optional) \u2013 directory in which to save the object \nmap_location (optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) \nprogress (bool, optional) \u2013 whether or not to display a progress bar to stderr. Default: True \ncheck_hash (bool, optional) \u2013 If True, the filename part of the URL should follow the naming convention filename-<sha256>.ext where <sha256> is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False \nfile_name (string, optional) \u2013 name for the downloaded file. Filename from url will be used if not set.    Example >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n \n"}, {"name": "torch.utils.tensorboard", "path": "tensorboard", "type": "torch.utils.tensorboard", "text": "torch.utils.tensorboard Before going further, more details on TensorBoard can be found at https://www.tensorflow.org/tensorboard/ Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs. The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example: import torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()\n This can then be visualized with TensorBoard, which should be installable and runnable with: pip install tensorboard\ntensorboard --logdir=runs\n Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped together, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately in the TensorBoard interface. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n Expected result:   \nclass torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='') [source]\n \nWrites entries directly to event files in the log_dir to be consumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.  \n__init__(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='') [source]\n \nCreates a SummaryWriter that will write out events and summaries to the event file.  Parameters \n \nlog_dir (string) \u2013 Save directory location. Default is runs/CURRENT_DATETIME_HOSTNAME, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc. for each new experiment to compare across them. \ncomment (string) \u2013 Comment log_dir suffix appended to the default log_dir. If log_dir is assigned, this argument has no effect. \npurge_step (int) \u2013 When logging crashes at step T+XT+X  and restarts at step TT , any events whose global_step larger or equal to TT  will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same log_dir. \nmax_queue (int) \u2013 Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk. Default is ten items. \nflush_secs (int) \u2013 How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes. \nfilename_suffix (string) \u2013 Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.    Examples: from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n \n  \nadd_scalar(tag, scalar_value, global_step=None, walltime=None) [source]\n \nAdd scalar data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nscalar_value (float or string/blobname) \u2013 Value to save \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) with seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n Expected result:  \n  \nadd_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None) [source]\n \nAdds many scalar data to summary.  Parameters \n \nmain_tag (string) \u2013 The parent name for the tags \ntag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n Expected result:  \n  \nadd_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) [source]\n \nAdd histogram to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nvalues (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram \nglobal_step (int) \u2013 Global step value to record \nbins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n Expected result:  \n  \nadd_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') [source]\n \nAdd image data to summary. Note that this requires the pillow package.  Parameters \n \ntag (string) \u2013 Data identifier \nimg_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nimg_tensor: Default is (3,H,W)(3, H, W) . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W) , (H,W)(H, W) , (H,W,3)(H, W, 3)  is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.   Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n Expected result:  \n  \nadd_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW') [source]\n \nAdd batched image data to summary. Note that this requires the pillow package.  Parameters \n \ntag (string) \u2013 Data identifier \nimg_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event \ndataformats (string) \u2013 Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.     Shape:\n\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W) . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.   Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n Expected result:  \n  \nadd_figure(tag, figure, global_step=None, close=True, walltime=None) [source]\n \nRender matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package.  Parameters \n \ntag (string) \u2013 Data identifier \nfigure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures \nglobal_step (int) \u2013 Global step value to record \nclose (bool) \u2013 Flag to automatically close the figure \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    \n  \nadd_video(tag, vid_tensor, global_step=None, fps=4, walltime=None) [source]\n \nAdd video data to summary. Note that this requires the moviepy package.  Parameters \n \ntag (string) \u2013 Data identifier \nvid_tensor (torch.Tensor) \u2013 Video data \nglobal_step (int) \u2013 Global step value to record \nfps (float or int) \u2013 Frames per second \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.   \n  \nadd_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None) [source]\n \nAdd audio data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nsnd_tensor (torch.Tensor) \u2013 Sound data \nglobal_step (int) \u2013 Global step value to record \nsample_rate (int) \u2013 sample rate in Hz \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nsnd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].   \n  \nadd_text(tag, text_string, global_step=None, walltime=None) [source]\n \nAdd text data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \ntext_string (string) \u2013 String to save \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n \n  \nadd_graph(model, input_to_model=None, verbose=False) [source]\n \nAdd graph data to summary.  Parameters \n \nmodel (torch.nn.Module) \u2013 Model to draw. \ninput_to_model (torch.Tensor or list of torch.Tensor) \u2013 A variable or a tuple of variables to be fed. \nverbose (bool) \u2013 Whether to print graph structure in console.    \n  \nadd_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) [source]\n \nAdd embedding projector data to summary.  Parameters \n \nmat (torch.Tensor or numpy.array) \u2013 A matrix which each row is the feature vector of the data point \nmetadata (list) \u2013 A list of labels, each element will be convert to string \nlabel_img (torch.Tensor) \u2013 Images correspond to each data point \nglobal_step (int) \u2013 Global step value to record \ntag (string) \u2013 Name for the embedding     Shape:\n\nmat: (N,D)(N, D) , where N is number of data and D is feature dimension label_img: (N,C,H,W)(N, C, H, W)    Examples: import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n \n  \nadd_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None) [source]\n \nAdds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.  Parameters \n \ntag (string) \u2013 Data identifier \nlabels (torch.Tensor, numpy.array, or string/blobname) \u2013 Ground truth data. Binary label for each element. \npredictions (torch.Tensor, numpy.array, or string/blobname) \u2013 The probability that an element be classified as true. Value should be in [0, 1] \nglobal_step (int) \u2013 Global step value to record \nnum_thresholds (int) \u2013 Number of thresholds used to draw the curve. \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n \n  \nadd_custom_scalars(layout) [source]\n \nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.  Parameters \nlayout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.   Examples: layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n \n  \nadd_mesh(tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None) [source]\n \nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.  Parameters \n \ntag (string) \u2013 Data identifier \nvertices (torch.Tensor) \u2013 List of the 3D coordinates of vertices. \ncolors (torch.Tensor) \u2013 Colors for each vertex \nfaces (torch.Tensor) \u2013 Indices of vertices within each triangle. (Optional) \nconfig_dict \u2013 Dictionary with ThreeJS classes names and configuration. \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nvertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels) colors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. faces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for type uint8.   Examples: from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n \n  \nadd_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None) [source]\n \nAdd a set of hyperparameters to be compared in TensorBoard.  Parameters \n \nhparam_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the hyper parameter and it\u2019s corresponding value. The type of the value can be one of bool, string, float, int, or None. \nmetric_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the metric and it\u2019s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by add_scalar will be displayed in hparam plugin. In most cases, this is unwanted. \nhparam_domain_discrete \u2013 (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold \nrun_name (str) \u2013 Name of the run, to be included as part of the logdir. If unspecified, will use current timestamp.    Examples: from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n Expected result:  \n  \nflush() [source]\n \nFlushes the event file to disk. Call this method to make sure that all pending events have been written to disk. \n  \nclose() [source]\n\n \n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter", "type": "torch.utils.tensorboard", "text": " \nclass torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='') [source]\n \nWrites entries directly to event files in the log_dir to be consumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.  \n__init__(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='') [source]\n \nCreates a SummaryWriter that will write out events and summaries to the event file.  Parameters \n \nlog_dir (string) \u2013 Save directory location. Default is runs/CURRENT_DATETIME_HOSTNAME, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc. for each new experiment to compare across them. \ncomment (string) \u2013 Comment log_dir suffix appended to the default log_dir. If log_dir is assigned, this argument has no effect. \npurge_step (int) \u2013 When logging crashes at step T+XT+X  and restarts at step TT , any events whose global_step larger or equal to TT  will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same log_dir. \nmax_queue (int) \u2013 Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk. Default is ten items. \nflush_secs (int) \u2013 How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes. \nfilename_suffix (string) \u2013 Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.    Examples: from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n \n  \nadd_scalar(tag, scalar_value, global_step=None, walltime=None) [source]\n \nAdd scalar data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nscalar_value (float or string/blobname) \u2013 Value to save \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) with seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n Expected result:  \n  \nadd_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None) [source]\n \nAdds many scalar data to summary.  Parameters \n \nmain_tag (string) \u2013 The parent name for the tags \ntag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n Expected result:  \n  \nadd_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) [source]\n \nAdd histogram to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nvalues (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram \nglobal_step (int) \u2013 Global step value to record \nbins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n Expected result:  \n  \nadd_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') [source]\n \nAdd image data to summary. Note that this requires the pillow package.  Parameters \n \ntag (string) \u2013 Data identifier \nimg_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nimg_tensor: Default is (3,H,W)(3, H, W) . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W) , (H,W)(H, W) , (H,W,3)(H, W, 3)  is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.   Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n Expected result:  \n  \nadd_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW') [source]\n \nAdd batched image data to summary. Note that this requires the pillow package.  Parameters \n \ntag (string) \u2013 Data identifier \nimg_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event \ndataformats (string) \u2013 Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.     Shape:\n\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W) . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.   Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n Expected result:  \n  \nadd_figure(tag, figure, global_step=None, close=True, walltime=None) [source]\n \nRender matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package.  Parameters \n \ntag (string) \u2013 Data identifier \nfigure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures \nglobal_step (int) \u2013 Global step value to record \nclose (bool) \u2013 Flag to automatically close the figure \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    \n  \nadd_video(tag, vid_tensor, global_step=None, fps=4, walltime=None) [source]\n \nAdd video data to summary. Note that this requires the moviepy package.  Parameters \n \ntag (string) \u2013 Data identifier \nvid_tensor (torch.Tensor) \u2013 Video data \nglobal_step (int) \u2013 Global step value to record \nfps (float or int) \u2013 Frames per second \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.   \n  \nadd_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None) [source]\n \nAdd audio data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nsnd_tensor (torch.Tensor) \u2013 Sound data \nglobal_step (int) \u2013 Global step value to record \nsample_rate (int) \u2013 sample rate in Hz \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nsnd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].   \n  \nadd_text(tag, text_string, global_step=None, walltime=None) [source]\n \nAdd text data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \ntext_string (string) \u2013 String to save \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n \n  \nadd_graph(model, input_to_model=None, verbose=False) [source]\n \nAdd graph data to summary.  Parameters \n \nmodel (torch.nn.Module) \u2013 Model to draw. \ninput_to_model (torch.Tensor or list of torch.Tensor) \u2013 A variable or a tuple of variables to be fed. \nverbose (bool) \u2013 Whether to print graph structure in console.    \n  \nadd_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) [source]\n \nAdd embedding projector data to summary.  Parameters \n \nmat (torch.Tensor or numpy.array) \u2013 A matrix which each row is the feature vector of the data point \nmetadata (list) \u2013 A list of labels, each element will be convert to string \nlabel_img (torch.Tensor) \u2013 Images correspond to each data point \nglobal_step (int) \u2013 Global step value to record \ntag (string) \u2013 Name for the embedding     Shape:\n\nmat: (N,D)(N, D) , where N is number of data and D is feature dimension label_img: (N,C,H,W)(N, C, H, W)    Examples: import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n \n  \nadd_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None) [source]\n \nAdds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.  Parameters \n \ntag (string) \u2013 Data identifier \nlabels (torch.Tensor, numpy.array, or string/blobname) \u2013 Ground truth data. Binary label for each element. \npredictions (torch.Tensor, numpy.array, or string/blobname) \u2013 The probability that an element be classified as true. Value should be in [0, 1] \nglobal_step (int) \u2013 Global step value to record \nnum_thresholds (int) \u2013 Number of thresholds used to draw the curve. \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n \n  \nadd_custom_scalars(layout) [source]\n \nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.  Parameters \nlayout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.   Examples: layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n \n  \nadd_mesh(tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None) [source]\n \nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.  Parameters \n \ntag (string) \u2013 Data identifier \nvertices (torch.Tensor) \u2013 List of the 3D coordinates of vertices. \ncolors (torch.Tensor) \u2013 Colors for each vertex \nfaces (torch.Tensor) \u2013 Indices of vertices within each triangle. (Optional) \nconfig_dict \u2013 Dictionary with ThreeJS classes names and configuration. \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nvertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels) colors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. faces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for type uint8.   Examples: from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n \n  \nadd_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None) [source]\n \nAdd a set of hyperparameters to be compared in TensorBoard.  Parameters \n \nhparam_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the hyper parameter and it\u2019s corresponding value. The type of the value can be one of bool, string, float, int, or None. \nmetric_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the metric and it\u2019s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by add_scalar will be displayed in hparam plugin. In most cases, this is unwanted. \nhparam_domain_discrete \u2013 (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold \nrun_name (str) \u2013 Name of the run, to be included as part of the logdir. If unspecified, will use current timestamp.    Examples: from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n Expected result:  \n  \nflush() [source]\n \nFlushes the event file to disk. Call this method to make sure that all pending events have been written to disk. \n  \nclose() [source]\n\n \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_audio()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_audio", "type": "torch.utils.tensorboard", "text": " \nadd_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None) [source]\n \nAdd audio data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nsnd_tensor (torch.Tensor) \u2013 Sound data \nglobal_step (int) \u2013 Global step value to record \nsample_rate (int) \u2013 sample rate in Hz \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nsnd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].   \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars", "type": "torch.utils.tensorboard", "text": " \nadd_custom_scalars(layout) [source]\n \nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.  Parameters \nlayout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.   Examples: layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_embedding", "type": "torch.utils.tensorboard", "text": " \nadd_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None) [source]\n \nAdd embedding projector data to summary.  Parameters \n \nmat (torch.Tensor or numpy.array) \u2013 A matrix which each row is the feature vector of the data point \nmetadata (list) \u2013 A list of labels, each element will be convert to string \nlabel_img (torch.Tensor) \u2013 Images correspond to each data point \nglobal_step (int) \u2013 Global step value to record \ntag (string) \u2013 Name for the embedding     Shape:\n\nmat: (N,D)(N, D) , where N is number of data and D is feature dimension label_img: (N,C,H,W)(N, C, H, W)    Examples: import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_figure()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_figure", "type": "torch.utils.tensorboard", "text": " \nadd_figure(tag, figure, global_step=None, close=True, walltime=None) [source]\n \nRender matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package.  Parameters \n \ntag (string) \u2013 Data identifier \nfigure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures \nglobal_step (int) \u2013 Global step value to record \nclose (bool) \u2013 Flag to automatically close the figure \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_graph()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_graph", "type": "torch.utils.tensorboard", "text": " \nadd_graph(model, input_to_model=None, verbose=False) [source]\n \nAdd graph data to summary.  Parameters \n \nmodel (torch.nn.Module) \u2013 Model to draw. \ninput_to_model (torch.Tensor or list of torch.Tensor) \u2013 A variable or a tuple of variables to be fed. \nverbose (bool) \u2013 Whether to print graph structure in console.    \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_histogram", "type": "torch.utils.tensorboard", "text": " \nadd_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None) [source]\n \nAdd histogram to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nvalues (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram \nglobal_step (int) \u2013 Global step value to record \nbins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n Expected result:  \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_hparams", "type": "torch.utils.tensorboard", "text": " \nadd_hparams(hparam_dict, metric_dict, hparam_domain_discrete=None, run_name=None) [source]\n \nAdd a set of hyperparameters to be compared in TensorBoard.  Parameters \n \nhparam_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the hyper parameter and it\u2019s corresponding value. The type of the value can be one of bool, string, float, int, or None. \nmetric_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the metric and it\u2019s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by add_scalar will be displayed in hparam plugin. In most cases, this is unwanted. \nhparam_domain_discrete \u2013 (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold \nrun_name (str) \u2013 Name of the run, to be included as part of the logdir. If unspecified, will use current timestamp.    Examples: from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n Expected result:  \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_image()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_image", "type": "torch.utils.tensorboard", "text": " \nadd_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW') [source]\n \nAdd image data to summary. Note that this requires the pillow package.  Parameters \n \ntag (string) \u2013 Data identifier \nimg_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nimg_tensor: Default is (3,H,W)(3, H, W) . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W) , (H,W)(H, W) , (H,W,3)(H, W, 3)  is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.   Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n Expected result:  \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_images()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_images", "type": "torch.utils.tensorboard", "text": " \nadd_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW') [source]\n \nAdd batched image data to summary. Note that this requires the pillow package.  Parameters \n \ntag (string) \u2013 Data identifier \nimg_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event \ndataformats (string) \u2013 Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.     Shape:\n\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W) . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.   Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n Expected result:  \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_mesh", "type": "torch.utils.tensorboard", "text": " \nadd_mesh(tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None) [source]\n \nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.  Parameters \n \ntag (string) \u2013 Data identifier \nvertices (torch.Tensor) \u2013 List of the 3D coordinates of vertices. \ncolors (torch.Tensor) \u2013 Colors for each vertex \nfaces (torch.Tensor) \u2013 Indices of vertices within each triangle. (Optional) \nconfig_dict \u2013 Dictionary with ThreeJS classes names and configuration. \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nvertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels) colors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. faces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for type uint8.   Examples: from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve", "type": "torch.utils.tensorboard", "text": " \nadd_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None) [source]\n \nAdds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.  Parameters \n \ntag (string) \u2013 Data identifier \nlabels (torch.Tensor, numpy.array, or string/blobname) \u2013 Ground truth data. Binary label for each element. \npredictions (torch.Tensor, numpy.array, or string/blobname) \u2013 The probability that an element be classified as true. Value should be in [0, 1] \nglobal_step (int) \u2013 Global step value to record \nnum_thresholds (int) \u2013 Number of thresholds used to draw the curve. \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalar", "type": "torch.utils.tensorboard", "text": " \nadd_scalar(tag, scalar_value, global_step=None, walltime=None) [source]\n \nAdd scalar data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \nscalar_value (float or string/blobname) \u2013 Value to save \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) with seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n Expected result:  \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalars", "type": "torch.utils.tensorboard", "text": " \nadd_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None) [source]\n \nAdds many scalar data to summary.  Parameters \n \nmain_tag (string) \u2013 The parent name for the tags \ntag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n Expected result:  \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_text()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_text", "type": "torch.utils.tensorboard", "text": " \nadd_text(tag, text_string, global_step=None, walltime=None) [source]\n \nAdd text data to summary.  Parameters \n \ntag (string) \u2013 Data identifier \ntext_string (string) \u2013 String to save \nglobal_step (int) \u2013 Global step value to record \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_video()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_video", "type": "torch.utils.tensorboard", "text": " \nadd_video(tag, vid_tensor, global_step=None, fps=4, walltime=None) [source]\n \nAdd video data to summary. Note that this requires the moviepy package.  Parameters \n \ntag (string) \u2013 Data identifier \nvid_tensor (torch.Tensor) \u2013 Video data \nglobal_step (int) \u2013 Global step value to record \nfps (float or int) \u2013 Frames per second \nwalltime (float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:\n\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.   \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.close()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.close", "type": "torch.utils.tensorboard", "text": " \nclose() [source]\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.flush()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.flush", "type": "torch.utils.tensorboard", "text": " \nflush() [source]\n \nFlushes the event file to disk. Call this method to make sure that all pending events have been written to disk. \n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.__init__()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.__init__", "type": "torch.utils.tensorboard", "text": " \n__init__(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='') [source]\n \nCreates a SummaryWriter that will write out events and summaries to the event file.  Parameters \n \nlog_dir (string) \u2013 Save directory location. Default is runs/CURRENT_DATETIME_HOSTNAME, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc. for each new experiment to compare across them. \ncomment (string) \u2013 Comment log_dir suffix appended to the default log_dir. If log_dir is assigned, this argument has no effect. \npurge_step (int) \u2013 When logging crashes at step T+XT+X  and restarts at step TT , any events whose global_step larger or equal to TT  will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same log_dir. \nmax_queue (int) \u2013 Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk. Default is ten items. \nflush_secs (int) \u2013 How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes. \nfilename_suffix (string) \u2013 Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.    Examples: from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n \n"}, {"name": "torch.vander()", "path": "generated/torch.vander#torch.vander", "type": "torch", "text": " \ntorch.vander(x, N=None, increasing=False) \u2192 Tensor  \nGenerates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0 . If increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)} . Such a matrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde.  Parameters \n \nx (Tensor) \u2013 1-D input tensor. \nN (int, optional) \u2013 Number of columns in the output. If N is not specified, a square array is returned (N=len(x))(N = len(x)) . \nincreasing (bool, optional) \u2013 Order of the powers of the columns. If True, the powers increase from left to right, if False (the default) they are reversed.   Returns \nVandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)} , the second x(N\u22122)x^{(N-2)}  and so forth. If increasing is True, the columns are x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)} .  Return type \nTensor   Example: >>> x = torch.tensor([1, 2, 3, 5])\n>>> torch.vander(x)\ntensor([[  1,   1,   1,   1],\n        [  8,   4,   2,   1],\n        [ 27,   9,   3,   1],\n        [125,  25,   5,   1]])\n>>> torch.vander(x, N=3)\ntensor([[ 1,  1,  1],\n        [ 4,  2,  1],\n        [ 9,  3,  1],\n        [25,  5,  1]])\n>>> torch.vander(x, N=3, increasing=True)\ntensor([[ 1,  1,  1],\n        [ 1,  2,  4],\n        [ 1,  3,  9],\n        [ 1,  5, 25]])\n \n"}, {"name": "torch.var()", "path": "generated/torch.var#torch.var", "type": "torch", "text": " \ntorch.var(input, unbiased=True) \u2192 Tensor  \nReturns the variance of all elements in the input tensor. If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.3425, -1.2636, -0.4864]])\n>>> torch.var(a)\ntensor(0.2455)\n  \ntorch.var(input, dim, unbiased=True, keepdim=False, *, out=None) \u2192 Tensor \n Returns the variance of each row of the input tensor in the given dimension dim. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.3567,  1.7385, -1.3042,  0.7423],\n        [ 1.3436, -0.1015, -0.9834, -0.8438],\n        [ 0.6056,  0.1089, -0.3112, -1.4085],\n        [-0.7700,  0.6074, -0.1469,  0.7777]])\n>>> torch.var(a, 1)\ntensor([ 1.7444,  1.1363,  0.7356,  0.5112])\n \n"}, {"name": "torch.var_mean()", "path": "generated/torch.var_mean#torch.var_mean", "type": "torch", "text": " \ntorch.var_mean(input, unbiased=True) -> (Tensor, Tensor)  \nReturns the variance and mean of all elements in the input tensor. If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: >>> a = torch.randn(1, 3)\n>>> a\ntensor([[0.0146, 0.4258, 0.2211]])\n>>> torch.var_mean(a)\n(tensor(0.0423), tensor(0.2205))\n  \ntorch.var_mean(input, dim, keepdim=False, unbiased=True) -> (Tensor, Tensor) \n Returns the variance and mean of each row of the input tensor in the given dimension dim. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. \nunbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.5650,  2.0415, -0.1024, -0.5790],\n        [ 0.2325, -2.6145, -1.6428, -0.3537],\n        [-0.2159, -1.1069,  1.2882, -1.3265],\n        [-0.6706, -1.5893,  0.6827,  1.6727]])\n>>> torch.var_mean(a, 1)\n(tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))\n \n"}, {"name": "torch.vdot()", "path": "generated/torch.vdot#torch.vdot", "type": "torch", "text": " \ntorch.vdot(input, other, *, out=None) \u2192 Tensor  \nComputes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex, the complex conjugate of the first argument is used for the calculation of the dot product.  Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.   Parameters \n \ninput (Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. \nother (Tensor) \u2013 second tensor in the dot product, must be 1D.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])\n \n"}, {"name": "torch.view_as_complex()", "path": "generated/torch.view_as_complex#torch.view_as_complex", "type": "torch", "text": " \ntorch.view_as_complex(input) \u2192 Tensor  \nReturns a view of input as a complex tensor. For an input complex tensor of size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , this function returns a new complex tensor of size m1,m2,\u2026,mim1, m2, \\dots, mi  where the last dimension of the input tensor is expected to represent the real and imaginary components of complex numbers.  Warning view_as_complex() is only supported for tensors with torch.dtype torch.float64 and torch.float32. The input is expected to have the last dimension of size 2. In addition, the tensor must have a stride of 1 for its last dimension. The strides of all other dimensions must be even numbers.   Parameters \ninput (Tensor) \u2013 the input tensor.    Example::\n\n>>> x=torch.randn(4, 2)\n>>> x\ntensor([[ 1.6116, -0.5772],\n        [-1.4606, -0.9120],\n        [ 0.0786, -1.7497],\n        [-0.6561, -1.6623]])\n>>> torch.view_as_complex(x)\ntensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])\n   \n"}, {"name": "torch.view_as_real()", "path": "generated/torch.view_as_real#torch.view_as_real", "type": "torch", "text": " \ntorch.view_as_real(input) \u2192 Tensor  \nReturns a view of input as a real tensor. For an input complex tensor of size m1,m2,\u2026,mim1, m2, \\dots, mi , this function returns a new real tensor of size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , where the last dimension of size 2 represents the real and imaginary components of complex numbers.  Warning view_as_real() is only supported for tensors with complex dtypes.   Parameters \ninput (Tensor) \u2013 the input tensor.    Example::\n\n>>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])\n>>> torch.view_as_real(x)\ntensor([[ 0.4737, -0.3839],\n        [-0.2098, -0.6699],\n        [ 0.3470, -0.9451],\n        [-0.5174, -1.3136]])\n   \n"}, {"name": "torch.vstack()", "path": "generated/torch.vstack#torch.vstack", "type": "torch", "text": " \ntorch.vstack(tensors, *, out=None) \u2192 Tensor  \nStack tensors in sequence vertically (row wise). This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by torch.atleast_2d().  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.vstack((a,b))\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.vstack((a,b))\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n \n"}, {"name": "torch.where()", "path": "generated/torch.where#torch.where", "type": "torch", "text": " \ntorch.where(condition, x, y) \u2192 Tensor  \nReturn a tensor of elements selected from either x or y, depending on condition. The operation is defined as:  outi={xiif conditioniyiotherwise\\text{out}_i = \\begin{cases} \\text{x}_i & \\text{if } \\text{condition}_i \\\\ \\text{y}_i & \\text{otherwise} \\\\ \\end{cases}  \n Note The tensors condition, x, y must be broadcastable.   Note Currently valid scalar and tensor combination are 1. Scalar of floating dtype and torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of complex dtype and torch.complex128   Parameters \n \ncondition (BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y \nx (Tensor or Scalar) \u2013 value (if :attr:x is a scalar) or values selected at indices where condition is True\n \ny (Tensor or Scalar) \u2013 value (if :attr:x is a scalar) or values selected at indices where condition is False\n   Returns \nA tensor of shape equal to the broadcasted shape of condition, x, y  Return type \nTensor   Example: >>> x = torch.randn(3, 2)\n>>> y = torch.ones(3, 2)\n>>> x\ntensor([[-0.4620,  0.3139],\n        [ 0.3898, -0.7197],\n        [ 0.0478, -0.1657]])\n>>> torch.where(x > 0, x, y)\ntensor([[ 1.0000,  0.3139],\n        [ 0.3898,  1.0000],\n        [ 0.0478,  1.0000]])\n>>> x = torch.randn(2, 2, dtype=torch.double)\n>>> x\ntensor([[ 1.0779,  0.0383],\n        [-0.8785, -1.1089]], dtype=torch.float64)\n>>> torch.where(x > 0, x, 0.)\ntensor([[1.0779, 0.0383],\n        [0.0000, 0.0000]], dtype=torch.float64)\n  \ntorch.where(condition) \u2192 tuple of LongTensor \n torch.where(condition) is identical to torch.nonzero(condition, as_tuple=True).  Note See also torch.nonzero().  \n"}, {"name": "torch.xlogy()", "path": "generated/torch.xlogy#torch.xlogy", "type": "torch", "text": " \ntorch.xlogy(input, other, *, out=None) \u2192 Tensor  \nComputes input * log(other) with the following cases.  outi={NaNif otheri=NaN0if inputi=0.0inputi\u2217log\u2061(otheri)otherwise\\text{out}_{i} = \\begin{cases} \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\ 0 & \\text{if } \\text{input}_{i} = 0.0 \\\\ \\text{input}_{i} * \\log{(\\text{other}_{i})} & \\text{otherwise} \\end{cases}  \nSimilar to SciPy\u2019s scipy.special.xlogy.  Parameters \n \ninput (Number or Tensor) \u2013  \nother (Number or Tensor) \u2013      Note At least one of input or other must be a tensor.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])\n \n"}, {"name": "torch.zeros()", "path": "generated/torch.zeros#torch.zeros", "type": "torch", "text": " \ntorch.zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n>>> torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])\n \n"}, {"name": "torch.zeros_like()", "path": "generated/torch.zeros_like#torch.zeros_like", "type": "torch", "text": " \ntorch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor filled with the scalar value 0, with the same size as input. torch.zeros_like(input) is equivalent to torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Warning As of 0.4, this function does not support an out keyword. As an alternative, the old torch.zeros_like(input, out=output) is equivalent to torch.zeros(input.size(), out=output).   Parameters \ninput (Tensor) \u2013 the size of input will determine size of the output tensor.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    Example: >>> input = torch.empty(2, 3)\n>>> torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n \n"}, {"name": "torch._assert()", "path": "generated/torch._assert#torch._assert", "type": "torch", "text": " \ntorch._assert(condition, message) [source]\n \nA wrapper around Python\u2019s assert which is symbolically traceable. \n"}]