[{"name": "clear()", "path": "backends#clear", "type": "torch.backends", "text": ["Clears the cuFFT plan cache."]}, {"name": "max_size", "path": "backends#max_size", "type": "torch.backends", "text": ["A int that controls cache capacity of cuFFT plan."]}, {"name": "torch", "path": "torch", "type": "torch", "text": ["The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.", "It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0", "Returns True if obj is a PyTorch tensor.", "Returns True if obj is a PyTorch storage object.", "Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.", "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.", "Returns True if the input is a single element tensor which is not equal to zero after type conversions.", "Sets the default floating point dtype to d.", "Get the current default floating point torch.dtype.", "Sets the default torch.Tensor type to floating point tensor type t.", "Returns the total number of elements in the input tensor.", "Set options for printing.", "Disables denormal floating numbers on CPU.", "Note", "Random sampling creation ops are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() You may also use torch.empty() with the In-place random sampling methods to create torch.Tensor s with values sampled from a broader range of distributions.", "Constructs a tensor with data.", "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.", "Convert the data into a torch.Tensor.", "Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.", "Creates a Tensor from a numpy.ndarray.", "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.", "Returns a tensor filled with the scalar value 0, with the same size as input.", "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.", "Returns a tensor filled with the scalar value 1, with the same size as input.", "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil  with values from the interval [start, end) taken with common difference step beginning from start.", "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1  with values from start to end with step step.", "Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.", "Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}  to baseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale with base base.", "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.", "Returns a tensor filled with uninitialized data.", "Returns an uninitialized tensor with the same size as input.", "Returns a tensor filled with uninitialized data.", "Creates a tensor of size size filled with fill_value.", "Returns a tensor with the same size as input filled with fill_value.", "Converts a float tensor to a quantized tensor with given scale and zero point.", "Converts a float tensor to a per-channel quantized tensor with given scales and zero points.", "Returns an fp32 Tensor by dequantizing a quantized Tensor", "Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.", "Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.", "Computes the Heaviside step function for each element in input.", "Concatenates the given sequence of seq tensors in the given dimension.", "Splits a tensor into a specific number of chunks.", "Creates a new tensor by horizontally stacking the tensors in tensors.", "Stack tensors in sequence depthwise (along third axis).", "Gathers values along an axis specified by dim.", "Stack tensors in sequence horizontally (column wise).", "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.", "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.", "Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.", "Alias for torch.movedim().", "Returns a new tensor that is a narrowed version of input tensor.", "Returns a tensor with the same data and number of elements as input, but with the specified shape.", "Alias of torch.vstack().", "Out-of-place version of torch.Tensor.scatter_()", "Out-of-place version of torch.Tensor.scatter_add_()", "Splits the tensor into chunks.", "Returns a tensor with all the dimensions of input of size 1 removed.", "Concatenates a sequence of tensors along a new dimension.", "Alias for torch.transpose().", "Alias for torch.transpose().", "Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.", "Returns a new tensor with the elements of input at the given indices.", "Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.", "Constructs a tensor by repeating the elements of input.", "Returns a tensor that is a transposed version of input.", "Removes a tensor dimension.", "Returns a new tensor with a dimension of size one inserted at the specified position.", "Stack tensors in sequence vertically (row wise).", "Return a tensor of elements selected from either x or y, depending on condition.", "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.", "Sets the seed for generating random numbers to a non-deterministic random number.", "Sets the seed for generating random numbers.", "Returns the initial seed for generating random numbers as a Python long.", "Returns the random number generator state as a torch.ByteTensor.", "Sets the random number generator state.", "Draws binary random numbers (0 or 1) from a Bernoulli distribution.", "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.", "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.", "Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,", "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) ", "Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) .", "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).", "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).", "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).", "Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.", "Returns a random permutation of integers from 0 to n - 1.", "There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:", "quasirandom.SobolEngine", "The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.", "Saves an object to a disk file.", "Loads an object saved with torch.save() from a file.", "Returns the number of threads used for parallelizing CPU operations", "Sets the number of threads used for intraop parallelism on CPU.", "Returns the number of threads used for inter-op parallelism on CPU (e.g.", "Sets the number of threads used for interop parallelism (e.g.", "The context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled() are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. These context managers are thread local, so they won\u2019t work if you send work to another thread using the threading module, etc.", "Examples:", "Context-manager that disabled gradient calculation.", "Context-manager that enables gradient calculation.", "Context-manager that sets gradient calculation to on or off.", "Computes the absolute value of each element in input.", "Alias for torch.abs()", "Computes the inverse cosine of each element in input.", "Alias for torch.acos().", "Returns a new tensor with the inverse hyperbolic cosine of the elements of input.", "Alias for torch.acosh().", "Adds the scalar other to each element of the input input and returns a new resulting tensor.", "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.", "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.", "Computes the element-wise angle (in radians) of the given input tensor.", "Returns a new tensor with the arcsine of the elements of input.", "Alias for torch.asin().", "Returns a new tensor with the inverse hyperbolic sine of the elements of input.", "Alias for torch.asinh().", "Returns a new tensor with the arctangent of the elements of input.", "Alias for torch.atan().", "Returns a new tensor with the inverse hyperbolic tangent of the elements of input.", "Alias for torch.atanh().", "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}  with consideration of the quadrant.", "Computes the bitwise NOT of the given input tensor.", "Computes the bitwise AND of input and other.", "Computes the bitwise OR of input and other.", "Computes the bitwise XOR of input and other.", "Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.", "Clamp all elements in input into the range [ min, max ].", "Alias for torch.clamp().", "Computes the element-wise conjugate of the given input tensor.", "Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.", "Returns a new tensor with the cosine of the elements of input.", "Returns a new tensor with the hyperbolic cosine of the elements of input.", "Returns a new tensor with each of the elements of input converted from angles in degrees to radians.", "Divides each element of the input input by the corresponding element of other.", "Alias for torch.div().", "Computes the logarithmic derivative of the gamma function on input.", "Computes the error function of each element.", "Computes the complementary error function of each element of input.", "Computes the inverse error function of each element of input.", "Returns a new tensor with the exponential of the elements of the input tensor input.", "Computes the base two exponential function of input.", "Returns a new tensor with the exponential of the elements minus 1 of input.", "Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.", "Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.", "Alias for torch.trunc()", "Raises input to the power of exponent, elementwise, in double precision.", "Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.", "Computes the element-wise remainder of division.", "Computes the fractional portion of each element in input.", "Returns a new tensor containing imaginary values of the self tensor.", "Multiplies input by 2**:attr:other.", "Does a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.", "Computes the logarithm of the gamma function on input.", "Returns a new tensor with the natural logarithm of the elements of input.", "Returns a new tensor with the logarithm to the base 10 of the elements of input.", "Returns a new tensor with the natural logarithm of (1 + input).", "Returns a new tensor with the logarithm to the base 2 of the elements of input.", "Logarithm of the sum of exponentiations of the inputs.", "Logarithm of the sum of exponentiations of the inputs in base-2.", "Computes the element-wise logical AND of the given input tensors.", "Computes the element-wise logical NOT of the given input tensor.", "Computes the element-wise logical OR of the given input tensors.", "Computes the element-wise logical XOR of the given input tensors.", "Returns a new tensor with the logit of the elements of input.", "Given the legs of a right triangle, return its hypotenuse.", "Computes the zeroth order modified Bessel function of the first kind for each element of input.", "Computes the regularized lower incomplete gamma function:", "Computes the regularized upper incomplete gamma function:", "Multiplies each element of the input input with the scalar other and returns a new resulting tensor.", "Alias for torch.mul().", "Computes the multivariate log-gamma function) with dimension pp  element-wise, given by", "Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively.", "Returns a new tensor with the negative of the elements of input.", "Alias for torch.neg()", "Return the next floating-point value after input towards other, elementwise.", "Computes the nthn^{th}  derivative of the digamma function on input.", "Takes the power of each element in input with exponent and returns a tensor with the result.", "Returns a new tensor with each of the elements of input converted from angles in radians to degrees.", "Returns a new tensor containing real values of the self tensor.", "Returns a new tensor with the reciprocal of the elements of input", "Computes the element-wise remainder of division.", "Returns a new tensor with each of the elements of input rounded to the closest integer.", "Returns a new tensor with the reciprocal of the square-root of each of the elements of input.", "Returns a new tensor with the sigmoid of the elements of input.", "Returns a new tensor with the signs of the elements of input.", "For complex tensors, this function returns a new tensor whose elemants have the same angle as that of the elements of input and absolute value 1.", "Tests if each element of input has its sign bit set (is less than zero) or not.", "Returns a new tensor with the sine of the elements of input.", "Computes the normalized sinc of input.", "Returns a new tensor with the hyperbolic sine of the elements of input.", "Returns a new tensor with the square-root of the elements of input.", "Returns a new tensor with the square of the elements of input.", "Subtracts other, scaled by alpha, from input.", "Alias for torch.sub().", "Returns a new tensor with the tangent of the elements of input.", "Returns a new tensor with the hyperbolic tangent of the elements of input.", "Alias for torch.div() with rounding_mode=None.", "Returns a new tensor with the truncated integer values of the elements of input.", "Computes input * log(other) with the following cases.", "Returns the indices of the maximum value of all elements in the input tensor.", "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension", "Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.", "Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.", "Tests if all elements in input evaluate to True.", "the input tensor.", "Returns the maximum value of all elements in the input tensor.", "Returns the minimum value of all elements in the input tensor.", "Returns the p-norm of (input - other)", "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.", "Returns the mean value of all elements in the input tensor.", "Returns the median of the values in input.", "Returns the median of the values in input, ignoring NaN values.", "Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.", "Returns the matrix norm or vector norm of a given tensor.", "Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.", "Returns the product of all elements in the input tensor.", "Returns the q-th quantiles of all elements in the input tensor, doing a linear interpolation when the q-th quantile lies between two data points.", "This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.", "Returns the standard-deviation of all elements in the input tensor.", "Returns the standard-deviation and mean of all elements in the input tensor.", "Returns the sum of all elements in the input tensor.", "Returns the unique elements of the input tensor.", "Eliminates all but the first element from every consecutive group of equivalent elements.", "Returns the variance of all elements in the input tensor.", "Returns the variance and mean of all elements in the input tensor.", "Counts the number of non-zero values in the tensor input along the given dim.", "This function checks if all input and other satisfy the condition:", "Returns the indices that sort a tensor along a given dimension in ascending order by value.", "Computes element-wise equality", "True if two tensors have the same size and elements, False otherwise.", "Computes input\u2265other\\text{input} \\geq \\text{other}  element-wise.", "Alias for torch.ge().", "Computes input>other\\text{input} > \\text{other}  element-wise.", "Alias for torch.gt().", "Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.", "Returns a new tensor with boolean elements representing if each element is finite or not.", "Tests if each element of input is infinite (positive or negative infinity) or not.", "Tests if each element of input is positive infinity or not.", "Tests if each element of input is negative infinity or not.", "Returns a new tensor with boolean elements representing if each element of input is NaN or not.", "Returns a new tensor with boolean elements representing if each element of input is real-valued or not.", "Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.", "Computes input\u2264other\\text{input} \\leq \\text{other}  element-wise.", "Alias for torch.le().", "Computes input<other\\text{input} < \\text{other}  element-wise.", "Alias for torch.lt().", "Computes the element-wise maximum of input and other.", "Computes the element-wise minimum of input and other.", "Computes the element-wise maximum of input and other.", "Computes the element-wise minimum of input and other.", "Computes input\u2260other\\text{input} \\neq \\text{other}  element-wise.", "Alias for torch.ne().", "Sorts the elements of the input tensor along a given dimension in ascending order by value.", "Returns the k largest elements of the given input tensor along a given dimension.", "Sorts the elements of the input tensor along its first dimension in ascending order by value.", "Short-time Fourier transform (STFT).", "Inverse short time Fourier Transform.", "Bartlett window function.", "Blackman window function.", "Hamming window function.", "Hann window function.", "Computes the Kaiser window with window length window_length and shape parameter beta.", "Returns a 1-dimensional view of each input tensor with zero dimensions.", "Returns a 2-dimensional view of each input tensor with zero dimensions.", "Returns a 3-dimensional view of each input tensor with zero dimensions.", "Count the frequency of each value in an array of non-negative ints.", "Create a block diagonal matrix from provided tensors.", "Broadcasts the given tensors according to Broadcasting semantics.", "Broadcasts input to the shape shape.", "Similar to broadcast_tensors() but for shapes.", "Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.", "Do cartesian product of the given sequence of tensors.", "Computes batched the p-norm distance between each pair of the two collections of row vectors.", "Returns a copy of input.", "Compute combinations of length rr  of the given tensor.", "Returns the cross product of vectors in dimension dim of input and other.", "Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.", "Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.", "Returns the cumulative product of elements of input in the dimension dim.", "Returns the cumulative sum of elements of input in the dimension dim.", "Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.", "Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.", "Computes the n-th forward difference along the given dimension.", "Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.", "Flattens input by reshaping it into a one-dimensional tensor.", "Reverse the order of a n-D tensor along given axis in dims.", "Flip tensor in the left/right direction, returning a new tensor.", "Flip tensor in the up/down direction, returning a new tensor.", "Computes the Kronecker product, denoted by \u2297\\otimes , of input and other.", "Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.", "Computes the element-wise greatest common divisor (GCD) of input and other.", "Computes the histogram of a tensor.", "Take NN  tensors, each of which can be either scalar or 1-dimensional vector, and create NN  N-dimensional grids, where the ii  th grid is defined by expanding the ii  th input over dimensions defined by other inputs.", "Computes the element-wise least common multiple (LCM) of input and other.", "Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.", "Return a contiguous flattened tensor.", "Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm", "Repeat elements of a tensor.", "Roll the tensor along the given dimension(s).", "Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, the order of the corresponding innermost dimension within sorted_sequence would be preserved.", "Returns a contraction of a and b over multiple dimensions.", "Returns the sum of the elements of the diagonal of the input 2-D matrix.", "Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.", "Returns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.", "Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.", "Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.", "Generates a Vandermonde matrix.", "Returns a view of input as a real tensor.", "Returns a view of input as a complex tensor.", "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).", "Performs a matrix multiplication of the matrices mat1 and mat2.", "Performs a matrix-vector product of the matrix mat and the vector vec.", "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.", "Performs a batch matrix-matrix product of matrices in batch1 and batch2.", "Performs a batch matrix-matrix product of matrices stored in input and mat2.", "Returns the matrix product of the NN  2-D tensors.", "Computes the Cholesky decomposition of a symmetric positive-definite matrix AA  or for batches of symmetric positive-definite matrices.", "Computes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv.", "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu .", "Computes the dot product of two 1D tensors.", "Computes the eigenvalues and eigenvectors of a real square matrix.", "This is a low-level function for calling LAPACK directly.", "Alias of torch.outer().", "Computes the dot product for 1D tensors.", "Takes the inverse of the square matrix input.", "Calculates determinant of a square matrix or batches of square matrices.", "Calculates log determinant of a square matrix or batches of square matrices.", "Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.", "Computes the solution to the least squares and least norm problems for a full rank matrix AA  of size (m\u00d7n)(m \\times n)  and a matrix BB  of size (m\u00d7k)(m \\times k) .", "Computes the LU factorization of a matrix or batches of matrices A.", "Returns the LU solve of the linear system Ax=bAx = b  using the partially pivoted LU factorization of A from torch.lu().", "Unpacks the data and pivots from a LU factorization of a tensor.", "Matrix product of two tensors.", "Returns the matrix raised to the power n for square matrices.", "Returns the numerical rank of a 2-D tensor.", "Returns the matrix exponential.", "Performs a matrix multiplication of the matrices input and mat2.", "Performs a matrix-vector product of the matrix input and the vector vec.", "Computes the orthogonal matrix Q of a QR factorization, from the (input, input2) tuple returned by torch.geqrf().", "Multiplies mat (given by input3) by the orthogonal Q matrix of the QR factorization formed by torch.geqrf() that is represented by (a, tau) (given by (input, input2)).", "Outer product of input and vec2.", "Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.", "Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices.", "This function returns the solution to the system of linear equations represented by AX=BAX = B  and the LU factorization of A, in order as a namedtuple solution, LU.", "Computes the singular value decomposition of either a matrix or batch of matrices input.", "Return the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix AA  such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T .", "Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.", "This function returns eigenvalues and eigenvectors of a real symmetric matrix input or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).", "Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.", "Estimate \u222bydx\\int y\\,dx  along dim, using the trapezoid rule.", "Solves a system of equations with a triangular coefficient matrix AA  and multiple right-hand sides bb .", "Computes the dot product of two 1D tensors.", "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1", "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.", "Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.", "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.", "Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.", "Returns True if the global deterministic flag is turned on.", "A wrapper around Python\u2019s assert which is symbolically traceable."]}, {"name": "torch.abs()", "path": "generated/torch.abs#torch.abs", "type": "torch", "text": ["Computes the absolute value of each element in input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.absolute()", "path": "generated/torch.absolute#torch.absolute", "type": "torch", "text": ["Alias for torch.abs()"]}, {"name": "torch.acos()", "path": "generated/torch.acos#torch.acos", "type": "torch", "text": ["Computes the inverse cosine of each element in input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.acosh()", "path": "generated/torch.acosh#torch.acosh", "type": "torch", "text": ["Returns a new tensor with the inverse hyperbolic cosine of the elements of input.", "Note", "The domain of the inverse hyperbolic cosine is [1, inf) and values outside this range will be mapped to NaN, except for + INF for which the output is mapped to + INF.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.add()", "path": "generated/torch.add#torch.add", "type": "torch", "text": ["Adds the scalar other to each element of the input input and returns a new resulting tensor.", "If input is of type FloatTensor or DoubleTensor, other must be a real number, otherwise it should be an integer.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Each element of the tensor other is multiplied by the scalar alpha and added to each element of the tensor input. The resulting tensor is returned.", "The shapes of input and other must be broadcastable.", "If other is of type FloatTensor or DoubleTensor, alpha must be a real number, otherwise it should be an integer.", "Example:"]}, {"name": "torch.addbmm()", "path": "generated/torch.addbmm#torch.addbmm", "type": "torch", "text": ["Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result.", "batch1 and batch2 must be 3-D tensors each containing the same number of matrices.", "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, input must be broadcastable with a (n\u00d7p)(n \\times p)  tensor and out will be a (n\u00d7p)(n \\times p)  tensor.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "Example:"]}, {"name": "torch.addcdiv()", "path": "generated/torch.addcdiv#torch.addcdiv", "type": "torch", "text": ["Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.", "Warning", "Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.", "The shapes of input, tensor1, and tensor2 must be broadcastable.", "For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.", "Example:"]}, {"name": "torch.addcmul()", "path": "generated/torch.addcmul#torch.addcmul", "type": "torch", "text": ["Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.", "The shapes of tensor, tensor1, and tensor2 must be broadcastable.", "For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.", "Example:"]}, {"name": "torch.addmm()", "path": "generated/torch.addmm#torch.addmm", "type": "torch", "text": ["Performs a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result.", "If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p)  tensor and out will be a (n\u00d7p)(n \\times p)  tensor.", "alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "Example:"]}, {"name": "torch.addmv()", "path": "generated/torch.addmv#torch.addmv", "type": "torch", "text": ["Performs a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result.", "If mat is a (n\u00d7m)(n \\times m)  tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n.", "alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers", "Example:"]}, {"name": "torch.addr()", "path": "generated/torch.addr#torch.addr", "type": "torch", "text": ["Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.", "Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m)  and out will be a matrix of size (n\u00d7m)(n \\times m) .", "Example:"]}, {"name": "torch.all()", "path": "generated/torch.all#torch.all", "type": "torch", "text": ["Tests if all elements in input evaluate to True.", "Note", "This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.", "Example:", "For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.allclose()", "path": "generated/torch.allclose#torch.allclose", "type": "torch", "text": ["This function checks if all input and other satisfy the condition:", "elementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose", "Example:"]}, {"name": "torch.amax()", "path": "generated/torch.amax#torch.amax", "type": "torch", "text": ["Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.", "Note", "If keepdim is ``True`, the output tensors are of the same size as input except in the dimension(s) dim where they are of size 1. Otherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in the output tensors having fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.amin()", "path": "generated/torch.amin#torch.amin", "type": "torch", "text": ["Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.", "Note", "If keepdim is True, the output tensors are of the same size as input except in the dimension(s) dim where they are of size 1. Otherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in the output tensors having fewer dimensions than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.angle()", "path": "generated/torch.angle#torch.angle", "type": "torch", "text": ["Computes the element-wise angle (in radians) of the given input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Note", "Starting in PyTorch 1.8, angle returns pi for negative real numbers, zero for non-negative real numbers, and propagates NaNs. Previously the function would return zero for all real numbers and not propagate floating-point NaNs.", "Example:"]}, {"name": "torch.any()", "path": "generated/torch.any#torch.any", "type": "torch", "text": ["input (Tensor) \u2013 the input tensor.", "Tests if any element in input evaluates to True.", "Note", "This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.", "Example:", "For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.arange()", "path": "generated/torch.arange#torch.arange", "type": "torch", "text": ["Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil  with values from the interval [start, end) taken with common difference step beginning from start.", "Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise adding a small epsilon to end in such cases.", "Example:"]}, {"name": "torch.arccos()", "path": "generated/torch.arccos#torch.arccos", "type": "torch", "text": ["Alias for torch.acos()."]}, {"name": "torch.arccosh()", "path": "generated/torch.arccosh#torch.arccosh", "type": "torch", "text": ["Alias for torch.acosh()."]}, {"name": "torch.arcsin()", "path": "generated/torch.arcsin#torch.arcsin", "type": "torch", "text": ["Alias for torch.asin()."]}, {"name": "torch.arcsinh()", "path": "generated/torch.arcsinh#torch.arcsinh", "type": "torch", "text": ["Alias for torch.asinh()."]}, {"name": "torch.arctan()", "path": "generated/torch.arctan#torch.arctan", "type": "torch", "text": ["Alias for torch.atan()."]}, {"name": "torch.arctanh()", "path": "generated/torch.arctanh#torch.arctanh", "type": "torch", "text": ["Alias for torch.atanh()."]}, {"name": "torch.are_deterministic_algorithms_enabled()", "path": "generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled", "type": "torch", "text": ["Returns True if the global deterministic flag is turned on. Refer to torch.use_deterministic_algorithms() documentation for more details."]}, {"name": "torch.argmax()", "path": "generated/torch.argmax#torch.argmax", "type": "torch", "text": ["Returns the indices of the maximum value of all elements in the input tensor.", "This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.", "Note", "If there are multiple minimal values then the indices of the first minimal value are returned.", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns the indices of the maximum values of a tensor across a dimension.", "This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.", "Example:"]}, {"name": "torch.argmin()", "path": "generated/torch.argmin#torch.argmin", "type": "torch", "text": ["Returns the indices of the minimum value(s) of the flattened tensor or along a dimension", "This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.", "Note", "If there are multiple minimal values then the indices of the first minimal value are returned.", "Example:"]}, {"name": "torch.argsort()", "path": "generated/torch.argsort#torch.argsort", "type": "torch", "text": ["Returns the indices that sort a tensor along a given dimension in ascending order by value.", "This is the second value returned by torch.sort(). See its documentation for the exact semantics of this method.", "Example:"]}, {"name": "torch.asin()", "path": "generated/torch.asin#torch.asin", "type": "torch", "text": ["Returns a new tensor with the arcsine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.asinh()", "path": "generated/torch.asinh#torch.asinh", "type": "torch", "text": ["Returns a new tensor with the inverse hyperbolic sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.as_strided()", "path": "generated/torch.as_strided#torch.as_strided", "type": "torch", "text": ["Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.", "Warning", "More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like torch.Tensor.expand(), are easier to read and are therefore more advisable to use.", "Example:"]}, {"name": "torch.as_tensor()", "path": "generated/torch.as_tensor#torch.as_tensor", "type": "torch", "text": ["Convert the data into a torch.Tensor. If the data is already a Tensor with the same dtype and device, no copy will be performed, otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True. Similarly, if the data is an ndarray of the corresponding dtype and the device is the cpu, no copy will be performed.", "Example:"]}, {"name": "torch.atan()", "path": "generated/torch.atan#torch.atan", "type": "torch", "text": ["Returns a new tensor with the arctangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atan2()", "path": "generated/torch.atan2#torch.atan2", "type": "torch", "text": ["Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}  with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})  and vector (1,0)(1, 0) . (Note that otheri\\text{other}_{i} , the second parameter, is the x-coordinate, while inputi\\text{input}_{i} , the first parameter, is the y-coordinate.)", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atanh()", "path": "generated/torch.atanh#torch.atanh", "type": "torch", "text": ["Returns a new tensor with the inverse hyperbolic tangent of the elements of input.", "Note", "The domain of the inverse hyperbolic tangent is (-1, 1) and values outside this range will be mapped to NaN, except for the values 1 and -1 for which the output is mapped to +/-INF respectively.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atleast_1d()", "path": "generated/torch.atleast_1d#torch.atleast_1d", "type": "torch", "text": ["Returns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)"]}, {"name": "torch.atleast_2d()", "path": "generated/torch.atleast_2d#torch.atleast_2d", "type": "torch", "text": ["Returns a 2-dimensional view of each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors", "output (Tensor or tuple of Tensors)"]}, {"name": "torch.atleast_3d()", "path": "generated/torch.atleast_3d#torch.atleast_3d", "type": "torch", "text": ["Returns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors", "output (Tensor or tuple of Tensors)"]}, {"name": "torch.autograd", "path": "autograd", "type": "torch.autograd", "text": ["torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).", "Computes the sum of gradients of given tensors w.r.t. graph leaves.", "The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors).", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.", "Note", "If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Computes and returns the sum of gradients of outputs w.r.t. the inputs.", "grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None).", "If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.", "Note", "If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Warning", "This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.", "This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.", "This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set, you can use a lambda to capture them. For example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as f(input, constant, flag=flag) you can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input).", "Function that computes the Jacobian of a given function.", "if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.", "Jacobian (Tensor or nested tuple of Tensors)", "Function that computes the Hessian of a given scalar function.", "if there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.", "Hessian (Tensor or a tuple of tuple of Tensors)", "Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)", "Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.", "output (tuple)", "Note", "The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don\u2019t have support for forward mode AD in PyTorch at the moment.", "Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)", "Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)", "Note", "This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.", "Context-manager that disabled gradient calculation.", "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.", "In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator. (Make sure to instantiate with parenthesis.)", "Example:", "Context-manager that enables gradient calculation.", "Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator. (Make sure to instantiate with parenthesis.)", "Example:", "Context-manager that sets gradient calculation to on or off.", "set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function.", "This context manager is thread local; it will not affect computation in other threads.", "mode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.", "Example:", "When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows.", "If param.grad is initially None:", "If param already has a non-sparse .grad attribute:", "The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts.", "In fact, resetting all .grads to None before each accumulation phase, e.g.:", "such that they\u2019re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks.", "If you need manual control over .grad\u2019s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True.", "Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you\u2019re operating under heavy memory pressure, you might never need to use them.", "All Tensor s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you\u2019re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.", "Warning", "The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True. Below please find a quick guide on what has changed:", "In addition, one can now create tensors with requires_grad=True using factory methods such as torch.randn(), torch.zeros(), torch.ones(), and others like the following:", "autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)", "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it.", "Is True if gradients need to be computed for this Tensor, False otherwise.", "Note", "The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.", "All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.", "Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad().", "Example:", "Computes the gradient of current tensor w.r.t. graph leaves.", "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Returns a new Tensor, detached from the current graph.", "The result will never require gradient.", "Note", "Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.", "Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.", "Registers a backward hook.", "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Example:", "Enables .grad attribute for non-leaf Tensors.", "Records operation history and defines formulas for differentiating ops.", "See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd", "Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s.", "Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd.", "Examples:", "Defines a formula for differentiating the operation.", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.", "The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output.", "Performs the operation.", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).", "The context can be used to store tensors that can be then retrieved during the backward pass.", "When creating a new Function, the following methods are available to ctx.", "Marks given tensors as modified in an in-place operation.", "This should be called at most once, only from inside the forward() method, and all arguments should be inputs.", "Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification.", "Marks outputs as non-differentiable.", "This should be called at most once, only from inside the forward() method, and all arguments should be outputs.", "This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output.", "This is used e.g. for indices returned from a max Function.", "Saves given tensors for a future call to backward().", "This should be called at most once, and only from inside the forward() method.", "Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content.", "Arguments can also be None.", "Sets whether to materialize output grad tensors. Default is true.", "This should be called only from inside the forward() method", "If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method.", "Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point or complex type and with requires_grad=True.", "The check between numerical and analytical gradients uses allclose().", "For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of grad_output: 1 and 1j. For more details, check out Autograd for Complex Numbers.", "Note", "The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.", "Warning", "If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition", "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True.", "This function checks that backpropagating through the gradients computed to the given grad_outputs are correct.", "The check between numerical and analytical gradients uses allclose().", "Note", "The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.", "Warning", "If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition", "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using profile. and nvprof based (registers both CPU and GPU activity) using emit_nvtx.", "Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks", "Exports an EventList as a Chrome tracing tools file.", "The checkpoint can be later loaded and inspected under chrome://tracing URL.", "path (str) \u2013 Path where the trace will be written.", "Averages all function events over their keys.", "An EventList containing FunctionEventAvg objects.", "Returns total time spent on CPU obtained as a sum of all self times across all the events.", "Prints an EventList as a nicely formatted table.", "A string containing the table.", "Averages all events.", "A FunctionEventAvg object.", "Context manager that makes every autograd operation emit an NVTX range.", "It is useful when running the program under nvprof:", "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.", "Forward-backward correlation", "When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates.", "During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function.", "Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass.", "Double-backward", "If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass.", "Opens an nvprof trace file and parses autograd annotations.", "path (str) \u2013 path to nvprof trace", "Context-manager that enable anomaly detection for the autograd engine.", "This does two things:", "Warning", "This mode should be enabled only for debugging as the different tests will slow down your program execution.", "Context-manager that sets the anomaly detection for the autograd engine on or off.", "set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function.", "See detect_anomaly above for details of the anomaly detection behaviour.", "mode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False)."]}, {"name": "torch.autograd.backward()", "path": "autograd#torch.autograd.backward", "type": "torch.autograd", "text": ["Computes the sum of gradients of given tensors w.r.t. graph leaves.", "The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors).", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.", "Note", "If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes."]}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "torch.autograd", "text": ["Context-manager that enable anomaly detection for the autograd engine.", "This does two things:", "Warning", "This mode should be enabled only for debugging as the different tests will slow down your program execution."]}, {"name": "torch.autograd.enable_grad", "path": "autograd#torch.autograd.enable_grad", "type": "torch.autograd", "text": ["Context-manager that enables gradient calculation.", "Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator. (Make sure to instantiate with parenthesis.)", "Example:"]}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "torch.autograd", "text": ["Records operation history and defines formulas for differentiating ops.", "See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd", "Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s.", "Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd.", "Examples:", "Defines a formula for differentiating the operation.", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.", "The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output.", "Performs the operation.", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).", "The context can be used to store tensors that can be then retrieved during the backward pass."]}, {"name": "torch.autograd.Function.backward()", "path": "autograd#torch.autograd.Function.backward", "type": "torch.autograd", "text": ["Defines a formula for differentiating the operation.", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.", "The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output."]}, {"name": "torch.autograd.Function.forward()", "path": "autograd#torch.autograd.Function.forward", "type": "torch.autograd", "text": ["Performs the operation.", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).", "The context can be used to store tensors that can be then retrieved during the backward pass."]}, {"name": "torch.autograd.function._ContextMethodMixin", "path": "autograd#torch.autograd.function._ContextMethodMixin", "type": "torch.autograd", "text": ["Marks given tensors as modified in an in-place operation.", "This should be called at most once, only from inside the forward() method, and all arguments should be inputs.", "Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification.", "Marks outputs as non-differentiable.", "This should be called at most once, only from inside the forward() method, and all arguments should be outputs.", "This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output.", "This is used e.g. for indices returned from a max Function.", "Saves given tensors for a future call to backward().", "This should be called at most once, and only from inside the forward() method.", "Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content.", "Arguments can also be None.", "Sets whether to materialize output grad tensors. Default is true.", "This should be called only from inside the forward() method", "If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method."]}, {"name": "torch.autograd.function._ContextMethodMixin.mark_dirty()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_dirty", "type": "torch.autograd", "text": ["Marks given tensors as modified in an in-place operation.", "This should be called at most once, only from inside the forward() method, and all arguments should be inputs.", "Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification."]}, {"name": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "type": "torch.autograd", "text": ["Marks outputs as non-differentiable.", "This should be called at most once, only from inside the forward() method, and all arguments should be outputs.", "This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output.", "This is used e.g. for indices returned from a max Function."]}, {"name": "torch.autograd.function._ContextMethodMixin.save_for_backward()", "path": "autograd#torch.autograd.function._ContextMethodMixin.save_for_backward", "type": "torch.autograd", "text": ["Saves given tensors for a future call to backward().", "This should be called at most once, and only from inside the forward() method.", "Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content.", "Arguments can also be None."]}, {"name": "torch.autograd.function._ContextMethodMixin.set_materialize_grads()", "path": "autograd#torch.autograd.function._ContextMethodMixin.set_materialize_grads", "type": "torch.autograd", "text": ["Sets whether to materialize output grad tensors. Default is true.", "This should be called only from inside the forward() method", "If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method."]}, {"name": "torch.autograd.functional.hessian()", "path": "autograd#torch.autograd.functional.hessian", "type": "torch.autograd", "text": ["Function that computes the Hessian of a given scalar function.", "if there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.", "Hessian (Tensor or a tuple of tuple of Tensors)"]}, {"name": "torch.autograd.functional.hvp()", "path": "autograd#torch.autograd.functional.hvp", "type": "torch.autograd", "text": ["Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)", "Note", "This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation."]}, {"name": "torch.autograd.functional.jacobian()", "path": "autograd#torch.autograd.functional.jacobian", "type": "torch.autograd", "text": ["Function that computes the Jacobian of a given function.", "if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.", "Jacobian (Tensor or nested tuple of Tensors)"]}, {"name": "torch.autograd.functional.jvp()", "path": "autograd#torch.autograd.functional.jvp", "type": "torch.autograd", "text": ["Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.", "output (tuple)", "Note", "The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don\u2019t have support for forward mode AD in PyTorch at the moment."]}, {"name": "torch.autograd.functional.vhp()", "path": "autograd#torch.autograd.functional.vhp", "type": "torch.autograd", "text": ["Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)"]}, {"name": "torch.autograd.functional.vjp()", "path": "autograd#torch.autograd.functional.vjp", "type": "torch.autograd", "text": ["Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)"]}, {"name": "torch.autograd.grad()", "path": "autograd#torch.autograd.grad", "type": "torch.autograd", "text": ["Computes and returns the sum of gradients of outputs w.r.t. the inputs.", "grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None).", "If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.", "Note", "If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes."]}, {"name": "torch.autograd.gradcheck()", "path": "autograd#torch.autograd.gradcheck", "type": "torch.autograd", "text": ["Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point or complex type and with requires_grad=True.", "The check between numerical and analytical gradients uses allclose().", "For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of grad_output: 1 and 1j. For more details, check out Autograd for Complex Numbers.", "Note", "The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.", "Warning", "If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition"]}, {"name": "torch.autograd.gradgradcheck()", "path": "autograd#torch.autograd.gradgradcheck", "type": "torch.autograd", "text": ["Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True.", "This function checks that backpropagating through the gradients computed to the given grad_outputs are correct.", "The check between numerical and analytical gradients uses allclose().", "Note", "The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.", "Warning", "If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition"]}, {"name": "torch.autograd.no_grad", "path": "autograd#torch.autograd.no_grad", "type": "torch.autograd", "text": ["Context-manager that disabled gradient calculation.", "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.", "In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator. (Make sure to instantiate with parenthesis.)", "Example:"]}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "torch.autograd", "text": ["Context manager that makes every autograd operation emit an NVTX range.", "It is useful when running the program under nvprof:", "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.", "Forward-backward correlation", "When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates.", "During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function.", "Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass.", "Double-backward", "If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass."]}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "autograd#torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "text": ["Opens an nvprof trace file and parses autograd annotations.", "path (str) \u2013 path to nvprof trace"]}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "torch.autograd", "text": ["Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks", "Exports an EventList as a Chrome tracing tools file.", "The checkpoint can be later loaded and inspected under chrome://tracing URL.", "path (str) \u2013 Path where the trace will be written.", "Averages all function events over their keys.", "An EventList containing FunctionEventAvg objects.", "Returns total time spent on CPU obtained as a sum of all self times across all the events.", "Prints an EventList as a nicely formatted table.", "A string containing the table.", "Averages all events.", "A FunctionEventAvg object."]}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "autograd#torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "text": ["Exports an EventList as a Chrome tracing tools file.", "The checkpoint can be later loaded and inspected under chrome://tracing URL.", "path (str) \u2013 Path where the trace will be written."]}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "autograd#torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "text": ["Averages all function events over their keys.", "An EventList containing FunctionEventAvg objects."]}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total()", "path": "autograd#torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "text": ["Returns total time spent on CPU obtained as a sum of all self times across all the events."]}, {"name": "torch.autograd.profiler.profile.table()", "path": "autograd#torch.autograd.profiler.profile.table", "type": "torch.autograd", "text": ["Prints an EventList as a nicely formatted table.", "A string containing the table."]}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "autograd#torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "text": ["Averages all events.", "A FunctionEventAvg object."]}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "torch.autograd", "text": ["Context-manager that sets the anomaly detection for the autograd engine on or off.", "set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function.", "See detect_anomaly above for details of the anomaly detection behaviour.", "mode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False)."]}, {"name": "torch.autograd.set_grad_enabled", "path": "autograd#torch.autograd.set_grad_enabled", "type": "torch.autograd", "text": ["Context-manager that sets gradient calculation to on or off.", "set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function.", "This context manager is thread local; it will not affect computation in other threads.", "mode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.", "Example:"]}, {"name": "torch.backends", "path": "backends", "type": "torch.backends", "text": ["torch.backends controls the behavior of various backends that PyTorch supports.", "These backends include:", "Returns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.", "A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.", "cufft_plan_cache caches the cuFFT plans", "A readonly int that shows the number of plans currently in the cuFFT plan cache.", "A int that controls cache capacity of cuFFT plan.", "Clears the cuFFT plan cache.", "Returns the version of cuDNN", "Returns a bool indicating if CUDNN is currently available.", "A bool that controls whether cuDNN is enabled.", "A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.", "A bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms().", "A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.", "Returns whether PyTorch is built with MKL support.", "Returns whether PyTorch is built with MKL-DNN support.", "Returns whether PyTorch is built with OpenMP support."]}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "torch.backends", "text": ["cufft_plan_cache caches the cuFFT plans", "A readonly int that shows the number of plans currently in the cuFFT plan cache.", "A int that controls cache capacity of cuFFT plan.", "Clears the cuFFT plan cache."]}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "torch.backends", "text": ["Returns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it."]}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "torch.backends", "text": ["A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices."]}, {"name": "torch.backends.cuda.size", "path": "backends#torch.backends.cuda.size", "type": "torch.backends", "text": ["A readonly int that shows the number of plans currently in the cuFFT plan cache."]}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "torch.backends", "text": ["A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices."]}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "torch.backends", "text": ["A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."]}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "torch.backends", "text": ["A bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms()."]}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "torch.backends", "text": ["A bool that controls whether cuDNN is enabled."]}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "torch.backends", "text": ["Returns a bool indicating if CUDNN is currently available."]}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "torch.backends", "text": ["Returns the version of cuDNN"]}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "torch.backends", "text": ["Returns whether PyTorch is built with MKL support."]}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "torch.backends", "text": ["Returns whether PyTorch is built with MKL-DNN support."]}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "torch.backends", "text": ["Returns whether PyTorch is built with OpenMP support."]}, {"name": "torch.baddbmm()", "path": "generated/torch.baddbmm#torch.baddbmm", "type": "torch", "text": ["Performs a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result.", "batch1 and batch2 must be 3-D tensors each containing the same number of matrices.", "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "Example:"]}, {"name": "torch.bartlett_window()", "path": "generated/torch.bartlett_window#torch.bartlett_window", "type": "torch", "text": ["Bartlett window function.", "where NN  is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1 , the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window", "Tensor"]}, {"name": "torch.bernoulli()", "path": "generated/torch.bernoulli#torch.bernoulli", "type": "torch", "text": ["Draws binary random numbers (0 or 1) from a Bernoulli distribution.", "The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1 .", "The ith\\text{i}^{th}  element of the output tensor will draw a value 11  according to the ith\\text{i}^{th}  probability value given in input.", "The returned out tensor only has values 0 or 1 and is of the same shape as input.", "out can have integral dtype, but input must have floating point dtype.", "input (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution", "Example:"]}, {"name": "torch.bincount()", "path": "generated/torch.bincount#torch.bincount", "type": "torch", "text": ["Count the frequency of each value in an array of non-negative ints.", "The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "a tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)", "output (Tensor)", "Example:"]}, {"name": "torch.bitwise_and()", "path": "generated/torch.bitwise_and#torch.bitwise_and", "type": "torch", "text": ["Computes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.bitwise_not()", "path": "generated/torch.bitwise_not#torch.bitwise_not", "type": "torch", "text": ["Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.bitwise_or()", "path": "generated/torch.bitwise_or#torch.bitwise_or", "type": "torch", "text": ["Computes the bitwise OR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.bitwise_xor()", "path": "generated/torch.bitwise_xor#torch.bitwise_xor", "type": "torch", "text": ["Computes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.blackman_window()", "path": "generated/torch.blackman_window#torch.blackman_window", "type": "torch", "text": ["Blackman window function.", "where NN  is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1 , the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window", "Tensor"]}, {"name": "torch.block_diag()", "path": "generated/torch.block_diag#torch.block_diag", "type": "torch", "text": ["Create a block diagonal matrix from provided tensors.", "*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.", "order such that their upper left and lower right corners are diagonally adjacent. All other elements are set to 0.", "Tensor", "Example:"]}, {"name": "torch.bmm()", "path": "generated/torch.bmm#torch.bmm", "type": "torch", "text": ["Performs a batch matrix-matrix product of matrices stored in input and mat2.", "input and mat2 must be 3-D tensors each containing the same number of matrices.", "If input is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor.", "This operator supports TensorFloat32.", "Note", "This function does not broadcast. For broadcasting matrix products, see torch.matmul().", "Example:"]}, {"name": "torch.broadcast_shapes()", "path": "generated/torch.broadcast_shapes#torch.broadcast_shapes", "type": "torch", "text": ["Similar to broadcast_tensors() but for shapes.", "This is equivalent to torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape but avoids the need create to intermediate tensors. This is useful for broadcasting tensors of common batch shape but different rightmost shape, e.g. to broadcast mean vectors with covariance matrices.", "Example:", "*shapes (torch.Size) \u2013 Shapes of tensors.", "A shape compatible with all input shapes.", "shape (torch.Size)", "RuntimeError \u2013 If shapes are incompatible."]}, {"name": "torch.broadcast_tensors()", "path": "generated/torch.broadcast_tensors#torch.broadcast_tensors", "type": "torch", "text": ["Broadcasts the given tensors according to Broadcasting semantics.", "*tensors \u2013 any number of tensors of the same type", "Warning", "More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.broadcast_to()", "path": "generated/torch.broadcast_to#torch.broadcast_to", "type": "torch", "text": ["Broadcasts input to the shape shape. Equivalent to calling input.expand(shape). See expand() for details.", "Example:"]}, {"name": "torch.bucketize()", "path": "generated/torch.bucketize#torch.bucketize", "type": "torch", "text": ["Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Return a new tensor with the same size as input. If right is False (default), then the left boundary is closed. More formally, the returned index satisfies the following rules:", "right", "returned index satisfies", "False", "boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]", "True", "boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]", "Example:"]}, {"name": "torch.can_cast()", "path": "generated/torch.can_cast#torch.can_cast", "type": "torch", "text": ["Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.", "Example:"]}, {"name": "torch.cartesian_prod()", "path": "generated/torch.cartesian_prod#torch.cartesian_prod", "type": "torch", "text": ["Do cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.", "*tensors \u2013 any number of 1 dimensional tensors.", "do itertools.product on these lists, and finally convert the resulting list into tensor.", "Tensor", "Example:"]}, {"name": "torch.cat()", "path": "generated/torch.cat#torch.cat", "type": "torch", "text": ["Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.", "torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk().", "torch.cat() can be best understood via examples.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cdist()", "path": "generated/torch.cdist#torch.cdist", "type": "torch", "text": ["Computes batched the p-norm distance between each pair of the two collections of row vectors.", "If x1 has shape B\u00d7P\u00d7MB \\times P \\times M  and x2 has shape B\u00d7R\u00d7MB \\times R \\times M  then the output will have shape B\u00d7P\u00d7RB \\times P \\times R .", "This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())."]}, {"name": "torch.ceil()", "path": "generated/torch.ceil#torch.ceil", "type": "torch", "text": ["Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.chain_matmul()", "path": "generated/torch.chain_matmul#torch.chain_matmul", "type": "torch", "text": ["Returns the matrix product of the NN  2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NN  needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NN  is 1, then this is a no-op - the original matrix is returned as is.", "matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined.", "if the ithi^{th}  tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1} , then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1} .", "Tensor", "Example:"]}, {"name": "torch.cholesky()", "path": "generated/torch.cholesky#torch.cholesky", "type": "torch", "text": ["Computes the Cholesky decomposition of a symmetric positive-definite matrix AA  or for batches of symmetric positive-definite matrices.", "If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:", "If upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:", "If upper is True, and AA  is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.", "Note", "torch.linalg.cholesky() should be used over torch.cholesky when possible. Note however that torch.linalg.cholesky() does not yet support the upper parameter and instead always returns the lower triangular matrix.", "out (Tensor, optional) \u2013 the output matrix", "Example:"]}, {"name": "torch.cholesky_inverse()", "path": "generated/torch.cholesky_inverse#torch.cholesky_inverse", "type": "torch", "text": ["Computes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines).", "If upper is False, uu  is lower triangular such that the returned tensor is", "If upper is True or not provided, uu  is upper triangular such that the returned tensor is", "out (Tensor, optional) \u2013 the output tensor for inv", "Example:"]}, {"name": "torch.cholesky_solve()", "path": "generated/torch.cholesky_solve#torch.cholesky_solve", "type": "torch", "text": ["Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu .", "If upper is False, uu  is and lower triangular and c is returned such that:", "If upper is True or not provided, uu  is upper triangular and c is returned such that:", "torch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c", "Supports real-valued and complex-valued inputs. For the complex-valued inputs the transpose operator above is the conjugate transpose.", "out (Tensor, optional) \u2013 the output tensor for c", "Example:"]}, {"name": "torch.chunk()", "path": "generated/torch.chunk#torch.chunk", "type": "torch", "text": ["Splits a tensor into a specific number of chunks. Each chunk is a view of the input tensor.", "Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks."]}, {"name": "torch.clamp()", "path": "generated/torch.clamp#torch.clamp", "type": "torch", "text": ["Clamp all elements in input into the range [ min, max ]. Let min_value and max_value be min and max, respectively, this returns:", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Clamps all elements in input to be larger or equal min.", "input (Tensor) \u2013 the input tensor.", "Example:", "Clamps all elements in input to be smaller or equal max.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.clip()", "path": "generated/torch.clip#torch.clip", "type": "torch", "text": ["Alias for torch.clamp()."]}, {"name": "torch.clone()", "path": "generated/torch.clone#torch.clone", "type": "torch", "text": ["Returns a copy of input.", "Note", "This function is differentiable, so gradients will flow back from the result of this operation to input. To create a tensor without an autograd relationship to input see detach().", "input (Tensor) \u2013 the input tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned tensor. Default: torch.preserve_format."]}, {"name": "torch.column_stack()", "path": "generated/torch.column_stack#torch.column_stack", "type": "torch", "text": ["Creates a new tensor by horizontally stacking the tensors in tensors.", "Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.combinations()", "path": "generated/torch.combinations#torch.combinations", "type": "torch", "text": ["Compute combinations of length rr  of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.", "A tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.", "Tensor", "Example:"]}, {"name": "torch.compiled_with_cxx11_abi()", "path": "generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi", "type": "torch", "text": ["Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"]}, {"name": "torch.complex()", "path": "generated/torch.complex#torch.complex", "type": "torch", "text": ["Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.", "out (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128."]}, {"name": "torch.conj()", "path": "generated/torch.conj#torch.conj", "type": "torch", "text": ["Computes the element-wise conjugate of the given input tensor. If :attr`input` has a non-complex dtype, this function just returns input.", "Warning", "In the future, torch.conj() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj() when input is of non-complex dtype to be compatible with this change.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.copysign()", "path": "generated/torch.copysign#torch.copysign", "type": "torch", "text": ["Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.", "Supports broadcasting to a common shape, and integer and float inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cos()", "path": "generated/torch.cos#torch.cos", "type": "torch", "text": ["Returns a new tensor with the cosine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cosh()", "path": "generated/torch.cosh#torch.cosh", "type": "torch", "text": ["Returns a new tensor with the hyperbolic cosine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "When input is on the CPU, the implementation of torch.cosh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details."]}, {"name": "torch.count_nonzero()", "path": "generated/torch.count_nonzero#torch.count_nonzero", "type": "torch", "text": ["Counts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.", "Example:"]}, {"name": "torch.cross()", "path": "generated/torch.cross#torch.cross", "type": "torch", "text": ["Returns the cross product of vectors in dimension dim of input and other.", "input and other must have the same size, and the size of their dim dimension should be 3.", "If dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cuda", "path": "cuda", "type": "torch.cuda", "text": ["This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.", "It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA.", "CUDA semantics has more details about working with CUDA.", "Checks if peer access between two devices is possible.", "Returns cublasHandle_t pointer to current cuBLAS handle", "Returns the index of a currently selected device.", "Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).", "Returns the default Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).", "Context-manager that changes the selected device.", "device (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.", "Returns the number of GPUs available.", "Context-manager that changes the current device to that of given object.", "You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.", "obj (Tensor or Storage) \u2013 object allocated on the selected device.", "Returns list CUDA architectures this library was compiled for.", "Gets the cuda capability of a device.", "device (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the major and minor cuda capability of the device", "tuple(int, int)", "Gets the name of a device.", "device (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the name of the device", "str", "Gets the properties of a device.", "device (torch.device or int or str) \u2013 device for which to return the properties of the device.", "the properties of the device", "_CudaDeviceProperties", "Returns NVCC gencode flags this library were compiled with.", "Initialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand.", "Does nothing if the CUDA state is already initialized.", "Force collects GPU memory after it has been released by CUDA IPC.", "Note", "Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.", "Returns a bool indicating if CUDA is currently available.", "Returns whether PyTorch\u2019s CUDA state has been initialized.", "Sets the current device.", "Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.", "device (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.", "Context-manager that selects a given stream.", "All CUDA kernels queued within its context will be enqueued on a selected stream.", "stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "Note", "Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.", "Waits for all kernels in all streams on a CUDA device to complete.", "device (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).", "Returns the random number generator state of the specified GPU as a ByteTensor.", "device (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).", "Warning", "This function eagerly initializes CUDA.", "Returns a list of ByteTensor representing the random number states of all devices.", "Sets the random number generator state of the specified GPU.", "Sets the random number generator state of all devices.", "new_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device", "Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Warning", "If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().", "Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Warning", "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().", "Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Returns the current random seed of the current GPU.", "Warning", "This function eagerly initializes CUDA.", "Broadcasts a tensor to specified GPU devices.", "Note", "Exactly one of devices and out must be specified.", "a tuple containing copies of tensor, placed on devices.", "a tuple containing out tensors, each containing a copy of tensor.", "Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.", "A tuple containing copies of tensor, placed on devices.", "Sums tensors from multiple GPUs.", "All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.", "A tensor containing an elementwise sum of all inputs, placed on the destination device.", "Scatters tensor across multiple GPUs.", "Note", "Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.", "a tuple containing chunks of tensor, placed on devices.", "a tuple containing out tensors, each containing a chunk of tensor.", "Gathers tensors from multiple GPU devices.", "Note", "destination must not be specified when out is specified.", "a tensor located on destination device, that is a result of concatenating tensors along dim.", "the out tensor, now containing results of concatenating tensors along dim.", "Wrapper around a CUDA stream.", "A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.", "Note", "Although CUDA versions >= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.", "Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed.", "Records an event.", "event (Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event.", "Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.", "Makes all future work submitted to the stream wait for an event.", "event (Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected.", "Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.", "Wrapper around a CUDA event.", "CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams.", "The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.", "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.", "Reconstruct an event from an IPC handle on the given device.", "Returns an IPC handle of this event. If not recorded yet, the event will use the current device.", "Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed.", "Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device.", "Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.", "Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified.", "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Note", "empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.", "Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "device (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).", "Returns a dictionary of CUDA memory allocator statistics for a given device.", "The return value of this function is a dictionary of statistics, each of which is a non-negative integer.", "Core statistics:", "For these core statistics, values are broken down as follows.", "Pool type:", "Metric type:", "In addition to the core statistics, we also provide some simple event counters:", "device (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management.", "Returns a human-readable printout of the current memory allocator statistics for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "Note", "See Memory management for more details about GPU memory management.", "Returns a snapshot of the CUDA memory allocator state across all devices.", "Interpreting the output of this function requires familiarity with the memory allocator internals.", "Note", "See Memory management for more details about GPU memory management.", "Returns the current GPU memory occupied by tensors in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.", "Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management.", "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "See max_memory_allocated() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management.", "Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management.", "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management.", "Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.", "Note", "In general, the total available free memory is less than the total capacity.", "Deprecated; see memory_reserved().", "Deprecated; see max_memory_reserved().", "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "See max_memory_cached() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management.", "Describe an instantaneous event that occurred at some point.", "msg (string) \u2013 ASCII message to associate with the event.", "Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (string) \u2013 ASCII message to associate with range", "Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.cuda.amp", "path": "amp", "type": "torch.cuda.amp", "text": ["torch.cuda.amp provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half). Some ops, like linear layers and convolutions, are much faster in float16. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype.", "Ordinarily, \u201cautomatic mixed precision training\u201d uses torch.cuda.amp.autocast and torch.cuda.amp.GradScaler together, as shown in the Automatic Mixed Precision examples and Automatic Mixed Precision recipe. However, autocast and GradScaler are modular, and may be used separately if desired.", "Autocast Op Reference", "Op-Specific Behavior", "Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision.", "In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details.", "When entering an autocast-enabled region, Tensors may be any type. You should not call .half() on your model(s) or inputs when using autocasting.", "autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.", "Example:", "See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).", "autocast can also be used as a decorator, e.g., on the forward method of your model:", "Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. Example:", "Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue.", "autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use:", "The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).", "enabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled in the region.", "Helper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.", "cast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.", "Note", "If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.", "Helper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail.", "If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost.", "To prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don\u2019t flush to zero.", "Each parameter\u2019s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.", "Returns a Python float containing the scale backoff factor.", "Returns a Python float containing the scale growth factor.", "Returns a Python int containing the growth interval.", "Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync.", "Returns a bool indicating whether this instance is enabled.", "Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().", "Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.", "new_scale (float) \u2013 Value to use as the new scale backoff factor.", "new_scale (float) \u2013 Value to use as the new scale growth factor.", "new_interval (int) \u2013 Value to use as the new growth interval.", "Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().", "step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported.", "Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.", "Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the scale directly.", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.", "Only CUDA ops are eligible for autocasting.", "Ops that run in float64 or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.", "Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an out=... Tensor are allowed in autocast-enabled regions, but won\u2019t go through autocasting. For example, in an autocast-enabled region a.addmm(b, c) can autocast, but a.addmm_(b, c) and a.addmm(b, c, out=d) cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.", "Ops called with an explicit dtype=... argument are not eligible, and will produce output that respects the dtype argument.", "The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.", "Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they\u2019re downstream from autocasted ops.", "If an op is unlisted, we assume it\u2019s numerically stable in float16. If you believe an unlisted op is numerically unstable in float16, please file an issue.", "__matmul__, addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, prelu, RNNCell", "__pow__, __rdiv__, __rpow__, __rtruediv__, acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, gelu, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss", "These ops don\u2019t require a particular dtype for stability, but take multiple inputs and require that the inputs\u2019 dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32.", "addcdiv, addcmul, atan2, bilinear, cat, cross, dot, equal, index_put, stack, tensordot", "Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting\u2019s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled.", "The backward passes of torch.nn.functional.binary_cross_entropy() (and torch.nn.BCELoss, which wraps it) can produce gradients that aren\u2019t representable in float16. In autocast-enabled regions, the forward input may be float16, which means the backward gradient must be representable in float16 (autocasting float16 forward inputs to float32 doesn\u2019t help, because that cast must be reversed in backward). Therefore, binary_cross_entropy and BCELoss raise an error in autocast-enabled regions.", "Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits() or torch.nn.BCEWithLogitsLoss. binary_cross_entropy_with_logits and BCEWithLogits are safe to autocast."]}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "torch.cuda.amp", "text": ["Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision.", "In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details.", "When entering an autocast-enabled region, Tensors may be any type. You should not call .half() on your model(s) or inputs when using autocasting.", "autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.", "Example:", "See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).", "autocast can also be used as a decorator, e.g., on the forward method of your model:", "Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. Example:", "Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue.", "autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use:", "The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).", "enabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled in the region."]}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "torch.cuda.amp", "text": ["Helper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail."]}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "torch.cuda.amp", "text": ["Helper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.", "cast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.", "Note", "If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect."]}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "torch.cuda.amp", "text": ["Returns a Python float containing the scale backoff factor.", "Returns a Python float containing the scale growth factor.", "Returns a Python int containing the growth interval.", "Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync.", "Returns a bool indicating whether this instance is enabled.", "Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().", "Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.", "new_scale (float) \u2013 Value to use as the new scale backoff factor.", "new_scale (float) \u2013 Value to use as the new scale growth factor.", "new_interval (int) \u2013 Value to use as the new growth interval.", "Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().", "step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported.", "Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.", "Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the scale directly.", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration."]}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "torch.cuda.amp", "text": ["Returns a Python float containing the scale backoff factor."]}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "torch.cuda.amp", "text": ["Returns a Python float containing the scale growth factor."]}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "torch.cuda.amp", "text": ["Returns a Python int containing the growth interval."]}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "torch.cuda.amp", "text": ["Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync."]}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "torch.cuda.amp", "text": ["Returns a bool indicating whether this instance is enabled."]}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "torch.cuda.amp", "text": ["Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "torch.cuda.amp", "text": ["Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale."]}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "torch.cuda.amp", "text": ["new_scale (float) \u2013 Value to use as the new scale backoff factor."]}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "torch.cuda.amp", "text": ["new_scale (float) \u2013 Value to use as the new scale growth factor."]}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "torch.cuda.amp", "text": ["new_interval (int) \u2013 Value to use as the new growth interval."]}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "torch.cuda.amp", "text": ["Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update()."]}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "torch.cuda.amp", "text": ["step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported."]}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "torch.cuda.amp", "text": ["Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute."]}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "torch.cuda.amp", "text": ["Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the scale directly.", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration."]}, {"name": "torch.cuda.can_device_access_peer()", "path": "cuda#torch.cuda.can_device_access_peer", "type": "torch.cuda", "text": ["Checks if peer access between two devices is possible."]}, {"name": "torch.cuda.comm.broadcast()", "path": "cuda#torch.cuda.comm.broadcast", "type": "torch.cuda", "text": ["Broadcasts a tensor to specified GPU devices.", "Note", "Exactly one of devices and out must be specified.", "a tuple containing copies of tensor, placed on devices.", "a tuple containing out tensors, each containing a copy of tensor."]}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "cuda#torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "text": ["Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.", "A tuple containing copies of tensor, placed on devices."]}, {"name": "torch.cuda.comm.gather()", "path": "cuda#torch.cuda.comm.gather", "type": "torch.cuda", "text": ["Gathers tensors from multiple GPU devices.", "Note", "destination must not be specified when out is specified.", "a tensor located on destination device, that is a result of concatenating tensors along dim.", "the out tensor, now containing results of concatenating tensors along dim."]}, {"name": "torch.cuda.comm.reduce_add()", "path": "cuda#torch.cuda.comm.reduce_add", "type": "torch.cuda", "text": ["Sums tensors from multiple GPUs.", "All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.", "A tensor containing an elementwise sum of all inputs, placed on the destination device."]}, {"name": "torch.cuda.comm.scatter()", "path": "cuda#torch.cuda.comm.scatter", "type": "torch.cuda", "text": ["Scatters tensor across multiple GPUs.", "Note", "Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.", "a tuple containing chunks of tensor, placed on devices.", "a tuple containing out tensors, each containing a chunk of tensor."]}, {"name": "torch.cuda.current_blas_handle()", "path": "cuda#torch.cuda.current_blas_handle", "type": "torch.cuda", "text": ["Returns cublasHandle_t pointer to current cuBLAS handle"]}, {"name": "torch.cuda.current_device()", "path": "cuda#torch.cuda.current_device", "type": "torch.cuda", "text": ["Returns the index of a currently selected device."]}, {"name": "torch.cuda.current_stream()", "path": "cuda#torch.cuda.current_stream", "type": "torch.cuda", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.default_stream()", "path": "cuda#torch.cuda.default_stream", "type": "torch.cuda", "text": ["Returns the default Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.device", "path": "cuda#torch.cuda.device", "type": "torch.cuda", "text": ["Context-manager that changes the selected device.", "device (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None."]}, {"name": "torch.cuda.device_count()", "path": "cuda#torch.cuda.device_count", "type": "torch.cuda", "text": ["Returns the number of GPUs available."]}, {"name": "torch.cuda.device_of", "path": "cuda#torch.cuda.device_of", "type": "torch.cuda", "text": ["Context-manager that changes the current device to that of given object.", "You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.", "obj (Tensor or Storage) \u2013 object allocated on the selected device."]}, {"name": "torch.cuda.empty_cache()", "path": "cuda#torch.cuda.empty_cache", "type": "torch.cuda", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Note", "empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.Event", "path": "cuda#torch.cuda.Event", "type": "torch.cuda", "text": ["Wrapper around a CUDA event.", "CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams.", "The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.", "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.", "Reconstruct an event from an IPC handle on the given device.", "Returns an IPC handle of this event. If not recorded yet, the event will use the current device.", "Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed.", "Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device.", "Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.", "Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified."]}, {"name": "torch.cuda.Event.elapsed_time()", "path": "cuda#torch.cuda.Event.elapsed_time", "type": "torch.cuda", "text": ["Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded."]}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "cuda#torch.cuda.Event.from_ipc_handle", "type": "torch.cuda", "text": ["Reconstruct an event from an IPC handle on the given device."]}, {"name": "torch.cuda.Event.ipc_handle()", "path": "cuda#torch.cuda.Event.ipc_handle", "type": "torch.cuda", "text": ["Returns an IPC handle of this event. If not recorded yet, the event will use the current device."]}, {"name": "torch.cuda.Event.query()", "path": "cuda#torch.cuda.Event.query", "type": "torch.cuda", "text": ["Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed."]}, {"name": "torch.cuda.Event.record()", "path": "cuda#torch.cuda.Event.record", "type": "torch.cuda", "text": ["Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device."]}, {"name": "torch.cuda.Event.synchronize()", "path": "cuda#torch.cuda.Event.synchronize", "type": "torch.cuda", "text": ["Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.Event.wait()", "path": "cuda#torch.cuda.Event.wait", "type": "torch.cuda", "text": ["Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified."]}, {"name": "torch.cuda.get_arch_list()", "path": "cuda#torch.cuda.get_arch_list", "type": "torch.cuda", "text": ["Returns list CUDA architectures this library was compiled for."]}, {"name": "torch.cuda.get_device_capability()", "path": "cuda#torch.cuda.get_device_capability", "type": "torch.cuda", "text": ["Gets the cuda capability of a device.", "device (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the major and minor cuda capability of the device", "tuple(int, int)"]}, {"name": "torch.cuda.get_device_name()", "path": "cuda#torch.cuda.get_device_name", "type": "torch.cuda", "text": ["Gets the name of a device.", "device (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the name of the device", "str"]}, {"name": "torch.cuda.get_device_properties()", "path": "cuda#torch.cuda.get_device_properties", "type": "torch.cuda", "text": ["Gets the properties of a device.", "device (torch.device or int or str) \u2013 device for which to return the properties of the device.", "the properties of the device", "_CudaDeviceProperties"]}, {"name": "torch.cuda.get_gencode_flags()", "path": "cuda#torch.cuda.get_gencode_flags", "type": "torch.cuda", "text": ["Returns NVCC gencode flags this library were compiled with."]}, {"name": "torch.cuda.get_rng_state()", "path": "cuda#torch.cuda.get_rng_state", "type": "torch.cuda", "text": ["Returns the random number generator state of the specified GPU as a ByteTensor.", "device (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).", "Warning", "This function eagerly initializes CUDA."]}, {"name": "torch.cuda.get_rng_state_all()", "path": "cuda#torch.cuda.get_rng_state_all", "type": "torch.cuda", "text": ["Returns a list of ByteTensor representing the random number states of all devices."]}, {"name": "torch.cuda.init()", "path": "cuda#torch.cuda.init", "type": "torch.cuda", "text": ["Initialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand.", "Does nothing if the CUDA state is already initialized."]}, {"name": "torch.cuda.initial_seed()", "path": "cuda#torch.cuda.initial_seed", "type": "torch.cuda", "text": ["Returns the current random seed of the current GPU.", "Warning", "This function eagerly initializes CUDA."]}, {"name": "torch.cuda.ipc_collect()", "path": "cuda#torch.cuda.ipc_collect", "type": "torch.cuda", "text": ["Force collects GPU memory after it has been released by CUDA IPC.", "Note", "Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory."]}, {"name": "torch.cuda.is_available()", "path": "cuda#torch.cuda.is_available", "type": "torch.cuda", "text": ["Returns a bool indicating if CUDA is currently available."]}, {"name": "torch.cuda.is_initialized()", "path": "cuda#torch.cuda.is_initialized", "type": "torch.cuda", "text": ["Returns whether PyTorch\u2019s CUDA state has been initialized."]}, {"name": "torch.cuda.list_gpu_processes()", "path": "cuda#torch.cuda.list_gpu_processes", "type": "torch.cuda", "text": ["Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "device (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.manual_seed()", "path": "cuda#torch.cuda.manual_seed", "type": "torch.cuda", "text": ["Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Warning", "If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all()."]}, {"name": "torch.cuda.manual_seed_all()", "path": "cuda#torch.cuda.manual_seed_all", "type": "torch.cuda", "text": ["Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.cuda.max_memory_allocated()", "path": "cuda#torch.cuda.max_memory_allocated", "type": "torch.cuda", "text": ["Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.max_memory_cached()", "path": "cuda#torch.cuda.max_memory_cached", "type": "torch.cuda", "text": ["Deprecated; see max_memory_reserved()."]}, {"name": "torch.cuda.max_memory_reserved()", "path": "cuda#torch.cuda.max_memory_reserved", "type": "torch.cuda", "text": ["Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_allocated()", "path": "cuda#torch.cuda.memory_allocated", "type": "torch.cuda", "text": ["Returns the current GPU memory occupied by tensors in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_cached()", "path": "cuda#torch.cuda.memory_cached", "type": "torch.cuda", "text": ["Deprecated; see memory_reserved()."]}, {"name": "torch.cuda.memory_reserved()", "path": "cuda#torch.cuda.memory_reserved", "type": "torch.cuda", "text": ["Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_snapshot()", "path": "cuda#torch.cuda.memory_snapshot", "type": "torch.cuda", "text": ["Returns a snapshot of the CUDA memory allocator state across all devices.", "Interpreting the output of this function requires familiarity with the memory allocator internals.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_stats()", "path": "cuda#torch.cuda.memory_stats", "type": "torch.cuda", "text": ["Returns a dictionary of CUDA memory allocator statistics for a given device.", "The return value of this function is a dictionary of statistics, each of which is a non-negative integer.", "Core statistics:", "For these core statistics, values are broken down as follows.", "Pool type:", "Metric type:", "In addition to the core statistics, we also provide some simple event counters:", "device (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_summary()", "path": "cuda#torch.cuda.memory_summary", "type": "torch.cuda", "text": ["Returns a human-readable printout of the current memory allocator statistics for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.nvtx.mark()", "path": "cuda#torch.cuda.nvtx.mark", "type": "torch.cuda", "text": ["Describe an instantaneous event that occurred at some point.", "msg (string) \u2013 ASCII message to associate with the event."]}, {"name": "torch.cuda.nvtx.range_pop()", "path": "cuda#torch.cuda.nvtx.range_pop", "type": "torch.cuda", "text": ["Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.cuda.nvtx.range_push()", "path": "cuda#torch.cuda.nvtx.range_push", "type": "torch.cuda", "text": ["Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (string) \u2013 ASCII message to associate with range"]}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "cuda#torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "text": ["Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "See max_memory_allocated() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "cuda#torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "text": ["Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "See max_memory_cached() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.seed()", "path": "cuda#torch.cuda.seed", "type": "torch.cuda", "text": ["Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Warning", "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all()."]}, {"name": "torch.cuda.seed_all()", "path": "cuda#torch.cuda.seed_all", "type": "torch.cuda", "text": ["Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored."]}, {"name": "torch.cuda.set_device()", "path": "cuda#torch.cuda.set_device", "type": "torch.cuda", "text": ["Sets the current device.", "Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.", "device (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative."]}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "cuda#torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "text": ["Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.", "Note", "In general, the total available free memory is less than the total capacity."]}, {"name": "torch.cuda.set_rng_state()", "path": "cuda#torch.cuda.set_rng_state", "type": "torch.cuda", "text": ["Sets the random number generator state of the specified GPU."]}, {"name": "torch.cuda.set_rng_state_all()", "path": "cuda#torch.cuda.set_rng_state_all", "type": "torch.cuda", "text": ["Sets the random number generator state of all devices.", "new_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device"]}, {"name": "torch.cuda.Stream", "path": "cuda#torch.cuda.Stream", "type": "torch.cuda", "text": ["Wrapper around a CUDA stream.", "A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.", "Note", "Although CUDA versions >= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.", "Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed.", "Records an event.", "event (Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event.", "Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.", "Makes all future work submitted to the stream wait for an event.", "event (Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected.", "Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected."]}, {"name": "torch.cuda.stream()", "path": "cuda#torch.cuda.stream", "type": "torch.cuda", "text": ["Context-manager that selects a given stream.", "All CUDA kernels queued within its context will be enqueued on a selected stream.", "stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "Note", "Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream."]}, {"name": "torch.cuda.Stream.query()", "path": "cuda#torch.cuda.Stream.query", "type": "torch.cuda", "text": ["Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed."]}, {"name": "torch.cuda.Stream.record_event()", "path": "cuda#torch.cuda.Stream.record_event", "type": "torch.cuda", "text": ["Records an event.", "event (Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event."]}, {"name": "torch.cuda.Stream.synchronize()", "path": "cuda#torch.cuda.Stream.synchronize", "type": "torch.cuda", "text": ["Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info."]}, {"name": "torch.cuda.Stream.wait_event()", "path": "cuda#torch.cuda.Stream.wait_event", "type": "torch.cuda", "text": ["Makes all future work submitted to the stream wait for an event.", "event (Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected."]}, {"name": "torch.cuda.Stream.wait_stream()", "path": "cuda#torch.cuda.Stream.wait_stream", "type": "torch.cuda", "text": ["Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected."]}, {"name": "torch.cuda.synchronize()", "path": "cuda#torch.cuda.synchronize", "type": "torch.cuda", "text": ["Waits for all kernels in all streams on a CUDA device to complete.", "device (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cummax()", "path": "generated/torch.cummax#torch.cummax", "type": "torch", "text": ["Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.cummin()", "path": "generated/torch.cummin#torch.cummin", "type": "torch", "text": ["Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.cumprod()", "path": "generated/torch.cumprod#torch.cumprod", "type": "torch", "text": ["Returns the cumulative product of elements of input in the dimension dim.", "For example, if input is a vector of size N, the result will also be a vector of size N, with elements.", "Example:"]}, {"name": "torch.cumsum()", "path": "generated/torch.cumsum#torch.cumsum", "type": "torch", "text": ["Returns the cumulative sum of elements of input in the dimension dim.", "For example, if input is a vector of size N, the result will also be a vector of size N, with elements.", "Example:"]}, {"name": "torch.deg2rad()", "path": "generated/torch.deg2rad#torch.deg2rad", "type": "torch", "text": ["Returns a new tensor with each of the elements of input converted from angles in degrees to radians.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.dequantize()", "path": "generated/torch.dequantize#torch.dequantize", "type": "torch", "text": ["Returns an fp32 Tensor by dequantizing a quantized Tensor", "tensor (Tensor) \u2013 A quantized Tensor", "Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors", "tensors (sequence of Tensors) \u2013 A list of quantized Tensors"]}, {"name": "torch.det()", "path": "generated/torch.det#torch.det", "type": "torch", "text": ["Calculates determinant of a square matrix or batches of square matrices.", "Note", "torch.det() is deprecated. Please use torch.linalg.det() instead.", "Note", "Backward through detdet  internally uses SVD results when input is not invertible. In this case, double backward through detdet  will be unstable when input doesn\u2019t have distinct singular values. See torch.svd~torch.svd  for details.", "input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.", "Example:"]}, {"name": "torch.diag()", "path": "generated/torch.diag#torch.diag", "type": "torch", "text": ["The argument diagonal controls which diagonal to consider:", "out (Tensor, optional) \u2013 the output tensor.", "See also", "torch.diagonal() always returns the diagonal of its input.", "torch.diagflat() always constructs a tensor with diagonal elements specified by the input.", "Examples:", "Get the square matrix where the input vector is the diagonal:", "Get the k-th diagonal of a given matrix:"]}, {"name": "torch.diagflat()", "path": "generated/torch.diagflat#torch.diagflat", "type": "torch", "text": ["The argument offset controls which diagonal to consider:", "Examples:"]}, {"name": "torch.diagonal()", "path": "generated/torch.diagonal#torch.diagonal", "type": "torch", "text": ["Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.", "The argument offset controls which diagonal to consider:", "Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.", "Note", "To take a batch diagonal, pass in dim1=-2, dim2=-1.", "Examples:"]}, {"name": "torch.diag_embed()", "path": "generated/torch.diag_embed#torch.diag_embed", "type": "torch", "text": ["Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.", "The argument offset controls which diagonal to consider:", "The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 00 , the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset.", "Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.", "Example:"]}, {"name": "torch.diff()", "path": "generated/torch.diff#torch.diff", "type": "torch", "text": ["Computes the n-th forward difference along the given dimension.", "The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order differences are calculated by using torch.diff() recursively.", "Note", "Only n = 1 is currently supported", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.digamma()", "path": "generated/torch.digamma#torch.digamma", "type": "torch", "text": ["Computes the logarithmic derivative of the gamma function on input.", "input (Tensor) \u2013 the tensor to compute the digamma function on", "out (Tensor, optional) \u2013 the output tensor.", "Note", "This function is similar to SciPy\u2019s scipy.special.digamma.", "Note", "From PyTorch 1.8 onwards, the digamma function returns -Inf for 0. Previously it returned NaN for 0.", "Example:"]}, {"name": "torch.dist()", "path": "generated/torch.dist#torch.dist", "type": "torch", "text": ["Returns the p-norm of (input - other)", "The shapes of input and other must be broadcastable.", "Example:"]}, {"name": "torch.distributed", "path": "distributed", "type": "torch.distributed", "text": ["Note", "Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.", "torch.distributed supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.", "Backend", "gloo", "mpi", "nccl", "Device", "CPU", "GPU", "CPU", "GPU", "CPU", "GPU", "send", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2718", "recv", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2718", "broadcast", "\u2713", "\u2713", "\u2713", "?", "\u2718", "\u2713", "all_reduce", "\u2713", "\u2713", "\u2713", "?", "\u2718", "\u2713", "reduce", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2713", "all_gather", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2713", "gather", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2718", "scatter", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2718", "reduce_scatter", "\u2718", "\u2718", "\u2718", "\u2718", "\u2718", "\u2713", "all_to_all", "\u2718", "\u2718", "\u2713", "?", "\u2718", "\u2718", "barrier", "\u2713", "\u2718", "\u2713", "?", "\u2718", "\u2713", "PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)", "Note", "As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the init_method argument of init_process_group() points to a file it must adhere to the following schema:", "Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.", "In the past, we were often asked: \u201cwhich backend should I use?\u201d.", "Rule of thumb", "GPU hosts with InfiniBand interconnect", "GPU hosts with Ethernet interconnect", "CPU hosts with InfiniBand interconnect", "CPU hosts with Ethernet interconnect", "By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):", "If you\u2019re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.", "NCCL has also provided a number of environment variables for fine-tuning purposes.", "Commonly used ones include the following for debugging purposes:", "For the full list of NCCL environment variables, please refer to NVIDIA NCCL\u2019s official documentation", "The torch.distributed package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class torch.nn.parallel.DistributedDataParallel() builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and torch.nn.DataParallel() in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.", "In the single-machine synchronous case, torch.distributed or the torch.nn.parallel.DistributedDataParallel() wrapper may still have advantages over other approaches to data-parallelism, including torch.nn.DataParallel():", "The package needs to be initialized using the torch.distributed.init_process_group() function before calling any other methods. This blocks until all processes have joined.", "Returns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS.", "Initializes the default distributed process group, and this will also initialize the distributed package.", "If neither is specified, init_method is assumed to be \u201cenv://\u201d.", "To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI.", "An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.", "The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL.", "This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".", "Note", "The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.", "Returns the backend of the given process group.", "group (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.", "The backend of the given process group as a lower case string.", "Returns the rank of current process group", "Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The rank of the process group -1, if not part of the group", "Returns the number of processes in the current process group", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The world size of the process group -1, if not part of the group", "Checking if the default process group has been initialized", "Checks if the MPI backend is available.", "Checks if the NCCL backend is available.", "Currently three initialization methods are supported:", "There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired world_size. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.", "Note that multicast address is not supported anymore in the latest distributed package. group_name is deprecated as well.", "Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired world_size. The URL should start with file:// and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn\u2019t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next init_process_group() call on the same file path/name.", "Note that automatic rank assignment is not supported anymore in the latest distributed package and group_name is deprecated as well.", "Warning", "This method assumes that the file system supports locking using fcntl - most local systems and NFS support it.", "Warning", "This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call init_process_group() multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call init_process_group() again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time init_process_group() is called.", "This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:", "The machine with rank 0 will be used to set up all connections.", "This is the default method, meaning that init_method does not have to be specified (or can be env://).", "The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in torch.distributed.init_process_group() (by explicitly creating the store as an alternative to specifying init_method.) There are 3 choices for Key-Value Stores: TCPStore, FileStore, and HashStore.", "Base class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore).", "A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc.", "A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.", "A store implementation that uses a file to store the underlying key-value pairs.", "A wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store.", "Inserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value.", "Retrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.", "key (str) \u2013 The function will return the value associated with this key.", "Value associated with key if key is in the store.", "The first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception.", "Overloaded function.", "Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.", "keys (list) \u2013 List of keys on which to wait until they are set in the store.", "Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout.", "Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.", "Warning", "When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.", "The number of keys present in the store.", "Deletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.", "Warning", "The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.", "key (str) \u2013 The key to be deleted from the store", "True if key was deleted, otherwise False.", "Sets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().", "timeout (timedelta) \u2013 timeout to be set in the store.", "By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).", "Creates a new distributed group.", "This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.", "Warning", "Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.", "A handle of distributed group that can be given to collective calls.", "Sends a tensor synchronously.", "Receives a tensor synchronously.", "Sender rank -1, if not part of the group", "isend() and irecv() return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:", "Sends a tensor asynchronously.", "A distributed request object. None, if not part of the group", "Receives a tensor asynchronously.", "A distributed request object. None, if not part of the group", "Every collective operation function supports the following two kinds of operations, depending on the setting of the async_op flag passed into the collective:", "Synchronous operation - the default mode, when async_op is set to False. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see CUDA Semantics. See the below script to see examples of differences in these semantics for CPU and CUDA operations.", "Asynchronous operation - when async_op is set to True. The collective operation function returns a distributed request object. In general, you don\u2019t need to create it manually and it is guaranteed to support two methods:", "Example", "The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:", "Broadcasts the tensor to the whole group.", "tensor must have the same number of elements in all processes participating in the collective.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Broadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.", "None. If rank is part of the group, object_list will contain the broadcasted objects from src rank.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Reduces the tensor data across all machines in such a way that all get the final result.", "After the call tensor is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Reduces the tensor data across all machines.", "Only the process with rank dst is going to receive the final result.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Gathers tensors from the whole group in a list.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Gathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Warning", "all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Gathers a list of tensors in a single process.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Gathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. On the dst rank, object_gather_list will contain the output of the collective.", "Note", "Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "Note that this API is not supported when using the NCCL backend.", "Warning", "gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Scatters a list of tensors to all processes in a group.", "Each process will receive exactly one tensor and store its data in the tensor argument.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Scatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.", "None. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.", "Note", "Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Reduces, then scatters a list of tensors to all processes in a group.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "all_to_all is experimental and subject to change.", "Synchronizes all processes.", "This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "An enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR.", "Note that BAND, BOR, and BXOR reductions are not available when using the NCCL backend.", "Additionally, MAX, MIN and PRODUCT are not supported for complex tensors.", "The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc.", "Members:", "SUM", "PRODUCT", "MIN", "MAX", "BAND", "BOR", "BXOR", "Deprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX.", "ReduceOp is recommended to use instead.", "If you want to use collective communication functions supporting autograd you can find an implementation of those in the torch.distributed.nn.* module.", "Functions here are synchronous and will be inserted in the autograd graph, so you need to ensure that all the processes that participated in the collective operation will do the backward pass for the backward communication to effectively happen and don\u2019t cause a deadlock.", "Please notice that currently the only backend where all the functions are guaranteed to work is gloo. .. autofunction:: torch.distributed.nn.broadcast .. autofunction:: torch.distributed.nn.gather .. autofunction:: torch.distributed.nn.scatter .. autofunction:: torch.distributed.nn.reduce .. autofunction:: torch.distributed.nn.all_gather .. autofunction:: torch.distributed.nn.all_to_all .. autofunction:: torch.distributed.nn.all_reduce", "If you have more than one GPU on each node, when using the NCCL and Gloo backend, broadcast_multigpu() all_reduce_multigpu() reduce_multigpu() all_gather_multigpu() and reduce_scatter_multigpu() support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend.", "For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference:", "Code running on Node 0", "Code running on Node 1", "After the call, all 16 tensors on the two nodes will have the all-reduced value of 16", "Broadcasts the tensor to the whole group with multiple GPU tensors per node.", "tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU", "Only nccl and gloo backend are currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.", "After the call, all tensor in tensor_list is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Only nccl and gloo backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Reduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU", "Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result.", "Only nccl backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, otherwise", "Gathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU", "Only nccl backend is currently supported tensors should only be GPU tensors", "Complex tensors are supported.", "output_tensor_lists (List[List[Tensor]]) \u2013 ", "Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i].", "Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j]", "Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.", "Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.", "output_tensor_list (List[Tensor]) \u2013 ", "Output tensors (on different GPUs) to receive the result of the operation.", "Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.", "input_tensor_lists (List[List[Tensor]]) \u2013 ", "Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i].", "Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j]", "Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to Tutorials - Custom C++ and CUDA Extensions and test/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party backends are decided by their own implementations.", "The new backend derives from c10d.ProcessGroup and registers the backend name and the instantiating interface through torch.distributed.Backend.register_backend() when imported.", "When manually importing this backend and invoking torch.distributed.init_process_group() with the corresponding backend name, the torch.distributed package runs on the new backend.", "Warning", "The support of third-party backend is experimental and subject to change.", "The torch.distributed package also provides a launch utility in torch.distributed.launch. This helper utility can be used to launch multiple processes per node for distributed training.", "torch.distributed.launch is a module that spawns up multiple distributed training processes on each of the training nodes.", "The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.", "In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (--nproc_per_node). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (nproc_per_node), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1).", "How to use this module:", "Node 1: (IP: 192.168.1.1, and has a free port: 1234)", "Node 2:", "Important Notices:", "1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training.", "2. In your training program, you must parse the command-line argument: --local_rank=LOCAL_PROCESS_RANK, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:", "Parsing the local_rank argument", "Set your device to local rank using either", "or", "3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. You need to make sure that the init_method uses env://, which is the only supported init_method by this module.", "4. In your training program, you can either use regular distributed functions or use torch.nn.parallel.DistributedDataParallel() module. If your training program uses GPUs for training and you would like to use torch.nn.parallel.DistributedDataParallel() module, here is how to configure it.", "Please ensure that device_ids argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the device_ids needs to be [args.local_rank], and output_device needs to be args.local_rank in order to use this utility", "5. Another way to pass local_rank to the subprocesses via environment variable LOCAL_RANK. This behavior is enabled when you launch the script with --use_env=True. You must adjust the subprocess example above to replace args.local_rank with os.environ['LOCAL_RANK']; the launcher will not pass --local_rank when you specify this flag.", "Warning", "local_rank is NOT globally unique: it is only unique per process on a machine. Thus, don\u2019t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don\u2019t do this correctly.", "The Multiprocessing package - torch.multiprocessing package also provides a spawn function in torch.multiprocessing.spawn(). This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.", "For references on how to use it, please refer to PyTorch example - ImageNet implementation", "Note that this function requires Python 3.4 or higher."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook just calls allreduce using GradBucket tensors. Once gradient tensors are aggregated across all workers, its then callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won\u2019t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements a simple gradient compression approach that converts GradBucket tensors whose type is assumed to be torch.float32 to half-precision floating point format (torch.float16). It allreduces those float16 gradient tensors. Once compressed gradient tensors are allreduced, its then callback called decompress converts the aggregated result back to float32 and takes the mean."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is faster than powerSGD_hook(), but usually results in a much lower accuracy, unless matrix_approximation_rank is 1.", "Warning", "Increasing matrix_approximation_rank here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider powerSGD_hook() first, and only consider this variant when a satisfactory accuracy can be achieved when matrix_approximation_rank is 1.", "Once gradient tensors are aggregated across all workers, this hook applies compression as follows:", "Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.", "Future handler of the communication, which updates the gradients in place."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": ["Stores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training. Particularly, matrix_approximation_rank and start_powerSGD_iter are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters use_error_feedback and warm_start on.", "matrix_approximation_rank controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.", "1.1. If matrix_approximation_rank is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.", "1.2. The increase of matrix_approximation_rank can substantially increase the computation costs of the compression, and the accuracy may not be futher improved beyond a certain matrix_approximation_rank threshold.", "To tune matrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an expoential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.", "To tune start_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached.", "Warning", "If error feedback or warm-up is enabled, the minimum value of start_powerSGD_iter allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:", "Handles rank-1 tensors by allreducing them without compression:", "2.1. Allocate contiguous memory for those rank-1 tensors, and allreduces all the rank-1 tensors as a batch, without compression;", "2.2. Copies the individual rank-1 tensors from the contiguous memory back to the input tensor.", "Handles high-rank tensors by PowerSGD compression:", "3.1. For each high-rank tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;", "3.2. Computes each P in Ps, which is equal to MQ;", "3.3. Allreduces Ps as a batch;", "3.4. Orthogonalizes each P in Ps;", "3.5. Computes each Q in Qs, which is approximately equal to M^TP;", "3.6. Allreduces Qs as a batch;", "3.7. Computes each M among all the high-rank tensors, which is approximately equal to PQ^T.", "Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.", "Future handler of the communication, which updates the gradients in place."]}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "torch.distributed", "text": ["Gathers tensors from the whole group in a list.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "torch.distributed", "text": ["Gathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU", "Only nccl backend is currently supported tensors should only be GPU tensors", "Complex tensors are supported.", "output_tensor_lists (List[List[Tensor]]) \u2013 ", "Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i].", "Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j]", "Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "torch.distributed", "text": ["Gathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Warning", "all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust."]}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "torch.distributed", "text": ["Reduces the tensor data across all machines in such a way that all get the final result.", "After the call tensor is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "torch.distributed", "text": ["Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.", "After the call, all tensor in tensor_list is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Only nccl and gloo backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "torch.distributed", "text": ["Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "all_to_all is experimental and subject to change."]}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC Framework", "text": ["Kicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.", "We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.", "We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context to be used is looked up given the context_id that is passed in when torch.distributed.autograd.backward() is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API."]}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC Framework", "text": ["Context object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass."]}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC Framework", "text": ["Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass.", "context_id (int) \u2013 The autograd context id for which we should retrieve the gradients.", "A map where the key is the Tensor and the value is the associated gradient for that Tensor."]}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "torch.distributed", "text": ["An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.", "The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL.", "This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".", "Note", "The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence."]}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "torch.distributed", "text": ["Synchronizes all processes.", "This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "torch.distributed", "text": ["Broadcasts the tensor to the whole group.", "tensor must have the same number of elements in all processes participating in the collective.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "torch.distributed", "text": ["Broadcasts the tensor to the whole group with multiple GPU tensors per node.", "tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU", "Only nccl and gloo backend are currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "torch.distributed", "text": ["Broadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.", "None. If rank is part of the group, object_list will contain the broadcasted objects from src rank.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust."]}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "torch.distributed", "text": ["A store implementation that uses a file to store the underlying key-value pairs."]}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "torch.distributed", "text": ["Gathers a list of tensors in a single process.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "torch.distributed", "text": ["Gathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. On the dst rank, object_gather_list will contain the output of the collective.", "Note", "Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "Note that this API is not supported when using the NCCL backend.", "Warning", "gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust."]}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "torch.distributed", "text": ["Returns the backend of the given process group.", "group (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.", "The backend of the given process group as a lower case string."]}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "torch.distributed", "text": ["Returns the rank of current process group", "Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The rank of the process group -1, if not part of the group"]}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "torch.distributed", "text": ["Returns the number of processes in the current process group", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The world size of the process group -1, if not part of the group"]}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "torch.distributed", "text": ["A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes."]}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "torch.distributed", "text": ["Initializes the default distributed process group, and this will also initialize the distributed package.", "If neither is specified, init_method is assumed to be \u201cenv://\u201d.", "To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI."]}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "torch.distributed", "text": ["Receives a tensor asynchronously.", "A distributed request object. None, if not part of the group"]}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "torch.distributed", "text": ["Sends a tensor asynchronously.", "A distributed request object. None, if not part of the group"]}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "torch.distributed", "text": ["Returns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS."]}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "torch.distributed", "text": ["Checking if the default process group has been initialized"]}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "torch.distributed", "text": ["Checks if the MPI backend is available."]}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "torch.distributed", "text": ["Checks if the NCCL backend is available."]}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "torch.distributed", "text": ["Creates a new distributed group.", "This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.", "Warning", "Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.", "A handle of distributed group that can be given to collective calls."]}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "rpc#torch.distributed.optim.DistributedOptimizer", "type": "Distributed RPC Framework", "text": ["DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.", "This class uses get_gradients() in order to retrieve the gradients for specific parameters.", "Concurrent calls to step(), either from the same or different clients, will be serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.", "DistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) during multithreaded training (e.g. Distributed Model Parallel). This feature is currently in beta stage, enabled for optimizers including Adagrad, Adam, SGD, RMSprop, AdamW and Adadelta. We are increasing the coverage to all optimizers in future releases.", "Performs a single optimization step.", "This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.", "context_id \u2013 the autograd context id for which we should run the optimizer step."]}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "rpc#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed RPC Framework", "text": ["Performs a single optimization step.", "This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.", "context_id \u2013 the autograd context id for which we should run the optimizer step."]}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Pipeline Parallelism", "text": ["Wraps an arbitrary nn.Sequential module to train on using synchronous pipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on a single GPU, pipeline parallelism is a useful technique to employ for training.", "The implementation is based on the torchgpipe paper.", "Pipe combines pipeline parallelism with checkpointing to reduce peak memory required to train while minimizing device under-utilization.", "You should place all the modules on the appropriate devices and wrap them into an nn.Sequential module defining the desired order of execution.", "Pipeline of two FC layers across GPUs 0 and 1.", "Note", "You can wrap a Pipe model with torch.nn.parallel.DistributedDataParallel only when the checkpoint parameter of Pipe is 'never'.", "Note", "Pipe only supports intra-node pipelining currently, but will be expanded to support inter-node pipelining in the future. The forward function returns an RRef to allow for inter-node pipelining in the future, where the output might be on a remote host. For intra-node pipelinining you can use local_value() to retrieve the output locally.", "Warning", "Pipe is experimental and subject to change.", "Processes a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to be a Tensor or a sequence of tensors. This restriction is applied at partition boundaries too.", "The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.", "input (torch.Tensor or sequence of Tensor) \u2013 input mini-batch", "RRef to the output of the mini-batch", "TypeError \u2013 input is not a tensor or sequence of tensors."]}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Pipeline Parallelism", "text": ["Processes a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to be a Tensor or a sequence of tensors. This restriction is applied at partition boundaries too.", "The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.", "input (torch.Tensor or sequence of Tensor) \u2013 input mini-batch", "RRef to the output of the mini-batch", "TypeError \u2013 input is not a tensor or sequence of tensors."]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Pipeline Parallelism", "text": ["The command to pop a skip tensor.", "name (str) \u2013 name of skip tensor", "the skip tensor previously stashed by another layer under the same name"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Pipeline Parallelism", "text": ["The decorator to define a nn.Module with skip connections. Decorated modules are called \u201cskippable\u201d. This functionality works perfectly fine even when the module is not wrapped by Pipe.", "Each skip tensor is managed by its name. Before manipulating skip tensors, a skippable module must statically declare the names for skip tensors by stash and/or pop parameters. Skip tensors with pre-declared name can be stashed by yield stash(name, tensor) or popped by tensor = yield\npop(name).", "Here is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed and popped at the first and last layer, respectively:", "One skippable module can stash or pop multiple skip tensors:", "Every skip tensor must be associated with exactly one pair of stash and pop. Pipe checks this restriction automatically when wrapping a module. You can also check the restriction by verify_skippables() without Pipe."]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Pipeline Parallelism", "text": ["The command to stash a skip tensor."]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Pipeline Parallelism", "text": ["Verifies if the underlying skippable modules satisfy integrity.", "Every skip tensor must have only one pair of stash and pop. If there are one or more unmatched pairs, it will raise TypeError with the detailed messages.", "Here are a few failure cases. verify_skippables() will report failure for these cases:", "To use the same name for multiple skip tensors, they must be isolated by different namespaces. See isolate().", "TypeError \u2013 one or more pairs of stash and pop are not matched."]}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "torch.distributed", "text": ["A wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store."]}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "torch.distributed", "text": ["Receives a tensor synchronously.", "Sender rank -1, if not part of the group"]}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "torch.distributed", "text": ["Reduces the tensor data across all machines.", "Only the process with rank dst is going to receive the final result.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "torch.distributed", "text": ["An enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR.", "Note that BAND, BOR, and BXOR reductions are not available when using the NCCL backend.", "Additionally, MAX, MIN and PRODUCT are not supported for complex tensors.", "The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc.", "Members:", "SUM", "PRODUCT", "MIN", "MAX", "BAND", "BOR", "BXOR"]}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "torch.distributed", "text": ["Reduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU", "Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result.", "Only nccl backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, otherwise"]}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "torch.distributed", "text": ["Deprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX.", "ReduceOp is recommended to use instead."]}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "torch.distributed", "text": ["Reduces, then scatters a list of tensors to all processes in a group.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group."]}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "torch.distributed", "text": ["Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.", "Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.", "output_tensor_list (List[Tensor]) \u2013 ", "Output tensors (on different GPUs) to receive the result of the operation.", "Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.", "input_tensor_lists (List[List[Tensor]]) \u2013 ", "Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i].", "Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j]", "Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group."]}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC Framework", "text": ["An enum class of available backends.", "PyTorch ships with two builtin backends: BackendType.TENSORPIPE and BackendType.PROCESS_GROUP. Additional ones can be registered using the register_backend() function."]}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC Framework", "text": ["A decorator for a function indicating that the return value of the function is guaranteed to be a Future object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the Future returned by the wrapped function and installs subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. That also means the returned Future only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function\u2019s (fn) execution needs to pause and resume due to, e.g., containing rpc_async() or waiting for other signals.", "Note", "To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a Future object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with @staticmethod or @classmethod, @rpc.functions.async_execution needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by @rpc.functions.async_execution.", "The returned Future object can come from rpc_async(), then(), or Future constructor. The example below shows directly using the Future returned by then().", "When combined with TorchScript decorators, this decorator must be the outmost one.", "When combined with static or class method, this decorator must be the inner one.", "This decorator also works with RRef helpers, i.e., . torch.distributed.rpc.RRef.rpc_sync(), torch.distributed.rpc.RRef.rpc_async(), and torch.distributed.rpc.RRef.remote()."]}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC Framework", "text": ["Get WorkerInfo of a given worker name. Use this WorkerInfo to avoid passing an expensive string on every invocation.", "worker_name (str) \u2013 the string name of a worker. If None, return the the id of the current worker. (default None)", "WorkerInfo instance for the given worker_name or WorkerInfo of the current worker if worker_name is None."]}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC Framework", "text": ["Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs."]}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions", "type": "Distributed RPC Framework", "text": ["The backend options class for ProcessGroupAgent, which is derived from RpcBackendOptions.", "URL specifying how to initialize the process group. Default is env://", "The number of threads in the thread-pool used by ProcessGroupAgent.", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads", "type": "Distributed RPC Framework", "text": ["The number of threads in the thread-pool used by ProcessGroupAgent."]}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC Framework", "text": ["Make a remote call to run func on worker to and return an RRef to the result value immediately. Worker to will be the owner of the returned RRef, and the worker calling remote is a user. The owner manages the global reference count of its RRef, and the owner RRef is only destructed when globally there are no living references to it.", "A user RRef instance to the result value. Use the blocking API torch.distributed.rpc.RRef.to_here() to retrieve the result value locally.", "Warning", "Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.", "Warning", "The remote API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the torch.distributed.rpc.RRef.confirmed_by_owner() API.", "Warning", "Errors such as timeouts for the remote API are handled on a best-effort basis. This means that when remote calls initiated by remote fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as to_here or fork call), then future uses of the RRef will appropriately raise errors. However, it is possible that the user application will use the RRef before the errors are handled. In this case, errors may not be raised as they have not yet been handled.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC Framework", "text": ["An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to init_rpc() in order to initialize RPC with specific configurations, such as the RPC timeout and init_method to be used.", "URL specifying how to initialize the process group. Default is env://", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC Framework", "text": ["Make a non-blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a Future that can be awaited on.", "Returns a Future object that can be waited on. When completed, the return value of func on args and kwargs can be retrieved from the Future object.", "Warning", "Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.", "Warning", "The rpc_async API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned Future completes.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC Framework", "text": ["Make a blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.", "Returns the result of running func with args and kwargs.", "Warning", "Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.RRef", "path": "rpc#torch.distributed.rpc.RRef", "type": "Distributed RPC Framework", "text": ["Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.", "Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef.", "Returns whether or not the current node is the owner of this RRef.", "If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.", "Returns worker information of the node that owns this RRef.", "Returns worker name of the node that owns this RRef.", "Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.", "Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.", "Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.", "Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.", "timeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used."]}, {"name": "torch.distributed.rpc.RRef.backward()", "path": "rpc#torch.distributed.rpc.RRef.backward", "type": "Distributed RPC Framework", "text": ["Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor."]}, {"name": "torch.distributed.rpc.RRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.RRef.confirmed_by_owner", "type": "Distributed RPC Framework", "text": ["Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef."]}, {"name": "torch.distributed.rpc.RRef.is_owner()", "path": "rpc#torch.distributed.rpc.RRef.is_owner", "type": "Distributed RPC Framework", "text": ["Returns whether or not the current node is the owner of this RRef."]}, {"name": "torch.distributed.rpc.RRef.local_value()", "path": "rpc#torch.distributed.rpc.RRef.local_value", "type": "Distributed RPC Framework", "text": ["If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception."]}, {"name": "torch.distributed.rpc.RRef.owner()", "path": "rpc#torch.distributed.rpc.RRef.owner", "type": "Distributed RPC Framework", "text": ["Returns worker information of the node that owns this RRef."]}, {"name": "torch.distributed.rpc.RRef.owner_name()", "path": "rpc#torch.distributed.rpc.RRef.owner_name", "type": "Distributed RPC Framework", "text": ["Returns worker name of the node that owns this RRef."]}, {"name": "torch.distributed.rpc.RRef.remote()", "path": "rpc#torch.distributed.rpc.RRef.remote", "type": "Distributed RPC Framework", "text": ["Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef."]}, {"name": "torch.distributed.rpc.RRef.rpc_async()", "path": "rpc#torch.distributed.rpc.RRef.rpc_async", "type": "Distributed RPC Framework", "text": ["Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used."]}, {"name": "torch.distributed.rpc.RRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.RRef.rpc_sync", "type": "Distributed RPC Framework", "text": ["Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used."]}, {"name": "torch.distributed.rpc.RRef.to_here()", "path": "rpc#torch.distributed.rpc.RRef.to_here", "type": "Distributed RPC Framework", "text": ["Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.", "timeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used."]}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC Framework", "text": ["Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If graceful=True, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if graceful=False, this is a local shutdown, and it does not wait for other RPC processes to reach this method.", "Warning", "For Future objects returned by rpc_async(), future.wait() should not be called after shutdown().", "graceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for UserRRefs and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "Then run the following code in two different processes:"]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC Framework", "text": ["The backend options for TensorPipeAgent, derived from RpcBackendOptions.", "The device map locations.", "URL specifying how to initialize the process group. Default is env://", "The number of threads in the thread-pool used by TensorPipeAgent to execute requests.", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.", "Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC Framework", "text": ["The device map locations."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC Framework", "text": ["The number of threads in the thread-pool used by TensorPipeAgent to execute requests."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC Framework", "text": ["Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations."]}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC Framework", "text": ["A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through get_worker_info() and the result can be passed in to functions such as rpc_sync(), rpc_async(), remote() to avoid copying a string on every invocation.", "Globally unique id to identify the worker.", "The name of the worker."]}, {"name": "torch.distributed.rpc.WorkerInfo.id()", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC Framework", "text": ["Globally unique id to identify the worker."]}, {"name": "torch.distributed.rpc.WorkerInfo.name()", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC Framework", "text": ["The name of the worker."]}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "torch.distributed", "text": ["Scatters a list of tensors to all processes in a group.", "Each process will receive exactly one tensor and store its data in the tensor argument.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "torch.distributed", "text": ["Scatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.", "None. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.", "Note", "Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust."]}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "torch.distributed", "text": ["Sends a tensor synchronously."]}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "torch.distributed", "text": ["Base class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore)."]}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "torch.distributed", "text": ["The first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception."]}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "torch.distributed", "text": ["Deletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.", "Warning", "The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.", "key (str) \u2013 The key to be deleted from the store", "True if key was deleted, otherwise False."]}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "torch.distributed", "text": ["Retrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.", "key (str) \u2013 The function will return the value associated with this key.", "Value associated with key if key is in the store."]}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "torch.distributed", "text": ["Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.", "Warning", "When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.", "The number of keys present in the store."]}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "torch.distributed", "text": ["Inserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value."]}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "torch.distributed", "text": ["Sets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().", "timeout (timedelta) \u2013 timeout to be set in the store."]}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "torch.distributed", "text": ["Overloaded function.", "Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.", "keys (list) \u2013 List of keys on which to wait until they are set in the store.", "Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout."]}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "torch.distributed", "text": ["A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc."]}, {"name": "torch.distributions", "path": "distributions", "type": "torch.distributions", "text": ["The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package.", "It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples f(x)f(x) , the pathwise derivative requires the derivative f\u2032(x)f'(x) . The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs .", "When the probability density function is differentiable with respect to its parameters, we only need sample() and log_prob() to implement REINFORCE:", "where \u03b8\\theta  are the parameters, \u03b1\\alpha  is the learning rate, rr  is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))  is the probability of taking action aa  in state ss  given policy \u03c0\u03b8\\pi^\\theta .", "In practice we would sample an action from the output of a network, apply this action in an environment, and then use log_prob to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:", "The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the rsample() method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:", "Bases: object", "Distribution is the abstract base class for probability distributions.", "Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.", "Returns the shape over which parameters are batched.", "Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0.", "Returns the shape of a single sample (without batching).", "Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size.", "Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Returns the mean of the distribution.", "Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.", "Generates n samples or n batches of samples if the distribution parameters are batched.", "Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation.", "Returns the standard deviation of the distribution.", "Returns a Constraint object representing this distribution\u2019s support.", "Returns the variance of the distribution.", "Bases: torch.distributions.distribution.Distribution", "ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below", "where \u03b8\\theta  denotes the natural parameters, t(x)t(x)  denotes the sufficient statistic, F(\u03b8)F(\\theta)  is the log normalizer function for a given family and k(x)k(x)  is the carrier measure.", "Note", "This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).", "Method to compute the entropy using Bregman divergence of the log normalizer.", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Bernoulli distribution parameterized by probs or logits (but not both).", "Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p.", "Example:", "Bases: torch.distributions.exp_family.ExponentialFamily", "Beta distribution parameterized by concentration1 and concentration0.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a categorical distribution parameterized by either probs or logits (but not both).", "Note", "It is equivalent to the distribution that torch.multinomial() samples from.", "Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}  where K is probs.size(-1).", "If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index.", "If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.", "See also: torch.multinomial()", "Example:", "Bases: torch.distributions.distribution.Distribution", "Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution.", "Example:", "Bases: torch.distributions.gamma.Gamma", "Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5)", "Example:", "df (float or Tensor) \u2013 shape parameter of the distribution", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both).", "The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.", "Example:", "[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Dirichlet distribution parameterized by concentration concentration.", "Example:", "concentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Exponential distribution parameterized by rate.", "Example:", "rate (float or Tensor) \u2013 rate = 1 / scale of the distribution", "Bases: torch.distributions.distribution.Distribution", "Creates a Fisher-Snedecor distribution parameterized by df1 and df2.", "Example:", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Gamma distribution parameterized by shape concentration and rate.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1  Bernoulli trials, the first kk  trials failed, before seeing a success.", "Samples are non-negative integers [0, inf\u2061\\inf ).", "Example:", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a Gumbel Distribution.", "Examples:", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a half-Cauchy distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Cauchy distribution", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a half-normal distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Normal distribution", "Bases: torch.distributions.distribution.Distribution", "Reinterprets some of the batch dims of a distribution as event dims.", "This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a Kumaraswamy distribution.", "Example:", "Bases: torch.distributions.distribution.Distribution", "LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta  to make the probability of the correlation matrix MM  generated from a Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3.", "L ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration)", "Example:", "References", "[1] Generating random correlation matrices based on vines and extended onion method, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.", "Bases: torch.distributions.distribution.Distribution", "Creates a Laplace distribution parameterized by loc and scale.", "Example:", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a log-normal distribution parameterized by loc and scale where:", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag:", "Note", "The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:", "Bases: torch.distributions.distribution.Distribution", "The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component.", "Examples:", "Bases: torch.distributions.distribution.Distribution", "Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches.", "Note that total_count need not be specified if only log_prob() is called (see example below)", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.", "The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}  or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}  or a lower-triangular matrix L\\mathbf{L}  with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.", "Note", "Only one of covariance_matrix or precision_matrix or scale_tril can be specified.", "Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.", "Bases: torch.distributions.distribution.Distribution", "Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of failure of each Bernoulli trial is probs.", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a normal (also called Gaussian) distribution parameterized by loc and scale.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a one-hot categorical distribution parameterized by probs or logits.", "Samples are one-hot coded vectors of size probs.size(-1).", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.", "See also: torch.distributions.Categorical() for specifications of probs and logits.", "Example:", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a Pareto Type 1 distribution.", "Example:", "Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Poisson distribution parameterized by rate, the rate parameter.", "Samples are nonnegative integers, with a pmf given by", "Example:", "rate (Number, Tensor) \u2013 the rate parameter", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution.", "Samples are logits of values in (0, 1). See [1] for more details.", "[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)", "[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale.", "Example:", "Bases: torch.distributions.distribution.Distribution", "Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:", "Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.", "An example for the usage of TransformedDistribution would be:", "For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical", "Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.", "Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.", "Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.", "Bases: torch.distributions.distribution.Distribution", "Generates uniformly distributed random samples from the half-open interval [low, high).", "Example:", "Bases: torch.distributions.distribution.Distribution", "A circular von Mises distribution.", "This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.", "The provided mean is the circular one.", "The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157.", "The provided variance is the circular one.", "Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a two-parameter Weibull distribution.", "Compute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)  between two distributions.", "A batch of KL divergences of shape batch_shape.", "Tensor", "NotImplementedError \u2013 If the distribution types have not been registered via register_kl().", "Decorator to register a pairwise function with kl_divergence(). Usage:", "Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation:", "you should register a third most-specific implementation, e.g.:", "Abstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution.", "Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:", "However the following will error when caching due to dependency reversal:", "Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().", "cache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.", "Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t.", "Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.", "Computes the log det jacobian log |dy/dx| given input and output.", "Infers the shape of the forward computation, given the input shape. Defaults to preserving shape.", "Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape.", "Composes multiple transforms in a chain. The transforms being composed are responsible for caching.", "Wrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian().", "Unit Jacobian transform to reshape the rightmost part of a tensor.", "Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape().", "Transform via the mapping y=exp\u2061(x)y = \\exp(x) .", "Transform via the mapping y=xexponenty = x^{\\text{exponent}} .", "Transform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}  and x=logit(y)x = \\text{logit}(y) .", "Transform via the mapping y=tanh\u2061(x)y = \\tanh(x) .", "It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead.", "Note that one should use cache_size=1 when it comes to NaN/Inf values.", "Transform via the mapping y=\u2223x\u2223y = |x| .", "Transform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x .", "Transforms an uncontrained real vector xx  with length D\u2217(D\u22121)/2D*(D-1)/2  into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:", "Transform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)  then normalizing.", "This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms.", "Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.", "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.", "This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization.", "Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.", "This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization.", "Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().", "x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)", "The following constraints are implemented:", "Abstract base class for constraints.", "A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.", "Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint.", "alias of torch.distributions.constraints._DependentProperty", "alias of torch.distributions.constraints._IndependentConstraint", "alias of torch.distributions.constraints._IntegerInterval", "alias of torch.distributions.constraints._GreaterThan", "alias of torch.distributions.constraints._GreaterThanEq", "alias of torch.distributions.constraints._LessThan", "alias of torch.distributions.constraints._Multinomial", "alias of torch.distributions.constraints._Interval", "alias of torch.distributions.constraints._HalfOpenInterval", "alias of torch.distributions.constraints._Cat", "alias of torch.distributions.constraints._Stack", "PyTorch provides two global ConstraintRegistry objects that link Constraint objects to Transform objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.", "The transform_to() registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution\u2019s .arg_constraints dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:", "The biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant.:", "Note", "An example where transform_to and biject_to differ is constraints.simplex: transform_to(constraints.simplex) returns a SoftmaxTransform that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, biject_to(constraints.simplex) returns a StickBreakingTransform that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.", "The biject_to and transform_to objects can be extended by user-defined constraints and transforms using their .register() method either as a function on singleton constraints:", "or as a decorator on parameterized constraints:", "You can create your own registry by creating a new ConstraintRegistry object.", "Registry to link constraints to transforms.", "Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Bernoulli distribution parameterized by probs or logits (but not both).", "Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p.", "Example:"]}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.mean()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.variance()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Beta distribution parameterized by concentration1 and concentration0.", "Example:"]}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.concentration0()", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.concentration1()", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.mean()", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.beta.Beta.variance()", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits.", "Example:"]}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.mean()", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.param_shape()", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.support()", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.variance()", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a categorical distribution parameterized by either probs or logits (but not both).", "Note", "It is equivalent to the distribution that torch.multinomial() samples from.", "Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}  where K is probs.size(-1).", "If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index.", "If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.", "See also: torch.multinomial()", "Example:"]}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.mean()", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.param_shape()", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.support()", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.variance()", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution.", "Example:"]}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.mean()", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.variance()", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "torch.distributions", "text": ["Bases: torch.distributions.gamma.Gamma", "Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5)", "Example:", "df (float or Tensor) \u2013 shape parameter of the distribution"]}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2.df()", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._Cat"]}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "torch.distributions", "text": ["Abstract base class for constraints.", "A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.", "Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint."]}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "torch.distributions", "text": ["Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint."]}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._DependentProperty"]}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._GreaterThan"]}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._GreaterThanEq"]}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._HalfOpenInterval"]}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._IndependentConstraint"]}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._IntegerInterval"]}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._Interval"]}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._LessThan"]}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._Multinomial"]}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "torch.distributions", "text": ["alias of torch.distributions.constraints._Stack"]}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "torch.distributions", "text": ["Registry to link constraints to transforms.", "Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "torch.distributions", "text": ["Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both).", "The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.", "Example:", "[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845"]}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Dirichlet distribution parameterized by concentration concentration.", "Example:", "concentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)"]}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.mean()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.variance()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "torch.distributions", "text": ["Bases: object", "Distribution is the abstract base class for probability distributions.", "Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.", "Returns the shape over which parameters are batched.", "Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0.", "Returns the shape of a single sample (without batching).", "Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size.", "Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Returns the mean of the distribution.", "Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.", "Generates n samples or n batches of samples if the distribution parameters are batched.", "Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation.", "Returns the standard deviation of the distribution.", "Returns a Constraint object representing this distribution\u2019s support.", "Returns the variance of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.arg_constraints()", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "torch.distributions", "text": ["Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict."]}, {"name": "torch.distributions.distribution.Distribution.batch_shape()", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "torch.distributions", "text": ["Returns the shape over which parameters are batched."]}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "torch.distributions", "text": ["Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 "]}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "torch.distributions", "text": ["Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape."]}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "torch.distributions", "text": ["Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0."]}, {"name": "torch.distributions.distribution.Distribution.event_shape()", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "torch.distributions", "text": ["Returns the shape of a single sample (without batching)."]}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "torch.distributions", "text": ["Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size."]}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "torch.distributions", "text": ["Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 "]}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "torch.distributions", "text": ["Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 "]}, {"name": "torch.distributions.distribution.Distribution.mean()", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "torch.distributions", "text": ["Returns the mean of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "torch.distributions", "text": ["Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape."]}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "torch.distributions", "text": ["Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched."]}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "torch.distributions", "text": ["Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched."]}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "torch.distributions", "text": ["Generates n samples or n batches of samples if the distribution parameters are batched."]}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "torch.distributions", "text": ["Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation."]}, {"name": "torch.distributions.distribution.Distribution.stddev()", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "torch.distributions", "text": ["Returns the standard deviation of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.support()", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "torch.distributions", "text": ["Returns a Constraint object representing this distribution\u2019s support."]}, {"name": "torch.distributions.distribution.Distribution.variance()", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "torch.distributions", "text": ["Returns the variance of the distribution."]}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Exponential distribution parameterized by rate.", "Example:", "rate (float or Tensor) \u2013 rate = 1 / scale of the distribution"]}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.mean()", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.stddev()", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.variance()", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below", "where \u03b8\\theta  denotes the natural parameters, t(x)t(x)  denotes the sufficient statistic, F(\u03b8)F(\\theta)  is the log normalizer function for a given family and k(x)k(x)  is the carrier measure.", "Note", "This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).", "Method to compute the entropy using Bregman divergence of the log normalizer."]}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "torch.distributions", "text": ["Method to compute the entropy using Bregman divergence of the log normalizer."]}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Fisher-Snedecor distribution parameterized by df1 and df2.", "Example:"]}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Gamma distribution parameterized by shape concentration and rate.", "Example:"]}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.mean()", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.variance()", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1  Bernoulli trials, the first kk  trials failed, before seeing a success.", "Samples are non-negative integers [0, inf\u2061\\inf ).", "Example:"]}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.mean()", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.variance()", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a Gumbel Distribution.", "Examples:"]}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.mean()", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.stddev()", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.variance()", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a half-Cauchy distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Cauchy distribution"]}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a half-normal distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Normal distribution"]}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.mean()", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.scale()", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.variance()", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Reinterprets some of the batch dims of a distribution as event dims.", "This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:"]}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.has_enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.has_rsample()", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.mean()", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.support()", "path": "distributions#torch.distributions.independent.Independent.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.independent.Independent.variance()", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "torch.distributions", "text": ["Compute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)  between two distributions.", "A batch of KL divergences of shape batch_shape.", "Tensor", "NotImplementedError \u2013 If the distribution types have not been registered via register_kl()."]}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "torch.distributions", "text": ["Decorator to register a pairwise function with kl_divergence(). Usage:", "Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation:", "you should register a third most-specific implementation, e.g.:"]}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a Kumaraswamy distribution.", "Example:"]}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Laplace distribution parameterized by loc and scale.", "Example:"]}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.mean()", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.stddev()", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.variance()", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta  to make the probability of the correlation matrix MM  generated from a Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3.", "L ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration)", "Example:", "References", "[1] Generating random correlation matrices based on vines and extended onion method, Daniel Lewandowski, Dorota Kurowicka, Harry Joe."]}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a log-normal distribution parameterized by loc and scale where:", "Example:"]}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.loc()", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.mean()", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.scale()", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.variance()", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag:", "Note", "The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:"]}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component.", "Examples:"]}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches.", "Note that total_count need not be specified if only log_prob() is called (see example below)", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.", "Example:"]}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.logits()", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.mean()", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.param_shape()", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.probs()", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.support()", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.variance()", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.", "The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}  or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}  or a lower-triangular matrix L\\mathbf{L}  with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.", "Note", "Only one of covariance_matrix or precision_matrix or scale_tril can be specified.", "Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition."]}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of failure of each Bernoulli trial is probs."]}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a normal (also called Gaussian) distribution parameterized by loc and scale.", "Example:"]}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.mean()", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.stddev()", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.normal.Normal.variance()", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a one-hot categorical distribution parameterized by probs or logits.", "Samples are one-hot coded vectors of size probs.size(-1).", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.", "See also: torch.distributions.Categorical() for specifications of probs and logits.", "Example:"]}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a Pareto Type 1 distribution.", "Example:"]}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.mean()", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.support()", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.variance()", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "torch.distributions", "text": ["Bases: torch.distributions.exp_family.ExponentialFamily", "Creates a Poisson distribution parameterized by rate, the rate parameter.", "Samples are nonnegative integers, with a pmf given by", "Example:", "rate (Number, Tensor) \u2013 the rate parameter"]}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.mean()", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.variance()", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution.", "Samples are logits of values in (0, 1). See [1] for more details.", "[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)", "[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)"]}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples.", "Example:"]}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable.", "Example:"]}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale.", "Example:"]}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.mean()", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.variance()", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:", "Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.", "An example for the usage of TransformedDistribution would be:", "For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical", "Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.", "Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.", "Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "torch.distributions", "text": ["Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "torch.distributions", "text": ["Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "torch.distributions", "text": ["Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "torch.distributions", "text": ["Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "torch.distributions", "text": ["Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "torch.distributions", "text": ["Transform via the mapping y=\u2223x\u2223y = |x| ."]}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "torch.distributions", "text": ["Transform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x ."]}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "torch.distributions", "text": ["Composes multiple transforms in a chain. The transforms being composed are responsible for caching."]}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "torch.distributions", "text": ["Transforms an uncontrained real vector xx  with length D\u2217(D\u22121)/2D*(D-1)/2  into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:"]}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "torch.distributions", "text": ["Transform via the mapping y=exp\u2061(x)y = \\exp(x) ."]}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "torch.distributions", "text": ["Wrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian()."]}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "torch.distributions", "text": ["Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.", "This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization."]}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "torch.distributions", "text": ["Transform via the mapping y=xexponenty = x^{\\text{exponent}} ."]}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "torch.distributions", "text": ["Unit Jacobian transform to reshape the rightmost part of a tensor.", "Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape()."]}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "torch.distributions", "text": ["Transform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}  and x=logit(y)x = \\text{logit}(y) ."]}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "torch.distributions", "text": ["Transform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)  then normalizing.", "This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms."]}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "torch.distributions", "text": ["Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().", "x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)"]}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "torch.distributions", "text": ["Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.", "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.", "This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization."]}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "torch.distributions", "text": ["Transform via the mapping y=tanh\u2061(x)y = \\tanh(x) .", "It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead.", "Note that one should use cache_size=1 when it comes to NaN/Inf values."]}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "torch.distributions", "text": ["Abstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution.", "Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:", "However the following will error when caching due to dependency reversal:", "Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().", "cache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.", "Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t.", "Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.", "Computes the log det jacobian log |dy/dx| given input and output.", "Infers the shape of the forward computation, given the input shape. Defaults to preserving shape.", "Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "torch.distributions", "text": ["Infers the shape of the forward computation, given the input shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.inv()", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "torch.distributions", "text": ["Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t."]}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "torch.distributions", "text": ["Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "torch.distributions", "text": ["Computes the log det jacobian log |dy/dx| given input and output."]}, {"name": "torch.distributions.transforms.Transform.sign()", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "torch.distributions", "text": ["Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms."]}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "Generates uniformly distributed random samples from the half-open interval [low, high).", "Example:"]}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.mean()", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.stddev()", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.support()", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.variance()", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "torch.distributions", "text": ["Bases: torch.distributions.distribution.Distribution", "A circular von Mises distribution.", "This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.", "The provided mean is the circular one.", "The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157.", "The provided variance is the circular one."]}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.mean()", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "torch.distributions", "text": ["The provided mean is the circular one."]}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "torch.distributions", "text": ["The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157."]}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "torch.distributions", "text": ["The provided variance is the circular one."]}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "torch.distributions", "text": ["Bases: torch.distributions.transformed_distribution.TransformedDistribution", "Samples from a two-parameter Weibull distribution."]}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.mean()", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "torch.distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.variance()", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "torch.distributions", "text": []}, {"name": "torch.div()", "path": "generated/torch.div#torch.div", "type": "torch", "text": ["Divides each element of the input input by the corresponding element of other.", "Note", "By default, this performs a \u201ctrue\u201d division like Python 3. See the rounding_mode argument for floor division.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.", "rounding_mode (str, optional) \u2013 ", "Type of rounding applied to the result:", "Examples:"]}, {"name": "torch.divide()", "path": "generated/torch.divide#torch.divide", "type": "torch", "text": ["Alias for torch.div()."]}, {"name": "torch.dot()", "path": "generated/torch.dot#torch.dot", "type": "torch", "text": ["Computes the dot product of two 1D tensors.", "Note", "Unlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.", "{out} \u2013 ", "Example:"]}, {"name": "torch.dstack()", "path": "generated/torch.dstack#torch.dstack", "type": "torch", "text": ["Stack tensors in sequence depthwise (along third axis).", "This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d().", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.eig()", "path": "generated/torch.eig#torch.eig", "type": "torch", "text": ["Computes the eigenvalues and eigenvectors of a real square matrix.", "Note", "Since eigenvalues and eigenvectors might be complex, backward pass is supported only if eigenvalues and eigenvectors are all real valued.", "When input is on CUDA, torch.eig() causes host-device synchronization.", "out (tuple, optional) \u2013 the output tensors", "A namedtuple (eigenvalues, eigenvectors) containing", "(Tensor, Tensor)", "Example:"]}, {"name": "torch.einsum()", "path": "generated/torch.einsum#torch.einsum", "type": "torch", "text": ["Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.", "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention, given by equation. The details of this format are described below, but the general idea is to label every dimension of the input operands with some subscript and define which subscripts are part of the output. The output is then computed by summing the product of the elements of the operands along the dimensions whose subscripts are not part of the output. For example, matrix multiplication can be computed using einsum as torch.einsum(\u201cij,jk->ik\u201d, A, B). Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).", "Equation:", "The equation string specifies the subscripts (lower case letters [\u2018a\u2019, \u2018z\u2019]) for each dimension of the input operands in the same order as the dimensions, separating subcripts for each operand by a comma (\u2018,\u2019), e.g. \u2018ij,jk\u2019 specify subscripts for two 2D operands. The dimensions labeled with the same subscript must be broadcastable, that is, their size must either match or be 1. The exception is if a subscript is repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order. The output is computed by multiplying the input operands element-wise, with their dimensions aligned based on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.", "Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation followed by the subscripts for the output. For instance, the following equation computes the transpose of a matrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and at most once for the output.", "Ellipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis. Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts, e.g. for an input operand with 5 dimensions, the ellipsis in the equation \u2018ab\u2026c\u2019 cover the third and fourth dimensions. The ellipsis does not need to cover the same number of dimensions across the operands but the \u2018shape\u2019 of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis will come first in the output (left-most dimensions), before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements batch matrix multiplication \u2018\u2026ij,\u2026jk\u2019.", "A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis, arrow and comma) but something like \u2018\u2026\u2019 is not valid. An empty string \u2018\u2019 is valid for scalar operands.", "Note", "torch.einsum handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.", "Note", "This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) can optimize the formula for you.", "Examples:"]}, {"name": "torch.empty()", "path": "generated/torch.empty#torch.empty", "type": "torch", "text": ["Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.empty_like()", "path": "generated/torch.empty_like#torch.empty_like", "type": "torch", "text": ["Returns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}, {"name": "torch.empty_strided()", "path": "generated/torch.empty_strided#torch.empty_strided", "type": "torch", "text": ["Returns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument size and stride respectively. torch.empty_strided(size, stride) is equivalent to torch.empty(size).as_strided(size, stride).", "Warning", "More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "torch", "text": ["Context-manager that enables gradient calculation.", "Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator. (Make sure to instantiate with parenthesis.)", "Example:"]}, {"name": "torch.eq()", "path": "generated/torch.eq#torch.eq", "type": "torch", "text": ["Computes element-wise equality", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is equal to other and False elsewhere", "Example:"]}, {"name": "torch.equal()", "path": "generated/torch.equal#torch.equal", "type": "torch", "text": ["True if two tensors have the same size and elements, False otherwise.", "Example:"]}, {"name": "torch.erf()", "path": "generated/torch.erf#torch.erf", "type": "torch", "text": ["Computes the error function of each element. The error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.erfc()", "path": "generated/torch.erfc#torch.erfc", "type": "torch", "text": ["Computes the complementary error function of each element of input. The complementary error function is defined as follows:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.erfinv()", "path": "generated/torch.erfinv#torch.erfinv", "type": "torch", "text": ["Computes the inverse error function of each element of input. The inverse error function is defined in the range (\u22121,1)(-1, 1)  as:", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.exp()", "path": "generated/torch.exp#torch.exp", "type": "torch", "text": ["Returns a new tensor with the exponential of the elements of the input tensor input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.exp2()", "path": "generated/torch.exp2#torch.exp2", "type": "torch", "text": ["Computes the base two exponential function of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.expm1()", "path": "generated/torch.expm1#torch.expm1", "type": "torch", "text": ["Returns a new tensor with the exponential of the elements minus 1 of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.eye()", "path": "generated/torch.eye#torch.eye", "type": "torch", "text": ["Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.", "A 2-D tensor with ones on the diagonal and zeros elsewhere", "Tensor", "Example:"]}, {"name": "torch.fake_quantize_per_channel_affine()", "path": "generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine", "type": "torch", "text": ["Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.", "A newly fake_quantized per channel tensor", "Tensor", "Example:"]}, {"name": "torch.fake_quantize_per_tensor_affine()", "path": "generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine", "type": "torch", "text": ["Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.", "A newly fake_quantized tensor", "Tensor", "Example:"]}, {"name": "torch.fft", "path": "fft", "type": "torch.fft", "text": ["Discrete Fourier transforms and related functions.", "Computes the one dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft()), these correspond to:", "Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (no normalization).", "Computes the one dimensional inverse discrete Fourier transform of input.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft()), these correspond to:", "Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Computes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (no normalization).", "The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls:", "Computes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls:", "Computes the N dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (no normalization).", "The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls:", "Computes the N dimensional inverse discrete Fourier transform of input.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls:", "Computes the one dimensional Fourier transform of real-valued input.", "The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft()), these correspond to:", "Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (no normalization).", "Compare against the full output from fft():", "Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued.", "Computes the inverse of rfft().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft()), these correspond to:", "Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length:", "So, it is recommended to always pass the signal length n:", "Computes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default.", "The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "Compared against the full output from fft2(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft():", "Computes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s.", "Computes the N-dimensional discrete Fourier transform of real input.", "The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "Compared against the full output from fftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft():", "Computes the inverse of rfftn().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s.", "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Note", "hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().", "Note", "Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft()), these correspond to:", "Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (no normalization).", "Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:", "Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies:", "Like with irfft(), the output length must be given in order to recover an even length output:", "Computes the inverse of hfft().", "input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft()), these correspond to:", "Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Compare against the full output from ifft():", "Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Note", "By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.", "For even input, we can see the Nyquist frequency at f[2] is given as negative:", "Computes the sample frequencies for rfft() with a signal of size n.", "Note", "rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.", "Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500])", "Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.", "Note", "By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().", "Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor.", "This also works for multi-dimensional transforms:", "fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift().", "Similarly, we can convert the frequency domain components to centered convention by applying fftshift().", "The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order:", "Inverse of fftshift().", "A round-trip through fftshift() and ifftshift() gives the same result:"]}, {"name": "torch.fft.fft()", "path": "fft#torch.fft.fft", "type": "torch.fft", "text": ["Computes the one dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft()), these correspond to:", "Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (no normalization)."]}, {"name": "torch.fft.fft2()", "path": "fft#torch.fft.fft2", "type": "torch.fft", "text": ["Computes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (no normalization).", "The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.fftfreq()", "path": "fft#torch.fft.fftfreq", "type": "torch.fft", "text": ["Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Note", "By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.", "For even input, we can see the Nyquist frequency at f[2] is given as negative:"]}, {"name": "torch.fft.fftn()", "path": "fft#torch.fft.fftn", "type": "torch.fft", "text": ["Computes the N dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (no normalization).", "The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.fftshift()", "path": "fft#torch.fft.fftshift", "type": "torch.fft", "text": ["Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.", "Note", "By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().", "Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor.", "This also works for multi-dimensional transforms:", "fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift().", "Similarly, we can convert the frequency domain components to centered convention by applying fftshift().", "The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order:"]}, {"name": "torch.fft.hfft()", "path": "fft#torch.fft.hfft", "type": "torch.fft", "text": ["Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Note", "hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().", "Note", "Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft()), these correspond to:", "Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (no normalization).", "Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:", "Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies:", "Like with irfft(), the output length must be given in order to recover an even length output:"]}, {"name": "torch.fft.ifft()", "path": "fft#torch.fft.ifft", "type": "torch.fft", "text": ["Computes the one dimensional inverse discrete Fourier transform of input.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft()), these correspond to:", "Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (normalize by 1/n)."]}, {"name": "torch.fft.ifft2()", "path": "fft#torch.fft.ifft2", "type": "torch.fft", "text": ["Computes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.ifftn()", "path": "fft#torch.fft.ifftn", "type": "torch.fft", "text": ["Computes the N dimensional inverse discrete Fourier transform of input.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.ifftshift()", "path": "fft#torch.fft.ifftshift", "type": "torch.fft", "text": ["Inverse of fftshift().", "A round-trip through fftshift() and ifftshift() gives the same result:"]}, {"name": "torch.fft.ihfft()", "path": "fft#torch.fft.ihfft", "type": "torch.fft", "text": ["Computes the inverse of hfft().", "input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft()), these correspond to:", "Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Compare against the full output from ifft():"]}, {"name": "torch.fft.irfft()", "path": "fft#torch.fft.irfft", "type": "torch.fft", "text": ["Computes the inverse of rfft().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft()), these correspond to:", "Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length:", "So, it is recommended to always pass the signal length n:"]}, {"name": "torch.fft.irfft2()", "path": "fft#torch.fft.irfft2", "type": "torch.fft", "text": ["Computes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.irfftn()", "path": "fft#torch.fft.irfftn", "type": "torch.fft", "text": ["Computes the inverse of rfftn().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.rfft()", "path": "fft#torch.fft.rfft", "type": "torch.fft", "text": ["Computes the one dimensional Fourier transform of real-valued input.", "The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft()), these correspond to:", "Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (no normalization).", "Compare against the full output from fft():", "Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued."]}, {"name": "torch.fft.rfft2()", "path": "fft#torch.fft.rfft2", "type": "torch.fft", "text": ["Computes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default.", "The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "Compared against the full output from fft2(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.rfftfreq()", "path": "fft#torch.fft.rfftfreq", "type": "torch.fft", "text": ["Computes the sample frequencies for rfft() with a signal of size n.", "Note", "rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.", "Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500])"]}, {"name": "torch.fft.rfftn()", "path": "fft#torch.fft.rfftn", "type": "torch.fft", "text": ["Computes the N-dimensional discrete Fourier transform of real input.", "The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "Compared against the full output from fftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fix()", "path": "generated/torch.fix#torch.fix", "type": "torch", "text": ["Alias for torch.trunc()"]}, {"name": "torch.flatten()", "path": "generated/torch.flatten#torch.flatten", "type": "torch", "text": ["Flattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim are passed, only dimensions starting with start_dim and ending with end_dim are flattened. The order of elements in input is unchanged.", "Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view, or copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned.", "Note", "Flattening a zero-dimensional tensor will return a one-dimensional view.", "Example:"]}, {"name": "torch.flip()", "path": "generated/torch.flip#torch.flip", "type": "torch", "text": ["Reverse the order of a n-D tensor along given axis in dims.", "Note", "torch.flip makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flip, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flip is expected to be slower than np.flip.", "Example:"]}, {"name": "torch.fliplr()", "path": "generated/torch.fliplr#torch.fliplr", "type": "torch", "text": ["Flip tensor in the left/right direction, returning a new tensor.", "Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.", "Note", "Requires the tensor to be at least 2-D.", "Note", "torch.fliplr makes a copy of input\u2019s data. This is different from NumPy\u2019s np.fliplr, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.fliplr is expected to be slower than np.fliplr.", "input (Tensor) \u2013 Must be at least 2-dimensional.", "Example:"]}, {"name": "torch.flipud()", "path": "generated/torch.flipud#torch.flipud", "type": "torch", "text": ["Flip tensor in the up/down direction, returning a new tensor.", "Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.", "Note", "Requires the tensor to be at least 1-D.", "Note", "torch.flipud makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flipud, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.", "input (Tensor) \u2013 Must be at least 1-dimensional.", "Example:"]}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "torch.Storage", "text": ["Casts this storage to bfloat16 type", "Casts this storage to bool type", "Casts this storage to byte type", "Casts this storage to char type", "Returns a copy of this storage", "Casts this storage to complex double type", "Casts this storage to complex float type", "Returns a CPU copy of this storage if it\u2019s not already on the CPU", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "Casts this storage to double type", "Casts this storage to float type", "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.", "Casts this storage to half type", "Casts this storage to int type", "Casts this storage to long type", "Copies the storage to pinned memory, if it\u2019s not already pinned.", "Moves the storage to shared memory.", "This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.", "Returns: self", "Casts this storage to short type", "Returns a list containing the elements of this storage", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.FloatStorage.bfloat16()", "path": "storage#torch.FloatStorage.bfloat16", "type": "torch.Storage", "text": ["Casts this storage to bfloat16 type"]}, {"name": "torch.FloatStorage.bool()", "path": "storage#torch.FloatStorage.bool", "type": "torch.Storage", "text": ["Casts this storage to bool type"]}, {"name": "torch.FloatStorage.byte()", "path": "storage#torch.FloatStorage.byte", "type": "torch.Storage", "text": ["Casts this storage to byte type"]}, {"name": "torch.FloatStorage.char()", "path": "storage#torch.FloatStorage.char", "type": "torch.Storage", "text": ["Casts this storage to char type"]}, {"name": "torch.FloatStorage.clone()", "path": "storage#torch.FloatStorage.clone", "type": "torch.Storage", "text": ["Returns a copy of this storage"]}, {"name": "torch.FloatStorage.complex_double()", "path": "storage#torch.FloatStorage.complex_double", "type": "torch.Storage", "text": ["Casts this storage to complex double type"]}, {"name": "torch.FloatStorage.complex_float()", "path": "storage#torch.FloatStorage.complex_float", "type": "torch.Storage", "text": ["Casts this storage to complex float type"]}, {"name": "torch.FloatStorage.copy_()", "path": "storage#torch.FloatStorage.copy_", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.cpu()", "path": "storage#torch.FloatStorage.cpu", "type": "torch.Storage", "text": ["Returns a CPU copy of this storage if it\u2019s not already on the CPU"]}, {"name": "torch.FloatStorage.cuda()", "path": "storage#torch.FloatStorage.cuda", "type": "torch.Storage", "text": ["Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned."]}, {"name": "torch.FloatStorage.data_ptr()", "path": "storage#torch.FloatStorage.data_ptr", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.device", "path": "storage#torch.FloatStorage.device", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.double()", "path": "storage#torch.FloatStorage.double", "type": "torch.Storage", "text": ["Casts this storage to double type"]}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.element_size()", "path": "storage#torch.FloatStorage.element_size", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.fill_()", "path": "storage#torch.FloatStorage.fill_", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.float()", "path": "storage#torch.FloatStorage.float", "type": "torch.Storage", "text": ["Casts this storage to float type"]}, {"name": "torch.FloatStorage.from_buffer()", "path": "storage#torch.FloatStorage.from_buffer", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.from_file()", "path": "storage#torch.FloatStorage.from_file", "type": "torch.Storage", "text": ["If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed."]}, {"name": "torch.FloatStorage.get_device()", "path": "storage#torch.FloatStorage.get_device", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.half()", "path": "storage#torch.FloatStorage.half", "type": "torch.Storage", "text": ["Casts this storage to half type"]}, {"name": "torch.FloatStorage.int()", "path": "storage#torch.FloatStorage.int", "type": "torch.Storage", "text": ["Casts this storage to int type"]}, {"name": "torch.FloatStorage.is_cuda", "path": "storage#torch.FloatStorage.is_cuda", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.is_pinned()", "path": "storage#torch.FloatStorage.is_pinned", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.is_shared()", "path": "storage#torch.FloatStorage.is_shared", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.is_sparse", "path": "storage#torch.FloatStorage.is_sparse", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.long()", "path": "storage#torch.FloatStorage.long", "type": "torch.Storage", "text": ["Casts this storage to long type"]}, {"name": "torch.FloatStorage.new()", "path": "storage#torch.FloatStorage.new", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.pin_memory()", "path": "storage#torch.FloatStorage.pin_memory", "type": "torch.Storage", "text": ["Copies the storage to pinned memory, if it\u2019s not already pinned."]}, {"name": "torch.FloatStorage.resize_()", "path": "storage#torch.FloatStorage.resize_", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.share_memory_()", "path": "storage#torch.FloatStorage.share_memory_", "type": "torch.Storage", "text": ["Moves the storage to shared memory.", "This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.", "Returns: self"]}, {"name": "torch.FloatStorage.short()", "path": "storage#torch.FloatStorage.short", "type": "torch.Storage", "text": ["Casts this storage to short type"]}, {"name": "torch.FloatStorage.size()", "path": "storage#torch.FloatStorage.size", "type": "torch.Storage", "text": []}, {"name": "torch.FloatStorage.tolist()", "path": "storage#torch.FloatStorage.tolist", "type": "torch.Storage", "text": ["Returns a list containing the elements of this storage"]}, {"name": "torch.FloatStorage.type()", "path": "storage#torch.FloatStorage.type", "type": "torch.Storage", "text": ["Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.float_power()", "path": "generated/torch.float_power#torch.float_power", "type": "torch", "text": ["Raises input to the power of exponent, elementwise, in double precision. If neither input is complex returns a torch.float64 tensor, and if one or more inputs is complex returns a torch.complex128 tensor.", "Note", "This function always computes in double precision, unlike torch.pow(), which implements more typical type promotion. This is useful when the computation needs to be performed in a wider or more precise dtype, or the results of the computation may contain fractional values not representable in the input dtypes, like when an integer base is raised to a negative integer exponent.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.floor()", "path": "generated/torch.floor#torch.floor", "type": "torch", "text": ["Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.floor_divide()", "path": "generated/torch.floor_divide#torch.floor_divide", "type": "torch", "text": ["Warning", "This function\u2019s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.", "Computes input divided by other, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmax()", "path": "generated/torch.fmax#torch.fmax", "type": "torch", "text": ["Computes the element-wise maximum of input and other.", "This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated.", "This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function.", "Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmin()", "path": "generated/torch.fmin#torch.fmin", "type": "torch", "text": ["Computes the element-wise minimum of input and other.", "This is like torch.minimum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. Only if both elements are NaN is NaN propagated.", "This function is a wrapper around C++\u2019s std::fmin and is similar to NumPy\u2019s fmin function.", "Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmod()", "path": "generated/torch.fmod#torch.fmod", "type": "torch", "text": ["Computes the element-wise remainder of division.", "The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend input.", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "Note", "When the divisor is zero, returns NaN for floating point dtypes on both CPU and GPU; raises RuntimeError for integer division by zero on CPU; Integer division by zero on GPU may return any value.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.frac()", "path": "generated/torch.frac#torch.frac", "type": "torch", "text": ["Computes the fractional portion of each element in input.", "Example:"]}, {"name": "torch.from_numpy()", "path": "generated/torch.from_numpy#torch.from_numpy", "type": "torch", "text": ["Creates a Tensor from a numpy.ndarray.", "The returned tensor and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable.", "It currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16, numpy.complex64, numpy.complex128, numpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8, and numpy.bool.", "Example:"]}, {"name": "torch.full()", "path": "generated/torch.full#torch.full", "type": "torch", "text": ["Creates a tensor of size size filled with fill_value. The tensor\u2019s dtype is inferred from fill_value.", "Example:"]}, {"name": "torch.full_like()", "path": "generated/torch.full_like#torch.full_like", "type": "torch", "text": ["Returns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)."]}, {"name": "torch.futures", "path": "futures", "type": "torch.futures", "text": ["Warning", "The torch.futures package is experimental and subject to change.", "This package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.", "Wrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.", "Return True if this Future is done. A Future is done if it has a result or an exception.", "Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future.", "Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "result (object) \u2013 the result object of this Future.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Block until the value of this Future is ready.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.", "Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.", "futures (list) \u2013 a list of Future objects.", "Returns a Future object to a list of the passed in Futures.", "Waits for all provided futures to be complete, and returns the list of completed values.", "futures (list) \u2013 a list of Future object.", "A list of the completed Future results. This method will throw an error if wait on any Future throws."]}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "torch.futures", "text": ["Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.", "futures (list) \u2013 a list of Future objects.", "Returns a Future object to a list of the passed in Futures."]}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "torch.futures", "text": ["Wrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.", "Return True if this Future is done. A Future is done if it has a result or an exception.", "Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future.", "Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "result (object) \u2013 the result object of this Future.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Block until the value of this Future is ready.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error."]}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "torch.futures", "text": []}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "torch.futures", "text": ["Return True if this Future is done. A Future is done if it has a result or an exception."]}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "torch.futures", "text": ["Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future."]}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "torch.futures", "text": ["Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "result (object) \u2013 the result object of this Future."]}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "torch.futures", "text": ["Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes."]}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "torch.futures", "text": []}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "torch.futures", "text": ["Block until the value of this Future is ready.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error."]}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "torch.futures", "text": ["Waits for all provided futures to be complete, and returns the list of completed values.", "futures (list) \u2013 a list of Future object.", "A list of the completed Future results. This method will throw an error if wait on any Future throws."]}, {"name": "torch.fx", "path": "fx", "type": "torch.fx", "text": ["This feature is under a Beta release and its API may change.", "FX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action:", "The symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation.", "The intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied.", "Python code generation is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph\u2019s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph.", "Taken together, this pipeline of components (symbolic tracing \u2192 intermediate representation \u2192 transforms \u2192 Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX!", "Several example transformations can be found at the examples repository.", "What is an FX transform? Essentially, it\u2019s a function that looks like this.", "Your transform will take in an torch.nn.Module, acquire a Graph from it, do some modifications, and return a new torch.nn.Module. You should think of the torch.nn.Module that your FX transform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for composability.", "Note", "It is also possible to modify an existing GraphModule instead of creating a new one, like so:", "Note that you MUST call GraphModule.recompile() to bring the generated forward() method on the GraphModule in sync with the modified Graph.", "Given that you\u2019ve passed in a torch.nn.Module that has been traced into a Graph, there are now two primary approaches you can take to building a new Graph.", "Full treatment of the semantics of graphs can be found in the Graph documentation, but we are going to cover the basics here. A Graph is a data structure that represents a method on a GraphModule. The information that this requires is:", "All three of these concepts are represented with Node instances. Let\u2019s see what we mean by that with a short example:", "Here we define a module MyModule for demonstration purposes, instantiate it, symbolically trace it, then call the Graph.print_tabular() method to print out a table showing the nodes of this Graph:", "opcode", "name", "target", "args", "kwargs", "placeholder", "x", "x", "()", "{}", "get_attr", "linear_weight", "linear.weight", "()", "{}", "call_function", "add_1", "<built-in function add>", "(x, linear_weight)", "{}", "call_module", "linear_1", "linear", "(add_1,)", "{}", "call_method", "relu_1", "relu", "(linear_1,)", "{}", "call_function", "sum_1", "<built-in method sum \u2026>", "(relu_1,)", "{\u2018dim\u2019: -1}", "call_function", "topk_1", "<built-in method topk \u2026>", "(sum_1, 3)", "{}", "output", "output", "output", "(topk_1,)", "{}", "We can use this information to answer the questions we posed above.", "Given that we now know the basics of how code is represented in FX, we can now explore how we would edit a Graph.", "One approach to building this new Graph is to directly manipulate your old one. To aid in this, we can simply take the Graph we obtain from symbolic tracing and modify it. For example, let\u2019s say we desire to replace torch.add() calls with torch.mul() calls.", "We can also do more involved Graph rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the Graph documentation. An example of using these APIs to append a torch.relu() call can be found below.", "For simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter.", "FX also provides another level of automation on top of direct graph manipulation. The replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing Graphs. It allows you to specify a pattern and replacement function and it will trace through those functions, find instances of the group of operations in the pattern graph, and replace those instances with copies of the replacement graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.", "Another way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph.", "To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph.", "In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules.", "A worked example of using Proxys for Graph manipulation can be found here.", "A useful code organizational pattern in FX is to loop over all the Nodes in a Graph and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys. For example, suppose we want to run a GraphModule and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime. That might look like:", "As you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the Interpreter class, which encompasses the above logic in a way that certain aspects of the interpreter\u2019s execution can be overridden via method overrides.", "In addition to executing operations, we can also generate a new Graph by feeding Proxy values through an interpreter. Similarly, we provide the Transformer class to encompass this pattern. Transformer behaves similarly to Interpreter, but instead of calling the run method to get a concrete output value from the Module, you would call the Transformer.transform() method to return a new GraphModule which was subject to any transformation rules you installed as overridden methods.", "Often in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code.", "If you\u2019re not familiar with debuggers, please see the auxiliary section Available Debuggers.", "Because the output of most deep learning modules consists of floating point torch.Tensor instances, checking for equivalence between the results of two torch.nn.Module is not as straightforward as doing a simple equality check. To motivate this, let\u2019s use an example:", "Here, we\u2019ve tried to check equality of the values of two deep learning models with the == equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use torch.allclose() instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold:", "This is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation.", "Because FX generates the forward() function on GraphModules, using traditional debugging techniques like print statements or pdb is not as straightfoward. Luckily, we have several techniques we can use for debugging the generated code.", "Invoke pdb to step into the running program. Although the code that represents the Graph is not in any source file, we can still step into it manually using pdb when the forward pass is invoked.", "If you\u2019d like to run the same code multiple times, then it can be a bit tedious to step to the right code with pdb. In that case, one approach is to simply copy-paste the generated forward pass into your code and examine it from there.", "GraphModule.to_folder() is a method in GraphModule that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using to_folder.", "After running the above example, we can then look at the code within foo/module.py and modify it as desired (e.g. adding print statements or using pdb) to debug the generated code.", "Now that we\u2019ve identified that a transformation is creating incorrect code, it\u2019s time to debug the transformation itself. First, we\u2019ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our GraphModule transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module:", "Using the utility functions above, we can compare our traced Module before and after we\u2019ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it\u2019s still not clear what\u2019s going wrong, a debugger like pdb can be a good next step.", "Going off of the example above, consider the following code:", "Using the above example, let\u2019s say that the call to print(traced) showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a pdb session. We can see what\u2019s happening during the transform by breaking on transform_graph(traced), then pressing s to \u201cstep into\u201d the call to transform_graph(traced).", "We may also have good luck by editing the print_tabular method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node\u2019s input_nodes and users.)", "The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d.", "IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb).", "FX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is tracing in that it executes the program (really a torch.nn.Module or function) to record operations. It is symbolic in that the data flowing through the program during this execution is not real data, but rather symbols (Proxy in FX parlance).", "Although symbolic tracing works for most neural net code, it has some limitations.", "The main limitation of symbolic tracing is it does not currently support dynamic control flow. That is, loops or if statements where the condition may depend on the input values of the program.", "For example, let\u2019s examine the following program:", "The condition to the if statement relies on the value of dim0, which eventually relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens.", "On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model\u2019s architecture based on hyper-parameters. As a concrete example:", "The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing.", "Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by passing constant values during symbolic tracing:", "In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather than tracing through them.", "FX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are things that are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example:", "The error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API:", "The Tracer class is the class that underlies the implementation of symbolic_trace. The behavior of tracing can be customized by subclassing Tracer, like so:", "Leaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard torch.nn module instances. For example:", "The set of leaf modules can be customized by overriding Tracer.is_leaf_module().", "Tensor constructors (e.g. torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_coo_tensor) are currently not traceable.", "Type annotations", "Symbolic tracing API", "Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.", "a Module created from the recorded operations from root.", "GraphModule", "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through:", "This function can also equivalently be used as a decorator:", "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.", "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called", "GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.", "Warning", "When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.", "Construct a GraphModule.", "Return the Python code generated from the Graph underlying this GraphModule.", "Return the Graph underlying this GraphModule", "Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date.", "Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>", "Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function.", "For example, the following code", "Will produce the following Graph:", "For the semantics of operations represented in the Graph, please see Node.", "Construct an empty Graph.", "Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be", "Returns", "The newly created and inserted call_function node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.", "The newly created and inserted call_method node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Insert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.", "The newly-created and inserted call_module node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Create a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().", "The newly-created and inserted node.", "Erases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.", "to_erase (Node) \u2013 The Node to erase from the Graph.", "Insert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.", "The newly-created and inserted get_attr node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Copy all nodes from a given graph into self.", "The value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.", "Set the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "n (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Set the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "n (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root", "root (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.", "Copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:", "Get the list of Nodes that constitute this Graph.", "Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.", "A doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.", "Insert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Insert a placeholder node into the Graph. A placeholder represents a function input.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Prints the intermediate representation of the graph in tabular format.", "Turn this Graph into valid Python code.", "root_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.", "The string source code generated from this Graph.", "Node is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:", "Return all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.", "List of Nodes that appear in the args and kwargs of this Node, in that order.", "Insert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)", "x (Node) \u2013 The node to put after this node. Must be a member of the same graph.", "The tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "The dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "Returns the next Node in the linked list of Nodes.", "The next Node in the linked list of Nodes.", "Insert x before this node in the list of nodes in the graph. Example:", "x (Node) \u2013 The node to put before this node. Must be a member of the same graph.", "Returns the previous Node in the linked list of Nodes.", "The previous Node in the linked list of Nodes.", "Replace all uses of self in the Graph with the Node replace_with.", "replace_with (Node) \u2013 The node to replace all uses of self with.", "The list of Nodes on which this change was made.", "Tracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m).", "Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.", "Method that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.", "By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.", "This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.", "The return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.", "A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.", "By default, the behavior includes:", "Given a non-Proxy Tensor object, emit IR for various cases:", "This method can be overridden to support more types.", "a (Any) \u2013 The value to be emitted as an Argument in the Graph.", "The value a converted into the appropriate Argument", "Create placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs.", "A method to specify whether a given nn.Module is a \u201cleaf\u201d module.", "Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.", "Helper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.", "mod (str) \u2013 The Module to retrieve the qualified name for.", "Trace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.", "Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.", "root (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.", "A Graph representing the semantics of the passed-in root.", "Proxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph.", "If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph.", "An Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.", "Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so:", "module (GraphModule) \u2013 The module to be executed", "Execute a call_function node and return the result.", "Any: The value returned by the function invocation", "Execute a call_method node and return the result.", "Any: The value returned by the method invocation", "Execute a call_module node and return the result.", "Any: The value returned by the module invocation", "Fetch the concrete values of args and kwargs of node n from the current execution environment.", "n (Node) \u2013 The node for which args and kwargs should be fetched.", "args and kwargs with concrete values for n.", "Tuple[Tuple, Dict]", "Fetch an attribute from the Module hierarchy of self.module.", "target (str) \u2013 The fully-qualfiied name of the attribute to fetch", "The value of the attribute.", "Any", "Execute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.", "The value of the attribute that was retrieved", "Any", "Recursively descend through args and look up the concrete value for each Node in the current execution environment.", "Execute an output node. This really just retrieves the value referenced by the output node and returns it.", "The return value referenced by the output node", "Any", "Execute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.", "The argument value that was retrieved.", "Any", "Run module via interpretation and return the result.", "The value returned from executing the Module", "Any", "Run a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op", "n (Node) \u2013 The Node to execute", "The result of executing n", "Any", "Transformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically.", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so:", "module (GraphModule) \u2013 The Module to be transformed.", "Execute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.", "Execute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.", "Transform self.module and return the transformed GraphModule.", "Matches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).", "A list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as:", "List[Match]", "Examples:", "The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m).", "The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph.", "When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.)", "One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing", "with", "In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement.", "After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this:"]}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "torch.fx", "text": ["Graph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function.", "For example, the following code", "Will produce the following Graph:", "For the semantics of operations represented in the Graph, please see Node.", "Construct an empty Graph.", "Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be", "Returns", "The newly created and inserted call_function node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.", "The newly created and inserted call_method node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Insert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.", "The newly-created and inserted call_module node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node().", "Create a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().", "The newly-created and inserted node.", "Erases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.", "to_erase (Node) \u2013 The Node to erase from the Graph.", "Insert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.", "The newly-created and inserted get_attr node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Copy all nodes from a given graph into self.", "The value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.", "Set the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "n (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Set the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "n (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__.", "Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root", "root (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.", "Copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:", "Get the list of Nodes that constitute this Graph.", "Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.", "A doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.", "Insert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Insert a placeholder node into the Graph. A placeholder represents a function input.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node.", "Prints the intermediate representation of the graph in tabular format.", "Turn this Graph into valid Python code.", "root_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.", "The string source code generated from this Graph."]}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "torch.fx", "text": ["Insert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be", "Returns", "The newly created and inserted call_function node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node()."]}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "torch.fx", "text": ["Insert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.", "The newly created and inserted call_method node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node()."]}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "torch.fx", "text": ["Insert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.", "The newly-created and inserted call_module node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node()."]}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "torch.fx", "text": ["Create a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().", "The newly-created and inserted node."]}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "torch.fx", "text": ["Erases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.", "to_erase (Node) \u2013 The Node to erase from the Graph."]}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "torch.fx", "text": ["Insert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.", "The newly-created and inserted get_attr node.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node."]}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "torch.fx", "text": ["Copy all nodes from a given graph into self.", "The value in self that is now equivalent to the output value in g, if g had an output node. None otherwise."]}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "torch.fx", "text": ["Set the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "n (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__."]}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "torch.fx", "text": ["Set the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits:", "n (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.", "A resource manager that will restore the insert point on __exit__."]}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "torch.fx", "text": ["Runs various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root", "root (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule."]}, {"name": "torch.fx.Graph.nodes()", "path": "fx#torch.fx.Graph.nodes", "type": "torch.fx", "text": ["Get the list of Nodes that constitute this Graph.", "Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.", "A doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order."]}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "torch.fx", "text": ["Copy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example:"]}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "torch.fx", "text": ["Insert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node."]}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "torch.fx", "text": ["Insert a placeholder node into the Graph. A placeholder represents a function input.", "Note", "The same insertion point and type expression rules apply for this method as Graph.create_node."]}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "torch.fx", "text": ["Prints the intermediate representation of the graph in tabular format."]}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "torch.fx", "text": ["Turn this Graph into valid Python code.", "root_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.", "The string source code generated from this Graph."]}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "torch.fx", "text": ["Construct an empty Graph."]}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "torch.fx", "text": ["GraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.", "Warning", "When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.", "Construct a GraphModule.", "Return the Python code generated from the Graph underlying this GraphModule.", "Return the Graph underlying this GraphModule", "Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date.", "Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>"]}, {"name": "torch.fx.GraphModule.code()", "path": "fx#torch.fx.GraphModule.code", "type": "torch.fx", "text": ["Return the Python code generated from the Graph underlying this GraphModule."]}, {"name": "torch.fx.GraphModule.graph()", "path": "fx#torch.fx.GraphModule.graph", "type": "torch.fx", "text": ["Return the Graph underlying this GraphModule"]}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "torch.fx", "text": ["Recompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date."]}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "torch.fx", "text": ["Dumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>"]}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "torch.fx", "text": ["Construct a GraphModule."]}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "torch.fx", "text": ["An Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.", "Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so:", "module (GraphModule) \u2013 The module to be executed", "Execute a call_function node and return the result.", "Any: The value returned by the function invocation", "Execute a call_method node and return the result.", "Any: The value returned by the method invocation", "Execute a call_module node and return the result.", "Any: The value returned by the module invocation", "Fetch the concrete values of args and kwargs of node n from the current execution environment.", "n (Node) \u2013 The node for which args and kwargs should be fetched.", "args and kwargs with concrete values for n.", "Tuple[Tuple, Dict]", "Fetch an attribute from the Module hierarchy of self.module.", "target (str) \u2013 The fully-qualfiied name of the attribute to fetch", "The value of the attribute.", "Any", "Execute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.", "The value of the attribute that was retrieved", "Any", "Recursively descend through args and look up the concrete value for each Node in the current execution environment.", "Execute an output node. This really just retrieves the value referenced by the output node and returns it.", "The return value referenced by the output node", "Any", "Execute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.", "The argument value that was retrieved.", "Any", "Run module via interpretation and return the result.", "The value returned from executing the Module", "Any", "Run a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op", "n (Node) \u2013 The Node to execute", "The result of executing n", "Any"]}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "torch.fx", "text": ["Execute a call_function node and return the result.", "Any: The value returned by the function invocation"]}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "torch.fx", "text": ["Execute a call_method node and return the result.", "Any: The value returned by the method invocation"]}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "torch.fx", "text": ["Execute a call_module node and return the result.", "Any: The value returned by the module invocation"]}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "torch.fx", "text": ["Fetch the concrete values of args and kwargs of node n from the current execution environment.", "n (Node) \u2013 The node for which args and kwargs should be fetched.", "args and kwargs with concrete values for n.", "Tuple[Tuple, Dict]"]}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "torch.fx", "text": ["Fetch an attribute from the Module hierarchy of self.module.", "target (str) \u2013 The fully-qualfiied name of the attribute to fetch", "The value of the attribute.", "Any"]}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "torch.fx", "text": ["Execute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.", "The value of the attribute that was retrieved", "Any"]}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "torch.fx", "text": ["Recursively descend through args and look up the concrete value for each Node in the current execution environment."]}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "torch.fx", "text": ["Execute an output node. This really just retrieves the value referenced by the output node and returns it.", "The return value referenced by the output node", "Any"]}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "torch.fx", "text": ["Execute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.", "The argument value that was retrieved.", "Any"]}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "torch.fx", "text": ["Run module via interpretation and return the result.", "The value returned from executing the Module", "Any"]}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "torch.fx", "text": ["Run a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op", "n (Node) \u2013 The Node to execute", "The result of executing n", "Any"]}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "torch.fx", "text": ["Node is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:", "Return all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.", "List of Nodes that appear in the args and kwargs of this Node, in that order.", "Insert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)", "x (Node) \u2013 The node to put after this node. Must be a member of the same graph.", "The tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "The dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.", "Returns the next Node in the linked list of Nodes.", "The next Node in the linked list of Nodes.", "Insert x before this node in the list of nodes in the graph. Example:", "x (Node) \u2013 The node to put before this node. Must be a member of the same graph.", "Returns the previous Node in the linked list of Nodes.", "The previous Node in the linked list of Nodes.", "Replace all uses of self in the Graph with the Node replace_with.", "replace_with (Node) \u2013 The node to replace all uses of self with.", "The list of Nodes on which this change was made."]}, {"name": "torch.fx.Node.all_input_nodes()", "path": "fx#torch.fx.Node.all_input_nodes", "type": "torch.fx", "text": ["Return all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.", "List of Nodes that appear in the args and kwargs of this Node, in that order."]}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "torch.fx", "text": ["Insert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)", "x (Node) \u2013 The node to put after this node. Must be a member of the same graph."]}, {"name": "torch.fx.Node.args()", "path": "fx#torch.fx.Node.args", "type": "torch.fx", "text": ["The tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment."]}, {"name": "torch.fx.Node.kwargs()", "path": "fx#torch.fx.Node.kwargs", "type": "torch.fx", "text": ["The dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information.", "Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment."]}, {"name": "torch.fx.Node.next()", "path": "fx#torch.fx.Node.next", "type": "torch.fx", "text": ["Returns the next Node in the linked list of Nodes.", "The next Node in the linked list of Nodes."]}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "torch.fx", "text": ["Insert x before this node in the list of nodes in the graph. Example:", "x (Node) \u2013 The node to put before this node. Must be a member of the same graph."]}, {"name": "torch.fx.Node.prev()", "path": "fx#torch.fx.Node.prev", "type": "torch.fx", "text": ["Returns the previous Node in the linked list of Nodes.", "The previous Node in the linked list of Nodes."]}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "torch.fx", "text": ["Replace all uses of self in the Graph with the Node replace_with.", "replace_with (Node) \u2013 The node to replace all uses of self with.", "The list of Nodes on which this change was made."]}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "torch.fx", "text": ["Proxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph.", "If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph."]}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "torch.fx", "text": ["Matches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).", "A list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as:", "List[Match]", "Examples:", "The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m).", "The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph.", "When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.)", "One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing", "with", "In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement.", "After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this:"]}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "torch.fx", "text": ["Symbolic tracing API", "Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.", "a Module created from the recorded operations from root.", "GraphModule"]}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "torch.fx", "text": ["Tracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m).", "Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.", "Method that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.", "By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.", "This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.", "The return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.", "A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.", "By default, the behavior includes:", "Given a non-Proxy Tensor object, emit IR for various cases:", "This method can be overridden to support more types.", "a (Any) \u2013 The value to be emitted as an Argument in the Graph.", "The value a converted into the appropriate Argument", "Create placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs.", "A method to specify whether a given nn.Module is a \u201cleaf\u201d module.", "Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.", "Helper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.", "mod (str) \u2013 The Module to retrieve the qualified name for.", "Trace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.", "Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.", "root (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.", "A Graph representing the semantics of the passed-in root."]}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "torch.fx", "text": ["Method that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance.", "By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function.", "This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.", "The return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation."]}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "torch.fx", "text": ["A method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph.", "By default, the behavior includes:", "Given a non-Proxy Tensor object, emit IR for various cases:", "This method can be overridden to support more types.", "a (Any) \u2013 The value to be emitted as an Argument in the Graph.", "The value a converted into the appropriate Argument"]}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "torch.fx", "text": ["Create placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs."]}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "torch.fx", "text": ["A method to specify whether a given nn.Module is a \u201cleaf\u201d module.", "Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter."]}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "torch.fx", "text": ["Helper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.", "mod (str) \u2013 The Module to retrieve the qualified name for."]}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "torch.fx", "text": ["Trace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable.", "Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.", "root (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.", "A Graph representing the semantics of the passed-in root."]}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "torch.fx", "text": ["Transformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically.", "Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so:", "module (GraphModule) \u2013 The Module to be transformed.", "Execute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.", "Execute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.", "Transform self.module and return the transformed GraphModule."]}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "torch.fx", "text": ["Execute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph."]}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "torch.fx", "text": ["Execute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph."]}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "torch.fx", "text": ["Transform self.module and return the transformed GraphModule."]}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "torch.fx", "text": ["This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through:", "This function can also equivalently be used as a decorator:", "A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.", "fn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called"]}, {"name": "torch.gather()", "path": "generated/torch.gather#torch.gather", "type": "torch", "text": ["Gathers values along an axis specified by dim.", "For a 3-D tensor the output is specified by:", "input and index must have the same number of dimensions. It is also required that index.size(d) <= input.size(d) for all dimensions d != dim. out will have the same shape as index. Note that input and index do not broadcast against each other.", "Example:"]}, {"name": "torch.gcd()", "path": "generated/torch.gcd#torch.gcd", "type": "torch", "text": ["Computes the element-wise greatest common divisor (GCD) of input and other.", "Both input and other must have integer types.", "Note", "This defines gcd(0,0)=0gcd(0, 0) = 0 .", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ge()", "path": "generated/torch.ge#torch.ge", "type": "torch", "text": ["Computes input\u2265other\\text{input} \\geq \\text{other}  element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is greater than or equal to other and False elsewhere", "Example:"]}, {"name": "torch.Generator", "path": "generated/torch.generator#torch.Generator", "type": "torch", "text": ["Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers. Used as a keyword argument in many In-place random sampling functions.", "device (torch.device, optional) \u2013 the desired device for the generator.", "An torch.Generator object.", "Generator", "Example:", "Generator.device -> device", "Gets the current device of the generator.", "Example:", "Returns the Generator state as a torch.ByteTensor.", "A torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.", "Tensor", "Example:", "Returns the initial seed for generating random numbers.", "Example:", "Sets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "An torch.Generator object.", "Generator", "Example:", "Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.", "Example:", "Sets the Generator state.", "new_state (torch.ByteTensor) \u2013 The desired state.", "Example:"]}, {"name": "torch.Generator.device", "path": "generated/torch.generator#torch.Generator.device", "type": "torch", "text": ["Generator.device -> device", "Gets the current device of the generator.", "Example:"]}, {"name": "torch.Generator.get_state()", "path": "generated/torch.generator#torch.Generator.get_state", "type": "torch", "text": ["Returns the Generator state as a torch.ByteTensor.", "A torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.", "Tensor", "Example:"]}, {"name": "torch.Generator.initial_seed()", "path": "generated/torch.generator#torch.Generator.initial_seed", "type": "torch", "text": ["Returns the initial seed for generating random numbers.", "Example:"]}, {"name": "torch.Generator.manual_seed()", "path": "generated/torch.generator#torch.Generator.manual_seed", "type": "torch", "text": ["Sets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "An torch.Generator object.", "Generator", "Example:"]}, {"name": "torch.Generator.seed()", "path": "generated/torch.generator#torch.Generator.seed", "type": "torch", "text": ["Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator.", "Example:"]}, {"name": "torch.Generator.set_state()", "path": "generated/torch.generator#torch.Generator.set_state", "type": "torch", "text": ["Sets the Generator state.", "new_state (torch.ByteTensor) \u2013 The desired state.", "Example:"]}, {"name": "torch.geqrf()", "path": "generated/torch.geqrf#torch.geqrf", "type": "torch", "text": ["This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf .", "You\u2019ll generally want to use torch.qr() instead.", "Computes a QR decomposition of input, but without constructing QQ  and RR  as explicit separate matrices.", "Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of \u2018elementary reflectors\u2019.", "See LAPACK documentation for geqrf for further details.", "input (Tensor) \u2013 the input matrix", "out (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)"]}, {"name": "torch.ger()", "path": "generated/torch.ger#torch.ger", "type": "torch", "text": ["Alias of torch.outer().", "Warning", "This function is deprecated and will be removed in a future PyTorch release. Use torch.outer() instead."]}, {"name": "torch.get_default_dtype()", "path": "generated/torch.get_default_dtype#torch.get_default_dtype", "type": "torch", "text": ["Get the current default floating point torch.dtype.", "Example:"]}, {"name": "torch.get_num_interop_threads()", "path": "generated/torch.get_num_interop_threads#torch.get_num_interop_threads", "type": "torch", "text": ["Returns the number of threads used for inter-op parallelism on CPU (e.g. in JIT interpreter)"]}, {"name": "torch.get_num_threads()", "path": "generated/torch.get_num_threads#torch.get_num_threads", "type": "torch", "text": ["Returns the number of threads used for parallelizing CPU operations"]}, {"name": "torch.get_rng_state()", "path": "generated/torch.get_rng_state#torch.get_rng_state", "type": "torch", "text": ["Returns the random number generator state as a torch.ByteTensor."]}, {"name": "torch.greater()", "path": "generated/torch.greater#torch.greater", "type": "torch", "text": ["Alias for torch.gt()."]}, {"name": "torch.greater_equal()", "path": "generated/torch.greater_equal#torch.greater_equal", "type": "torch", "text": ["Alias for torch.ge()."]}, {"name": "torch.gt()", "path": "generated/torch.gt#torch.gt", "type": "torch", "text": ["Computes input>other\\text{input} > \\text{other}  element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is greater than other and False elsewhere", "Example:"]}, {"name": "torch.hamming_window()", "path": "generated/torch.hamming_window#torch.hamming_window", "type": "torch", "text": ["Hamming window function.", "where NN  is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.hamming_window(L, periodic=True) equal to torch.hamming_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1 , the returned window contains a single value 1.", "Note", "This is a generalized version of torch.hann_window().", "A 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window", "Tensor"]}, {"name": "torch.hann_window()", "path": "generated/torch.hann_window#torch.hann_window", "type": "torch", "text": ["Hann window function.", "where NN  is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.hann_window(L, periodic=True) equal to torch.hann_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1 , the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window", "Tensor"]}, {"name": "torch.heaviside()", "path": "generated/torch.heaviside#torch.heaviside", "type": "torch", "text": ["Computes the Heaviside step function for each element in input. The Heaviside step function is defined as:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.histc()", "path": "generated/torch.histc#torch.histc", "type": "torch", "text": ["Computes the histogram of a tensor.", "The elements are sorted into equal width bins between min and max. If min and max are both zero, the minimum and maximum values of the data are used.", "Elements lower than min and higher than max are ignored.", "out (Tensor, optional) \u2013 the output tensor.", "Histogram represented as a tensor", "Tensor", "Example:"]}, {"name": "torch.hspmm()", "path": "sparse#torch.hspmm", "type": "torch.sparse", "text": ["Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The result is a (1 + 1)-dimensional hybrid COO matrix.", "{out} \u2013 "]}, {"name": "torch.hstack()", "path": "generated/torch.hstack#torch.hstack", "type": "torch", "text": ["Stack tensors in sequence horizontally (column wise).", "This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors.", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.hub", "path": "hub", "type": "torch.hub", "text": ["Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.", "Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple hubconf.py file;", "hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).", "Here is a code snippet specifies an entrypoint for resnet18 model if we expand the implementation in pytorch/vision/hubconf.py. In most case importing the right function in hubconf.py is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in pytorch/vision repo", "Pytorch Hub provides convenient APIs to explore all available models in hub through torch.hub.list(), show docstring and examples through torch.hub.help() and load the pre-trained models using torch.hub.load().", "List all entrypoints available in github hubconf.", "a list of available entrypoint names", "entrypoints", "Show the docstring of entrypoint model.", "Load a model from a github repo or a local directory.", "Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.", "If source is 'github', repo_or_dir is expected to be of the form repo_owner/repo_name[:tag_name] with an optional tag/branch.", "If source is 'local', repo_or_dir is expected to be a path to a local directory.", "The output of the model callable when called with the given *args and **kwargs.", "Download object at the given URL to a local path.", "Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir().", "Note that *args and **kwargs in torch.hub.load() are used to instantiate a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is", "To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It\u2019s also helpful to include a minimal working example.", "The locations are used in the order of", "Get the Torch Hub cache directory used for storing downloaded models & weights.", "If set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set.", "Optionally set the Torch Hub directory used to save downloaded models & weights.", "d (string) \u2013 path to a local folder to save downloaded models & weights.", "By default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by get_dir().", "Users can force a reload by calling hub.load(..., force_reload=True). This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.", "Torch hub works by importing the package as if it was installed. There\u2019re some side effects introduced by importing in Python. For example, you can see new items in Python caches sys.modules and sys.path_importer_cache which is normal Python behavior.", "A known limitation that worth mentioning here is user CANNOT load two different branches of the same repo in the same python process. It\u2019s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it\u2019s totally fine to load them in separate processes."]}, {"name": "torch.hub.download_url_to_file()", "path": "hub#torch.hub.download_url_to_file", "type": "torch.hub", "text": ["Download object at the given URL to a local path."]}, {"name": "torch.hub.get_dir()", "path": "hub#torch.hub.get_dir", "type": "torch.hub", "text": ["Get the Torch Hub cache directory used for storing downloaded models & weights.", "If set_dir() is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesystem layout, with a default value ~/.cache if the environment variable is not set."]}, {"name": "torch.hub.help()", "path": "hub#torch.hub.help", "type": "torch.hub", "text": ["Show the docstring of entrypoint model."]}, {"name": "torch.hub.list()", "path": "hub#torch.hub.list", "type": "torch.hub", "text": ["List all entrypoints available in github hubconf.", "a list of available entrypoint names", "entrypoints"]}, {"name": "torch.hub.load()", "path": "hub#torch.hub.load", "type": "torch.hub", "text": ["Load a model from a github repo or a local directory.", "Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.", "If source is 'github', repo_or_dir is expected to be of the form repo_owner/repo_name[:tag_name] with an optional tag/branch.", "If source is 'local', repo_or_dir is expected to be a path to a local directory.", "The output of the model callable when called with the given *args and **kwargs."]}, {"name": "torch.hub.load_state_dict_from_url()", "path": "hub#torch.hub.load_state_dict_from_url", "type": "torch.hub", "text": ["Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir()."]}, {"name": "torch.hub.set_dir()", "path": "hub#torch.hub.set_dir", "type": "torch.hub", "text": ["Optionally set the Torch Hub directory used to save downloaded models & weights.", "d (string) \u2013 path to a local folder to save downloaded models & weights."]}, {"name": "torch.hypot()", "path": "generated/torch.hypot#torch.hypot", "type": "torch", "text": ["Given the legs of a right triangle, return its hypotenuse.", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.i0()", "path": "generated/torch.i0#torch.i0", "type": "torch", "text": ["Computes the zeroth order modified Bessel function of the first kind for each element of input.", "input (Tensor) \u2013 the input tensor", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.igamma()", "path": "generated/torch.igamma#torch.igamma", "type": "torch", "text": ["Computes the regularized lower incomplete gamma function:", "where both inputi\\text{input}_i  and otheri\\text{other}_i  are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot)  in the equation above is the gamma function,", "See torch.igammac() and torch.lgamma() for related functions.", "Supports broadcasting to a common shape and float inputs.", "Note", "The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.igammac()", "path": "generated/torch.igammac#torch.igammac", "type": "torch", "text": ["Computes the regularized upper incomplete gamma function:", "where both inputi\\text{input}_i  and otheri\\text{other}_i  are weakly positive and at least one is strictly positive. If both are zero or either is negative then outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot)  in the equation above is the gamma function,", "See torch.igamma() and torch.lgamma() for related functions.", "Supports broadcasting to a common shape and float inputs.", "Note", "The backward pass with respect to input is not yet supported. Please open an issue on PyTorch\u2019s Github to request it.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.imag()", "path": "generated/torch.imag#torch.imag", "type": "torch", "text": ["Returns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "imag() is only supported for tensors with complex dtypes.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.index_select()", "path": "generated/torch.index_select#torch.index_select", "type": "torch", "text": ["Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.", "The returned tensor has the same number of dimensions as the original tensor (input). The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor.", "Note", "The returned tensor does not use the same storage as the original tensor. If out has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.initial_seed()", "path": "generated/torch.initial_seed#torch.initial_seed", "type": "torch", "text": ["Returns the initial seed for generating random numbers as a Python long."]}, {"name": "torch.inner()", "path": "generated/torch.inner#torch.inner", "type": "torch", "text": ["Computes the dot product for 1D tensors. For higher dimensions, sums the product of elements from input and other along their last dimension.", "Note", "If either input or other is a scalar, the result is equivalent to torch.mul(input, other).", "If both input and other are non-scalars, the size of their last dimension must match and the result is equivalent to torch.tensordot(input, other, dims=([-1], [-1]))", "out (Tensor, optional) \u2013 Optional output tensor to write result into. The output shape is input.shape[:-1] + other.shape[:-1].", "Example:"]}, {"name": "torch.inverse()", "path": "generated/torch.inverse#torch.inverse", "type": "torch", "text": ["Takes the inverse of the square matrix input. input can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses.", "Supports real and complex input.", "Note", "torch.inverse() is deprecated. Please use torch.linalg.inv() instead.", "Note", "Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like input.contiguous().transpose(-2, -1).stride()", "input (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  where * is zero or more batch dimensions", "out (Tensor, optional) \u2013 the output tensor.", "Examples:"]}, {"name": "torch.isclose()", "path": "generated/torch.isclose#torch.isclose", "type": "torch", "text": ["Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other. Closeness is defined as:", "where input and other are finite. Where input and/or other are nonfinite they are close if and only if they are equal, with NaNs being considered equal to each other when equal_nan is True.", "Examples:"]}, {"name": "torch.isfinite()", "path": "generated/torch.isfinite#torch.isfinite", "type": "torch", "text": ["Returns a new tensor with boolean elements representing if each element is finite or not.", "Real values are finite when they are not NaN, negative infinity, or infinity. Complex values are finite when both their real and imaginary parts are finite.", "input (Tensor): the input tensor.", "A boolean tensor that is True where input is finite and False elsewhere", "Example:"]}, {"name": "torch.isinf()", "path": "generated/torch.isinf#torch.isinf", "type": "torch", "text": ["Tests if each element of input is infinite (positive or negative infinity) or not.", "Note", "Complex values are infinite when their real or imaginary part is infinite.", "{input}", "A boolean tensor that is True where input is infinite and False elsewhere", "Example:"]}, {"name": "torch.isnan()", "path": "generated/torch.isnan#torch.isnan", "type": "torch", "text": ["Returns a new tensor with boolean elements representing if each element of input is NaN or not. Complex values are considered NaN when either their real and/or imaginary part is NaN.", "input (Tensor) \u2013 the input tensor.", "A boolean tensor that is True where input is NaN and False elsewhere", "Example:"]}, {"name": "torch.isneginf()", "path": "generated/torch.isneginf#torch.isneginf", "type": "torch", "text": ["Tests if each element of input is negative infinity or not.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.isposinf()", "path": "generated/torch.isposinf#torch.isposinf", "type": "torch", "text": ["Tests if each element of input is positive infinity or not.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.isreal()", "path": "generated/torch.isreal#torch.isreal", "type": "torch", "text": ["Returns a new tensor with boolean elements representing if each element of input is real-valued or not. All real-valued types are considered real. Complex values are considered real when their imaginary part is 0.", "input (Tensor) \u2013 the input tensor.", "A boolean tensor that is True where input is real and False elsewhere", "Example:"]}, {"name": "torch.istft()", "path": "generated/torch.istft#torch.istft", "type": "torch", "text": ["Inverse short time Fourier Transform. This is expected to be the inverse of stft(). It has the same parameters (+ additional optional parameter of length) and it should return the least squares estimation of the original signal. The algorithm will check using the NOLA condition ( nonzero overlap).", "Important consideration in the parameters window and center so that the envelop created by the summation of all the windows is never zero at certain point in time. Specifically, \u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times hop\\_length] \\cancel{=} 0 .", "Since stft() discards elements at the end of the signal if they do not fit in a frame, istft may return a shorter signal than the original signal (can occur if center is False since the signal isn\u2019t padded).", "If center is True, then there will be padding e.g. 'constant', 'reflect', etc. Left padding can be trimmed off exactly because they can be calculated but right padding cannot be calculated without additional information.", "Example: Suppose the last window is: [17, 18, 0, 0, 0] vs [18, 0, 0, 0, 0]", "The n_fft, hop_length, win_length are all the same which prevents the calculation of right padding. These additional values could be zeros or a reflection of the signal so providing length could be useful. If length is None then padding will be aggressively removed (some loss of signal).", "[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time Fourier transform,\u201d IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.", "input (Tensor) \u2013 ", "The input tensor. Expected to be output of stft(), can either be complex (channel, fft_size, n_frame), or real (channel, fft_size, n_frame, 2) where the channel dimension is optional.", "Deprecated since version 1.8.0: Real input is deprecated, use complex inputs as returned by stft(..., return_complex=True) instead.", "Least squares estimation of the original signal of size (\u2026, signal_length)", "Tensor"]}, {"name": "torch.is_complex()", "path": "generated/torch.is_complex#torch.is_complex", "type": "torch", "text": ["Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.is_floating_point()", "path": "generated/torch.is_floating_point#torch.is_floating_point", "type": "torch", "text": ["Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.is_nonzero()", "path": "generated/torch.is_nonzero#torch.is_nonzero", "type": "torch", "text": ["Returns True if the input is a single element tensor which is not equal to zero after type conversions. i.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or torch.tensor([False]). Throws a RuntimeError if torch.numel() != 1 (even in case of sparse tensors).", "input (Tensor) \u2013 the input tensor.", "Examples:"]}, {"name": "torch.is_storage()", "path": "generated/torch.is_storage#torch.is_storage", "type": "torch", "text": ["Returns True if obj is a PyTorch storage object.", "obj (Object) \u2013 Object to test"]}, {"name": "torch.is_tensor()", "path": "generated/torch.is_tensor#torch.is_tensor", "type": "torch", "text": ["Returns True if obj is a PyTorch tensor.", "Note that this function is simply doing isinstance(obj, Tensor). Using that isinstance check is better for typechecking with mypy, and more explicit - so it\u2019s recommended to use that instead of is_tensor.", "obj (Object) \u2013 Object to test"]}, {"name": "torch.jit.export()", "path": "jit#torch.jit.export", "type": "TorchScript", "text": ["This decorator indicates that a method on an nn.Module is used as an entry point into a ScriptModule and should be compiled.", "forward implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from forward are compiled as they are seen by the compiler, so they do not need this decorator either.", "Example (using @torch.jit.export on a method):"]}, {"name": "torch.jit.fork()", "path": "generated/torch.jit.fork#torch.jit.fork", "type": "TorchScript", "text": ["Creates an asynchronous task executing func and a reference to the value of the result of this execution. fork will return immediately, so the return value of func may not have been computed yet. To force completion of the task and access the return value invoke torch.jit.wait on the Future. fork invoked with a func which returns T is typed as torch.jit.Future[T]. fork calls can be arbitrarily nested, and may be invoked with positional and keyword arguments. Asynchronous execution will only occur when run in TorchScript. If run in pure python, fork will not execute in parallel. fork will also not execute in parallel when invoked while tracing, however the fork and wait calls will be captured in the exported IR Graph. .. warning:", "a reference to the execution of func. The value T can only be accessed by forcing completion of func through torch.jit.wait.", "torch.jit.Future[T]", "Example (fork a free function):", "Example (fork a module method):"]}, {"name": "torch.jit.freeze()", "path": "generated/torch.jit.freeze#torch.jit.freeze", "type": "TorchScript", "text": ["Freezing a ScriptModule will clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, forward will be preserved, as well as attributes & methods specified in preserved_attrs. Additionally, any attribute that is modified within a preserved method will be preserved.", "Freezing currently only accepts ScriptModules that are in eval mode.", "Frozen ScriptModule.", "Example (Freezing a simple module with a Parameter):", "Example (Freezing a module with preserved attributes)", "Note", "If you\u2019re not sure why an attribute is not being inlined as a constant, you can run dump_alias_db on frozen_module.forward.graph to see if freezing has detected the attribute is being modified."]}, {"name": "torch.jit.ignore()", "path": "generated/torch.jit.ignore#torch.jit.ignore", "type": "TorchScript", "text": ["This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. If called from TorchScript, ignored functions will dispatch the call to the Python interpreter. Models with ignored functions cannot be exported; use @torch.jit.unused instead.", "Example (using @torch.jit.ignore on a method):", "Example (using @torch.jit.ignore(drop=True) on a method):"]}, {"name": "torch.jit.isinstance()", "path": "generated/torch.jit.isinstance#torch.jit.isinstance", "type": "TorchScript", "text": ["This function provides for conatiner type refinement in TorchScript. It can refine parameterized containers of the List, Dict, Tuple, and Optional types. E.g. List[str], Dict[str, List[torch.Tensor]], Optional[Tuple[int,str,int]]. It can also refine basic types such as bools and ints that are available in TorchScript.", "False otherwise with no new type refinement", "bool", "Example (using torch.jit.isinstance for type refinement): .. testcode:"]}, {"name": "torch.jit.is_scripting()", "path": "jit_language_reference#torch.jit.is_scripting", "type": "TorchScript", "text": ["Function that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode:"]}, {"name": "torch.jit.load()", "path": "generated/torch.jit.load#torch.jit.load", "type": "TorchScript", "text": ["Load a ScriptModule or ScriptFunction previously saved with torch.jit.save", "All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised.", "A ScriptModule object.", "Example:"]}, {"name": "torch.jit.save()", "path": "generated/torch.jit.save#torch.jit.save", "type": "TorchScript", "text": ["Save an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using torch::jit::load(filename) or into the Python API with torch.jit.load.", "To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of ScriptModule as well.", "Danger", "All modules, no matter their device, are always loaded onto the CPU during loading. This is different from torch.load()\u2019s semantics and may change in the future.", "Note", "torch.jit.save attempts to preserve the behavior of some operators across versions. For example, dividing two integer tensors in PyTorch 1.5 performed floor division, and if the module containing that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6 its division behavior will be preserved. The same module saved in PyTorch 1.6 will fail to load in PyTorch 1.5, however, since the behavior of division changed in 1.6, and 1.5 does not know how to replicate the 1.6 behavior.", "Example:"]}, {"name": "torch.jit.script()", "path": "generated/torch.jit.script#torch.jit.script", "type": "TorchScript", "text": ["Scripting a function or nn.Module will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a ScriptModule or ScriptFunction. TorchScript itself is a subset of the Python language, so not all features in Python work, but we provide enough functionality to compute on tensors and do control-dependent operations. For a complete guide, see the TorchScript Language Reference.", "torch.jit.script can be used as a function for modules and functions, and as a decorator @torch.jit.script for TorchScript Classes and functions.", "obj (callable, class, or nn.Module) \u2013 The nn.Module, function, or class type to compile.", "If obj is nn.Module, script returns a ScriptModule object. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If obj is a standalone function, a ScriptFunction will be returned.", "The @torch.jit.script decorator will construct a ScriptFunction by compiling the body of the function.", "Example (scripting a function):", "Scripting an nn.Module by default will compile the forward method and recursively compile any methods, submodules, and functions called by forward. If a nn.Module only uses features supported in TorchScript, no changes to the original module code should be necessary. script will construct ScriptModule that has copies of the attributes, parameters, and methods of the original module.", "Example (scripting a simple module with a Parameter):", "Example (scripting a module with traced submodules):", "To compile a method other than forward (and recursively compile anything it calls), add the @torch.jit.export decorator to the method. To opt out of compilation use @torch.jit.ignore or @torch.jit.unused.", "Example (an exported and ignored method in a module):"]}, {"name": "torch.jit.ScriptFunction", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction", "type": "TorchScript", "text": ["Functionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters."]}, {"name": "torch.jit.ScriptFunction.get_debug_state()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.get_debug_state", "type": "TorchScript", "text": []}, {"name": "torch.jit.ScriptFunction.save()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save", "type": "TorchScript", "text": []}, {"name": "torch.jit.ScriptFunction.save_to_buffer()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer", "type": "TorchScript", "text": []}, {"name": "torch.jit.ScriptModule", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule", "type": "TorchScript", "text": ["A wrapper around C++ torch::jit::Module. ScriptModules contain methods, attributes, parameters, and constants. These can be accessed the same as on a normal nn.Module.", "Adds a child module to the current module.", "The module can be accessed as an attribute using the given name.", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:", "Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module", "Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:", "Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details.", "Returns a tuple of:", "[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See code. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant\u2019s values.", "See Inspecting Code for details.", "Moves all model parameters and buffers to the CPU.", "self", "Module", "Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Casts all floating point parameters and buffers to double datatype.", "self", "Module", "Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module", "Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.", "Casts all floating point parameters and buffers to float datatype.", "self", "Module", "Returns a string representation of the internal graph for the forward method. See Interpreting Graphs for details.", "Casts all floating point parameters and buffers to half datatype.", "self", "Module", "Returns a string representation of the internal graph for the forward method. This graph will be preprocessed to inline all function and method calls. See Interpreting Graphs for details.", "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields", "Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:", "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:", "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:", "Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:", "Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:", "Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name.", "Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module", "See torch.jit.save for details.", "Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:", "Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:", "Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module", "Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module", "Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.jit.ScriptModule.add_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.add_module", "type": "TorchScript", "text": ["Adds a child module to the current module.", "The module can be accessed as an attribute using the given name."]}, {"name": "torch.jit.ScriptModule.apply()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.apply", "type": "TorchScript", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:"]}, {"name": "torch.jit.ScriptModule.bfloat16()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.bfloat16", "type": "TorchScript", "text": ["Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.buffers", "type": "TorchScript", "text": ["Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:"]}, {"name": "torch.jit.ScriptModule.children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.children", "type": "TorchScript", "text": ["Returns an iterator over immediate children modules.", "Module \u2013 a child module"]}, {"name": "torch.jit.ScriptModule.code()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code", "type": "TorchScript", "text": ["Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details."]}, {"name": "torch.jit.ScriptModule.code_with_constants()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code_with_constants", "type": "TorchScript", "text": ["Returns a tuple of:", "[0] a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See code. [1] a ConstMap following the CONSTANT.cN format of the output in [0]. The indices in the [0] output are keys to the underlying constant\u2019s values.", "See Inspecting Code for details."]}, {"name": "torch.jit.ScriptModule.cpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cpu", "type": "TorchScript", "text": ["Moves all model parameters and buffers to the CPU.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.cuda()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cuda", "type": "TorchScript", "text": ["Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.jit.ScriptModule.double()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.double", "type": "TorchScript", "text": ["Casts all floating point parameters and buffers to double datatype.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.eval()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.eval", "type": "TorchScript", "text": ["Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module"]}, {"name": "torch.jit.ScriptModule.extra_repr()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.extra_repr", "type": "TorchScript", "text": ["Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable."]}, {"name": "torch.jit.ScriptModule.float()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.float", "type": "TorchScript", "text": ["Casts all floating point parameters and buffers to float datatype.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.graph", "type": "TorchScript", "text": ["Returns a string representation of the internal graph for the forward method. See Interpreting Graphs for details."]}, {"name": "torch.jit.ScriptModule.half()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.half", "type": "TorchScript", "text": ["Casts all floating point parameters and buffers to half datatype.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.inlined_graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.inlined_graph", "type": "TorchScript", "text": ["Returns a string representation of the internal graph for the forward method. This graph will be preprocessed to inline all function and method calls. See Interpreting Graphs for details."]}, {"name": "torch.jit.ScriptModule.load_state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.load_state_dict", "type": "TorchScript", "text": ["Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields"]}, {"name": "torch.jit.ScriptModule.modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.modules", "type": "TorchScript", "text": ["Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.jit.ScriptModule.named_buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_buffers", "type": "TorchScript", "text": ["Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:"]}, {"name": "torch.jit.ScriptModule.named_children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_children", "type": "TorchScript", "text": ["Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:"]}, {"name": "torch.jit.ScriptModule.named_modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_modules", "type": "TorchScript", "text": ["Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.jit.ScriptModule.named_parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_parameters", "type": "TorchScript", "text": ["Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:"]}, {"name": "torch.jit.ScriptModule.parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.parameters", "type": "TorchScript", "text": ["Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:"]}, {"name": "torch.jit.ScriptModule.register_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_backward_hook", "type": "TorchScript", "text": ["Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_buffer", "type": "TorchScript", "text": ["Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:"]}, {"name": "torch.jit.ScriptModule.register_forward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_hook", "type": "TorchScript", "text": ["Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_forward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_pre_hook", "type": "TorchScript", "text": ["Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_full_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_hook", "type": "TorchScript", "text": ["Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.jit.ScriptModule.register_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_parameter", "type": "TorchScript", "text": ["Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name."]}, {"name": "torch.jit.ScriptModule.requires_grad_()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.requires_grad_", "type": "TorchScript", "text": ["Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.save()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.save", "type": "TorchScript", "text": ["See torch.jit.save for details."]}, {"name": "torch.jit.ScriptModule.state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.state_dict", "type": "TorchScript", "text": ["Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:"]}, {"name": "torch.jit.ScriptModule.to()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to", "type": "TorchScript", "text": ["Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:"]}, {"name": "torch.jit.ScriptModule.train()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.train", "type": "TorchScript", "text": ["Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module"]}, {"name": "torch.jit.ScriptModule.type()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.type", "type": "TorchScript", "text": ["Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module"]}, {"name": "torch.jit.ScriptModule.xpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.xpu", "type": "TorchScript", "text": ["Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.jit.ScriptModule.zero_grad()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.zero_grad", "type": "TorchScript", "text": ["Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.jit.script_if_tracing()", "path": "generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing", "type": "TorchScript", "text": ["Compiles fn when it is first called during tracing. torch.jit.script has a non-negligible start up time when it is first called due to lazy-initializations of many compiler builtins. Therefore you should not use it in library code. However, you may want to have parts of your library work in tracing even if they use control flow. In these cases, you should use @torch.jit.script_if_tracing to substitute for torch.jit.script.", "fn \u2013 A function to compile.", "If called during tracing, a ScriptFunction created by torch.jit.script is returned. Otherwise, the original function fn is returned."]}, {"name": "torch.jit.trace()", "path": "generated/torch.jit.trace#torch.jit.trace", "type": "TorchScript", "text": ["Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on Tensors and lists, dictionaries, and tuples of Tensors.", "Using torch.jit.trace and torch.jit.trace_module, you can turn an existing module or Python function into a TorchScript ScriptFunction or ScriptModule. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.", "This module also contains any parameters that the original module had as well.", "Warning", "Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned ScriptModule will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,", "In cases like these, tracing would not be appropriate and scripting is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.", "If func is nn.Module or forward of nn.Module, trace returns a ScriptModule object with a single forward method containing the traced code. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If func is a standalone function, trace returns ScriptFunction.", "Example (tracing a function):", "Example (tracing an existing module):"]}, {"name": "torch.jit.trace_module()", "path": "generated/torch.jit.trace_module#torch.jit.trace_module", "type": "TorchScript", "text": ["Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation. When a module is passed to torch.jit.trace, only the forward method is run and traced. With trace_module, you can specify a dictionary of method names to example inputs to trace (see the inputs) argument below.", "See torch.jit.trace for more information on tracing.", "A ScriptModule object with a single forward method containing the traced code. When func is a torch.nn.Module, the returned ScriptModule will have the same set of sub-modules and parameters as func.", "Example (tracing a module with multiple methods):"]}, {"name": "torch.jit.unused()", "path": "generated/torch.jit.unused#torch.jit.unused", "type": "TorchScript", "text": ["This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.", "Example (using @torch.jit.unused on a method):"]}, {"name": "torch.jit.wait()", "path": "generated/torch.jit.wait#torch.jit.wait", "type": "TorchScript", "text": ["Forces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task. See fork() for docs and examples. :param func: an asynchronous task reference, created through torch.jit.fork :type func: torch.jit.Future[T]", "the return value of the the completed task", "T"]}, {"name": "torch.kaiser_window()", "path": "generated/torch.kaiser_window#torch.kaiser_window", "type": "torch", "text": ["Computes the Kaiser window with window length window_length and shape parameter beta.", "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and N = L - 1 if periodic is False and L if periodic is True, where L is the window_length. This function computes:", "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]). The periodic argument is intended as a helpful shorthand to produce a periodic window as input to functions like torch.stft().", "Note", "If window_length is one, then the returned window is a single element tensor containing a one."]}, {"name": "torch.kron()", "path": "generated/torch.kron#torch.kron", "type": "torch", "text": ["Computes the Kronecker product, denoted by \u2297\\otimes , of input and other.", "If input is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n)  tensor and other is a (b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n)  tensor, the result will be a (a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots \\times a_n*b_n)  tensor with the following entries:", "where kt=it\u2217bt+jtk_t = i_t * b_t + j_t  for 0\u2264t\u2264n0 \\leq t \\leq n . If one tensor has fewer dimensions than the other it is unsqueezed until it has the same number of dimensions.", "Supports real-valued and complex-valued inputs.", "Note", "This function generalizes the typical definition of the Kronecker product for two matrices to two tensors, as described above. When input is a (m\u00d7n)(m \\times n)  matrix and other is a (p\u00d7q)(p \\times q)  matrix, the result will be a (p\u2217m\u00d7q\u2217n)(p*m \\times q*n)  block matrix:", "where input is A\\mathbf{A}  and other is B\\mathbf{B} .", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:"]}, {"name": "torch.kthvalue()", "path": "generated/torch.kthvalue#torch.kthvalue", "type": "torch", "text": ["Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim. And indices is the index location of each element found.", "If dim is not given, the last dimension of the input is chosen.", "If keepdim is True, both the values and indices tensors are the same size as input, except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in both the values and indices tensors having 1 fewer dimension than the input tensor.", "Note", "When input is a CUDA tensor and there are multiple valid k th values, this function may nondeterministically return indices for any of them.", "out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers", "Example:"]}, {"name": "torch.lcm()", "path": "generated/torch.lcm#torch.lcm", "type": "torch", "text": ["Computes the element-wise least common multiple (LCM) of input and other.", "Both input and other must have integer types.", "Note", "This defines lcm(0,0)=0lcm(0, 0) = 0  and lcm(0,a)=0lcm(0, a) = 0 .", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ldexp()", "path": "generated/torch.ldexp#torch.ldexp", "type": "torch", "text": ["Multiplies input by 2**:attr:other.", "Typically this function is used to construct floating point numbers by multiplying mantissas in input with integral powers of two created from the exponents in :attr:\u2019other\u2019.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.le()", "path": "generated/torch.le#torch.le", "type": "torch", "text": ["Computes input\u2264other\\text{input} \\leq \\text{other}  element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is less than or equal to other and False elsewhere", "Example:"]}, {"name": "torch.lerp()", "path": "generated/torch.lerp#torch.lerp", "type": "torch", "text": ["Does a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.", "The shapes of start and end must be broadcastable. If weight is a tensor, then the shapes of weight, start, and end must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.less()", "path": "generated/torch.less#torch.less", "type": "torch", "text": ["Alias for torch.lt()."]}, {"name": "torch.less_equal()", "path": "generated/torch.less_equal#torch.less_equal", "type": "torch", "text": ["Alias for torch.le()."]}, {"name": "torch.lgamma()", "path": "generated/torch.lgamma#torch.lgamma", "type": "torch", "text": ["Computes the logarithm of the gamma function on input.", "Example:"]}, {"name": "torch.linalg", "path": "linalg", "type": "torch.linalg", "text": ["Common linear algebra operations.", "This module is in BETA. New functions are still being added, and some functions may change in future PyTorch releases. See the documentation of each function for details.", "Computes the Cholesky decomposition of a Hermitian (or symmetric for real-valued matrices) positive-definite matrix or the Cholesky decompositions for a batch of such matrices. Each decomposition has the form:", "where LL  is a lower-triangular matrix and LHL^H  is the conjugate transpose of LL , which is just a transpose for the case of real-valued input matrices. In code it translates to input = L @ L.t() if input is real-valued and input = L @ L.conj().t() if input is complex-valued. The batch of LL  matrices is returned.", "Supports real-valued and complex-valued inputs.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "LAPACK\u2019s potrf is used for CPU inputs, and MAGMA\u2019s potrf is used for CUDA inputs.", "Note", "If input is not a Hermitian positive-definite matrix, or if it\u2019s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown. If input is a batch of matrices, then the error message will include the batch index of the first matrix that is not Hermitian positive-definite.", "input (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  consisting of Hermitian positive-definite n\u00d7nn \\times n  matrices, where \u2217*  is zero or more batch dimensions.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:", "Computes the condition number of a matrix input, or of each matrix in a batched input, using the matrix norm defined by p.", "For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} this is defined as the matrix norm of input times the matrix norm of the inverse of input computed using torch.linalg.norm(). While for norms {None, 2, -2} this is defined as the ratio between the largest and smallest singular values computed using torch.linalg.svd().", "This function supports float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function may synchronize that device with the CPU depending on which norm p is used.", "Note", "For norms {None, 2, -2}, input may be a non-square matrix or batch of non-square matrices. For other norms, however, input must be a square matrix or a batch of square matrices, and if this requirement is not satisfied a RuntimeError will be thrown.", "Note", "For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} if input is a non-invertible matrix then a tensor containing infinity will be returned. If input is a batch of matrices and one or more of them is not invertible then a RuntimeError will be thrown.", "p (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 ", "the type of the matrix norm to use in the computations. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be used:", "p", "norm for matrices", "None", "ratio of the largest singular value to the smallest singular value", "\u2019fro\u2019", "Frobenius norm", "\u2019nuc\u2019", "nuclear norm", "inf", "max(sum(abs(x), dim=1))", "-inf", "min(sum(abs(x), dim=1))", "1", "max(sum(abs(x), dim=0))", "-1", "min(sum(abs(x), dim=0))", "2", "ratio of the largest singular value to the smallest singular value", "-2", "ratio of the smallest singular value to the largest singular value", "Default: None", "out (Tensor, optional) \u2013 tensor to write the output to. Default is None.", "The condition number of input. The output dtype is always real valued even for complex inputs (e.g. float if input is cfloat).", "Examples:", "Computes the determinant of a square matrix input, or of each square matrix in a batched input.", "This function supports float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.", "Note", "Backward through det internally uses torch.linalg.svd() when input is not invertible. In this case, double backward through det will be unstable when input doesn\u2019t have distinct singular values. See torch.linalg.svd() for more details.", "input (Tensor) \u2013 the input matrix of size (n, n) or the batch of matrices of size (*, n, n) where * is one or more batch dimensions.", "Example:", "Calculates the sign and natural logarithm of the absolute value of a square matrix\u2019s determinant, or of the absolute values of the determinants of a batch of square matrices input. The determinant can be computed with sign * exp(logabsdet).", "Supports input of float, double, cfloat and cdouble datatypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.", "Note", "For matrices that have zero determinant, this returns (0, -inf). If input is batched then the entries in the result tensors corresponding to matrices with the zero determinant have sign 0 and the natural logarithm of the absolute value of the determinant -inf.", "input (Tensor) \u2013 the input matrix of size (n,n)(n, n)  or the batch of matrices of size (\u2217,n,n)(*, n, n)  where \u2217*  is one or more batch dimensions.", "out (tuple, optional) \u2013 tuple of two tensors to write the output to.", "A namedtuple (sign, logabsdet) containing the sign of the determinant and the natural logarithm of the absolute value of determinant, respectively.", "Example:", "Computes the eigenvalues and eigenvectors of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input.", "For a single matrix input, the tensor of eigenvalues w and the tensor of eigenvectors V decompose the input such that input = V diag(w) V\u1d34, where V\u1d34 is the transpose of V for real-valued input, or the conjugate transpose of V for complex-valued input.", "Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The eigenvalues/eigenvectors are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.", "Note", "The eigenvalues of real symmetric or complex Hermitian matrices are always real.", "Note", "The eigenvectors of matrices are not unique, so any eigenvector multiplied by a constant remains a valid eigenvector. This function may compute different eigenvector representations on different device types. Usually the difference is only in the sign of the eigenvector.", "Note", "See torch.linalg.eigvalsh() for a related function that computes only eigenvalues. However, that function is not differentiable.", "out (tuple, optional) \u2013 tuple of two tensors to write the output to. Default is None.", "A namedtuple (eigenvalues, eigenvectors) containing", "The eigenvalues in ascending order.", "The orthonormal eigenvectors of the input.", "(Tensor, Tensor)", "Examples:", "Computes the eigenvalues of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input. The eigenvalues are returned in ascending order.", "Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The eigenvalues are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.", "Note", "The eigenvalues of real symmetric or complex Hermitian matrices are always real.", "Note", "This function doesn\u2019t support backpropagation, please use torch.linalg.eigh() instead, which also computes the eigenvectors.", "Note", "See torch.linalg.eigh() for a related function that computes both eigenvalues and eigenvectors.", "out (Tensor, optional) \u2013 tensor to write the output to. Default is None.", "Examples:", "Computes the numerical rank of a matrix input, or of each matrix in a batched input.", "The matrix rank is computed as the number of singular values (or absolute eigenvalues when hermitian is True) that are greater than the specified tol threshold.", "If tol is not specified, tol is set to S.max(dim=-1)*max(input.shape[-2:])*eps, where S is the singular values (or absolute eigenvalues when hermitian is True), and eps is the epsilon value for the datatype of input. The epsilon value can be obtained using the eps attribute of torch.finfo.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The matrix rank is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation is done by obtaining the eigenvalues (see torch.linalg.eigvalsh()).", "out (Tensor, optional) \u2013 tensor to write the output to. Default is None.", "Examples:", "Returns the matrix norm or vector norm of a given tensor.", "This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the ord parameter.", "ord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 ", "The order of norm. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be calculated:", "ord", "norm for matrices", "norm for vectors", "None", "Frobenius norm", "2-norm", "\u2019fro\u2019", "Frobenius norm", "\u2013 not supported \u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013 not supported \u2013", "inf", "max(sum(abs(x), dim=1))", "max(abs(x))", "-inf", "min(sum(abs(x), dim=1))", "min(abs(x))", "0", "\u2013 not supported \u2013", "sum(x != 0)", "1", "max(sum(abs(x), dim=0))", "as below", "-1", "min(sum(abs(x), dim=0))", "as below", "2", "2-norm (largest sing. value)", "as below", "-2", "smallest singular value", "as below", "other", "\u2013 not supported \u2013", "sum(abs(x)**ord)**(1./ord)", "Default: None", "Examples:", "Using the dim argument to compute vector norms:", "Using the dim argument to compute matrix norms:", "Computes the pseudo-inverse (also known as the Moore-Penrose inverse) of a matrix input, or of each matrix in a batched input.", "The singular values (or the absolute values of the eigenvalues when hermitian is True) that are below the specified rcond threshold are treated as zero and discarded in the computation.", "Supports input of float, double, cfloat and cdouble datatypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The pseudo-inverse is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation of the pseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see torch.linalg.eigh()).", "Note", "If singular value decomposition or eigenvalue decomposition algorithms do not converge then a RuntimeError will be thrown.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.", "Examples:", "Computes the singular value decomposition of either a matrix or batch of matrices input.\u201d The singular value decomposition is represented as a namedtuple (U, S, Vh), such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@} diag(S) \\times Vh . If input is a batch of tensors, then U, S, and Vh are also batched with the same batch dimensions as input.", "If full_matrices is False (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n,m)min(n, m)  orthonormal columns.", "If compute_uv is False, the returned U and Vh will be empy tensors with no elements and the same device as input. The full_matrices argument has no effect when compute_uv is False.", "The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.", "Note", "Unlike NumPy\u2019s linalg.svd, this always returns a namedtuple of three tensors, even when compute_uv=False. This behavior may change in a future PyTorch release.", "Note", "The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.", "Note", "The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the cuSOLVER routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and uses the MAGMA routine gesdd on earlier versions of CUDA.", "Note", "The returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride().", "Note", "Gradients computed using U and Vh may be unstable if input is not full rank or has non-unique singular values.", "Note", "When full_matrices = True, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.", "Note", "The S tensor can only be used to compute gradients if compute_uv is True.", "Note", "Since U and V of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi}  while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different U and V tensors.", "Example:", "Computes the solution x to the matrix equation matmul(input, x) = other with a square matrix, or batches of such matrices, input and one or more right-hand side vectors other. If input is batched and other is not, then other is broadcast to have the same batch dimensions as input. The resulting tensor has the same shape as the (possibly broadcast) other.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "If input is a non-square or non-invertible matrix, or a batch containing non-square matrices or one or more non-invertible matrices, then a RuntimeError will be thrown.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:", "Batched input:", "Computes a tensor input_inv such that tensordot(input_inv, input, ind) == I_n (inverse tensor equation), where I_n is the n-dimensional identity tensor and n is equal to input.ndim. The resulting tensor input_inv has shape equal to input.shape[ind:] + input.shape[:ind].", "Supports input of float, double, cfloat and cdouble data types.", "Note", "If input is not invertible or does not satisfy the requirement prod(input.shape[ind:]) == prod(input.shape[:ind]), then a RuntimeError will be thrown.", "Note", "When input is a 2-dimensional tensor and ind=1, this function computes the (multiplicative) inverse of input, equivalent to calling torch.inverse().", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:", "Computes a tensor x such that tensordot(input, x, dims=x.ndim) = other. The resulting tensor x has the same shape as input[other.ndim:].", "Supports real-valued and complex-valued inputs.", "Note", "If input does not satisfy the requirement prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim]) after (optionally) moving the dimensions using dims, then a RuntimeError will be thrown.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:", "Computes the multiplicative inverse matrix of a square matrix input, or of each square matrix in a batched input. The result satisfies the relation:", "matmul(inv(input),input) = matmul(input,inv(input)) = eye(input.shape[0]).expand_as(input).", "Supports input of float, double, cfloat and cdouble data types.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The inverse matrix is computed using LAPACK\u2019s getrf and getri routines for CPU inputs. For CUDA inputs, cuSOLVER\u2019s getrf and getrs routines as well as cuBLAS\u2019 getrf and getri routines are used if CUDA version >= 10.1.243, otherwise MAGMA\u2019s getrf and getri routines are used instead.", "Note", "If input is a non-invertible matrix or non-square matrix, or batch with at least one such matrix, then a RuntimeError will be thrown.", "input (Tensor) \u2013 the square (n, n) matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.", "Examples:", "Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices.", "Depending on the value of mode this function returns the reduced or complete QR factorization. See below for a list of valid modes.", "Note", "Differences with numpy.linalg.qr:", "Note", "Backpropagation is not supported for mode='r'. Use mode='reduced' instead.", "Backpropagation is also not supported if the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))  columns of any matrix in input are not linearly independent. While no error will be thrown when this occurs the values of the \u201cgradient\u201d produced may be anything. This behavior may change in the future.", "Note", "This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.", "mode (str, optional) \u2013 ", "if k = min(m, n) then:", "out (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of mode above.", "Example:"]}, {"name": "torch.linalg.cholesky()", "path": "linalg#torch.linalg.cholesky", "type": "torch.linalg", "text": ["Computes the Cholesky decomposition of a Hermitian (or symmetric for real-valued matrices) positive-definite matrix or the Cholesky decompositions for a batch of such matrices. Each decomposition has the form:", "where LL  is a lower-triangular matrix and LHL^H  is the conjugate transpose of LL , which is just a transpose for the case of real-valued input matrices. In code it translates to input = L @ L.t() if input is real-valued and input = L @ L.conj().t() if input is complex-valued. The batch of LL  matrices is returned.", "Supports real-valued and complex-valued inputs.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "LAPACK\u2019s potrf is used for CPU inputs, and MAGMA\u2019s potrf is used for CUDA inputs.", "Note", "If input is not a Hermitian positive-definite matrix, or if it\u2019s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown. If input is a batch of matrices, then the error message will include the batch index of the first matrix that is not Hermitian positive-definite.", "input (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)  consisting of Hermitian positive-definite n\u00d7nn \\times n  matrices, where \u2217*  is zero or more batch dimensions.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:"]}, {"name": "torch.linalg.cond()", "path": "linalg#torch.linalg.cond", "type": "torch.linalg", "text": ["Computes the condition number of a matrix input, or of each matrix in a batched input, using the matrix norm defined by p.", "For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} this is defined as the matrix norm of input times the matrix norm of the inverse of input computed using torch.linalg.norm(). While for norms {None, 2, -2} this is defined as the ratio between the largest and smallest singular values computed using torch.linalg.svd().", "This function supports float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function may synchronize that device with the CPU depending on which norm p is used.", "Note", "For norms {None, 2, -2}, input may be a non-square matrix or batch of non-square matrices. For other norms, however, input must be a square matrix or a batch of square matrices, and if this requirement is not satisfied a RuntimeError will be thrown.", "Note", "For norms {\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1} if input is a non-invertible matrix then a tensor containing infinity will be returned. If input is a batch of matrices and one or more of them is not invertible then a RuntimeError will be thrown.", "p (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 ", "the type of the matrix norm to use in the computations. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be used:", "p", "norm for matrices", "None", "ratio of the largest singular value to the smallest singular value", "\u2019fro\u2019", "Frobenius norm", "\u2019nuc\u2019", "nuclear norm", "inf", "max(sum(abs(x), dim=1))", "-inf", "min(sum(abs(x), dim=1))", "1", "max(sum(abs(x), dim=0))", "-1", "min(sum(abs(x), dim=0))", "2", "ratio of the largest singular value to the smallest singular value", "-2", "ratio of the smallest singular value to the largest singular value", "Default: None", "out (Tensor, optional) \u2013 tensor to write the output to. Default is None.", "The condition number of input. The output dtype is always real valued even for complex inputs (e.g. float if input is cfloat).", "Examples:"]}, {"name": "torch.linalg.det()", "path": "linalg#torch.linalg.det", "type": "torch.linalg", "text": ["Computes the determinant of a square matrix input, or of each square matrix in a batched input.", "This function supports float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.", "Note", "Backward through det internally uses torch.linalg.svd() when input is not invertible. In this case, double backward through det will be unstable when input doesn\u2019t have distinct singular values. See torch.linalg.svd() for more details.", "input (Tensor) \u2013 the input matrix of size (n, n) or the batch of matrices of size (*, n, n) where * is one or more batch dimensions.", "Example:"]}, {"name": "torch.linalg.eigh()", "path": "linalg#torch.linalg.eigh", "type": "torch.linalg", "text": ["Computes the eigenvalues and eigenvectors of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input.", "For a single matrix input, the tensor of eigenvalues w and the tensor of eigenvectors V decompose the input such that input = V diag(w) V\u1d34, where V\u1d34 is the transpose of V for real-valued input, or the conjugate transpose of V for complex-valued input.", "Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The eigenvalues/eigenvectors are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.", "Note", "The eigenvalues of real symmetric or complex Hermitian matrices are always real.", "Note", "The eigenvectors of matrices are not unique, so any eigenvector multiplied by a constant remains a valid eigenvector. This function may compute different eigenvector representations on different device types. Usually the difference is only in the sign of the eigenvector.", "Note", "See torch.linalg.eigvalsh() for a related function that computes only eigenvalues. However, that function is not differentiable.", "out (tuple, optional) \u2013 tuple of two tensors to write the output to. Default is None.", "A namedtuple (eigenvalues, eigenvectors) containing", "The eigenvalues in ascending order.", "The orthonormal eigenvectors of the input.", "(Tensor, Tensor)", "Examples:"]}, {"name": "torch.linalg.eigvalsh()", "path": "linalg#torch.linalg.eigvalsh", "type": "torch.linalg", "text": ["Computes the eigenvalues of a complex Hermitian (or real symmetric) matrix input, or of each such matrix in a batched input. The eigenvalues are returned in ascending order.", "Since the matrix or matrices in input are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When UPLO is \u201cL\u201d, its default value, only the lower triangular part of each matrix is used in the computation. When UPLO is \u201cU\u201d only the upper triangular part of each matrix is used.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The eigenvalues are computed using LAPACK\u2019s syevd and heevd routines for CPU inputs, and MAGMA\u2019s syevd and heevd routines for CUDA inputs.", "Note", "The eigenvalues of real symmetric or complex Hermitian matrices are always real.", "Note", "This function doesn\u2019t support backpropagation, please use torch.linalg.eigh() instead, which also computes the eigenvectors.", "Note", "See torch.linalg.eigh() for a related function that computes both eigenvalues and eigenvectors.", "out (Tensor, optional) \u2013 tensor to write the output to. Default is None.", "Examples:"]}, {"name": "torch.linalg.inv()", "path": "linalg#torch.linalg.inv", "type": "torch.linalg", "text": ["Computes the multiplicative inverse matrix of a square matrix input, or of each square matrix in a batched input. The result satisfies the relation:", "matmul(inv(input),input) = matmul(input,inv(input)) = eye(input.shape[0]).expand_as(input).", "Supports input of float, double, cfloat and cdouble data types.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The inverse matrix is computed using LAPACK\u2019s getrf and getri routines for CPU inputs. For CUDA inputs, cuSOLVER\u2019s getrf and getrs routines as well as cuBLAS\u2019 getrf and getri routines are used if CUDA version >= 10.1.243, otherwise MAGMA\u2019s getrf and getri routines are used instead.", "Note", "If input is a non-invertible matrix or non-square matrix, or batch with at least one such matrix, then a RuntimeError will be thrown.", "input (Tensor) \u2013 the square (n, n) matrix or the batch of such matrices of size (*, n, n) where * is one or more batch dimensions.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.", "Examples:"]}, {"name": "torch.linalg.matrix_rank()", "path": "linalg#torch.linalg.matrix_rank", "type": "torch.linalg", "text": ["Computes the numerical rank of a matrix input, or of each matrix in a batched input.", "The matrix rank is computed as the number of singular values (or absolute eigenvalues when hermitian is True) that are greater than the specified tol threshold.", "If tol is not specified, tol is set to S.max(dim=-1)*max(input.shape[-2:])*eps, where S is the singular values (or absolute eigenvalues when hermitian is True), and eps is the epsilon value for the datatype of input. The epsilon value can be obtained using the eps attribute of torch.finfo.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The matrix rank is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation is done by obtaining the eigenvalues (see torch.linalg.eigvalsh()).", "out (Tensor, optional) \u2013 tensor to write the output to. Default is None.", "Examples:"]}, {"name": "torch.linalg.norm()", "path": "linalg#torch.linalg.norm", "type": "torch.linalg", "text": ["Returns the matrix norm or vector norm of a given tensor.", "This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the ord parameter.", "ord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 ", "The order of norm. inf refers to float('inf'), numpy\u2019s inf object, or any equivalent object. The following norms can be calculated:", "ord", "norm for matrices", "norm for vectors", "None", "Frobenius norm", "2-norm", "\u2019fro\u2019", "Frobenius norm", "\u2013 not supported \u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013 not supported \u2013", "inf", "max(sum(abs(x), dim=1))", "max(abs(x))", "-inf", "min(sum(abs(x), dim=1))", "min(abs(x))", "0", "\u2013 not supported \u2013", "sum(x != 0)", "1", "max(sum(abs(x), dim=0))", "as below", "-1", "min(sum(abs(x), dim=0))", "as below", "2", "2-norm (largest sing. value)", "as below", "-2", "smallest singular value", "as below", "other", "\u2013 not supported \u2013", "sum(abs(x)**ord)**(1./ord)", "Default: None", "Examples:", "Using the dim argument to compute vector norms:", "Using the dim argument to compute matrix norms:"]}, {"name": "torch.linalg.pinv()", "path": "linalg#torch.linalg.pinv", "type": "torch.linalg", "text": ["Computes the pseudo-inverse (also known as the Moore-Penrose inverse) of a matrix input, or of each matrix in a batched input.", "The singular values (or the absolute values of the eigenvalues when hermitian is True) that are below the specified rcond threshold are treated as zero and discarded in the computation.", "Supports input of float, double, cfloat and cdouble datatypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The pseudo-inverse is computed using singular value decomposition (see torch.linalg.svd()) by default. If hermitian is True, then input is assumed to be Hermitian (symmetric if real-valued), and the computation of the pseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see torch.linalg.eigh()).", "Note", "If singular value decomposition or eigenvalue decomposition algorithms do not converge then a RuntimeError will be thrown.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default is None.", "Examples:"]}, {"name": "torch.linalg.qr()", "path": "linalg#torch.linalg.qr", "type": "torch.linalg", "text": ["Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices.", "Depending on the value of mode this function returns the reduced or complete QR factorization. See below for a list of valid modes.", "Note", "Differences with numpy.linalg.qr:", "Note", "Backpropagation is not supported for mode='r'. Use mode='reduced' instead.", "Backpropagation is also not supported if the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))  columns of any matrix in input are not linearly independent. While no error will be thrown when this occurs the values of the \u201cgradient\u201d produced may be anything. This behavior may change in the future.", "Note", "This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.", "mode (str, optional) \u2013 ", "if k = min(m, n) then:", "out (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of mode above.", "Example:"]}, {"name": "torch.linalg.slogdet()", "path": "linalg#torch.linalg.slogdet", "type": "torch.linalg", "text": ["Calculates the sign and natural logarithm of the absolute value of a square matrix\u2019s determinant, or of the absolute values of the determinants of a batch of square matrices input. The determinant can be computed with sign * exp(logabsdet).", "Supports input of float, double, cfloat and cdouble datatypes.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "Note", "The determinant is computed using LU factorization. LAPACK\u2019s getrf is used for CPU inputs, and MAGMA\u2019s getrf is used for CUDA inputs.", "Note", "For matrices that have zero determinant, this returns (0, -inf). If input is batched then the entries in the result tensors corresponding to matrices with the zero determinant have sign 0 and the natural logarithm of the absolute value of the determinant -inf.", "input (Tensor) \u2013 the input matrix of size (n,n)(n, n)  or the batch of matrices of size (\u2217,n,n)(*, n, n)  where \u2217*  is one or more batch dimensions.", "out (tuple, optional) \u2013 tuple of two tensors to write the output to.", "A namedtuple (sign, logabsdet) containing the sign of the determinant and the natural logarithm of the absolute value of determinant, respectively.", "Example:"]}, {"name": "torch.linalg.solve()", "path": "linalg#torch.linalg.solve", "type": "torch.linalg", "text": ["Computes the solution x to the matrix equation matmul(input, x) = other with a square matrix, or batches of such matrices, input and one or more right-hand side vectors other. If input is batched and other is not, then other is broadcast to have the same batch dimensions as input. The resulting tensor has the same shape as the (possibly broadcast) other.", "Supports input of float, double, cfloat and cdouble dtypes.", "Note", "If input is a non-square or non-invertible matrix, or a batch containing non-square matrices or one or more non-invertible matrices, then a RuntimeError will be thrown.", "Note", "When given inputs on a CUDA device, this function synchronizes that device with the CPU.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:", "Batched input:"]}, {"name": "torch.linalg.svd()", "path": "linalg#torch.linalg.svd", "type": "torch.linalg", "text": ["Computes the singular value decomposition of either a matrix or batch of matrices input.\u201d The singular value decomposition is represented as a namedtuple (U, S, Vh), such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@} diag(S) \\times Vh . If input is a batch of tensors, then U, S, and Vh are also batched with the same batch dimensions as input.", "If full_matrices is False (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n,m)min(n, m)  orthonormal columns.", "If compute_uv is False, the returned U and Vh will be empy tensors with no elements and the same device as input. The full_matrices argument has no effect when compute_uv is False.", "The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.", "Note", "Unlike NumPy\u2019s linalg.svd, this always returns a namedtuple of three tensors, even when compute_uv=False. This behavior may change in a future PyTorch release.", "Note", "The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.", "Note", "The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the cuSOLVER routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and uses the MAGMA routine gesdd on earlier versions of CUDA.", "Note", "The returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride().", "Note", "Gradients computed using U and Vh may be unstable if input is not full rank or has non-unique singular values.", "Note", "When full_matrices = True, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.", "Note", "The S tensor can only be used to compute gradients if compute_uv is True.", "Note", "Since U and V of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi}  while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different U and V tensors.", "Example:"]}, {"name": "torch.linalg.tensorinv()", "path": "linalg#torch.linalg.tensorinv", "type": "torch.linalg", "text": ["Computes a tensor input_inv such that tensordot(input_inv, input, ind) == I_n (inverse tensor equation), where I_n is the n-dimensional identity tensor and n is equal to input.ndim. The resulting tensor input_inv has shape equal to input.shape[ind:] + input.shape[:ind].", "Supports input of float, double, cfloat and cdouble data types.", "Note", "If input is not invertible or does not satisfy the requirement prod(input.shape[ind:]) == prod(input.shape[:ind]), then a RuntimeError will be thrown.", "Note", "When input is a 2-dimensional tensor and ind=1, this function computes the (multiplicative) inverse of input, equivalent to calling torch.inverse().", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:"]}, {"name": "torch.linalg.tensorsolve()", "path": "linalg#torch.linalg.tensorsolve", "type": "torch.linalg", "text": ["Computes a tensor x such that tensordot(input, x, dims=x.ndim) = other. The resulting tensor x has the same shape as input[other.ndim:].", "Supports real-valued and complex-valued inputs.", "Note", "If input does not satisfy the requirement prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim]) after (optionally) moving the dimensions using dims, then a RuntimeError will be thrown.", "out (Tensor, optional) \u2013 The output tensor. Ignored if None. Default: None", "Examples:"]}, {"name": "torch.linspace()", "path": "generated/torch.linspace#torch.linspace", "type": "torch", "text": ["Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive. That is, the value are:", "Warning", "Not providing a value for steps is deprecated. For backwards compatibility, not providing a value for steps will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for steps will throw a runtime error.", "Example:"]}, {"name": "torch.load()", "path": "generated/torch.load#torch.load", "type": "torch", "text": ["Loads an object saved with torch.save() from a file.", "torch.load() uses Python\u2019s unpickling facilities but treats storages, which underlie tensors, specially. They are first deserialized on the CPU and are then moved to the device they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the map_location argument.", "If map_location is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. The builtin location tags are 'cpu' for CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors. map_location should return either None or a storage. If map_location returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, torch.load() will fall back to the default behavior, as if map_location wasn\u2019t specified.", "If map_location is a torch.device object or a string containing a device tag, it indicates the location where all tensors should be loaded.", "Otherwise, if map_location is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values).", "User extensions can register their own location tags and tagging and deserialization methods using torch.serialization.register_package().", "Warning", "torch.load() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never load data that could have come from an untrusted source, or that could have been tampered with. Only load data you trust.", "Note", "When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call torch.load(.., map_location='cpu') and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint.", "Note", "By default, we decode byte strings as utf-8. This is to avoid a common error case UnicodeDecodeError: 'ascii' codec can't decode byte 0x... when loading files saved by Python 2 in Python 3. If this default is incorrect, you may use an extra encoding keyword argument to specify how these objects should be loaded, e.g., encoding='latin1' decodes them to strings using latin1 encoding, and encoding='bytes' keeps them as byte arrays which can be decoded later with byte_array.decode(...)."]}, {"name": "torch.lobpcg()", "path": "generated/torch.lobpcg#torch.lobpcg", "type": "torch", "text": ["Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.", "This function is a front-end to the following LOBPCG algorithms selectable via method argument:", "method=\u201dbasic\u201d - the LOBPCG method introduced by Andrew Knyazev, see [Knyazev2001]. A less robust method, may fail when Cholesky is applied to singular input.", "method=\u201dortho\u201d - the LOBPCG method with orthogonal basis selection [StathopoulosEtal2002]. A robust method.", "Supported inputs are dense, sparse, and batches of dense matrices.", "Note", "In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.", "Warning", "The backward method does not support sparse and complex inputs. It works only when B is not provided (i.e. B == None). We are actively working on extensions, and the details of the algorithms are going to be published promptly.", "Warning", "While it is assumed that A is symmetric, A.grad is not. To make sure that A.grad is symmetric, so that A - t * A.grad is symmetric in first-order optimization routines, prior to running lobpcg we do the following symmetrization map: A -> (A + A.t()) / 2. The map is performed only when the A requires gradients.", "tracker (callable, optional) \u2013 ", "a function for tracing the iteration process. When specified, it is called at each iteration step with LOBPCG instance as an argument. The LOBPCG instance holds the full state of the iteration process in the following attributes:", "iparams, fparams, bparams - dictionaries of integer, float, and boolean valued input parameters, respectively", "ivars, fvars, bvars, tvars - dictionaries of integer, float, boolean, and Tensor valued iteration variables, respectively.", "A, B, iK - input Tensor arguments.", "E, X, S, R - iteration Tensor variables.", "For instance:", "ivars[\u201cistep\u201d] - the current iteration step X - the current approximation of eigenvectors E - the current approximation of eigenvalues R - the current residual ivars[\u201cconverged_count\u201d] - the current number of converged eigenpairs tvars[\u201crerr\u201d] - the current state of convergence criteria", "Note that when tracker stores Tensor objects from the LOBPCG instance, it must make copies of these.", "If tracker sets bvars[\u201cforce_stop\u201d] = True, the iteration process will be hard-stopped.", "tensor of eigenvalues of size (\u2217,k)(*, k) ", "X (Tensor): tensor of eigenvectors of size (\u2217,m,k)(*, m, k) ", "E (Tensor)", "[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) https://epubs.siam.org/doi/abs/10.1137/S1064827500366124", "[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) https://epubs.siam.org/doi/10.1137/S1064827500370883", "[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) https://epubs.siam.org/doi/abs/10.1137/17M1129830"]}, {"name": "torch.log()", "path": "generated/torch.log#torch.log", "type": "torch", "text": ["Returns a new tensor with the natural logarithm of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.log10()", "path": "generated/torch.log10#torch.log10", "type": "torch", "text": ["Returns a new tensor with the logarithm to the base 10 of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.log1p()", "path": "generated/torch.log1p#torch.log1p", "type": "torch", "text": ["Returns a new tensor with the natural logarithm of (1 + input).", "Note", "This function is more accurate than torch.log() for small values of input", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.log2()", "path": "generated/torch.log2#torch.log2", "type": "torch", "text": ["Returns a new tensor with the logarithm to the base 2 of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logaddexp()", "path": "generated/torch.logaddexp#torch.logaddexp", "type": "torch", "text": ["Logarithm of the sum of exponentiations of the inputs.", "Calculates pointwise log\u2061(ex+ey)\\log\\left(e^x + e^y\\right) . This function is useful in statistics where the calculated probabilities of events may be so small as to exceed the range of normal floating point numbers. In such cases the logarithm of the calculated probability is stored. This function allows adding probabilities stored in such a fashion.", "This op should be disambiguated with torch.logsumexp() which performs a reduction on a single tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logaddexp2()", "path": "generated/torch.logaddexp2#torch.logaddexp2", "type": "torch", "text": ["Logarithm of the sum of exponentiations of the inputs in base-2.", "Calculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right) . See torch.logaddexp() for more details.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.logcumsumexp()", "path": "generated/torch.logcumsumexp#torch.logcumsumexp", "type": "torch", "text": ["Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.", "For summation index jj  given by dim and other indices ii , the result is", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.logdet()", "path": "generated/torch.logdet#torch.logdet", "type": "torch", "text": ["Calculates log determinant of a square matrix or batches of square matrices.", "Note", "Result is -inf if input has zero log determinant, and is nan if input has negative determinant.", "Note", "Backward through logdet() internally uses SVD results when input is not invertible. In this case, double backward through logdet() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.", "input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.", "Example:"]}, {"name": "torch.logical_and()", "path": "generated/torch.logical_and#torch.logical_and", "type": "torch", "text": ["Computes the element-wise logical AND of the given input tensors. Zeros are treated as False and nonzeros are treated as True.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logical_not()", "path": "generated/torch.logical_not#torch.logical_not", "type": "torch", "text": ["Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as False and non-zeros are treated as True.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logical_or()", "path": "generated/torch.logical_or#torch.logical_or", "type": "torch", "text": ["Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logical_xor()", "path": "generated/torch.logical_xor#torch.logical_xor", "type": "torch", "text": ["Computes the element-wise logical XOR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logit()", "path": "generated/torch.logit#torch.logit", "type": "torch", "text": ["Returns a new tensor with the logit of the elements of input. input is clamped to [eps, 1 - eps] when eps is not None. When eps is None and input < 0 or input > 1, the function will yields NaN.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.logspace()", "path": "generated/torch.logspace#torch.logspace", "type": "torch", "text": ["Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}  to baseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale with base base. That is, the values are:", "Warning", "Not providing a value for steps is deprecated. For backwards compatibility, not providing a value for steps will create a tensor with 100 elements. Note that this behavior is not reflected in the documented function signature and should not be relied on. In a future PyTorch release, failing to provide a value for steps will throw a runtime error.", "Example:"]}, {"name": "torch.logsumexp()", "path": "generated/torch.logsumexp#torch.logsumexp", "type": "torch", "text": ["Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. The computation is numerically stabilized.", "For summation index jj  given by dim and other indices ii , the result is", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.lstsq()", "path": "generated/torch.lstsq#torch.lstsq", "type": "torch", "text": ["Computes the solution to the least squares and least norm problems for a full rank matrix AA  of size (m\u00d7n)(m \\times n)  and a matrix BB  of size (m\u00d7k)(m \\times k) .", "If m\u2265nm \\geq n , lstsq() solves the least-squares problem:", "If m<nm < n , lstsq() solves the least-norm problem:", "Returned tensor XX  has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k) . The first nn  rows of XX  contains the solution. If m\u2265nm \\geq n , the residual sum of squares for the solution in each column is given by the sum of squares of elements in the remaining m\u2212nm - n  rows of that column.", "Note", "The case when m<nm < n  is not supported on the GPU.", "out (tuple, optional) \u2013 the optional destination tensor", "A namedtuple (solution, QR) containing:", "(Tensor, Tensor)", "Note", "The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride (1, m) instead of (m, 1).", "Example:"]}, {"name": "torch.lt()", "path": "generated/torch.lt#torch.lt", "type": "torch", "text": ["Computes input<other\\text{input} < \\text{other}  element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is less than other and False elsewhere", "Example:"]}, {"name": "torch.lu()", "path": "generated/torch.lu#torch.lu", "type": "torch", "text": ["Computes the LU factorization of a matrix or batches of matrices A. Returns a tuple containing the LU factorization and pivots of A. Pivoting is done if pivot is set to True.", "Note", "The pivots returned by the function are 1-indexed. If pivot is False, then the returned pivots is a tensor filled with zeros of the appropriate size.", "Note", "LU factorization with pivot = False is not available for CPU, and attempting to do so will throw an error. However, LU factorization with pivot = False is available for CUDA.", "Note", "This function does not check if the factorization was successful or not if get_infos is True since the status of the factorization is present in the third element of the return tuple.", "Note", "In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).", "Note", "L, U, and P can be derived using torch.lu_unpack().", "Warning", "The LU factorization does have backward support, but only for square inputs of full rank.", "A tuple of tensors containing", "(Tensor, IntTensor, IntTensor (optional))", "Example:"]}, {"name": "torch.lu_solve()", "path": "generated/torch.lu_solve#torch.lu_solve", "type": "torch", "text": ["Returns the LU solve of the linear system Ax=bAx = b  using the partially pivoted LU factorization of A from torch.lu().", "This function supports float, double, cfloat and cdouble dtypes for input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.lu_unpack()", "path": "generated/torch.lu_unpack#torch.lu_unpack", "type": "torch", "text": ["Unpacks the data and pivots from a LU factorization of a tensor.", "Returns a tuple of tensors as (the pivots, the L tensor, the U tensor).", "Examples:"]}, {"name": "torch.manual_seed()", "path": "generated/torch.manual_seed#torch.manual_seed", "type": "torch", "text": ["Sets the seed for generating random numbers. Returns a torch.Generator object.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed."]}, {"name": "torch.masked_select()", "path": "generated/torch.masked_select#torch.masked_select", "type": "torch", "text": ["Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.", "The shapes of the mask tensor and the input tensor don\u2019t need to match, but they must be broadcastable.", "Note", "The returned tensor does not use the same storage as the original tensor", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.matmul()", "path": "generated/torch.matmul#torch.matmul", "type": "torch", "text": ["Matrix product of two tensors.", "The behavior depends on the dimensionality of the tensors as follows:", "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), then a batched matrix multiply is returned. If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after. If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable). For example, if input is a (j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)  tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)  tensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)  tensor.", "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions. For example, if input is a (j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)  tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)  tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the matrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)  tensor.", "This operator supports TensorFloat32.", "Note", "The 1-dimensional dot product version of this function does not support an out parameter.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.matrix_exp()", "path": "generated/torch.matrix_exp#torch.matrix_exp", "type": "torch", "text": ["Returns the matrix exponential. Supports batched input. For a matrix A, the matrix exponential is defined as", "The implementation is based on:", "Bader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.matrix_power()", "path": "generated/torch.matrix_power#torch.matrix_power", "type": "torch", "text": ["Returns the matrix raised to the power n for square matrices. For batch of matrices, each individual matrix is raised to the power n.", "If n is negative, then the inverse of the matrix (if invertible) is raised to the power n. For a batch of matrices, the batched inverse (if invertible) is raised to the power n. If n is 0, then an identity matrix is returned.", "Example:"]}, {"name": "torch.matrix_rank()", "path": "generated/torch.matrix_rank#torch.matrix_rank", "type": "torch", "text": ["Returns the numerical rank of a 2-D tensor. The method to compute the matrix rank is done using SVD by default. If symmetric is True, then input is assumed to be symmetric, and the computation of the rank is done by obtaining the eigenvalues.", "tol is the threshold below which the singular values (or the eigenvalues when symmetric is True) are considered to be 0. If tol is not specified, tol is set to S.max() * max(S.size()) * eps where S is the singular values (or the eigenvalues when symmetric is True), and eps is the epsilon value for the datatype of input.", "Note", "torch.matrix_rank() is deprecated. Please use torch.linalg.matrix_rank() instead. The parameter symmetric was renamed in torch.linalg.matrix_rank() to hermitian.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.max()", "path": "generated/torch.max#torch.max", "type": "torch", "text": ["Returns the maximum value of all elements in the input tensor.", "Warning", "This function produces deterministic (sub)gradients unlike max(dim=0)", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.", "Note", "If there are multiple maximal values in a reduced row then the indices of the first maximal value are returned.", "out (tuple, optional) \u2013 the result tuple of two output tensors (max, max_indices)", "Example:", "See torch.maximum()."]}, {"name": "torch.maximum()", "path": "generated/torch.maximum#torch.maximum", "type": "torch", "text": ["Computes the element-wise maximum of input and other.", "Note", "If one of the elements being compared is a NaN, then that element is returned. maximum() is not supported for tensors with complex dtypes.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mean()", "path": "generated/torch.mean#torch.mean", "type": "torch", "text": ["Returns the mean value of all elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.median()", "path": "generated/torch.median#torch.median", "type": "torch", "text": ["Returns the median of the values in input.", "Note", "The median is not unique for input tensors with an even number of elements. In this case the lower of the two medians is returned. To compute the mean of both medians, use torch.quantile() with q=0.5 instead.", "Warning", "This function produces deterministic (sub)gradients unlike median(dim=0)", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values contains the median of each row of input in the dimension dim, and indices contains the index of the median values found in the dimension dim.", "By default, dim is the last dimension of the input tensor.", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the outputs tensor having 1 fewer dimension than input.", "Note", "The median is not unique for input tensors with an even number of elements in the dimension dim. In this case the lower of the two medians is returned. To compute the mean of both medians in input, use torch.quantile() with q=0.5 instead.", "Warning", "indices does not necessarily contain the first occurrence of each median value found, unless it is unique. The exact implementation details are device-specific. Do not expect the same result when run on CPU and GPU in general. For the same reason do not expect the gradients to be deterministic.", "out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the dimension dim of input.", "Example:"]}, {"name": "torch.meshgrid()", "path": "generated/torch.meshgrid#torch.meshgrid", "type": "torch", "text": ["Take NN  tensors, each of which can be either scalar or 1-dimensional vector, and create NN  N-dimensional grids, where the ii  th grid is defined by expanding the ii  th input over dimensions defined by other inputs.", "tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be treated as tensors of size (1,)(1,)  automatically", "If the input has kk  tensors of size (N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,) , then the output would also have kk  tensors, where all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k) .", "seq (sequence of Tensors)", "Example:"]}, {"name": "torch.min()", "path": "generated/torch.min#torch.min", "type": "torch", "text": ["Returns the minimum value of all elements in the input tensor.", "Warning", "This function produces deterministic (sub)gradients unlike min(dim=0)", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values is the minimum value of each row of the input tensor in the given dimension dim. And indices is the index location of each minimum value found (argmin).", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.", "Note", "If there are multiple minimal values in a reduced row then the indices of the first minimal value are returned.", "out (tuple, optional) \u2013 the tuple of two output tensors (min, min_indices)", "Example:", "See torch.minimum()."]}, {"name": "torch.minimum()", "path": "generated/torch.minimum#torch.minimum", "type": "torch", "text": ["Computes the element-wise minimum of input and other.", "Note", "If one of the elements being compared is a NaN, then that element is returned. minimum() is not supported for tensors with complex dtypes.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mm()", "path": "generated/torch.mm#torch.mm", "type": "torch", "text": ["Performs a matrix multiplication of the matrices input and mat2.", "If input is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, out will be a (n\u00d7p)(n \\times p)  tensor.", "Note", "This function does not broadcast. For broadcasting matrix products, see torch.matmul().", "Supports strided and sparse 2-D tensors as inputs, autograd with respect to strided inputs.", "This operator supports TensorFloat32.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mode()", "path": "generated/torch.mode#torch.mode", "type": "torch", "text": ["Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often in that row, and indices is the index location of each mode value found.", "By default, dim is the last dimension of the input tensor.", "If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.", "Note", "This function is not defined for torch.cuda.Tensor yet.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.moveaxis()", "path": "generated/torch.moveaxis#torch.moveaxis", "type": "torch", "text": ["Alias for torch.movedim().", "This function is equivalent to NumPy\u2019s moveaxis function.", "Examples:"]}, {"name": "torch.movedim()", "path": "generated/torch.movedim#torch.movedim", "type": "torch", "text": ["Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.", "Other dimensions of input that are not explicitly moved remain in their original order and appear at the positions not specified in destination.", "Examples:"]}, {"name": "torch.msort()", "path": "generated/torch.msort#torch.msort", "type": "torch", "text": ["Sorts the elements of the input tensor along its first dimension in ascending order by value.", "Note", "torch.msort(t) is equivalent to torch.sort(t, dim=0)[0]. See also torch.sort().", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mul()", "path": "generated/torch.mul#torch.mul", "type": "torch", "text": ["Multiplies each element of the input input with the scalar other and returns a new resulting tensor.", "If input is of type FloatTensor or DoubleTensor, other should be a real number, otherwise it should be an integer", "{out} \u2013 ", "Example:", "Each element of the tensor input is multiplied by the corresponding element of the Tensor other. The resulting tensor is returned.", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.multinomial()", "path": "generated/torch.multinomial#torch.multinomial", "type": "torch", "text": ["Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.", "Note", "The rows of input do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.", "Indices are ordered from left to right according to when each was sampled (first samples are placed in first column).", "If input is a vector, out is a vector of size num_samples.", "If input is a matrix with m rows, out is an matrix of shape (m\u00d7num_samples)(m \\times \\text{num\\_samples}) .", "If replacement is True, samples are drawn with replacement.", "If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.", "Note", "When drawn without replacement, num_samples must be lower than number of non-zero elements in input (or the min number of non-zero elements in each row of input if it is a matrix).", "Example:"]}, {"name": "torch.multiply()", "path": "generated/torch.multiply#torch.multiply", "type": "torch", "text": ["Alias for torch.mul()."]}, {"name": "torch.multiprocessing", "path": "multiprocessing", "type": "torch.multiprocessing", "text": ["torch.multiprocessing is a wrapper around the native multiprocessing module. It registers custom reducers, that use shared memory to provide shared views on the same data in different processes. Once the tensor/storage is moved to shared_memory (see share_memory_()), it will be possible to send it to other processes without making any copies.", "The API is 100% compatible with the original module - it\u2019s enough to change import multiprocessing to import torch.multiprocessing to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory.", "Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.", "Warning", "If the main process exits abruptly (e.g. because of an incoming signal), Python\u2019s multiprocessing sometimes fails to clean up its children. It\u2019s a known caveat, so if you\u2019re seeing any resource leaks after interrupting the interpreter, it probably means that this has just happened to you.", "Returns a set of sharing strategies supported on a current system.", "Returns the current strategy for sharing CPU tensors.", "Sets the strategy for sharing CPU tensors.", "new_strategy (str) \u2013 Name of the selected strategy. Should be one of the values returned by get_all_sharing_strategies().", "Sharing CUDA tensors between processes is supported only in Python 3, using a spawn or forkserver start methods.", "Unlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. The refcounting is implemented under the hood but requires users to follow the next best practices.", "Warning", "If the consumer process dies abnormally to a fatal signal, the shared tensor could be forever kept in memory as long as the sending process is running.", "2. Keep producer process running until all consumers exits. This will prevent the situation when the producer process releasing memory which is still in use by the consumer.", "This section provides a brief overview into how different sharing strategies work. Note that it applies only to CPU tensor - CUDA tensors will always use the CUDA API, as that\u2019s the only way they can be shared.", "Note", "This is the default strategy (except for macOS and OS X where it\u2019s not supported).", "This strategy will use file descriptors as shared memory handles. Whenever a storage is moved to shared memory, a file descriptor obtained from shm_open is cached with the object, and when it\u2019s going to be sent to other processes, the file descriptor will be transferred (e.g. via UNIX sockets) to it. The receiver will also cache the file descriptor and mmap it, to obtain a shared view onto the storage data.", "Note that if there will be a lot of tensors shared, this strategy will keep a large number of file descriptors open most of the time. If your system has low limits for the number of open file descriptors, and you can\u2019t raise them, you should use the file_system strategy.", "This strategy will use file names given to shm_open to identify the shared memory regions. This has a benefit of not requiring the implementation to cache the file descriptors obtained from it, but at the same time is prone to shared memory leaks. The file can\u2019t be deleted right after its creation, because other processes need to access it to open their views. If the processes fatally crash, or are killed, and don\u2019t call the storage destructors, the files will remain in the system. This is very serious, because they keep using up the memory until the system is restarted, or they\u2019re freed manually.", "To counter the problem of shared memory file leaks, torch.multiprocessing will spawn a daemon named torch_shm_manager that will isolate itself from the current process group, and will keep track of all shared memory allocations. Once all processes connected to it exit, it will wait a moment to ensure there will be no new connections, and will iterate over all shared memory files allocated by the group. If it finds that any of them still exist, they will be deallocated. We\u2019ve tested this method and it proved to be robust to various failures. Still, if your system has high enough limits, and file_descriptor is a supported strategy, we do not recommend switching to this one.", "Note", "Available for Python >= 3.4.", "This depends on the spawn start method in Python\u2019s multiprocessing package.", "Spawning a number of subprocesses to perform some function can be done by creating Process instances and calling join to wait for their completion. This approach works fine when dealing with a single subprocess but presents potential issues when dealing with multiple processes.", "Namely, joining processes sequentially implies they will terminate sequentially. If they don\u2019t, and the first process does not terminate, the process termination will go unnoticed. Also, there are no native facilities for error propagation.", "The spawn function below addresses these concerns and takes care of error propagation, out of order termination, and will actively terminate processes upon detecting an error in one of them.", "Spawns nprocs processes that run fn with args.", "If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.", "fn (function) \u2013 ", "Function is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing.", "The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.", "None if join is True, ProcessContext if join is False", "Returned by spawn() when called with join=False.", "Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.", "Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.", "timeout (float) \u2013 Wait this long before giving up on waiting."]}, {"name": "torch.multiprocessing.get_all_sharing_strategies()", "path": "multiprocessing#torch.multiprocessing.get_all_sharing_strategies", "type": "torch.multiprocessing", "text": ["Returns a set of sharing strategies supported on a current system."]}, {"name": "torch.multiprocessing.get_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.get_sharing_strategy", "type": "torch.multiprocessing", "text": ["Returns the current strategy for sharing CPU tensors."]}, {"name": "torch.multiprocessing.set_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.set_sharing_strategy", "type": "torch.multiprocessing", "text": ["Sets the strategy for sharing CPU tensors.", "new_strategy (str) \u2013 Name of the selected strategy. Should be one of the values returned by get_all_sharing_strategies()."]}, {"name": "torch.multiprocessing.spawn()", "path": "multiprocessing#torch.multiprocessing.spawn", "type": "torch.multiprocessing", "text": ["Spawns nprocs processes that run fn with args.", "If one of the processes exits with a non-zero exit status, the remaining processes are killed and an exception is raised with the cause of termination. In the case an exception was caught in the child process, it is forwarded and its traceback is included in the exception raised in the parent process.", "fn (function) \u2013 ", "Function is called as the entrypoint of the spawned process. This function must be defined at the top level of a module so it can be pickled and spawned. This is a requirement imposed by multiprocessing.", "The function is called as fn(i, *args), where i is the process index and args is the passed through tuple of arguments.", "None if join is True, ProcessContext if join is False"]}, {"name": "torch.multiprocessing.SpawnContext", "path": "multiprocessing#torch.multiprocessing.SpawnContext", "type": "torch.multiprocessing", "text": ["Returned by spawn() when called with join=False.", "Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.", "Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.", "timeout (float) \u2013 Wait this long before giving up on waiting."]}, {"name": "torch.multiprocessing.SpawnContext.join()", "path": "multiprocessing#torch.multiprocessing.SpawnContext.join", "type": "torch.multiprocessing", "text": ["Tries to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting.", "Returns True if all processes have been joined successfully, False if there are more processes that need to be joined.", "timeout (float) \u2013 Wait this long before giving up on waiting."]}, {"name": "torch.mv()", "path": "generated/torch.mv#torch.mv", "type": "torch", "text": ["Performs a matrix-vector product of the matrix input and the vector vec.", "If input is a (n\u00d7m)(n \\times m)  tensor, vec is a 1-D tensor of size mm , out will be 1-D of size nn .", "Note", "This function does not broadcast.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.mvlgamma()", "path": "generated/torch.mvlgamma#torch.mvlgamma", "type": "torch", "text": ["Computes the multivariate log-gamma function) with dimension pp  element-wise, given by", "where C=log\u2061(\u03c0)\u00d7p(p\u22121)4C = \\log(\\pi) \\times \\frac{p (p - 1)}{4}  and \u0393(\u22c5)\\Gamma(\\cdot)  is the Gamma function.", "All elements must be greater than p\u221212\\frac{p - 1}{2} , otherwise an error would be thrown.", "Example:"]}, {"name": "torch.nanmedian()", "path": "generated/torch.nanmedian#torch.nanmedian", "type": "torch", "text": ["Returns the median of the values in input, ignoring NaN values.", "This function is identical to torch.median() when there are no NaN values in input. When input has one or more NaN values, torch.median() will always return NaN, while this function will return the median of the non-NaN elements in input. If all the elements in input are NaN it will also return NaN.", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns a namedtuple (values, indices) where values contains the median of each row of input in the dimension dim, ignoring NaN values, and indices contains the index of the median values found in the dimension dim.", "This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has one or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the median of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too.", "out ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the median values and the second tensor, which must have dtype long, with their indices in the dimension dim of input.", "Example:"]}, {"name": "torch.nanquantile()", "path": "generated/torch.nanquantile#torch.nanquantile", "type": "torch", "text": ["This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist. If all values in a reduced row are NaN then the quantiles for that reduction will be NaN. See the documentation for torch.quantile().", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.nansum()", "path": "generated/torch.nansum#torch.nansum", "type": "torch", "text": ["Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the sum of each row of the input tensor in the given dimension dim, treating Not a Numbers (NaNs) as zero. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:"]}, {"name": "torch.nan_to_num()", "path": "generated/torch.nan_to_num#torch.nan_to_num", "type": "torch", "text": ["Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively. By default, NaN`s are replaced with zero, positive infinity is replaced with the\ngreatest finite value representable by :attr:`input\u2019s dtype, and negative infinity is replaced with the least finite value representable by input\u2019s dtype.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.narrow()", "path": "generated/torch.narrow#torch.narrow", "type": "torch", "text": ["Returns a new tensor that is a narrowed version of input tensor. The dimension dim is input from start to start + length. The returned tensor and input tensor share the same underlying storage.", "Example:"]}, {"name": "torch.ne()", "path": "generated/torch.ne#torch.ne", "type": "torch", "text": ["Computes input\u2260other\\text{input} \\neq \\text{other}  element-wise.", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is not equal to other and False elsewhere", "Example:"]}, {"name": "torch.neg()", "path": "generated/torch.neg#torch.neg", "type": "torch", "text": ["Returns a new tensor with the negative of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.negative()", "path": "generated/torch.negative#torch.negative", "type": "torch", "text": ["Alias for torch.neg()"]}, {"name": "torch.nextafter()", "path": "generated/torch.nextafter#torch.nextafter", "type": "torch", "text": ["Return the next floating-point value after input towards other, elementwise.", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.nn", "path": "nn", "type": "torch.nn", "text": ["These are the basic building block for graphs", "torch.nn", "A kind of Tensor that is to be considered a module parameter.", "A parameter that is not initialized.", "Base class for all neural network modules.", "A sequential container.", "Holds submodules in a list.", "Holds submodules in a dictionary.", "Holds parameters in a list.", "Holds parameters in a dictionary.", "Global Hooks For Module", "Registers a forward pre-hook common to all modules.", "Registers a global forward hook for all the modules", "Registers a backward hook common to all the modules.", "nn.Conv1d", "Applies a 1D convolution over an input signal composed of several input planes.", "nn.Conv2d", "Applies a 2D convolution over an input signal composed of several input planes.", "nn.Conv3d", "Applies a 3D convolution over an input signal composed of several input planes.", "nn.ConvTranspose1d", "Applies a 1D transposed convolution operator over an input image composed of several input planes.", "nn.ConvTranspose2d", "Applies a 2D transposed convolution operator over an input image composed of several input planes.", "nn.ConvTranspose3d", "Applies a 3D transposed convolution operator over an input image composed of several input planes.", "nn.LazyConv1d", "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).", "nn.LazyConv2d", "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).", "nn.LazyConv3d", "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).", "nn.LazyConvTranspose1d", "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).", "nn.LazyConvTranspose2d", "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).", "nn.LazyConvTranspose3d", "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).", "nn.Unfold", "Extracts sliding local blocks from a batched input tensor.", "nn.Fold", "Combines an array of sliding local blocks into a large containing tensor.", "nn.MaxPool1d", "Applies a 1D max pooling over an input signal composed of several input planes.", "nn.MaxPool2d", "Applies a 2D max pooling over an input signal composed of several input planes.", "nn.MaxPool3d", "Applies a 3D max pooling over an input signal composed of several input planes.", "nn.MaxUnpool1d", "Computes a partial inverse of MaxPool1d.", "nn.MaxUnpool2d", "Computes a partial inverse of MaxPool2d.", "nn.MaxUnpool3d", "Computes a partial inverse of MaxPool3d.", "nn.AvgPool1d", "Applies a 1D average pooling over an input signal composed of several input planes.", "nn.AvgPool2d", "Applies a 2D average pooling over an input signal composed of several input planes.", "nn.AvgPool3d", "Applies a 3D average pooling over an input signal composed of several input planes.", "nn.FractionalMaxPool2d", "Applies a 2D fractional max pooling over an input signal composed of several input planes.", "nn.LPPool1d", "Applies a 1D power-average pooling over an input signal composed of several input planes.", "nn.LPPool2d", "Applies a 2D power-average pooling over an input signal composed of several input planes.", "nn.AdaptiveMaxPool1d", "Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "nn.AdaptiveMaxPool2d", "Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "nn.AdaptiveMaxPool3d", "Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "nn.AdaptiveAvgPool1d", "Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "nn.AdaptiveAvgPool2d", "Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "nn.AdaptiveAvgPool3d", "Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "nn.ReflectionPad1d", "Pads the input tensor using the reflection of the input boundary.", "nn.ReflectionPad2d", "Pads the input tensor using the reflection of the input boundary.", "nn.ReplicationPad1d", "Pads the input tensor using replication of the input boundary.", "nn.ReplicationPad2d", "Pads the input tensor using replication of the input boundary.", "nn.ReplicationPad3d", "Pads the input tensor using replication of the input boundary.", "nn.ZeroPad2d", "Pads the input tensor boundaries with zero.", "nn.ConstantPad1d", "Pads the input tensor boundaries with a constant value.", "nn.ConstantPad2d", "Pads the input tensor boundaries with a constant value.", "nn.ConstantPad3d", "Pads the input tensor boundaries with a constant value.", "nn.ELU", "Applies the element-wise function:", "nn.Hardshrink", "Applies the hard shrinkage function element-wise:", "nn.Hardsigmoid", "Applies the element-wise function:", "nn.Hardtanh", "Applies the HardTanh function element-wise", "nn.Hardswish", "Applies the hardswish function, element-wise, as described in the paper:", "nn.LeakyReLU", "Applies the element-wise function:", "nn.LogSigmoid", "Applies the element-wise function:", "nn.MultiheadAttention", "Allows the model to jointly attend to information from different representation subspaces.", "nn.PReLU", "Applies the element-wise function:", "nn.ReLU", "Applies the rectified linear unit function element-wise:", "nn.ReLU6", "Applies the element-wise function:", "nn.RReLU", "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:", "nn.SELU", "Applied element-wise, as:", "nn.CELU", "Applies the element-wise function:", "nn.GELU", "Applies the Gaussian Error Linear Units function:", "nn.Sigmoid", "Applies the element-wise function:", "nn.SiLU", "Applies the silu function, element-wise.", "nn.Softplus", "Applies the element-wise function:", "nn.Softshrink", "Applies the soft shrinkage function elementwise:", "nn.Softsign", "Applies the element-wise function:", "nn.Tanh", "Applies the element-wise function:", "nn.Tanhshrink", "Applies the element-wise function:", "nn.Threshold", "Thresholds each element of the input Tensor.", "nn.Softmin", "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.", "nn.Softmax", "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.", "nn.Softmax2d", "Applies SoftMax over features to each spatial location.", "nn.LogSoftmax", "Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))  function to an n-dimensional input Tensor.", "nn.AdaptiveLogSoftmaxWithLoss", "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.", "nn.BatchNorm1d", "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.BatchNorm2d", "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.BatchNorm3d", "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.GroupNorm", "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization", "nn.SyncBatchNorm", "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "nn.InstanceNorm1d", "Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "nn.InstanceNorm2d", "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "nn.InstanceNorm3d", "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "nn.LayerNorm", "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization", "nn.LocalResponseNorm", "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.", "nn.RNNBase", "nn.RNN", "Applies a multi-layer Elman RNN with tanh\u2061\\tanh  or ReLU\\text{ReLU}  non-linearity to an input sequence.", "nn.LSTM", "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.", "nn.GRU", "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.", "nn.RNNCell", "An Elman RNN cell with tanh or ReLU non-linearity.", "nn.LSTMCell", "A long short-term memory (LSTM) cell.", "nn.GRUCell", "A gated recurrent unit (GRU) cell", "nn.Transformer", "A transformer model.", "nn.TransformerEncoder", "TransformerEncoder is a stack of N encoder layers", "nn.TransformerDecoder", "TransformerDecoder is a stack of N decoder layers", "nn.TransformerEncoderLayer", "TransformerEncoderLayer is made up of self-attn and feedforward network.", "nn.TransformerDecoderLayer", "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.", "nn.Identity", "A placeholder identity operator that is argument-insensitive.", "nn.Linear", "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b ", "nn.Bilinear", "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b ", "nn.LazyLinear", "A torch.nn.Linear module with lazy initialization.", "nn.Dropout", "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "nn.Dropout2d", "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ).", "nn.Dropout3d", "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ).", "nn.AlphaDropout", "Applies Alpha Dropout over the input.", "nn.Embedding", "A simple lookup table that stores embeddings of a fixed dictionary and size.", "nn.EmbeddingBag", "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.", "nn.CosineSimilarity", "Returns cosine similarity between x1x_1  and x2x_2 , computed along dim.", "nn.PairwiseDistance", "Computes the batchwise pairwise distance between vectors v1v_1 , v2v_2  using the p-norm:", "nn.L1Loss", "Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx  and target yy .", "nn.MSELoss", "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx  and target yy .", "nn.CrossEntropyLoss", "This criterion combines LogSoftmax and NLLLoss in one single class.", "nn.CTCLoss", "The Connectionist Temporal Classification loss.", "nn.NLLLoss", "The negative log likelihood loss.", "nn.PoissonNLLLoss", "Negative log likelihood loss with Poisson distribution of target.", "nn.GaussianNLLLoss", "Gaussian negative log likelihood loss.", "nn.KLDivLoss", "The Kullback-Leibler divergence loss measure", "nn.BCELoss", "Creates a criterion that measures the Binary Cross Entropy between the target and the output:", "nn.BCEWithLogitsLoss", "This loss combines a Sigmoid layer and the BCELoss in one single class.", "nn.MarginRankingLoss", "Creates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy  (containing 1 or -1).", "nn.HingeEmbeddingLoss", "Measures the loss given an input tensor xx  and a labels tensor yy  (containing 1 or -1).", "nn.MultiLabelMarginLoss", "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 2D Tensor of target class indices).", "nn.SmoothL1Loss", "Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "nn.SoftMarginLoss", "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx  and target tensor yy  (containing 1 or -1).", "nn.MultiLabelSoftMarginLoss", "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx  and target yy  of size (N,C)(N, C) .", "nn.CosineEmbeddingLoss", "Creates a criterion that measures the loss given input tensors x1x_1 , x2x_2  and a Tensor label yy  with values 1 or -1.", "nn.MultiMarginLoss", "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-1 ):", "nn.TripletMarginLoss", "Creates a criterion that measures the triplet loss given an input tensors x1x1 , x2x2 , x3x3  and a margin with a value greater than 00 .", "nn.TripletMarginWithDistanceLoss", "Creates a criterion that measures the triplet loss given input tensors aa , pp , and nn  (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).", "nn.PixelShuffle", "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is an upscale factor.", "nn.PixelUnshuffle", "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.", "nn.Upsample", "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.", "nn.UpsamplingNearest2d", "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.", "nn.UpsamplingBilinear2d", "Applies a 2D bilinear upsampling to an input signal composed of several input channels.", "nn.ChannelShuffle", "Divide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W)  into g groups and rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the original tensor shape.", "nn.DataParallel", "Implements data parallelism at the module level.", "nn.parallel.DistributedDataParallel", "Implements distributed data parallelism that is based on torch.distributed package at the module level.", "From the torch.nn.utils module", "Clips gradient norm of an iterable of parameters.", "Clips gradient of an iterable of parameters at specified value.", "Convert parameters to one vector", "Convert one vector to the parameters", "prune.BasePruningMethod", "Abstract base class for creation of new pruning techniques.", "prune.PruningContainer", "Container holding a sequence of pruning methods for iterative pruning.", "prune.Identity", "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "prune.RandomUnstructured", "Prune (currently unpruned) units in a tensor at random.", "prune.L1Unstructured", "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.", "prune.RandomStructured", "Prune entire (currently unpruned) channels in a tensor at random.", "prune.LnStructured", "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.", "prune.CustomFromMask", "prune.identity", "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units.", "prune.random_unstructured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random.", "prune.l1_unstructured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm.", "prune.random_structured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random.", "prune.ln_structured", "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm.", "prune.global_unstructured", "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.", "prune.custom_from_mask", "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask.", "prune.remove", "Removes the pruning reparameterization from a module and the pruning method from the forward hook.", "prune.is_pruned", "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.", "Applies weight normalization to a parameter in the given module.", "Removes the weight normalization reparameterization from a module.", "Applies spectral normalization to a parameter in the given module.", "Removes the spectral normalization reparameterization from a module.", "Utility functions in other modules", "nn.utils.rnn.PackedSequence", "Holds the data and list of batch_sizes of a packed sequence.", "nn.utils.rnn.pack_padded_sequence", "Packs a Tensor containing padded sequences of variable length.", "nn.utils.rnn.pad_packed_sequence", "Pads a packed batch of variable length sequences.", "nn.utils.rnn.pad_sequence", "Pad a list of variable length Tensors with padding_value", "nn.utils.rnn.pack_sequence", "Packs a list of variable length Tensors", "nn.Flatten", "Flattens a contiguous range of dims into a tensor.", "nn.Unflatten", "Unflattens a tensor dim expanding it to a desired shape.", "Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.", "nn.modules.lazy.LazyModuleMixin", "A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d"]}, {"name": "torch.nn.AdaptiveAvgPool1d", "path": "generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d", "type": "torch.nn", "text": ["Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "The output size is H, for any input size. The number of output features is equal to the number of input planes.", "output_size \u2013 the target output size H"]}, {"name": "torch.nn.AdaptiveAvgPool2d", "path": "generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d", "type": "torch.nn", "text": ["Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.", "output_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input."]}, {"name": "torch.nn.AdaptiveAvgPool3d", "path": "generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d", "type": "torch.nn", "text": ["Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.", "output_size \u2013 the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input."]}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "torch.nn", "text": ["Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.", "Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf\u2019s law.", "Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated.", "The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute \u2013 that is, contain a small number of assigned labels.", "We highly recommend taking a look at the original paper for more details.", "Warning", "Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1.", "Note", "This module returns a NamedTuple with output and loss fields. See further documentation for details.", "Note", "To compute log-probabilities for all classes, the log_prob method can be used.", "NamedTuple with output and loss fields", "Computes log probabilities for all n_classes\\texttt{n\\_classes} ", "input (Tensor) \u2013 a minibatch of examples", "log-probabilities of for each class cc  in range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes} , where n_classes\\texttt{n\\_classes}  is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.", "This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.", "input (Tensor) \u2013 a minibatch of examples", "a class with the highest probability for each example", "output (Tensor)"]}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "torch.nn", "text": ["Computes log probabilities for all n_classes\\texttt{n\\_classes} ", "input (Tensor) \u2013 a minibatch of examples", "log-probabilities of for each class cc  in range 0<=c<=n_classes0 <= c <= \\texttt{n\\_classes} , where n_classes\\texttt{n\\_classes}  is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor."]}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "torch.nn", "text": ["This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.", "input (Tensor) \u2013 a minibatch of examples", "a class with the highest probability for each example", "output (Tensor)"]}, {"name": "torch.nn.AdaptiveMaxPool1d", "path": "generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d", "type": "torch.nn", "text": ["Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "The output size is H, for any input size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.AdaptiveMaxPool2d", "path": "generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d", "type": "torch.nn", "text": ["Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "The output is of size H x W, for any input size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.AdaptiveMaxPool3d", "path": "generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d", "type": "torch.nn", "text": ["Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.AlphaDropout", "path": "generated/torch.nn.alphadropout#torch.nn.AlphaDropout", "type": "torch.nn", "text": ["Applies Alpha Dropout over the input.", "Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation.", "During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation.", "During evaluation the module simply computes an identity function.", "More details can be found in the paper Self-Normalizing Neural Networks .", "Examples:"]}, {"name": "torch.nn.AvgPool1d", "path": "generated/torch.nn.avgpool1d#torch.nn.AvgPool1d", "type": "torch.nn", "text": ["Applies a 1D average pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L) , output (N,C,Lout)(N, C, L_{out})  and kernel_size kk  can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding can each be an int or a one-element tuple.", "Output: (N,C,Lout)(N, C, L_{out}) , where", "Examples:"]}, {"name": "torch.nn.AvgPool2d", "path": "generated/torch.nn.avgpool2d#torch.nn.AvgPool2d", "type": "torch.nn", "text": ["Applies a 2D average pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  and kernel_size (kH,kW)(kH, kW)  can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding can either be:", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where", "Examples:"]}, {"name": "torch.nn.AvgPool3d", "path": "generated/torch.nn.avgpool3d#torch.nn.AvgPool3d", "type": "torch.nn", "text": ["Applies a 3D average pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  and kernel_size (kD,kH,kW)(kD, kH, kW)  can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on all three sides for padding number of points.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride can either be:", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where", "Examples:"]}, {"name": "torch.nn.BatchNorm1d", "path": "generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d", "type": "torch.nn", "text": ["Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are set to 1 and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it\u2019s common terminology to call this Temporal Batch Normalization.", "Examples:"]}, {"name": "torch.nn.BatchNorm2d", "path": "generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d", "type": "torch.nn", "text": ["Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are set to 1 and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it\u2019s common terminology to call this Spatial Batch Normalization.", "Examples:"]}, {"name": "torch.nn.BatchNorm3d", "path": "generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d", "type": "torch.nn", "text": ["Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are set to 1 and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.", "Examples:"]}, {"name": "torch.nn.BCELoss", "path": "generated/torch.nn.bceloss#torch.nn.BCELoss", "type": "torch.nn", "text": ["Creates a criterion that measures the Binary Cross Entropy between the target and the output:", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN  is the batch size. If reduction is not 'none' (default 'mean'), then", "This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yy  should be numbers between 0 and 1.", "Notice that if xnx_n  is either 0 or 1, one of the log terms would be mathematically undefined in the above loss equation. PyTorch chooses to set log\u2061(0)=\u2212\u221e\\log (0) = -\\infty , since lim\u2061x\u21920log\u2061(x)=\u2212\u221e\\lim_{x\\to 0} \\log (x) = -\\infty . However, an infinite term in the loss equation is not desirable for several reasons.", "For one, if either yn=0y_n = 0  or (1\u2212yn)=0(1 - y_n) = 0 , then we would be multiplying 0 with infinity. Secondly, if we have an infinite loss value, then we would also have an infinite term in our gradient, since lim\u2061x\u21920ddxlog\u2061(x)=\u221e\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty . This would make BCELoss\u2019s backward method nonlinear with respect to xnx_n , and using it for things like linear regression would not be straight-forward.", "Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.", "Examples:"]}, {"name": "torch.nn.BCEWithLogitsLoss", "path": "generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss", "type": "torch.nn", "text": ["This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN  is the batch size. If reduction is not 'none' (default 'mean'), then", "This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1.", "It\u2019s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:", "where cc  is the class number (c>1c > 1  for multi-label binary classification, c=1c = 1  for single-label binary classification), nn  is the number of the sample in the batch and pcp_c  is the weight of the positive answer for the class cc .", "pc>1p_c > 1  increases the recall, pc<1p_c < 1  increases the precision.", "For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to 300100=3\\frac{300}{100}=3 . The loss would act as if the dataset contains 3\u00d7100=3003\\times 100=300  positive examples.", "Examples:", "Examples:"]}, {"name": "torch.nn.Bilinear", "path": "generated/torch.nn.bilinear#torch.nn.Bilinear", "type": "torch.nn", "text": ["Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b ", "Examples:"]}, {"name": "torch.nn.CELU", "path": "generated/torch.nn.celu#torch.nn.CELU", "type": "torch.nn", "text": ["Applies the element-wise function:", "More details can be found in the paper Continuously Differentiable Exponential Linear Units .", "Examples:"]}, {"name": "torch.nn.ChannelShuffle", "path": "generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle", "type": "torch.nn", "text": ["Divide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W)  into g groups and rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the original tensor shape.", "groups (int) \u2013 number of groups to divide channels in.", "Examples:"]}, {"name": "torch.nn.ConstantPad1d", "path": "generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d", "type": "torch.nn", "text": ["Pads the input tensor boundaries with a constant value.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} )", "Output: (N,C,Wout)(N, C, W_{out})  where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.ConstantPad2d", "path": "generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d", "type": "torch.nn", "text": ["Pads the input tensor boundaries with a constant value.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom} ", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.ConstantPad3d", "path": "generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d", "type": "torch.nn", "text": ["Pads the input tensor boundaries with a constant value.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} , padding_front\\text{padding\\_front} , padding_back\\text{padding\\_back} )", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  where", "Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back} ", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom} ", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.Conv1d", "path": "generated/torch.nn.conv1d#torch.nn.Conv1d", "type": "torch.nn", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,Cin,L)(N, C_{\\text{in}}, L)  and output (N,Cout,Lout)(N, C_{\\text{out}}, L_{\\text{out}})  can be precisely described as:", "where \u22c6\\star  is the valid cross-correlation operator, NN  is a batch size, CC  denotes a number of channels, LL  is a length of signal sequence.", "This module supports TensorFloat32.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "Note", "When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d.", "In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Output: (N,Cout,Lout)(N, C_{out}, L_{out})  where", "Examples:"]}, {"name": "torch.nn.Conv2d", "path": "generated/torch.nn.conv2d#torch.nn.Conv2d", "type": "torch.nn", "text": ["Applies a 2D convolution over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,Cin,H,W)(N, C_{\\text{in}}, H, W)  and output (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})  can be precisely described as:", "where \u22c6\\star  is the valid 2D cross-correlation operator, NN  is a batch size, CC  denotes a number of channels, HH  is a height of input planes in pixels, and WW  is width in pixels.", "This module supports TensorFloat32.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, dilation can either be:", "Note", "When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d.", "In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})  where"]}, {"name": "torch.nn.Conv3d", "path": "generated/torch.nn.conv3d#torch.nn.Conv3d", "type": "torch.nn", "text": ["Applies a 3D convolution over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,Cin,D,H,W)(N, C_{in}, D, H, W)  and output (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})  can be precisely described as:", "where \u22c6\\star  is the valid 3D cross-correlation operator", "This module supports TensorFloat32.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, dilation can either be:", "Note", "When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also known as a \u201cdepthwise convolution\u201d.", "In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a depthwise convolution with a depthwise multiplier K can be performed with the arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})  where", "Examples:"]}, {"name": "torch.nn.ConvTranspose1d", "path": "generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d", "type": "torch.nn", "text": ["Applies a 1D transposed convolution operator over an input image composed of several input planes.", "This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).", "This module supports TensorFloat32.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "Note", "The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv1d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.", "Note", "In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic =\nTrue. Please see the notes on Reproducibility for background.", "Output: (N,Cout,Lout)(N, C_{out}, L_{out})  where"]}, {"name": "torch.nn.ConvTranspose2d", "path": "generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d", "type": "torch.nn", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes.", "This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).", "This module supports TensorFloat32.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, output_padding can either be:", "Note", "The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv2d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.ConvTranspose3d", "path": "generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d", "type": "torch.nn", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes.", "This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).", "This module supports TensorFloat32.", "groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,", "The parameters kernel_size, stride, padding, output_padding can either be:", "Note", "The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride > 1, Conv3d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.CosineEmbeddingLoss", "path": "generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss", "type": "torch.nn", "text": ["Creates a criterion that measures the loss given input tensors x1x_1 , x2x_2  and a Tensor label yy  with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning.", "The loss function for each sample is:"]}, {"name": "torch.nn.CosineSimilarity", "path": "generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity", "type": "torch.nn", "text": ["Returns cosine similarity between x1x_1  and x2x_2 , computed along dim."]}, {"name": "torch.nn.CrossEntropyLoss", "path": "generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss", "type": "torch.nn", "text": ["This criterion combines LogSoftmax and NLLLoss in one single class.", "It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.", "The input is expected to contain raw, unnormalized scores for each class.", "input has to be a Tensor of size either (minibatch,C)(minibatch, C)  or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  for the K-dimensional case (described later).", "This criterion expects a class index in the range [0,C\u22121][0, C-1]  as the target for each value of a 1D tensor of size minibatch; if ignore_index is specified, this criterion also accepts this class index (this index may not necessarily be in the class range).", "The loss can be described as:", "or in the case of the weight argument being specified:", "The losses are averaged across observations for each minibatch. If the weight argument is specified then this is a weighted average:", "Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1 , where KK  is the number of dimensions, and a target of appropriate shape (see below).", "Examples:"]}, {"name": "torch.nn.CTCLoss", "path": "generated/torch.nn.ctcloss#torch.nn.CTCLoss", "type": "torch.nn", "text": ["The Connectionist Temporal Classification loss.", "Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d, which limits the length of the target sequence such that it must be \u2264\\leq  the input length.", "Examples:", "A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf", "Note", "In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T. blank=0blank=0 , target_lengths \u2264256\\leq 256 , the integer arguments must be of dtype torch.int32.", "The regular implementation uses the (more common in PyTorch) torch.long dtype.", "Note", "In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic =\nTrue. Please see the notes on Reproducibility for background."]}, {"name": "torch.nn.DataParallel", "path": "generated/torch.nn.dataparallel#torch.nn.DataParallel", "type": "torch.nn", "text": ["Implements data parallelism at the module level.", "This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.", "The batch size should be larger than the number of GPUs used.", "Warning", "It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node. See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel.", "Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be scattered on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model\u2019s forward pass.", "The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module.", "Warning", "In each forward, module is replicated on each device, so any updates to the running module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value because the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers.", "Warning", "Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device.", "Warning", "When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.", "Note", "There is a subtlety in using the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn\u2019t work with data parallelism section in FAQ for details.", "~DataParallel.module (Module) \u2013 the module to be parallelized", "Example:"]}, {"name": "torch.nn.Dropout", "path": "generated/torch.nn.dropout#torch.nn.Dropout", "type": "torch.nn", "text": ["During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.", "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors .", "Furthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p}  during training. This means that during evaluation the module simply computes an identity function.", "Examples:"]}, {"name": "torch.nn.Dropout2d", "path": "generated/torch.nn.dropout2d#torch.nn.Dropout2d", "type": "torch.nn", "text": ["Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "Usually the input comes from nn.Conv2d modules.", "As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.", "In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead.", "Examples:"]}, {"name": "torch.nn.Dropout3d", "path": "generated/torch.nn.dropout3d#torch.nn.Dropout3d", "type": "torch.nn", "text": ["Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "Usually the input comes from nn.Conv3d modules.", "As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease.", "In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead.", "Examples:"]}, {"name": "torch.nn.ELU", "path": "generated/torch.nn.elu#torch.nn.ELU", "type": "torch.nn", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.Embedding", "path": "generated/torch.nn.embedding#torch.nn.Embedding", "type": "torch.nn", "text": ["A simple lookup table that stores embeddings of a fixed dictionary and size.", "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.", "~Embedding.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1) ", "Note", "Keep in mind that only a limited number of optimizers support sparse gradients: currently it\u2019s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)", "Note", "With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero.", "Note", "When max_norm is not None, Embedding\u2019s forward method will modify the weight tensor in-place. Since tensors needed for gradient computations cannot be modified in-place, performing a differentiable operation on Embedding.weight before calling Embedding\u2019s forward method requires cloning Embedding.weight when max_norm is not None. For example:", "Examples:", "Creates Embedding instance from given 2-dimensional FloatTensor.", "Examples:"]}, {"name": "torch.nn.Embedding.from_pretrained()", "path": "generated/torch.nn.embedding#torch.nn.Embedding.from_pretrained", "type": "torch.nn", "text": ["Creates Embedding instance from given 2-dimensional FloatTensor.", "Examples:"]}, {"name": "torch.nn.EmbeddingBag", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag", "type": "torch.nn", "text": ["Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings.", "For bags of constant length and no per_sample_weights and 2D inputs, this class", "However, EmbeddingBag is much more time and memory efficient than using a chain of these operations.", "EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by mode. If per_sample_weights` is passed, the only supported mode is \"sum\", which computes a weighted sum according to per_sample_weights.", "~EmbeddingBag.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1) .", "per_index_weights (Tensor, optional)", "If input is 2D of shape (B, N),", "it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.", "If input is 1D of shape (N),", "it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.", "to indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None. Only supported for mode='sum'.", "Output shape: (B, embedding_dim)", "Examples:", "Creates EmbeddingBag instance from given 2-dimensional FloatTensor.", "Examples:"]}, {"name": "torch.nn.EmbeddingBag.from_pretrained()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.from_pretrained", "type": "torch.nn", "text": ["Creates EmbeddingBag instance from given 2-dimensional FloatTensor.", "Examples:"]}, {"name": "torch.nn.Flatten", "path": "generated/torch.nn.flatten#torch.nn.Flatten", "type": "torch.nn", "text": ["Flattens a contiguous range of dims into a tensor. For use with Sequential.", "Adds a child module to the current module.", "The module can be accessed as an attribute using the given name.", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:", "Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module", "Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:", "Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Moves all model parameters and buffers to the CPU.", "self", "Module", "Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Casts all floating point parameters and buffers to double datatype.", "self", "Module", "Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module", "Casts all floating point parameters and buffers to float datatype.", "self", "Module", "Casts all floating point parameters and buffers to half datatype.", "self", "Module", "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields", "Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:", "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:", "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:", "Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:", "Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:", "Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name.", "Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module", "Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:", "Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:", "Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module", "Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module", "Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.Flatten.add_module()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.add_module", "type": "torch.nn", "text": ["Adds a child module to the current module.", "The module can be accessed as an attribute using the given name."]}, {"name": "torch.nn.Flatten.apply()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.apply", "type": "torch.nn", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:"]}, {"name": "torch.nn.Flatten.bfloat16()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.bfloat16", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module"]}, {"name": "torch.nn.Flatten.buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.buffers", "type": "torch.nn", "text": ["Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:"]}, {"name": "torch.nn.Flatten.children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.children", "type": "torch.nn", "text": ["Returns an iterator over immediate children modules.", "Module \u2013 a child module"]}, {"name": "torch.nn.Flatten.cpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cpu", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the CPU.", "self", "Module"]}, {"name": "torch.nn.Flatten.cuda()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cuda", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Flatten.double()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.double", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to double datatype.", "self", "Module"]}, {"name": "torch.nn.Flatten.eval()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.eval", "type": "torch.nn", "text": ["Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module"]}, {"name": "torch.nn.Flatten.float()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.float", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to float datatype.", "self", "Module"]}, {"name": "torch.nn.Flatten.half()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.half", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to half datatype.", "self", "Module"]}, {"name": "torch.nn.Flatten.load_state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.load_state_dict", "type": "torch.nn", "text": ["Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields"]}, {"name": "torch.nn.Flatten.modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.modules", "type": "torch.nn", "text": ["Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Flatten.named_buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_buffers", "type": "torch.nn", "text": ["Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:"]}, {"name": "torch.nn.Flatten.named_children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_children", "type": "torch.nn", "text": ["Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:"]}, {"name": "torch.nn.Flatten.named_modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_modules", "type": "torch.nn", "text": ["Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Flatten.named_parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_parameters", "type": "torch.nn", "text": ["Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:"]}, {"name": "torch.nn.Flatten.parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.parameters", "type": "torch.nn", "text": ["Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:"]}, {"name": "torch.nn.Flatten.register_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Flatten.register_buffer()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_buffer", "type": "torch.nn", "text": ["Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:"]}, {"name": "torch.nn.Flatten.register_forward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_hook", "type": "torch.nn", "text": ["Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Flatten.register_forward_pre_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_pre_hook", "type": "torch.nn", "text": ["Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Flatten.register_full_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_full_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Flatten.register_parameter()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_parameter", "type": "torch.nn", "text": ["Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name."]}, {"name": "torch.nn.Flatten.requires_grad_()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.requires_grad_", "type": "torch.nn", "text": ["Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module"]}, {"name": "torch.nn.Flatten.state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.state_dict", "type": "torch.nn", "text": ["Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:"]}, {"name": "torch.nn.Flatten.to()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.to", "type": "torch.nn", "text": ["Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:"]}, {"name": "torch.nn.Flatten.train()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.train", "type": "torch.nn", "text": ["Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module"]}, {"name": "torch.nn.Flatten.type()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.type", "type": "torch.nn", "text": ["Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module"]}, {"name": "torch.nn.Flatten.xpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.xpu", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Flatten.zero_grad()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.zero_grad", "type": "torch.nn", "text": ["Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.Fold", "path": "generated/torch.nn.fold#torch.nn.Fold", "type": "torch.nn", "text": ["Combines an array of sliding local blocks into a large containing tensor.", "Consider a batched input tensor containing sliding local blocks, e.g., patches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L) , where NN  is batch dimension, C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})  is the number of values within a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})  spatial locations each containing a CC -channeled vector), and LL  is the total number of blocks. (This is exactly the same specification as the output shape of Unfold.) This operation combines these local blocks into the large output tensor of shape (N,C,output_size[0],output_size[1],\u2026)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)  by summing the overlapping values. Similar to Unfold, the arguments must satisfy", "where dd  is over all spatial dimensions.", "The padding, stride and dilation arguments specify how the sliding blocks are retrieved.", "Note", "Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.", "In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters:", "Then for any (supported) input tensor the following equality holds:", "where divisor is a tensor that depends only on the shape and dtype of the input:", "When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).", "Warning", "Currently, only 4-D output tensors (batched image-like tensors) are supported.", "Examples:"]}, {"name": "torch.nn.FractionalMaxPool2d", "path": "generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d", "type": "torch.nn", "text": ["Applies a 2D fractional max pooling over an input signal composed of several input planes.", "Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham", "The max-pooling operation is applied in kH\u00d7kWkH \\times kW  regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes."]}, {"name": "torch.nn.functional", "path": "nn.functional", "type": "torch.nn.functional", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "This operator supports TensorFloat32.", "See Conv1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:", "Applies a 2D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:", "Applies a 3D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:", "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:", "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:", "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d", "This operator supports TensorFloat32.", "See ConvTranspose3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:", "Extracts sliding local blocks from a batched input tensor.", "Warning", "Currently, only 4-D input tensors (batched image-like tensors) are supported.", "Warning", "More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.", "See torch.nn.Unfold for details", "Combines an array of sliding local blocks into a large containing tensor.", "Warning", "Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.", "See torch.nn.Fold for details", "Applies a 1D average pooling over an input signal composed of several input planes.", "See AvgPool1d for details and output shape.", "Examples:", "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW  regions by step size sH\u00d7sWsH \\times sW  steps. The number of output features is equal to the number of input planes.", "See AvgPool2d for details and output shape.", "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW  regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW  steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor .", "See AvgPool3d for details and output shape.", "Applies a 1D max pooling over an input signal composed of several input planes.", "See MaxPool1d for details.", "Applies a 2D max pooling over an input signal composed of several input planes.", "See MaxPool2d for details.", "Applies a 3D max pooling over an input signal composed of several input planes.", "See MaxPool3d for details.", "Computes a partial inverse of MaxPool1d.", "See MaxUnpool1d for details.", "Computes a partial inverse of MaxPool2d.", "See MaxUnpool2d for details.", "Computes a partial inverse of MaxPool3d.", "See MaxUnpool3d for details.", "Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool1d for details.", "Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool2d for details.", "Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool1d for details and output shape.", "Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool2d for details and output shape.", "Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool3d for details and output shape.", "Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool1d for details and output shape.", "output_size \u2013 the target output size (single integer)", "Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool2d for details and output shape.", "output_size \u2013 the target output size (single integer or double-integer tuple)", "Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool3d for details and output shape.", "output_size \u2013 the target output size (single integer or triple-integer tuple)", "Thresholds each element of the input Tensor.", "See Threshold for more details.", "In-place version of threshold().", "Applies the rectified linear unit function element-wise. See ReLU for more details.", "In-place version of relu().", "Applies the HardTanh function element-wise. See Hardtanh for more details.", "In-place version of hardtanh().", "Applies the hardswish function, element-wise, as described in the paper:", "Searching for MobileNetV3.", "See Hardswish for more details.", "Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6) .", "See ReLU6 for more details.", "Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .", "See ELU for more details.", "In-place version of elu().", "Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717  and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 .", "See SELU for more details.", "Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .", "See CELU for more details.", "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x) ", "See LeakyReLU for more details.", "In-place version of leaky_relu().", "Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)  where weight is a learnable parameter.", "See PReLU for more details.", "Randomized leaky ReLU.", "See RReLU for more details.", "In-place version of rrelu().", "The gated linear unit. Computes:", "where input is split in half along dim to form a and b, \u03c3\\sigma  is the sigmoid function and \u2297\\otimes  is the element-wise product between matrices.", "See Language Modeling with Gated Convolutional Networks.", "Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x) ", "where \u03a6(x)\\Phi(x)  is the Cumulative Distribution Function for Gaussian Distribution.", "See Gaussian Error Linear Units (GELUs).", "Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right) ", "See LogSigmoid for more details.", "Applies the hard shrinkage function element-wise", "See Hardshrink for more details.", "Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x) ", "See Tanhshrink for more details.", "Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|} ", "See Softsign for more details.", "Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) .", "For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold .", "See Softplus for more details.", "Applies a softmin function.", "Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See softmax definition for mathematical formula.", "See Softmin for more details.", "Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} ", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.", "See Softmax for more details.", "Note", "This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties).", "Applies the soft shrinkage function elementwise", "See Softshrink for more details.", "Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.", "Sampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.", "Note", "This function is here for legacy reasons, may be removed from nn.Functional in the future.", "Note", "The main trick for hard is to do y_hard - y_soft.detach() + y_soft", "It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)", "Applies a softmax followed by a logarithm.", "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.", "See LogSoftmax for more details.", "Applies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)} ", "See Tanh for more details.", "Applies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)} ", "See Sigmoid for more details.", "Applies the element-wise function", "inplace \u2013 If set to True, will do this operation in-place. Default: False", "See Hardsigmoid for more details.", "Applies the silu function, element-wise.", "Note", "See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.", "See SiLU for more details.", "Applies Batch Normalization for each channel across a batch of data.", "See BatchNorm1d, BatchNorm2d, BatchNorm3d for details.", "Applies Instance Normalization for each channel in each data sample in a batch.", "See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details.", "Applies Layer Normalization for last certain number of dimensions.", "See LayerNorm for details.", "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.", "See LocalResponseNorm for details.", "Performs LpL_p  normalization of inputs over specified dimension.", "For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k) , each ndimn_{dim}  -element vector vv  along dimension dim is transformed as", "With the default arguments it uses the Euclidean norm over vectors along dimension 11  for normalization.", "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b .", "This operator supports TensorFloat32.", "Shape:", "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b ", "Shape:", "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "See Dropout for details.", "Applies alpha dropout to the input.", "See AlphaDropout for details.", "Randomly masks out entire channels (a channel is a feature map, e.g. the jj -th channel of the ii -th sample in the batch input is a tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function.", "Each element will be masked independently on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.", "See FeatureAlphaDropout for details.", "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout2d for details.", "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout3d for details.", "A simple lookup table that looks up embeddings in a fixed dictionary and size.", "This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.", "See torch.nn.Embedding for more details.", "where V = maximum index + 1 and embedding_dim = the embedding size", "Examples:", "Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.", "See torch.nn.EmbeddingBag for more details.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Shape:", "input (LongTensor) and offsets (LongTensor, optional)", "If input is 2D of shape (B, N),", "it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.", "If input is 1D of shape (N),", "it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.", "Examples:", "Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.", "See also One-hot on Wikipedia .", "LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.", "See torch.nn.PairwiseDistance for details", "Returns cosine similarity between x1 and x2, computed along dim.", "Example:", "Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous.", "If input has shape N\u00d7MN \\times M  then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1) .", "This function is equivalent to scipy.spatial.distance.pdist(input, \u2018minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()).", "Function that measures the Binary Cross Entropy between the target and the output.", "See BCELoss for details.", "Examples:", "Function that measures Binary Cross Entropy between target and output logits.", "See BCEWithLogitsLoss for details.", "Examples:", "Poisson negative log likelihood loss.", "See PoissonNLLLoss for details.", "See CosineEmbeddingLoss for details.", "This criterion combines log_softmax and nll_loss in a single function.", "See CrossEntropyLoss for details.", "Examples:", "The Connectionist Temporal Classification loss.", "See CTCLoss for details.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Example:", "See HingeEmbeddingLoss for details.", "The Kullback-Leibler divergence Loss", "See KLDivLoss for details.", "Note", "size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.", "Note", ":attr:reduction = 'mean' doesn\u2019t return the true kl divergence value, please use :attr:reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as \u2018batchmean\u2019.", "Function that takes the mean element-wise absolute value difference.", "See L1Loss for details.", "Measures the element-wise mean squared error.", "See MSELoss for details.", "See MarginRankingLoss for details.", "See MultiLabelMarginLoss for details.", "See MultiLabelSoftMarginLoss for details.", "reduce=None, reduction=\u2019mean\u2019) -> Tensor", "See MultiMarginLoss for details.", "The negative log likelihood loss.", "See NLLLoss for details.", "Example:", "Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "See SmoothL1Loss for details.", "See SoftMarginLoss for details.", "See TripletMarginLoss for details", "See TripletMarginWithDistanceLoss for details.", "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is the upscale_factor.", "See PixelShuffle for details.", "Examples:", "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the downscale_factor.", "See PixelUnshuffle for details.", "Examples:", "Pads tensor.", "The padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor  dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right}) ; to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom}) ; to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom}  padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back}) .", "See torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Examples:", "Down/up samples the input to either the given size or the given scale_factor", "The algorithm used for interpolation is determined by mode.", "Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.", "Warning", "When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "The algorithm used for upsampling is determined by mode.", "Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.", "Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').", "Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).", "Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.", "Currently, only spatial (4-D) and volumetric (5-D) input are supported.", "In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})  and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out}) .", "For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels.", "grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input.", "If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are", "Note", "This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Note", "NaN values in grid would be interpreted as -1.", "output Tensor", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Note", "mode='bicubic' is implemented using the cubic convolution algorithm with \u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha  might be different from packages to packages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This algorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func: torch.clamp to ensure they are within the valid range.", "Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.", "Note", "This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .", "output Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Warning", "When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are considered to be at `0 (the center of the input image).", "Evaluates module(input) in parallel across the GPUs given in device_ids.", "This is the functional version of the DataParallel module.", "a Tensor containing the result of module(input) located on output_device"]}, {"name": "torch.nn.functional.adaptive_avg_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool1d", "type": "torch.nn.functional", "text": ["Applies a 1D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool1d for details and output shape.", "output_size \u2013 the target output size (single integer)"]}, {"name": "torch.nn.functional.adaptive_avg_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool2d", "type": "torch.nn.functional", "text": ["Applies a 2D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool2d for details and output shape.", "output_size \u2013 the target output size (single integer or double-integer tuple)"]}, {"name": "torch.nn.functional.adaptive_avg_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool3d", "type": "torch.nn.functional", "text": ["Applies a 3D adaptive average pooling over an input signal composed of several input planes.", "See AdaptiveAvgPool3d for details and output shape.", "output_size \u2013 the target output size (single integer or triple-integer tuple)"]}, {"name": "torch.nn.functional.adaptive_max_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool1d", "type": "torch.nn.functional", "text": ["Applies a 1D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool1d for details and output shape."]}, {"name": "torch.nn.functional.adaptive_max_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool2d", "type": "torch.nn.functional", "text": ["Applies a 2D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool2d for details and output shape."]}, {"name": "torch.nn.functional.adaptive_max_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool3d", "type": "torch.nn.functional", "text": ["Applies a 3D adaptive max pooling over an input signal composed of several input planes.", "See AdaptiveMaxPool3d for details and output shape."]}, {"name": "torch.nn.functional.affine_grid()", "path": "nn.functional#torch.nn.functional.affine_grid", "type": "torch.nn.functional", "text": ["Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.", "Note", "This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .", "output Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Warning", "When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are considered to be at `0 (the center of the input image)."]}, {"name": "torch.nn.functional.alpha_dropout()", "path": "nn.functional#torch.nn.functional.alpha_dropout", "type": "torch.nn.functional", "text": ["Applies alpha dropout to the input.", "See AlphaDropout for details."]}, {"name": "torch.nn.functional.avg_pool1d()", "path": "nn.functional#torch.nn.functional.avg_pool1d", "type": "torch.nn.functional", "text": ["Applies a 1D average pooling over an input signal composed of several input planes.", "See AvgPool1d for details and output shape.", "Examples:"]}, {"name": "torch.nn.functional.avg_pool2d()", "path": "nn.functional#torch.nn.functional.avg_pool2d", "type": "torch.nn.functional", "text": ["Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW  regions by step size sH\u00d7sWsH \\times sW  steps. The number of output features is equal to the number of input planes.", "See AvgPool2d for details and output shape."]}, {"name": "torch.nn.functional.avg_pool3d()", "path": "nn.functional#torch.nn.functional.avg_pool3d", "type": "torch.nn.functional", "text": ["Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW  regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW  steps. The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor .", "See AvgPool3d for details and output shape."]}, {"name": "torch.nn.functional.batch_norm()", "path": "nn.functional#torch.nn.functional.batch_norm", "type": "torch.nn.functional", "text": ["Applies Batch Normalization for each channel across a batch of data.", "See BatchNorm1d, BatchNorm2d, BatchNorm3d for details."]}, {"name": "torch.nn.functional.bilinear()", "path": "nn.functional#torch.nn.functional.bilinear", "type": "torch.nn.functional", "text": ["Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + b ", "Shape:"]}, {"name": "torch.nn.functional.binary_cross_entropy()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy", "type": "torch.nn.functional", "text": ["Function that measures the Binary Cross Entropy between the target and the output.", "See BCELoss for details.", "Examples:"]}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy_with_logits", "type": "torch.nn.functional", "text": ["Function that measures Binary Cross Entropy between target and output logits.", "See BCEWithLogitsLoss for details.", "Examples:"]}, {"name": "torch.nn.functional.celu()", "path": "nn.functional#torch.nn.functional.celu", "type": "torch.nn.functional", "text": ["Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .", "See CELU for more details."]}, {"name": "torch.nn.functional.conv1d()", "path": "nn.functional#torch.nn.functional.conv1d", "type": "torch.nn.functional", "text": ["Applies a 1D convolution over an input signal composed of several input planes.", "This operator supports TensorFloat32.", "See Conv1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv2d()", "path": "nn.functional#torch.nn.functional.conv2d", "type": "torch.nn.functional", "text": ["Applies a 2D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv3d()", "path": "nn.functional#torch.nn.functional.conv3d", "type": "torch.nn.functional", "text": ["Applies a 3D convolution over an input image composed of several input planes.", "This operator supports TensorFloat32.", "See Conv3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv_transpose1d()", "path": "nn.functional#torch.nn.functional.conv_transpose1d", "type": "torch.nn.functional", "text": ["Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose1d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv_transpose2d()", "path": "nn.functional#torch.nn.functional.conv_transpose2d", "type": "torch.nn.functional", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.", "This operator supports TensorFloat32.", "See ConvTranspose2d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.conv_transpose3d()", "path": "nn.functional#torch.nn.functional.conv_transpose3d", "type": "torch.nn.functional", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d", "This operator supports TensorFloat32.", "See ConvTranspose3d for details and output shape.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Examples:"]}, {"name": "torch.nn.functional.cosine_embedding_loss()", "path": "nn.functional#torch.nn.functional.cosine_embedding_loss", "type": "torch.nn.functional", "text": ["See CosineEmbeddingLoss for details."]}, {"name": "torch.nn.functional.cosine_similarity()", "path": "nn.functional#torch.nn.functional.cosine_similarity", "type": "torch.nn.functional", "text": ["Returns cosine similarity between x1 and x2, computed along dim.", "Example:"]}, {"name": "torch.nn.functional.cross_entropy()", "path": "nn.functional#torch.nn.functional.cross_entropy", "type": "torch.nn.functional", "text": ["This criterion combines log_softmax and nll_loss in a single function.", "See CrossEntropyLoss for details.", "Examples:"]}, {"name": "torch.nn.functional.ctc_loss()", "path": "nn.functional#torch.nn.functional.ctc_loss", "type": "torch.nn.functional", "text": ["The Connectionist Temporal Classification loss.", "See CTCLoss for details.", "Note", "In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility for more information.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Example:"]}, {"name": "torch.nn.functional.dropout()", "path": "nn.functional#torch.nn.functional.dropout", "type": "torch.nn.functional", "text": ["During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.", "See Dropout for details."]}, {"name": "torch.nn.functional.dropout2d()", "path": "nn.functional#torch.nn.functional.dropout2d", "type": "torch.nn.functional", "text": ["Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout2d for details."]}, {"name": "torch.nn.functional.dropout3d()", "path": "nn.functional#torch.nn.functional.dropout3d", "type": "torch.nn.functional", "text": ["Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jj -th channel of the ii -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution.", "See Dropout3d for details."]}, {"name": "torch.nn.functional.elu()", "path": "nn.functional#torch.nn.functional.elu", "type": "torch.nn.functional", "text": ["Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .", "See ELU for more details."]}, {"name": "torch.nn.functional.elu_()", "path": "nn.functional#torch.nn.functional.elu_", "type": "torch.nn.functional", "text": ["In-place version of elu()."]}, {"name": "torch.nn.functional.embedding()", "path": "nn.functional#torch.nn.functional.embedding", "type": "torch.nn.functional", "text": ["A simple lookup table that looks up embeddings in a fixed dictionary and size.", "This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.", "See torch.nn.Embedding for more details.", "where V = maximum index + 1 and embedding_dim = the embedding size", "Examples:"]}, {"name": "torch.nn.functional.embedding_bag()", "path": "nn.functional#torch.nn.functional.embedding_bag", "type": "torch.nn.functional", "text": ["Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.", "See torch.nn.EmbeddingBag for more details.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "Shape:", "input (LongTensor) and offsets (LongTensor, optional)", "If input is 2D of shape (B, N),", "it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.", "If input is 1D of shape (N),", "it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.", "Examples:"]}, {"name": "torch.nn.functional.feature_alpha_dropout()", "path": "nn.functional#torch.nn.functional.feature_alpha_dropout", "type": "torch.nn.functional", "text": ["Randomly masks out entire channels (a channel is a feature map, e.g. the jj -th channel of the ii -th sample in the batch input is a tensor input[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function.", "Each element will be masked independently on every forward call with probability p using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.", "See FeatureAlphaDropout for details."]}, {"name": "torch.nn.functional.fold()", "path": "nn.functional#torch.nn.functional.fold", "type": "torch.nn.functional", "text": ["Combines an array of sliding local blocks into a large containing tensor.", "Warning", "Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.", "See torch.nn.Fold for details"]}, {"name": "torch.nn.functional.gelu()", "path": "nn.functional#torch.nn.functional.gelu", "type": "torch.nn.functional", "text": ["Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x) ", "where \u03a6(x)\\Phi(x)  is the Cumulative Distribution Function for Gaussian Distribution.", "See Gaussian Error Linear Units (GELUs)."]}, {"name": "torch.nn.functional.glu()", "path": "nn.functional#torch.nn.functional.glu", "type": "torch.nn.functional", "text": ["The gated linear unit. Computes:", "where input is split in half along dim to form a and b, \u03c3\\sigma  is the sigmoid function and \u2297\\otimes  is the element-wise product between matrices.", "See Language Modeling with Gated Convolutional Networks."]}, {"name": "torch.nn.functional.grid_sample()", "path": "nn.functional#torch.nn.functional.grid_sample", "type": "torch.nn.functional", "text": ["Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.", "Currently, only spatial (4-D) and volumetric (5-D) input are supported.", "In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})  and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out}) .", "For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels.", "grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values x = 1, y = 1 is the right-bottom pixel of input.", "If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are", "Note", "This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Note", "NaN values in grid would be interpreted as -1.", "output Tensor", "output (Tensor)", "Warning", "When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().", "Note", "mode='bicubic' is implemented using the cubic convolution algorithm with \u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha  might be different from packages to packages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This algorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func: torch.clamp to ensure they are within the valid range."]}, {"name": "torch.nn.functional.gumbel_softmax()", "path": "nn.functional#torch.nn.functional.gumbel_softmax", "type": "torch.nn.functional", "text": ["Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.", "Sampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.", "Note", "This function is here for legacy reasons, may be removed from nn.Functional in the future.", "Note", "The main trick for hard is to do y_hard - y_soft.detach() + y_soft", "It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)"]}, {"name": "torch.nn.functional.hardshrink()", "path": "nn.functional#torch.nn.functional.hardshrink", "type": "torch.nn.functional", "text": ["Applies the hard shrinkage function element-wise", "See Hardshrink for more details."]}, {"name": "torch.nn.functional.hardsigmoid()", "path": "nn.functional#torch.nn.functional.hardsigmoid", "type": "torch.nn.functional", "text": ["Applies the element-wise function", "inplace \u2013 If set to True, will do this operation in-place. Default: False", "See Hardsigmoid for more details."]}, {"name": "torch.nn.functional.hardswish()", "path": "nn.functional#torch.nn.functional.hardswish", "type": "torch.nn.functional", "text": ["Applies the hardswish function, element-wise, as described in the paper:", "Searching for MobileNetV3.", "See Hardswish for more details."]}, {"name": "torch.nn.functional.hardtanh()", "path": "nn.functional#torch.nn.functional.hardtanh", "type": "torch.nn.functional", "text": ["Applies the HardTanh function element-wise. See Hardtanh for more details."]}, {"name": "torch.nn.functional.hardtanh_()", "path": "nn.functional#torch.nn.functional.hardtanh_", "type": "torch.nn.functional", "text": ["In-place version of hardtanh()."]}, {"name": "torch.nn.functional.hinge_embedding_loss()", "path": "nn.functional#torch.nn.functional.hinge_embedding_loss", "type": "torch.nn.functional", "text": ["See HingeEmbeddingLoss for details."]}, {"name": "torch.nn.functional.instance_norm()", "path": "nn.functional#torch.nn.functional.instance_norm", "type": "torch.nn.functional", "text": ["Applies Instance Normalization for each channel in each data sample in a batch.", "See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details."]}, {"name": "torch.nn.functional.interpolate()", "path": "nn.functional#torch.nn.functional.interpolate", "type": "torch.nn.functional", "text": ["Down/up samples the input to either the given size or the given scale_factor", "The algorithm used for interpolation is determined by mode.", "Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.", "Warning", "When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.kl_div()", "path": "nn.functional#torch.nn.functional.kl_div", "type": "torch.nn.functional", "text": ["The Kullback-Leibler divergence Loss", "See KLDivLoss for details.", "Note", "size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.", "Note", ":attr:reduction = 'mean' doesn\u2019t return the true kl divergence value, please use :attr:reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as \u2018batchmean\u2019."]}, {"name": "torch.nn.functional.l1_loss()", "path": "nn.functional#torch.nn.functional.l1_loss", "type": "torch.nn.functional", "text": ["Function that takes the mean element-wise absolute value difference.", "See L1Loss for details."]}, {"name": "torch.nn.functional.layer_norm()", "path": "nn.functional#torch.nn.functional.layer_norm", "type": "torch.nn.functional", "text": ["Applies Layer Normalization for last certain number of dimensions.", "See LayerNorm for details."]}, {"name": "torch.nn.functional.leaky_relu()", "path": "nn.functional#torch.nn.functional.leaky_relu", "type": "torch.nn.functional", "text": ["Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x) ", "See LeakyReLU for more details."]}, {"name": "torch.nn.functional.leaky_relu_()", "path": "nn.functional#torch.nn.functional.leaky_relu_", "type": "torch.nn.functional", "text": ["In-place version of leaky_relu()."]}, {"name": "torch.nn.functional.linear()", "path": "nn.functional#torch.nn.functional.linear", "type": "torch.nn.functional", "text": ["Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b .", "This operator supports TensorFloat32.", "Shape:"]}, {"name": "torch.nn.functional.local_response_norm()", "path": "nn.functional#torch.nn.functional.local_response_norm", "type": "torch.nn.functional", "text": ["Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.", "See LocalResponseNorm for details."]}, {"name": "torch.nn.functional.logsigmoid()", "path": "nn.functional#torch.nn.functional.logsigmoid", "type": "torch.nn.functional", "text": ["Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right) ", "See LogSigmoid for more details."]}, {"name": "torch.nn.functional.log_softmax()", "path": "nn.functional#torch.nn.functional.log_softmax", "type": "torch.nn.functional", "text": ["Applies a softmax followed by a logarithm.", "While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.", "See LogSoftmax for more details."]}, {"name": "torch.nn.functional.lp_pool1d()", "path": "nn.functional#torch.nn.functional.lp_pool1d", "type": "torch.nn.functional", "text": ["Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool1d for details."]}, {"name": "torch.nn.functional.lp_pool2d()", "path": "nn.functional#torch.nn.functional.lp_pool2d", "type": "torch.nn.functional", "text": ["Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well.", "See LPPool2d for details."]}, {"name": "torch.nn.functional.margin_ranking_loss()", "path": "nn.functional#torch.nn.functional.margin_ranking_loss", "type": "torch.nn.functional", "text": ["See MarginRankingLoss for details."]}, {"name": "torch.nn.functional.max_pool1d()", "path": "nn.functional#torch.nn.functional.max_pool1d", "type": "torch.nn.functional", "text": ["Applies a 1D max pooling over an input signal composed of several input planes.", "See MaxPool1d for details."]}, {"name": "torch.nn.functional.max_pool2d()", "path": "nn.functional#torch.nn.functional.max_pool2d", "type": "torch.nn.functional", "text": ["Applies a 2D max pooling over an input signal composed of several input planes.", "See MaxPool2d for details."]}, {"name": "torch.nn.functional.max_pool3d()", "path": "nn.functional#torch.nn.functional.max_pool3d", "type": "torch.nn.functional", "text": ["Applies a 3D max pooling over an input signal composed of several input planes.", "See MaxPool3d for details."]}, {"name": "torch.nn.functional.max_unpool1d()", "path": "nn.functional#torch.nn.functional.max_unpool1d", "type": "torch.nn.functional", "text": ["Computes a partial inverse of MaxPool1d.", "See MaxUnpool1d for details."]}, {"name": "torch.nn.functional.max_unpool2d()", "path": "nn.functional#torch.nn.functional.max_unpool2d", "type": "torch.nn.functional", "text": ["Computes a partial inverse of MaxPool2d.", "See MaxUnpool2d for details."]}, {"name": "torch.nn.functional.max_unpool3d()", "path": "nn.functional#torch.nn.functional.max_unpool3d", "type": "torch.nn.functional", "text": ["Computes a partial inverse of MaxPool3d.", "See MaxUnpool3d for details."]}, {"name": "torch.nn.functional.mse_loss()", "path": "nn.functional#torch.nn.functional.mse_loss", "type": "torch.nn.functional", "text": ["Measures the element-wise mean squared error.", "See MSELoss for details."]}, {"name": "torch.nn.functional.multilabel_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_margin_loss", "type": "torch.nn.functional", "text": ["See MultiLabelMarginLoss for details."]}, {"name": "torch.nn.functional.multilabel_soft_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_soft_margin_loss", "type": "torch.nn.functional", "text": ["See MultiLabelSoftMarginLoss for details."]}, {"name": "torch.nn.functional.multi_margin_loss()", "path": "nn.functional#torch.nn.functional.multi_margin_loss", "type": "torch.nn.functional", "text": ["reduce=None, reduction=\u2019mean\u2019) -> Tensor", "See MultiMarginLoss for details."]}, {"name": "torch.nn.functional.nll_loss()", "path": "nn.functional#torch.nn.functional.nll_loss", "type": "torch.nn.functional", "text": ["The negative log likelihood loss.", "See NLLLoss for details.", "Example:"]}, {"name": "torch.nn.functional.normalize()", "path": "nn.functional#torch.nn.functional.normalize", "type": "torch.nn.functional", "text": ["Performs LpL_p  normalization of inputs over specified dimension.", "For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k) , each ndimn_{dim}  -element vector vv  along dimension dim is transformed as", "With the default arguments it uses the Euclidean norm over vectors along dimension 11  for normalization."]}, {"name": "torch.nn.functional.one_hot()", "path": "nn.functional#torch.nn.functional.one_hot", "type": "torch.nn.functional", "text": ["Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.", "See also One-hot on Wikipedia .", "LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else."]}, {"name": "torch.nn.functional.pad()", "path": "nn.functional#torch.nn.functional.pad", "type": "torch.nn.functional", "text": ["Pads tensor.", "The padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor  dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right}) ; to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom}) ; to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},  padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom}  padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back}) .", "See torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.", "Note", "When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.", "Examples:"]}, {"name": "torch.nn.functional.pairwise_distance()", "path": "nn.functional#torch.nn.functional.pairwise_distance", "type": "torch.nn.functional", "text": ["See torch.nn.PairwiseDistance for details"]}, {"name": "torch.nn.functional.pdist()", "path": "nn.functional#torch.nn.functional.pdist", "type": "torch.nn.functional", "text": ["Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous.", "If input has shape N\u00d7MN \\times M  then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1) .", "This function is equivalent to scipy.spatial.distance.pdist(input, \u2018minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())."]}, {"name": "torch.nn.functional.pixel_shuffle()", "path": "nn.functional#torch.nn.functional.pixel_shuffle", "type": "torch.nn.functional", "text": ["Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is the upscale_factor.", "See PixelShuffle for details.", "Examples:"]}, {"name": "torch.nn.functional.pixel_unshuffle()", "path": "nn.functional#torch.nn.functional.pixel_unshuffle", "type": "torch.nn.functional", "text": ["Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the downscale_factor.", "See PixelUnshuffle for details.", "Examples:"]}, {"name": "torch.nn.functional.poisson_nll_loss()", "path": "nn.functional#torch.nn.functional.poisson_nll_loss", "type": "torch.nn.functional", "text": ["Poisson negative log likelihood loss.", "See PoissonNLLLoss for details."]}, {"name": "torch.nn.functional.prelu()", "path": "nn.functional#torch.nn.functional.prelu", "type": "torch.nn.functional", "text": ["Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)  where weight is a learnable parameter.", "See PReLU for more details."]}, {"name": "torch.nn.functional.relu()", "path": "nn.functional#torch.nn.functional.relu", "type": "torch.nn.functional", "text": ["Applies the rectified linear unit function element-wise. See ReLU for more details."]}, {"name": "torch.nn.functional.relu6()", "path": "nn.functional#torch.nn.functional.relu6", "type": "torch.nn.functional", "text": ["Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6) .", "See ReLU6 for more details."]}, {"name": "torch.nn.functional.relu_()", "path": "nn.functional#torch.nn.functional.relu_", "type": "torch.nn.functional", "text": ["In-place version of relu()."]}, {"name": "torch.nn.functional.rrelu()", "path": "nn.functional#torch.nn.functional.rrelu", "type": "torch.nn.functional", "text": ["Randomized leaky ReLU.", "See RReLU for more details."]}, {"name": "torch.nn.functional.rrelu_()", "path": "nn.functional#torch.nn.functional.rrelu_", "type": "torch.nn.functional", "text": ["In-place version of rrelu()."]}, {"name": "torch.nn.functional.selu()", "path": "nn.functional#torch.nn.functional.selu", "type": "torch.nn.functional", "text": ["Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717  and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 .", "See SELU for more details."]}, {"name": "torch.nn.functional.sigmoid()", "path": "nn.functional#torch.nn.functional.sigmoid", "type": "torch.nn.functional", "text": ["Applies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)} ", "See Sigmoid for more details."]}, {"name": "torch.nn.functional.silu()", "path": "nn.functional#torch.nn.functional.silu", "type": "torch.nn.functional", "text": ["Applies the silu function, element-wise.", "Note", "See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.", "See SiLU for more details."]}, {"name": "torch.nn.functional.smooth_l1_loss()", "path": "nn.functional#torch.nn.functional.smooth_l1_loss", "type": "torch.nn.functional", "text": ["Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.", "See SmoothL1Loss for details."]}, {"name": "torch.nn.functional.softmax()", "path": "nn.functional#torch.nn.functional.softmax", "type": "torch.nn.functional", "text": ["Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} ", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.", "See Softmax for more details.", "Note", "This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties)."]}, {"name": "torch.nn.functional.softmin()", "path": "nn.functional#torch.nn.functional.softmin", "type": "torch.nn.functional", "text": ["Applies a softmin function.", "Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See softmax definition for mathematical formula.", "See Softmin for more details."]}, {"name": "torch.nn.functional.softplus()", "path": "nn.functional#torch.nn.functional.softplus", "type": "torch.nn.functional", "text": ["Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x)) .", "For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold .", "See Softplus for more details."]}, {"name": "torch.nn.functional.softshrink()", "path": "nn.functional#torch.nn.functional.softshrink", "type": "torch.nn.functional", "text": ["Applies the soft shrinkage function elementwise", "See Softshrink for more details."]}, {"name": "torch.nn.functional.softsign()", "path": "nn.functional#torch.nn.functional.softsign", "type": "torch.nn.functional", "text": ["Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|} ", "See Softsign for more details."]}, {"name": "torch.nn.functional.soft_margin_loss()", "path": "nn.functional#torch.nn.functional.soft_margin_loss", "type": "torch.nn.functional", "text": ["See SoftMarginLoss for details."]}, {"name": "torch.nn.functional.tanh()", "path": "nn.functional#torch.nn.functional.tanh", "type": "torch.nn.functional", "text": ["Applies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)} ", "See Tanh for more details."]}, {"name": "torch.nn.functional.tanhshrink()", "path": "nn.functional#torch.nn.functional.tanhshrink", "type": "torch.nn.functional", "text": ["Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x) ", "See Tanhshrink for more details."]}, {"name": "torch.nn.functional.threshold()", "path": "nn.functional#torch.nn.functional.threshold", "type": "torch.nn.functional", "text": ["Thresholds each element of the input Tensor.", "See Threshold for more details."]}, {"name": "torch.nn.functional.threshold_()", "path": "nn.functional#torch.nn.functional.threshold_", "type": "torch.nn.functional", "text": ["In-place version of threshold()."]}, {"name": "torch.nn.functional.triplet_margin_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_loss", "type": "torch.nn.functional", "text": ["See TripletMarginLoss for details"]}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_with_distance_loss", "type": "torch.nn.functional", "text": ["See TripletMarginWithDistanceLoss for details."]}, {"name": "torch.nn.functional.unfold()", "path": "nn.functional#torch.nn.functional.unfold", "type": "torch.nn.functional", "text": ["Extracts sliding local blocks from a batched input tensor.", "Warning", "Currently, only 4-D input tensors (batched image-like tensors) are supported.", "Warning", "More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.", "See torch.nn.Unfold for details"]}, {"name": "torch.nn.functional.upsample()", "path": "nn.functional#torch.nn.functional.upsample", "type": "torch.nn.functional", "text": ["Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "The algorithm used for upsampling is determined by mode.", "Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)", "Note", "With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs."]}, {"name": "torch.nn.functional.upsample_bilinear()", "path": "nn.functional#torch.nn.functional.upsample_bilinear", "type": "torch.nn.functional", "text": ["Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).", "Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.functional.upsample_nearest()", "path": "nn.functional#torch.nn.functional.upsample_nearest", "type": "torch.nn.functional", "text": ["Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').", "Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information."]}, {"name": "torch.nn.GaussianNLLLoss", "path": "generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss", "type": "torch.nn", "text": ["Gaussian negative log likelihood loss.", "The targets are treated as samples from Gaussian distributions with expectations and variances predicted by the neural network. For a D-dimensional target tensor modelled as having heteroscedastic Gaussian distributions with a D-dimensional tensor of expectations input and a D-dimensional tensor of positive variances var the loss is:", "where eps is used for stability. By default, the constant term of the loss function is omitted unless full is True. If var is a scalar (implying target tensor has homoscedastic Gaussian distributions) it is broadcasted to be the same size as the input.", "Examples:", "Note", "The clamping of var is ignored with respect to autograd, and so the gradients are unaffected by it.", "Nix, D. A. and Weigend, A. S., \u201cEstimating the mean and variance of the target probability distribution\u201d, Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN\u201994), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi: 10.1109/ICNN.1994.374138."]}, {"name": "torch.nn.GELU", "path": "generated/torch.nn.gelu#torch.nn.GELU", "type": "torch.nn", "text": ["Applies the Gaussian Error Linear Units function:", "where \u03a6(x)\\Phi(x)  is the Cumulative Distribution Function for Gaussian Distribution.", "Examples:"]}, {"name": "torch.nn.GroupNorm", "path": "generated/torch.nn.groupnorm#torch.nn.GroupNorm", "type": "torch.nn", "text": ["Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization", "The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. \u03b3\\gamma  and \u03b2\\beta  are learnable per-channel affine transform parameter vectors of size num_channels if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "This layer uses statistics computed from input data in both training and evaluation modes.", "Examples:"]}, {"name": "torch.nn.GRU", "path": "generated/torch.nn.gru#torch.nn.GRU", "type": "torch.nn", "text": ["Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t  is the hidden state at time t, xtx_t  is the input at time t, h(t\u22121)h_{(t-1)}  is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_t , ztz_t , ntn_t  are the reset, update, and new gates, respectively. \u03c3\\sigma  is the sigmoid function, and \u2217*  is the Hadamard product.", "In a multilayer GRU, the input xt(l)x^{(l)}_t  of the ll  -th layer (l>=2l >= 2 ) is the hidden state ht(l\u22121)h^{(l-1)}_t  of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t  where each \u03b4t(l\u22121)\\delta^{(l-1)}_t  is a Bernoulli random variable which is 00  with probability dropout.", "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.", "Similarly, the directions can be separated in the packed case.", "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len", "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}} ", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.nn.GRUCell", "path": "generated/torch.nn.grucell#torch.nn.GRUCell", "type": "torch.nn", "text": ["A gated recurrent unit (GRU) cell", "where \u03c3\\sigma  is the sigmoid function, and \u2217*  is the Hadamard product.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}} ", "Examples:"]}, {"name": "torch.nn.Hardshrink", "path": "generated/torch.nn.hardshrink#torch.nn.Hardshrink", "type": "torch.nn", "text": ["Applies the hard shrinkage function element-wise:", "lambd \u2013 the \u03bb\\lambda  value for the Hardshrink formulation. Default: 0.5", "Examples:"]}, {"name": "torch.nn.Hardsigmoid", "path": "generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid", "type": "torch.nn", "text": ["Applies the element-wise function:", "inplace \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.Hardswish", "path": "generated/torch.nn.hardswish#torch.nn.Hardswish", "type": "torch.nn", "text": ["Applies the hardswish function, element-wise, as described in the paper:", "Searching for MobileNetV3.", "inplace \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.Hardtanh", "path": "generated/torch.nn.hardtanh#torch.nn.Hardtanh", "type": "torch.nn", "text": ["Applies the HardTanh function element-wise", "HardTanh is defined as:", "The range of the linear region [\u22121,1][-1, 1]  can be adjusted using min_val and max_val.", "Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val.", "Examples:"]}, {"name": "torch.nn.HingeEmbeddingLoss", "path": "generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss", "type": "torch.nn", "text": ["Measures the loss given an input tensor xx  and a labels tensor yy  (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as xx , and is typically used for learning nonlinear embeddings or semi-supervised learning.", "The loss function for nn -th sample in the mini-batch is", "and the total loss functions is", "where L={l1,\u2026,lN}\u22a4L = \\{l_1,\\dots,l_N\\}^\\top ."]}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.identity#torch.nn.Identity", "type": "torch.nn", "text": ["A placeholder identity operator that is argument-insensitive.", "Examples:"]}, {"name": "torch.nn.init", "path": "nn.init", "type": "torch.nn.init", "text": ["Return the recommended gain value for the given nonlinearity function. The values are as follows:", "nonlinearity", "gain", "Linear / Identity", "11 ", "Conv{1,2,3}D", "11 ", "Sigmoid", "11 ", "Tanh", "53\\frac{5}{3} ", "ReLU", "2\\sqrt{2} ", "Leaky Relu", "21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}} ", "SELU", "34\\frac{3}{4} ", "Fills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b) .", "Fills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .", "Fills the input Tensor with the value val\\text{val} .", "Fills the input Tensor with the scalar value 1.", "tensor \u2013 an n-dimensional torch.Tensor", "Fills the input Tensor with the scalar value 0.", "tensor \u2013 an n-dimensional torch.Tensor", "Fills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.", "tensor \u2013 a 2-dimensional torch.Tensor", "Fills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity", "Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a)  where", "Also known as Glorot initialization.", "Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where", "Also known as Glorot initialization.", "Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})  where", "Also known as He initialization.", "Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where", "Also known as He initialization.", "Fills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.", "Fills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as described in Deep learning via Hessian-free optimization - Martens, J. (2010)."]}, {"name": "torch.nn.init.calculate_gain()", "path": "nn.init#torch.nn.init.calculate_gain", "type": "torch.nn.init", "text": ["Return the recommended gain value for the given nonlinearity function. The values are as follows:", "nonlinearity", "gain", "Linear / Identity", "11 ", "Conv{1,2,3}D", "11 ", "Sigmoid", "11 ", "Tanh", "53\\frac{5}{3} ", "ReLU", "2\\sqrt{2} ", "Leaky Relu", "21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}} ", "SELU", "34\\frac{3}{4} "]}, {"name": "torch.nn.init.constant_()", "path": "nn.init#torch.nn.init.constant_", "type": "torch.nn.init", "text": ["Fills the input Tensor with the value val\\text{val} ."]}, {"name": "torch.nn.init.dirac_()", "path": "nn.init#torch.nn.init.dirac_", "type": "torch.nn.init", "text": ["Fills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity"]}, {"name": "torch.nn.init.eye_()", "path": "nn.init#torch.nn.init.eye_", "type": "torch.nn.init", "text": ["Fills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.", "tensor \u2013 a 2-dimensional torch.Tensor"]}, {"name": "torch.nn.init.kaiming_normal_()", "path": "nn.init#torch.nn.init.kaiming_normal_", "type": "torch.nn.init", "text": ["Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where", "Also known as He initialization."]}, {"name": "torch.nn.init.kaiming_uniform_()", "path": "nn.init#torch.nn.init.kaiming_uniform_", "type": "torch.nn.init", "text": ["Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})  where", "Also known as He initialization."]}, {"name": "torch.nn.init.normal_()", "path": "nn.init#torch.nn.init.normal_", "type": "torch.nn.init", "text": ["Fills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) ."]}, {"name": "torch.nn.init.ones_()", "path": "nn.init#torch.nn.init.ones_", "type": "torch.nn.init", "text": ["Fills the input Tensor with the scalar value 1.", "tensor \u2013 an n-dimensional torch.Tensor"]}, {"name": "torch.nn.init.orthogonal_()", "path": "nn.init#torch.nn.init.orthogonal_", "type": "torch.nn.init", "text": ["Fills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened."]}, {"name": "torch.nn.init.sparse_()", "path": "nn.init#torch.nn.init.sparse_", "type": "torch.nn.init", "text": ["Fills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as described in Deep learning via Hessian-free optimization - Martens, J. (2010)."]}, {"name": "torch.nn.init.uniform_()", "path": "nn.init#torch.nn.init.uniform_", "type": "torch.nn.init", "text": ["Fills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b) ."]}, {"name": "torch.nn.init.xavier_normal_()", "path": "nn.init#torch.nn.init.xavier_normal_", "type": "torch.nn.init", "text": ["Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)  where", "Also known as Glorot initialization."]}, {"name": "torch.nn.init.xavier_uniform_()", "path": "nn.init#torch.nn.init.xavier_uniform_", "type": "torch.nn.init", "text": ["Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a)  where", "Also known as Glorot initialization."]}, {"name": "torch.nn.init.zeros_()", "path": "nn.init#torch.nn.init.zeros_", "type": "torch.nn.init", "text": ["Fills the input Tensor with the scalar value 0.", "tensor \u2013 an n-dimensional torch.Tensor"]}, {"name": "torch.nn.InstanceNorm1d", "path": "generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d", "type": "torch.nn", "text": ["Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "By default, this layer uses instance statistics computed from input data in both training and evaluation modes.", "If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Note", "InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don\u2019t apply affine transform.", "Examples:"]}, {"name": "torch.nn.InstanceNorm2d", "path": "generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d", "type": "torch.nn", "text": ["Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "By default, this layer uses instance statistics computed from input data in both training and evaluation modes.", "If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Note", "InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don\u2019t apply affine transform.", "Examples:"]}, {"name": "torch.nn.InstanceNorm3d", "path": "generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d", "type": "torch.nn", "text": ["Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.", "The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "By default, this layer uses instance statistics computed from input data in both training and evaluation modes.", "If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Note", "InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don\u2019t apply affine transform.", "Examples:"]}, {"name": "torch.nn.intrinsic.ConvBn1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.nn.intrinsic.ConvBn2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.nn.intrinsic.ConvBnReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.nn.intrinsic.ConvBnReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.nn.intrinsic.ConvReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv1d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.nn.intrinsic.ConvReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv2d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.nn.intrinsic.qat.ConvBn2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": ["A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default."]}, {"name": "torch.nn.intrinsic.qat.ConvBnReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": ["A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d and torch.nn.ReLU.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.", "~ConvBnReLU2d.weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.nn.intrinsic.qat.ConvReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": ["A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.", "We combined the interface of Conv2d and BatchNorm2d.", "~ConvReLU2d.weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.nn.intrinsic.qat.LinearReLU", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.", "We adopt the same interface as torch.nn.Linear.", "Similar to torch.nn.intrinsic.LinearReLU, with FakeQuantize modules initialized to default.", "~LinearReLU.weight \u2013 fake quant module for weight", "Examples:"]}, {"name": "torch.nn.intrinsic.quantized.ConvReLU2d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": ["A ConvReLU2d module is a fused module of Conv2d and ReLU", "We adopt the same interface as torch.nn.quantized.Conv2d.", "as torch.nn.quantized.Conv2d (Same) \u2013 "]}, {"name": "torch.nn.intrinsic.quantized.ConvReLU3d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": ["A ConvReLU3d module is a fused module of Conv3d and ReLU", "We adopt the same interface as torch.nn.quantized.Conv3d.", "Attributes: Same as torch.nn.quantized.Conv3d"]}, {"name": "torch.nn.intrinsic.quantized.LinearReLU", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules", "We adopt the same interface as torch.nn.quantized.Linear.", "as torch.nn.quantized.Linear (Same) \u2013 ", "Examples:"]}, {"name": "torch.nn.KLDivLoss", "path": "generated/torch.nn.kldivloss#torch.nn.KLDivLoss", "type": "torch.nn", "text": ["The Kullback-Leibler divergence loss measure", "Kullback-Leibler divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions.", "As with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are interpreted as probabilities by default, but could be considered as log-probabilities with log_target set to True.", "This criterion expects a target Tensor of the same size as the input Tensor.", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where the index NN  spans all dimensions of input and LL  has the same shape as input. If reduction is not 'none' (default 'mean'), then:", "In default reduction mode 'mean', the losses are averaged for each minibatch over observations as well as over dimensions. 'batchmean' mode gives the correct KL divergence where losses are averaged over batch dimension only. 'mean' mode\u2019s behavior will be changed to the same as 'batchmean' in the next major release.", "Note", "size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.", "Note", "reduction = 'mean' doesn\u2019t return the true kl divergence value, please use reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as 'batchmean'."]}, {"name": "torch.nn.L1Loss", "path": "generated/torch.nn.l1loss#torch.nn.L1Loss", "type": "torch.nn", "text": ["Creates a criterion that measures the mean absolute error (MAE) between each element in the input xx  and target yy .", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN  is the batch size. If reduction is not 'none' (default 'mean'), then:", "xx  and yy  are tensors of arbitrary shapes with a total of nn  elements each.", "The sum operation still operates over all the elements, and divides by nn .", "The division by nn  can be avoided if one sets reduction = 'sum'.", "Supports real-valued and complex-valued inputs.", "Examples:"]}, {"name": "torch.nn.LayerNorm", "path": "generated/torch.nn.layernorm#torch.nn.LayerNorm", "type": "torch.nn", "text": ["Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization", "The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape. \u03b3\\gamma  and \u03b2\\beta  are learnable affine transform parameters of normalized_shape if elementwise_affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Note", "Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine.", "This layer uses statistics computed from input data in both training and evaluation modes.", "normalized_shape (int or list or torch.Size) \u2013 ", "input shape from an expected input of size", "If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.", "Examples:"]}, {"name": "torch.nn.LazyConv1d", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d", "type": "torch.nn", "text": ["A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size(1).", "See also", "torch.nn.Conv1d and torch.nn.modules.lazy.LazyModuleMixin", "alias of Conv1d"]}, {"name": "torch.nn.LazyConv1d.cls_to_become", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d.cls_to_become", "type": "torch.nn", "text": ["alias of Conv1d"]}, {"name": "torch.nn.LazyConv2d", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d", "type": "torch.nn", "text": ["A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size(1).", "See also", "torch.nn.Conv2d and torch.nn.modules.lazy.LazyModuleMixin", "alias of Conv2d"]}, {"name": "torch.nn.LazyConv2d.cls_to_become", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d.cls_to_become", "type": "torch.nn", "text": ["alias of Conv2d"]}, {"name": "torch.nn.LazyConv3d", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d", "type": "torch.nn", "text": ["A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size(1).", "See also", "torch.nn.Conv3d and torch.nn.modules.lazy.LazyModuleMixin", "alias of Conv3d"]}, {"name": "torch.nn.LazyConv3d.cls_to_become", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d.cls_to_become", "type": "torch.nn", "text": ["alias of Conv3d"]}, {"name": "torch.nn.LazyConvTranspose1d", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d", "type": "torch.nn", "text": ["A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size(1).", "See also", "torch.nn.ConvTranspose1d and torch.nn.modules.lazy.LazyModuleMixin", "alias of ConvTranspose1d"]}, {"name": "torch.nn.LazyConvTranspose1d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d.cls_to_become", "type": "torch.nn", "text": ["alias of ConvTranspose1d"]}, {"name": "torch.nn.LazyConvTranspose2d", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d", "type": "torch.nn", "text": ["A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size(1).", "See also", "torch.nn.ConvTranspose2d and torch.nn.modules.lazy.LazyModuleMixin", "alias of ConvTranspose2d"]}, {"name": "torch.nn.LazyConvTranspose2d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d.cls_to_become", "type": "torch.nn", "text": ["alias of ConvTranspose2d"]}, {"name": "torch.nn.LazyConvTranspose3d", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d", "type": "torch.nn", "text": ["A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size(1).", "See also", "torch.nn.ConvTranspose3d and torch.nn.modules.lazy.LazyModuleMixin", "alias of ConvTranspose3d"]}, {"name": "torch.nn.LazyConvTranspose3d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d.cls_to_become", "type": "torch.nn", "text": ["alias of ConvTranspose3d"]}, {"name": "torch.nn.LazyLinear", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear", "type": "torch.nn", "text": ["A torch.nn.Linear module with lazy initialization.", "In this module, the weight and bias are of torch.nn.UninitializedParameter class. They will be initialized after the first call to forward is done and the module will become a regular torch.nn.Linear module.", "Check the torch.nn.modules.lazy.LazyModuleMixin for further documentation on lazy modules and their limitations.", "alias of Linear"]}, {"name": "torch.nn.LazyLinear.cls_to_become", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear.cls_to_become", "type": "torch.nn", "text": ["alias of Linear"]}, {"name": "torch.nn.LeakyReLU", "path": "generated/torch.nn.leakyrelu#torch.nn.LeakyReLU", "type": "torch.nn", "text": ["Applies the element-wise function:", "or", "Examples:"]}, {"name": "torch.nn.Linear", "path": "generated/torch.nn.linear#torch.nn.Linear", "type": "torch.nn", "text": ["Applies a linear transformation to the incoming data: y=xAT+by = xA^T + b ", "This module supports TensorFloat32.", "Examples:"]}, {"name": "torch.nn.LocalResponseNorm", "path": "generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm", "type": "torch.nn", "text": ["Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.", "Examples:"]}, {"name": "torch.nn.LogSigmoid", "path": "generated/torch.nn.logsigmoid#torch.nn.LogSigmoid", "type": "torch.nn", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.LogSoftmax", "path": "generated/torch.nn.logsoftmax#torch.nn.LogSoftmax", "type": "torch.nn", "text": ["Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))  function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:", "dim (int) \u2013 A dimension along which LogSoftmax will be computed.", "a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)", "Examples:"]}, {"name": "torch.nn.LPPool1d", "path": "generated/torch.nn.lppool1d#torch.nn.LPPool1d", "type": "torch.nn", "text": ["Applies a 1D power-average pooling over an input signal composed of several input planes.", "On each window, the function computed is:", "Note", "If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.", "Output: (N,C,Lout)(N, C, L_{out}) , where"]}, {"name": "torch.nn.LPPool2d", "path": "generated/torch.nn.lppool2d#torch.nn.LPPool2d", "type": "torch.nn", "text": ["Applies a 2D power-average pooling over an input signal composed of several input planes.", "On each window, the function computed is:", "The parameters kernel_size, stride can either be:", "Note", "If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where", "Examples:"]}, {"name": "torch.nn.LSTM", "path": "generated/torch.nn.lstm#torch.nn.LSTM", "type": "torch.nn", "text": ["Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t  is the hidden state at time t, ctc_t  is the cell state at time t, xtx_t  is the input at time t, ht\u22121h_{t-1}  is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and iti_t , ftf_t , gtg_t , oto_t  are the input, forget, cell, and output gates, respectively. \u03c3\\sigma  is the sigmoid function, and \u2299\\odot  is the Hadamard product.", "In a multilayer LSTM, the input xt(l)x^{(l)}_t  of the ll  -th layer (l>=2l >= 2 ) is the hidden state ht(l\u22121)h^{(l-1)}_t  of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t  where each \u03b4t(l\u22121)\\delta^{(l-1)}_t  is a Bernoulli random variable which is 00  with probability dropout.", "If proj_size > 0 is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the dimension of hth_t  will be changed from hidden_size to proj_size (dimensions of WhiW_{hi}  will be changed accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: ht=Whrhth_t = W_{hr}h_t . Note that as a consequence of this, the output of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.", "c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.", "If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.", "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. If proj_size > 0 was specified, output shape will be (seq_len, batch, num_directions * proj_size).", "For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.", "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. If proj_size > 0 was specified, h_n shape will be (num_layers * num_directions, batch, proj_size).", "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}} ", "Warning", "There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:", "On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1. This may affect performance.", "On CUDA 10.2 or later, set environment variable (note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2.", "See the cuDNN 8 Release Notes for more information.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.nn.LSTMCell", "path": "generated/torch.nn.lstmcell#torch.nn.LSTMCell", "type": "torch.nn", "text": ["A long short-term memory (LSTM) cell.", "where \u03c3\\sigma  is the sigmoid function, and \u2217*  is the Hadamard product.", "c_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch.", "If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}} ", "Examples:"]}, {"name": "torch.nn.MarginRankingLoss", "path": "generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss", "type": "torch.nn", "text": ["Creates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yy  (containing 1 or -1).", "If y=1y = 1  then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1 .", "The loss function for each pair of samples in the mini-batch is:", "Examples:"]}, {"name": "torch.nn.MaxPool1d", "path": "generated/torch.nn.maxpool1d#torch.nn.MaxPool1d", "type": "torch.nn", "text": ["Applies a 1D max pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L)  and output (N,C,Lout)(N, C, L_{out})  can be precisely described as:", "If padding is non-zero, then the input is implicitly padded with negative infinity on both sides for padding number of points. dilation is the stride between the elements within the sliding window. This link has a nice visualization of the pooling parameters.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "Output: (N,C,Lout)(N, C, L_{out}) , where", "Examples:"]}, {"name": "torch.nn.MaxPool2d", "path": "generated/torch.nn.maxpool2d#torch.nn.MaxPool2d", "type": "torch.nn", "text": ["Applies a 2D max pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  and kernel_size (kH,kW)(kH, kW)  can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding, dilation can either be:", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where", "Examples:"]}, {"name": "torch.nn.MaxPool3d", "path": "generated/torch.nn.maxpool3d#torch.nn.MaxPool3d", "type": "torch.nn", "text": ["Applies a 3D max pooling over an input signal composed of several input planes.", "In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  and kernel_size (kD,kH,kW)(kD, kH, kW)  can be precisely described as:", "If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.", "Note", "When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.", "The parameters kernel_size, stride, padding, dilation can either be:", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where", "Examples:"]}, {"name": "torch.nn.MaxUnpool1d", "path": "generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d", "type": "torch.nn", "text": ["Computes a partial inverse of MaxPool1d.", "MaxPool1d is not fully invertible, since the non-maximal values are lost.", "MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.", "Note", "MaxPool1d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.", "Output: (N,C,Hout)(N, C, H_{out}) , where", "or as given by output_size in the call operator", "Example:"]}, {"name": "torch.nn.MaxUnpool2d", "path": "generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d", "type": "torch.nn", "text": ["Computes a partial inverse of MaxPool2d.", "MaxPool2d is not fully invertible, since the non-maximal values are lost.", "MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.", "Note", "MaxPool2d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where", "or as given by output_size in the call operator", "Example:"]}, {"name": "torch.nn.MaxUnpool3d", "path": "generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d", "type": "torch.nn", "text": ["Computes a partial inverse of MaxPool3d.", "MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.", "Note", "MaxPool3d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs section below.", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where", "or as given by output_size in the call operator", "Example:"]}, {"name": "torch.nn.Module", "path": "generated/torch.nn.module#torch.nn.Module", "type": "torch.nn", "text": ["Base class for all neural network modules.", "Your models should also subclass this class.", "Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:", "Submodules assigned in this way will be registered, and will have their parameters converted too when you call to(), etc.", "training (bool) \u2013 Boolean represents whether this module is in training or evaluation mode.", "Adds a child module to the current module.", "The module can be accessed as an attribute using the given name.", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:", "Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module", "Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:", "Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Moves all model parameters and buffers to the CPU.", "self", "Module", "Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Casts all floating point parameters and buffers to double datatype.", "self", "Module", "This allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading.", "If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change.", "Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module", "Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.", "Casts all floating point parameters and buffers to float datatype.", "self", "Module", "Defines the computation performed at every call.", "Should be overridden by all subclasses.", "Note", "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.", "Casts all floating point parameters and buffers to half datatype.", "self", "Module", "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields", "Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:", "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:", "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:", "Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:", "Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:", "Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name.", "Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module", "Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:", "Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:", "Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module", "Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module", "Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.Module.add_module()", "path": "generated/torch.nn.module#torch.nn.Module.add_module", "type": "torch.nn", "text": ["Adds a child module to the current module.", "The module can be accessed as an attribute using the given name."]}, {"name": "torch.nn.Module.apply()", "path": "generated/torch.nn.module#torch.nn.Module.apply", "type": "torch.nn", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:"]}, {"name": "torch.nn.Module.bfloat16()", "path": "generated/torch.nn.module#torch.nn.Module.bfloat16", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module"]}, {"name": "torch.nn.Module.buffers()", "path": "generated/torch.nn.module#torch.nn.Module.buffers", "type": "torch.nn", "text": ["Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:"]}, {"name": "torch.nn.Module.children()", "path": "generated/torch.nn.module#torch.nn.Module.children", "type": "torch.nn", "text": ["Returns an iterator over immediate children modules.", "Module \u2013 a child module"]}, {"name": "torch.nn.Module.cpu()", "path": "generated/torch.nn.module#torch.nn.Module.cpu", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the CPU.", "self", "Module"]}, {"name": "torch.nn.Module.cuda()", "path": "generated/torch.nn.module#torch.nn.Module.cuda", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Module.double()", "path": "generated/torch.nn.module#torch.nn.Module.double", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to double datatype.", "self", "Module"]}, {"name": "torch.nn.Module.dump_patches", "path": "generated/torch.nn.module#torch.nn.Module.dump_patches", "type": "torch.nn", "text": ["This allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading.", "If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change."]}, {"name": "torch.nn.Module.eval()", "path": "generated/torch.nn.module#torch.nn.Module.eval", "type": "torch.nn", "text": ["Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module"]}, {"name": "torch.nn.Module.extra_repr()", "path": "generated/torch.nn.module#torch.nn.Module.extra_repr", "type": "torch.nn", "text": ["Set the extra representation of the module", "To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable."]}, {"name": "torch.nn.Module.float()", "path": "generated/torch.nn.module#torch.nn.Module.float", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to float datatype.", "self", "Module"]}, {"name": "torch.nn.Module.forward()", "path": "generated/torch.nn.module#torch.nn.Module.forward", "type": "torch.nn", "text": ["Defines the computation performed at every call.", "Should be overridden by all subclasses.", "Note", "Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them."]}, {"name": "torch.nn.Module.half()", "path": "generated/torch.nn.module#torch.nn.Module.half", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to half datatype.", "self", "Module"]}, {"name": "torch.nn.Module.load_state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.load_state_dict", "type": "torch.nn", "text": ["Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields"]}, {"name": "torch.nn.Module.modules()", "path": "generated/torch.nn.module#torch.nn.Module.modules", "type": "torch.nn", "text": ["Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Module.named_buffers()", "path": "generated/torch.nn.module#torch.nn.Module.named_buffers", "type": "torch.nn", "text": ["Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:"]}, {"name": "torch.nn.Module.named_children()", "path": "generated/torch.nn.module#torch.nn.Module.named_children", "type": "torch.nn", "text": ["Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:"]}, {"name": "torch.nn.Module.named_modules()", "path": "generated/torch.nn.module#torch.nn.Module.named_modules", "type": "torch.nn", "text": ["Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Module.named_parameters()", "path": "generated/torch.nn.module#torch.nn.Module.named_parameters", "type": "torch.nn", "text": ["Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:"]}, {"name": "torch.nn.Module.parameters()", "path": "generated/torch.nn.module#torch.nn.Module.parameters", "type": "torch.nn", "text": ["Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:"]}, {"name": "torch.nn.Module.register_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_buffer()", "path": "generated/torch.nn.module#torch.nn.Module.register_buffer", "type": "torch.nn", "text": ["Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:"]}, {"name": "torch.nn.Module.register_forward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_hook", "type": "torch.nn", "text": ["Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_forward_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_pre_hook", "type": "torch.nn", "text": ["Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_full_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_full_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Module.register_parameter()", "path": "generated/torch.nn.module#torch.nn.Module.register_parameter", "type": "torch.nn", "text": ["Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name."]}, {"name": "torch.nn.Module.requires_grad_()", "path": "generated/torch.nn.module#torch.nn.Module.requires_grad_", "type": "torch.nn", "text": ["Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module"]}, {"name": "torch.nn.Module.state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.state_dict", "type": "torch.nn", "text": ["Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:"]}, {"name": "torch.nn.Module.to()", "path": "generated/torch.nn.module#torch.nn.Module.to", "type": "torch.nn", "text": ["Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:"]}, {"name": "torch.nn.Module.train()", "path": "generated/torch.nn.module#torch.nn.Module.train", "type": "torch.nn", "text": ["Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module"]}, {"name": "torch.nn.Module.type()", "path": "generated/torch.nn.module#torch.nn.Module.type", "type": "torch.nn", "text": ["Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module"]}, {"name": "torch.nn.Module.xpu()", "path": "generated/torch.nn.module#torch.nn.Module.xpu", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Module.zero_grad()", "path": "generated/torch.nn.module#torch.nn.Module.zero_grad", "type": "torch.nn", "text": ["Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.ModuleDict", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict", "type": "torch.nn", "text": ["Holds submodules in a dictionary.", "ModuleDict can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all Module methods.", "ModuleDict is an ordered dictionary that respects", "Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict before Python version 3.6) does not preserve the order of the merged mapping.", "modules (iterable, optional) \u2013 a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)", "Example:", "Remove all items from the ModuleDict.", "Return an iterable of the ModuleDict key/value pairs.", "Return an iterable of the ModuleDict keys.", "Remove key from the ModuleDict and return its module.", "key (string) \u2013 key to pop from the ModuleDict", "Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)", "Return an iterable of the ModuleDict values."]}, {"name": "torch.nn.ModuleDict.clear()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.clear", "type": "torch.nn", "text": ["Remove all items from the ModuleDict."]}, {"name": "torch.nn.ModuleDict.items()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.items", "type": "torch.nn", "text": ["Return an iterable of the ModuleDict key/value pairs."]}, {"name": "torch.nn.ModuleDict.keys()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.keys", "type": "torch.nn", "text": ["Return an iterable of the ModuleDict keys."]}, {"name": "torch.nn.ModuleDict.pop()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.pop", "type": "torch.nn", "text": ["Remove key from the ModuleDict and return its module.", "key (string) \u2013 key to pop from the ModuleDict"]}, {"name": "torch.nn.ModuleDict.update()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.update", "type": "torch.nn", "text": ["Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)"]}, {"name": "torch.nn.ModuleDict.values()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.values", "type": "torch.nn", "text": ["Return an iterable of the ModuleDict values."]}, {"name": "torch.nn.ModuleList", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList", "type": "torch.nn", "text": ["Holds submodules in a list.", "ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.", "modules (iterable, optional) \u2013 an iterable of modules to add", "Example:", "Appends a given module to the end of the list.", "module (nn.Module) \u2013 module to append", "Appends modules from a Python iterable to the end of the list.", "modules (iterable) \u2013 iterable of modules to append", "Insert a given module before a given index in the list."]}, {"name": "torch.nn.ModuleList.append()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.append", "type": "torch.nn", "text": ["Appends a given module to the end of the list.", "module (nn.Module) \u2013 module to append"]}, {"name": "torch.nn.ModuleList.extend()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.extend", "type": "torch.nn", "text": ["Appends modules from a Python iterable to the end of the list.", "modules (iterable) \u2013 iterable of modules to append"]}, {"name": "torch.nn.ModuleList.insert()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.insert", "type": "torch.nn", "text": ["Insert a given module before a given index in the list."]}, {"name": "torch.nn.modules.lazy.LazyModuleMixin", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin", "type": "torch.nn", "text": ["A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d", "Modules that lazily initialize parameters, or \u201clazy modules\u201d, derive the shapes of their parameters from the first input(s) to their forward method. Until that first forward they contain torch.nn.UninitializedParameter`s that should not be accessed\nor used, and afterward they contain regular :class:`torch.nn.Parameter`s.\nLazy modules are convenient since they don't require computing some\nmodule arguments, like the `in_features argument of a typical torch.nn.Linear.", "After construction, networks with lazy modules should first be converted to the desired dtype and placed on the desired device. The lazy modules should then be initialized with one or more \u201cdry runs\u201d. These \u201cdry runs\u201d send inputs of the correct size, dtype, and device through the network and to each one of its lazy modules. After this the network can be used as usual.", "A final caveat when using lazy modules is that the order of initialization of a network\u2019s parameters may change, since the lazy modules are always initialized after other modules. This can cause the parameters of a network using lazy modules to be initialized differently than the parameters of a network without lazy modules. For example, if the LazyMLP class defined above had a torch.nn.LazyLinear module first and then a regular torch.nn.Linear second, the second module would be initialized on construction and the first module would be initialized during the first dry run.", "Lazy modules can be serialized with a state dict like other modules. For example:", "Lazy modules can also load regular torch.nn.Parameter s, which replace their torch.nn.UninitializedParameter s:", "Note, however, that lazy modules cannot validate that the shape of parameters they load is correct.", "Check if a module has parameters that are not initialized", "Initialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference."]}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params", "type": "torch.nn", "text": ["Check if a module has parameters that are not initialized"]}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters", "type": "torch.nn", "text": ["Initialize parameters according to the input batch properties. This adds an interface to isolate parameter initialization from the forward pass when doing parameter shape inference."]}, {"name": "torch.nn.modules.module.register_module_backward_hook()", "path": "generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook common to all the modules.", "This function is deprecated in favor of nn.module.register_module_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.modules.module.register_module_forward_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook", "type": "torch.nn", "text": ["Registers a global forward hook for all the modules", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "This hook will be executed before specific module hooks registered with register_forward_hook."]}, {"name": "torch.nn.modules.module.register_module_forward_pre_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook", "type": "torch.nn", "text": ["Registers a forward pre-hook common to all modules.", "Warning", "This adds global state to the nn.module module and it is only intended for debugging/profiling purposes.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "This hook has precedence over the specific module hooks registered with register_forward_pre_hook.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.MSELoss", "path": "generated/torch.nn.mseloss#torch.nn.MSELoss", "type": "torch.nn", "text": ["Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xx  and target yy .", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where NN  is the batch size. If reduction is not 'none' (default 'mean'), then:", "xx  and yy  are tensors of arbitrary shapes with a total of nn  elements each.", "The mean operation still operates over all the elements, and divides by nn .", "The division by nn  can be avoided if one sets reduction = 'sum'.", "Examples:"]}, {"name": "torch.nn.MultiheadAttention", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention", "type": "torch.nn", "text": ["Allows the model to jointly attend to information from different representation subspaces. See Attention Is All You Need", "where headi=Attention(QWiQ,KWiK,VWiV)head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) .", "Note that if kdim and vdim are None, they will be set to embed_dim such that query, key, and value have the same number of features.", "Examples:", "attn_mask: if a 2D mask: (L,S)(L, S)  where L is the target sequence length, S is the source sequence length.", "If a 3D mask: (N\u22c5num_heads,L,S)(N\\cdot\\text{num\\_heads}, L, S)  where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight."]}, {"name": "torch.nn.MultiheadAttention.forward()", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention.forward", "type": "torch.nn", "text": ["attn_mask: if a 2D mask: (L,S)(L, S)  where L is the target sequence length, S is the source sequence length.", "If a 3D mask: (N\u22c5num_heads,L,S)(N\\cdot\\text{num\\_heads}, L, S)  where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight."]}, {"name": "torch.nn.MultiLabelMarginLoss", "path": "generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss", "type": "torch.nn", "text": ["Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 2D Tensor of target class indices). For each sample in the mini-batch:", "where x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\} , y\u2208{0,\u22ef,y.size(0)\u22121}y \\in \\left\\{0, \\; \\cdots , \\; \\text{y.size}(0) - 1\\right\\} , 0\u2264y[j]\u2264x.size(0)\u221210 \\leq y[j] \\leq \\text{x.size}(0)-1 , and i\u2260y[j]i \\neq y[j]  for all ii  and jj .", "yy  and xx  must have the same size.", "The criterion only considers a contiguous block of non-negative targets that starts at the front.", "This allows for different samples to have variable amounts of target classes.", "Examples:"]}, {"name": "torch.nn.MultiLabelSoftMarginLoss", "path": "generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss", "type": "torch.nn", "text": ["Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xx  and target yy  of size (N,C)(N, C) . For each sample in the minibatch:", "where i\u2208{0,\u22ef,x.nElement()\u22121}i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.nElement}() - 1\\right\\} , y[i]\u2208{0,1}y[i] \\in \\left\\{0, \\; 1\\right\\} ."]}, {"name": "torch.nn.MultiMarginLoss", "path": "generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss", "type": "torch.nn", "text": ["Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xx  (a 2D mini-batch Tensor) and output yy  (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-1 ):", "For each mini-batch sample, the loss in terms of the 1D input xx  and scalar output yy  is:", "where x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}  and i\u2260yi \\neq y .", "Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor.", "The loss function then becomes:"]}, {"name": "torch.nn.NLLLoss", "path": "generated/torch.nn.nllloss#torch.nn.NLLLoss", "type": "torch.nn", "text": ["The negative log likelihood loss. It is useful to train a classification problem with C classes.", "If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set.", "The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)  or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1  for the K-dimensional case (described later).", "Obtaining log-probabilities in a neural network is easily achieved by adding a LogSoftmax layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer.", "The target that this loss expects should be a class index in the range [0,C\u22121][0, C-1]  where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range).", "The unreduced (i.e. with reduction set to 'none') loss can be described as:", "where xx  is the input, yy  is the target, ww  is the weight, and NN  is the batch size. If reduction is not 'none' (default 'mean'), then", "Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)  with K\u22651K \\geq 1 , where KK  is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.", "Examples:"]}, {"name": "torch.nn.PairwiseDistance", "path": "generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance", "type": "torch.nn", "text": ["Computes the batchwise pairwise distance between vectors v1v_1 , v2v_2  using the p-norm:"]}, {"name": "torch.nn.parallel.data_parallel()", "path": "nn.functional#torch.nn.parallel.data_parallel", "type": "torch.nn.functional", "text": ["Evaluates module(input) in parallel across the GPUs given in device_ids.", "This is the functional version of the DataParallel module.", "a Tensor containing the result of module(input) located on output_device"]}, {"name": "torch.nn.parallel.DistributedDataParallel", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel", "type": "torch.nn", "text": ["Implements distributed data parallelism that is based on torch.distributed package at the module level.", "This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged.", "The batch size should be larger than the number of GPUs used locally.", "See also: Basics and Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel. The same constraints on input as in torch.nn.DataParallel apply.", "Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group().", "DistributedDataParallel is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training.", "To use DistributedDataParallel on a host with N GPUs, you should spawn up N processes, ensuring that each process exclusively works on a single GPU from 0 to N-1. This can be done by either setting CUDA_VISIBLE_DEVICES for every process or by calling:", "where i is from 0 to N-1. In each process, you should refer the following to construct this module:", "In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn.", "Note", "Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.", "Note", "nccl backend is currently the fastest and highly recommended backend when using GPUs. This applies to both single-node and multi-node distributed training.", "Note", "This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine.", "Note", "If you use torch.save on one process to checkpoint the module, and torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location, torch.load would recover the module to devices where the module was saved from.", "Note", "When a model is trained on M nodes with batch=N, the gradient will be M times smaller when compared to the same model trained on a single node with batch=M*N if the loss is summed (NOT averaged as usual) across instances in a batch (because the gradients between different nodes are averaged). You should take this into consideration when you want to obtain a mathematically equivalent training process compared to the local training counterpart. But in most cases, you can just treat a DistributedDataParallel wrapped model, a DataParallel wrapped model and an ordinary model on a single GPU as the same (E.g. using the same learning rate for equivalent batch size).", "Note", "Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.", "Note", "If you are using DistributedDataParallel in conjunction with the Distributed RPC Framework, you should always use torch.distributed.autograd.backward() to compute gradients and torch.distributed.optim.DistributedOptimizer for optimizing parameters.", "Example:", "Warning", "Constructor, forward method, and differentiation of the output (or a function of the output of this module) are distributed synchronization points. Take that into account in case different processes might be executing different code.", "Warning", "This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.", "Warning", "This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient allreduce following the reverse order of the registered parameters of the model. In other words, it is users\u2019 responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.", "Warning", "This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose torch.memory_format is torch.contiguous_format and others whose format is torch.channels_last. However, corresponding parameters in different processes must have the same strides.", "Warning", "This module doesn\u2019t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters).", "Warning", "If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don\u2019t change this setting.", "Warning", "Forward and backward hooks defined on module and its submodules won\u2019t be invoked anymore, unless the hooks are initialized in the forward() method.", "Warning", "You should never try to change your model\u2019s parameters after wrapping up your model with DistributedDataParallel. Because, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model\u2019s parameters afterwards, gradient redunction functions no longer match the correct set of parameters.", "Warning", "Using DistributedDataParallel in conjunction with the Distributed RPC Framework is experimental and subject to change.", "Warning", "The gradient_as_bucket_view mode does not yet work with Automatic Mixed Precision (AMP). AMP maintains stashed gradients that are used for unscaling gradients. With gradient_as_bucket_view=True, these stashed gradients will point to communication buckets in the first iteration. In the next iteration, the communication buckets are mutated and thus these stashed gradients will be unexpectedly mutated as well, which might lead to wrong results.", "~DistributedDataParallel.module (Module) \u2013 the module to be parallelized.", "Example:", "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes.", "This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.", "Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).", "To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.", "Warning", "This module works only with the multi-process, single-device usage of torch.nn.parallel.DistributedDataParallel, which means that a single process works on a single GPU.", "Warning", "This module currently does not support custom distributed collective operations in the forward pass, such as SyncBatchNorm or other custom defined collectives in the model\u2019s forward pass.", "Example:", "A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context.", "Example:", "Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers.", "This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc.", "It is locally stored by each worker and shared by all the gradient tensors on the worker.", "hook (callable) \u2013 ", "Averages gradient tensors across workers and defined as: hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future:", "This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it can also just return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters.", "We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.work.", "Warning", "Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.", "Warning", "DDP communication hook can only be registered once and should be registered before calling backward.", "Warning", "The Future object that hook returns should contain a result that has the same shape with the tensors inside grad bucket.", "Warning", "DDP communication hook does not support single-process multiple-device mode. Gradbucket tensors should consist of only a single tensor.", "Warning", "get_future API supports only NCCL backend and will return a torch._C.Future which is an internal type and should be used with caution. It can still be used by register_comm_hook API, but it is subject to some subtle differences compared to torch.futures.Future.", "Warning", "DDP communication hook is experimental and subject to change.", "Below is an example of a noop hook that returns the same tensors.", "Below is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce."]}, {"name": "torch.nn.parallel.DistributedDataParallel.join()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.join", "type": "torch.nn", "text": ["A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes.", "This context manager will keep track of already-joined DDP processes, and \u201cshadow\u201d the forward and backward passes by inserting collective communication operations to match with the ones created by non-joined DDP processes. This will ensure each collective call has a corresponding call by already-joined DDP processes, preventing hangs or errors that would otherwise happen when training with uneven inputs across processes.", "Once all DDP processes have joined, the context manager will broadcast the model corresponding to the last joined process to all processes to ensure the model is the same across all processes (which is guaranteed by DDP).", "To use this to enable training with uneven inputs across processes, simply wrap this context manager around your training loop. No further modifications to the model or data loading is required.", "Warning", "This module works only with the multi-process, single-device usage of torch.nn.parallel.DistributedDataParallel, which means that a single process works on a single GPU.", "Warning", "This module currently does not support custom distributed collective operations in the forward pass, such as SyncBatchNorm or other custom defined collectives in the model\u2019s forward pass.", "Example:"]}, {"name": "torch.nn.parallel.DistributedDataParallel.no_sync()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.no_sync", "type": "torch.nn", "text": ["A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context.", "Example:"]}, {"name": "torch.nn.parallel.DistributedDataParallel.register_comm_hook()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.register_comm_hook", "type": "torch.nn", "text": ["Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers.", "This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc.", "It is locally stored by each worker and shared by all the gradient tensors on the worker.", "hook (callable) \u2013 ", "Averages gradient tensors across workers and defined as: hook(state: object, bucket: dist._GradBucket) -> torch.futures.Future:", "This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn\u2019t perform any communication, it can also just return a completed Future. The Future should hold the new value of grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters.", "We also provide an API called get_future to retrieve a Future associated with the completion of c10d.ProcessGroup.work.", "Warning", "Grad bucket\u2019s tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce.", "Warning", "DDP communication hook can only be registered once and should be registered before calling backward.", "Warning", "The Future object that hook returns should contain a result that has the same shape with the tensors inside grad bucket.", "Warning", "DDP communication hook does not support single-process multiple-device mode. Gradbucket tensors should consist of only a single tensor.", "Warning", "get_future API supports only NCCL backend and will return a torch._C.Future which is an internal type and should be used with caution. It can still be used by register_comm_hook API, but it is subject to some subtle differences compared to torch.futures.Future.", "Warning", "DDP communication hook is experimental and subject to change.", "Below is an example of a noop hook that returns the same tensors.", "Below is an example of a Parallel SGD algorithm where gradients are encoded before allreduce, and then decoded after allreduce."]}, {"name": "torch.nn.parameter.Parameter", "path": "generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter", "type": "torch.nn", "text": ["A kind of Tensor that is to be considered a module parameter.", "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn\u2019t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."]}, {"name": "torch.nn.parameter.UninitializedParameter", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter", "type": "torch.nn", "text": ["A parameter that is not initialized.", "Unitialized Parameters are a a special case of torch.nn.Parameter where the shape of the data is still unknown.", "Unlikely a torch.nn.Parameter, uninitialized parameters hold no data and attempting to access some properties, like their shape, will throw a runtime error. The only operations that can be performed on a uninitialized parameter are changing its datatype, moving it to a different device and converting it to a regular torch.nn.Parameter.", "Create a Parameter with the same properties of the uninitialized one. Given a shape, it materializes a parameter in the same device and with the same dtype as the current one or the specified ones in the arguments."]}, {"name": "torch.nn.parameter.UninitializedParameter.materialize()", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter.materialize", "type": "torch.nn", "text": ["Create a Parameter with the same properties of the uninitialized one. Given a shape, it materializes a parameter in the same device and with the same dtype as the current one or the specified ones in the arguments."]}, {"name": "torch.nn.ParameterDict", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict", "type": "torch.nn", "text": ["Holds parameters in a dictionary.", "ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods.", "ParameterDict is an ordered dictionary that respects", "Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict) does not preserve the order of the merged mapping.", "parameters (iterable, optional) \u2013 a mapping (dictionary) of (string : Parameter) or an iterable of key-value pairs of type (string, Parameter)", "Example:", "Remove all items from the ParameterDict.", "Return an iterable of the ParameterDict key/value pairs.", "Return an iterable of the ParameterDict keys.", "Remove key from the ParameterDict and return its parameter.", "key (string) \u2013 key to pop from the ParameterDict", "Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)", "Return an iterable of the ParameterDict values."]}, {"name": "torch.nn.ParameterDict.clear()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.clear", "type": "torch.nn", "text": ["Remove all items from the ParameterDict."]}, {"name": "torch.nn.ParameterDict.items()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.items", "type": "torch.nn", "text": ["Return an iterable of the ParameterDict key/value pairs."]}, {"name": "torch.nn.ParameterDict.keys()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.keys", "type": "torch.nn", "text": ["Return an iterable of the ParameterDict keys."]}, {"name": "torch.nn.ParameterDict.pop()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.pop", "type": "torch.nn", "text": ["Remove key from the ParameterDict and return its parameter.", "key (string) \u2013 key to pop from the ParameterDict"]}, {"name": "torch.nn.ParameterDict.update()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.update", "type": "torch.nn", "text": ["Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.", "Note", "If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.", "parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)"]}, {"name": "torch.nn.ParameterDict.values()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.values", "type": "torch.nn", "text": ["Return an iterable of the ParameterDict values."]}, {"name": "torch.nn.ParameterList", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList", "type": "torch.nn", "text": ["Holds parameters in a list.", "ParameterList can be indexed like a regular Python list, but parameters it contains are properly registered, and will be visible by all Module methods.", "parameters (iterable, optional) \u2013 an iterable of Parameter to add", "Example:", "Appends a given parameter at the end of the list.", "parameter (nn.Parameter) \u2013 parameter to append", "Appends parameters from a Python iterable to the end of the list.", "parameters (iterable) \u2013 iterable of parameters to append"]}, {"name": "torch.nn.ParameterList.append()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.append", "type": "torch.nn", "text": ["Appends a given parameter at the end of the list.", "parameter (nn.Parameter) \u2013 parameter to append"]}, {"name": "torch.nn.ParameterList.extend()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.extend", "type": "torch.nn", "text": ["Appends parameters from a Python iterable to the end of the list.", "parameters (iterable) \u2013 iterable of parameters to append"]}, {"name": "torch.nn.PixelShuffle", "path": "generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle", "type": "torch.nn", "text": ["Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)  to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is an upscale factor.", "This is useful for implementing efficient sub-pixel convolution with a stride of 1/r1/r .", "See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.", "upscale_factor (int) \u2013 factor to increase spatial resolution by", "Examples:"]}, {"name": "torch.nn.PixelUnshuffle", "path": "generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle", "type": "torch.nn", "text": ["Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)  to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.", "See the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.", "downscale_factor (int) \u2013 factor to decrease spatial resolution by", "Examples:"]}, {"name": "torch.nn.PoissonNLLLoss", "path": "generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss", "type": "torch.nn", "text": ["Negative log likelihood loss with Poisson distribution of target.", "The loss can be described as:", "The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.", "full (bool, optional) \u2013 ", "whether to compute full loss, i. e. to add the Stirling approximation term", "Examples:"]}, {"name": "torch.nn.PReLU", "path": "generated/torch.nn.prelu#torch.nn.PReLU", "type": "torch.nn", "text": ["Applies the element-wise function:", "or", "Here aa  is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter aa  across all input channels. If called with nn.PReLU(nChannels), a separate aa  is used for each input channel.", "Note", "weight decay should not be used when learning aa  for good performance.", "Note", "Channel dim is the 2nd dim of input. When input has dims < 2, then there is no channel dim and the number of channels = 1.", "~PReLU.weight (Tensor) \u2013 the learnable weights of shape (num_parameters).", "Examples:"]}, {"name": "torch.nn.qat.Conv2d", "path": "torch.nn.qat#torch.nn.qat.Conv2d", "type": "Quantization", "text": ["A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Conv2d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for documentation.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.", "~Conv2d.weight_fake_quant \u2013 fake quant module for weight", "Create a qat module from a float module or qparams_dict", "Args: mod a float module, either produced by torch.quantization utilities or directly from user"]}, {"name": "torch.nn.qat.Conv2d.from_float()", "path": "torch.nn.qat#torch.nn.qat.Conv2d.from_float", "type": "Quantization", "text": ["Create a qat module from a float module or qparams_dict", "Args: mod a float module, either produced by torch.quantization utilities or directly from user"]}, {"name": "torch.nn.qat.Linear", "path": "torch.nn.qat#torch.nn.qat.Linear", "type": "Quantization", "text": ["A linear module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, with FakeQuantize modules initialized to default.", "~Linear.weight \u2013 fake quant module for weight", "Create a qat module from a float module or qparams_dict", "Args: mod a float module, either produced by torch.quantization utilities or directly from user"]}, {"name": "torch.nn.qat.Linear.from_float()", "path": "torch.nn.qat#torch.nn.qat.Linear.from_float", "type": "Quantization", "text": ["Create a qat module from a float module or qparams_dict", "Args: mod a float module, either produced by torch.quantization utilities or directly from user"]}, {"name": "torch.nn.quantized.BatchNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm2d", "type": "Quantization", "text": ["This is the quantized version of BatchNorm2d."]}, {"name": "torch.nn.quantized.BatchNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm3d", "type": "Quantization", "text": ["This is the quantized version of BatchNorm3d."]}, {"name": "torch.nn.quantized.Conv1d", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d", "type": "Quantization", "text": ["Applies a 1D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv1d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv1d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.Conv1d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.Conv2d", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d", "type": "Quantization", "text": ["Applies a 2D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv2d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv2d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.Conv2d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.Conv3d", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d", "type": "Quantization", "text": ["Applies a 3D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv3d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv3d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.Conv3d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.DeQuantize", "path": "torch.nn.quantized#torch.nn.quantized.DeQuantize", "type": "Quantization", "text": ["Dequantizes an incoming tensor"]}, {"name": "torch.nn.quantized.dynamic", "path": "torch.nn.quantized.dynamic", "type": "torch.nn.quantized.dynamic", "text": ["A dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user", "A dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.", "Examples:", "A long short-term memory (LSTM) cell.", "A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.", "Examples:", "A gated recurrent unit (GRU) cell", "A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.", "Examples:", "An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.", "Examples:"]}, {"name": "torch.nn.quantized.dynamic.GRUCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.GRUCell", "type": "torch.nn.quantized.dynamic", "text": ["A gated recurrent unit (GRU) cell", "A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.", "Examples:"]}, {"name": "torch.nn.quantized.dynamic.Linear", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear", "type": "torch.nn.quantized.dynamic", "text": ["A dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.dynamic.Linear.from_float()", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear.from_float", "type": "torch.nn.quantized.dynamic", "text": ["Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.dynamic.LSTM", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTM", "type": "torch.nn.quantized.dynamic", "text": ["A dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.", "Examples:"]}, {"name": "torch.nn.quantized.dynamic.LSTMCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTMCell", "type": "torch.nn.quantized.dynamic", "text": ["A long short-term memory (LSTM) cell.", "A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.", "Examples:"]}, {"name": "torch.nn.quantized.dynamic.RNNCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.RNNCell", "type": "torch.nn.quantized.dynamic", "text": ["An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.", "Examples:"]}, {"name": "torch.nn.quantized.ELU", "path": "torch.nn.quantized#torch.nn.quantized.ELU", "type": "Quantization", "text": ["This is the quantized equivalent of ELU."]}, {"name": "torch.nn.quantized.Embedding", "path": "torch.nn.quantized#torch.nn.quantized.Embedding", "type": "Quantization", "text": ["A quantized Embedding module with quantized packed weights as inputs. We adopt the same interface as torch.nn.Embedding, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.", "Similar to Embedding, attributes will be randomly initialized at module creation time and will be overwritten later", "~Embedding.weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}) .", "Create a quantized embedding module from a float module", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user"]}, {"name": "torch.nn.quantized.Embedding.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Embedding.from_float", "type": "Quantization", "text": ["Create a quantized embedding module from a float module", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user"]}, {"name": "torch.nn.quantized.EmbeddingBag", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag", "type": "Quantization", "text": ["A quantized EmbeddingBag module with quantized packed weights as inputs. We adopt the same interface as torch.nn.EmbeddingBag, please see https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for documentation.", "Similar to EmbeddingBag, attributes will be randomly initialized at module creation time and will be overwritten later", "~EmbeddingBag.weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}) .", "Create a quantized embedding_bag module from a float module", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user"]}, {"name": "torch.nn.quantized.EmbeddingBag.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": ["Create a quantized embedding_bag module from a float module", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by user"]}, {"name": "torch.nn.quantized.FloatFunctional", "path": "torch.nn.quantized#torch.nn.quantized.FloatFunctional", "type": "Quantization", "text": ["State collector class for float operations.", "The instance of this class can be used instead of the torch. prefix for some operations. See example usage below.", "Note", "This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).", "Examples:"]}, {"name": "torch.nn.quantized.functional.adaptive_avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": ["Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters propagate to the output.", "See AdaptiveAvgPool2d for details and output shape.", "output_size \u2013 the target output size (single integer or double-integer tuple)"]}, {"name": "torch.nn.quantized.functional.avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": ["Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW  regions by step size sH\u00d7sWsH \\times sW  steps. The number of output features is equal to the number of input planes.", "Note", "The input quantization parameters propagate to the output.", "See AvgPool2d for details and output shape."]}, {"name": "torch.nn.quantized.functional.conv1d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv1d", "type": "Quantization", "text": ["Applies a 1D convolution over a quantized 1D input composed of several input planes.", "See Conv1d for details and output shape.", "Examples:"]}, {"name": "torch.nn.quantized.functional.conv2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv2d", "type": "Quantization", "text": ["Applies a 2D convolution over a quantized 2D input composed of several input planes.", "See Conv2d for details and output shape.", "Examples:"]}, {"name": "torch.nn.quantized.functional.conv3d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv3d", "type": "Quantization", "text": ["Applies a 3D convolution over a quantized 3D input composed of several input planes.", "See Conv3d for details and output shape.", "Examples:"]}, {"name": "torch.nn.quantized.functional.hardswish()", "path": "torch.nn.quantized#torch.nn.quantized.functional.hardswish", "type": "Quantization", "text": ["This is the quantized version of hardswish()."]}, {"name": "torch.nn.quantized.functional.interpolate()", "path": "torch.nn.quantized#torch.nn.quantized.functional.interpolate", "type": "Quantization", "text": ["Down/up samples the input to either the given size or the given scale_factor", "See torch.nn.functional.interpolate() for implementation details.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D/3D input is supported for quantized inputs", "Note", "Only the following modes are supported for the quantized inputs:"]}, {"name": "torch.nn.quantized.functional.linear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.linear", "type": "Quantization", "text": ["Applies a linear transformation to the incoming quantized data: y=xAT+by = xA^T + b . See Linear", "Note", "Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use Linear."]}, {"name": "torch.nn.quantized.functional.max_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": ["Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters are propagated to the output.", "See MaxPool2d for details."]}, {"name": "torch.nn.quantized.functional.upsample()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample", "type": "Quantization", "text": ["Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(...).", "See torch.nn.functional.interpolate() for implementation details.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D input is supported for quantized inputs", "Note", "Only the following modes are supported for the quantized inputs:", "Warning", "With align_corners = True, the linearly interpolating modes (bilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs."]}, {"name": "torch.nn.quantized.functional.upsample_bilinear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": ["Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True).", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D inputs are supported"]}, {"name": "torch.nn.quantized.functional.upsample_nearest()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": ["Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='nearest').", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D inputs are supported"]}, {"name": "torch.nn.quantized.GroupNorm", "path": "torch.nn.quantized#torch.nn.quantized.GroupNorm", "type": "Quantization", "text": ["This is the quantized version of GroupNorm."]}, {"name": "torch.nn.quantized.Hardswish", "path": "torch.nn.quantized#torch.nn.quantized.Hardswish", "type": "Quantization", "text": ["This is the quantized version of Hardswish."]}, {"name": "torch.nn.quantized.InstanceNorm1d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm1d."]}, {"name": "torch.nn.quantized.InstanceNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm2d."]}, {"name": "torch.nn.quantized.InstanceNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm3d."]}, {"name": "torch.nn.quantized.LayerNorm", "path": "torch.nn.quantized#torch.nn.quantized.LayerNorm", "type": "Quantization", "text": ["This is the quantized version of LayerNorm."]}, {"name": "torch.nn.quantized.Linear", "path": "torch.nn.quantized#torch.nn.quantized.Linear", "type": "Quantization", "text": ["A quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.Linear.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Linear.from_float", "type": "Quantization", "text": ["Create a quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user"]}, {"name": "torch.nn.quantized.QFunctional", "path": "torch.nn.quantized#torch.nn.quantized.QFunctional", "type": "Quantization", "text": ["Wrapper class for quantized operations.", "The instance of this class can be used instead of the torch.ops.quantized prefix. See example usage below.", "Note", "This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).", "Examples:"]}, {"name": "torch.nn.quantized.Quantize", "path": "torch.nn.quantized#torch.nn.quantized.Quantize", "type": "Quantization", "text": ["Quantizes an incoming tensor", "zero_point, dtype (`scale`,) \u2013 "]}, {"name": "torch.nn.quantized.ReLU6", "path": "torch.nn.quantized#torch.nn.quantized.ReLU6", "type": "Quantization", "text": ["Applies the element-wise function:", "ReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6)) , where x0x_0  is the zero_point, and q(6)q(6)  is the quantized representation of number 6.", "inplace \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.ReflectionPad1d", "path": "generated/torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d", "type": "torch.nn", "text": ["Pads the input tensor using the reflection of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} )", "Output: (N,C,Wout)(N, C, W_{out})  where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.ReflectionPad2d", "path": "generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d", "type": "torch.nn", "text": ["Pads the input tensor using the reflection of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom} ", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.ReLU", "path": "generated/torch.nn.relu#torch.nn.ReLU", "type": "torch.nn", "text": ["Applies the rectified linear unit function element-wise:", "ReLU(x)=(x)+=max\u2061(0,x)\\text{ReLU}(x) = (x)^+ = \\max(0, x) ", "inplace \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.ReLU6", "path": "generated/torch.nn.relu6#torch.nn.ReLU6", "type": "torch.nn", "text": ["Applies the element-wise function:", "inplace \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.ReplicationPad1d", "path": "generated/torch.nn.replicationpad1d#torch.nn.ReplicationPad1d", "type": "torch.nn", "text": ["Pads the input tensor using replication of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} )", "Output: (N,C,Wout)(N, C, W_{out})  where", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.ReplicationPad2d", "path": "generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d", "type": "torch.nn", "text": ["Pads the input tensor using replication of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom} ", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.ReplicationPad3d", "path": "generated/torch.nn.replicationpad3d#torch.nn.ReplicationPad3d", "type": "torch.nn", "text": ["Pads the input tensor using replication of the input boundary.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} , padding_front\\text{padding\\_front} , padding_back\\text{padding\\_back} )", "Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})  where", "Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back} ", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom} ", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nn.RNN", "path": "generated/torch.nn.rnn#torch.nn.RNN", "type": "torch.nn", "text": ["Applies a multi-layer Elman RNN with tanh\u2061\\tanh  or ReLU\\text{ReLU}  non-linearity to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t  is the hidden state at time t, xtx_t  is the input at time t, and h(t\u22121)h_{(t-1)}  is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is 'relu', then ReLU\\text{ReLU}  is used instead of tanh\u2061\\tanh .", "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.", "For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.", "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.", "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}} ", "Warning", "There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables:", "On CUDA 10.1, set environment variable CUDA_LAUNCH_BLOCKING=1. This may affect performance.", "On CUDA 10.2 or later, set environment variable (note the leading colon symbol) CUBLAS_WORKSPACE_CONFIG=:16:8 or CUBLAS_WORKSPACE_CONFIG=:4096:2.", "See the cuDNN 8 Release Notes for more information.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.nn.RNNBase", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase", "type": "torch.nn", "text": ["Resets parameter data pointer so that they can use faster code paths.", "Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op."]}, {"name": "torch.nn.RNNBase.flatten_parameters()", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase.flatten_parameters", "type": "torch.nn", "text": ["Resets parameter data pointer so that they can use faster code paths.", "Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op."]}, {"name": "torch.nn.RNNCell", "path": "generated/torch.nn.rnncell#torch.nn.RNNCell", "type": "torch.nn", "text": ["An Elman RNN cell with tanh or ReLU non-linearity.", "If nonlinearity is \u2018relu\u2019, then ReLU is used in place of tanh.", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})  where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}} ", "Examples:"]}, {"name": "torch.nn.RReLU", "path": "generated/torch.nn.rrelu#torch.nn.RReLU", "type": "torch.nn", "text": ["Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:", "Empirical Evaluation of Rectified Activations in Convolutional Network.", "The function is defined as:", "where aa  is randomly sampled from uniform distribution U(lower,upper)\\mathcal{U}(\\text{lower}, \\text{upper}) .", "See: https://arxiv.org/pdf/1505.00853.pdf", "Examples:"]}, {"name": "torch.nn.SELU", "path": "generated/torch.nn.selu#torch.nn.SELU", "type": "torch.nn", "text": ["Applied element-wise, as:", "with \u03b1=1.6732632423543772848170429916717\\alpha = 1.6732632423543772848170429916717  and scale=1.0507009873554804934193349852946\\text{scale} = 1.0507009873554804934193349852946 .", "More details can be found in the paper Self-Normalizing Neural Networks .", "inplace (bool, optional) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.nn.Sequential", "path": "generated/torch.nn.sequential#torch.nn.Sequential", "type": "torch.nn", "text": ["A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in.", "To make it easier to understand, here is a small example:"]}, {"name": "torch.nn.Sigmoid", "path": "generated/torch.nn.sigmoid#torch.nn.Sigmoid", "type": "torch.nn", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.SiLU", "path": "generated/torch.nn.silu#torch.nn.SiLU", "type": "torch.nn", "text": ["Applies the silu function, element-wise.", "Note", "See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.", "Examples:"]}, {"name": "torch.nn.SmoothL1Loss", "path": "generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss", "type": "torch.nn", "text": ["Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It is less sensitive to outliers than the torch.nn.MSELoss and in some cases prevents exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick). Omitting a scaling factor of beta, this loss is also known as the Huber loss:", "where ziz_{i}  is given by:", "xx  and yy  arbitrary shapes with a total of nn  elements each the sum operation still operates over all the elements, and divides by nn .", "beta is an optional parameter that defaults to 1.", "Note: When beta is set to 0, this is equivalent to L1Loss. Passing a negative value in for beta will result in an exception.", "The division by nn  can be avoided if sets reduction = 'sum'."]}, {"name": "torch.nn.SoftMarginLoss", "path": "generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss", "type": "torch.nn", "text": ["Creates a criterion that optimizes a two-class classification logistic loss between input tensor xx  and target tensor yy  (containing 1 or -1)."]}, {"name": "torch.nn.Softmax", "path": "generated/torch.nn.softmax#torch.nn.Softmax", "type": "torch.nn", "text": ["Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.", "Softmax is defined as:", "When the input Tensor is a sparse tensor then the unspecifed values are treated as -inf.", "a Tensor of the same dimension and shape as the input with values in the range [0, 1]", "dim (int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1).", "Note", "This module doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it\u2019s faster and has better numerical properties).", "Examples:"]}, {"name": "torch.nn.Softmax2d", "path": "generated/torch.nn.softmax2d#torch.nn.Softmax2d", "type": "torch.nn", "text": ["Applies SoftMax over features to each spatial location.", "When given an image of Channels x Height x Width, it will apply Softmax to each location (Channels,hi,wj)(Channels, h_i, w_j) ", "a Tensor of the same dimension and shape as the input with values in the range [0, 1]", "Examples:"]}, {"name": "torch.nn.Softmin", "path": "generated/torch.nn.softmin#torch.nn.Softmin", "type": "torch.nn", "text": ["Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1.", "Softmin is defined as:", "dim (int) \u2013 A dimension along which Softmin will be computed (so every slice along dim will sum to 1).", "a Tensor of the same dimension and shape as the input, with values in the range [0, 1]", "Examples:"]}, {"name": "torch.nn.Softplus", "path": "generated/torch.nn.softplus#torch.nn.Softplus", "type": "torch.nn", "text": ["Applies the element-wise function:", "SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive.", "For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2>thresholdinput \\times \\beta > threshold .", "Examples:"]}, {"name": "torch.nn.Softshrink", "path": "generated/torch.nn.softshrink#torch.nn.Softshrink", "type": "torch.nn", "text": ["Applies the soft shrinkage function elementwise:", "lambd \u2013 the \u03bb\\lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5", "Examples:"]}, {"name": "torch.nn.Softsign", "path": "generated/torch.nn.softsign#torch.nn.Softsign", "type": "torch.nn", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.SyncBatchNorm", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm", "type": "torch.nn", "text": ["Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .", "The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. \u03b3\\gamma  and \u03b2\\beta  are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma  are sampled from U(0,1)\\mathcal{U}(0, 1)  and the elements of \u03b2\\beta  are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).", "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.", "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.", "Note", "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where x^\\hat{x}  is the estimated statistic and xtx_t  is the new observed value.", "Because the Batch Normalization is done for each channel in the C dimension, computing statistics on (N, +) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.", "Currently SyncBatchNorm only supports DistributedDataParallel (DDP) with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm*D layer to SyncBatchNorm before wrapping Network with DDP.", "Examples:", "Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.", "The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.", "Example:"]}, {"name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm()", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm.convert_sync_batchnorm", "type": "torch.nn", "text": ["Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.", "The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.", "Example:"]}, {"name": "torch.nn.Tanh", "path": "generated/torch.nn.tanh#torch.nn.Tanh", "type": "torch.nn", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.Tanhshrink", "path": "generated/torch.nn.tanhshrink#torch.nn.Tanhshrink", "type": "torch.nn", "text": ["Applies the element-wise function:", "Examples:"]}, {"name": "torch.nn.Threshold", "path": "generated/torch.nn.threshold#torch.nn.Threshold", "type": "torch.nn", "text": ["Thresholds each element of the input Tensor.", "Threshold is defined as:", "Examples:"]}, {"name": "torch.nn.Transformer", "path": "generated/torch.nn.transformer#torch.nn.Transformer", "type": "torch.nn", "text": ["A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.", "Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model", "Take in and process masked source/target sequences.", "Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.", "Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode.", "where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number", "Generate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0)."]}, {"name": "torch.nn.Transformer.forward()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.forward", "type": "torch.nn", "text": ["Take in and process masked source/target sequences.", "Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.", "Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode.", "where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number"]}, {"name": "torch.nn.Transformer.generate_square_subsequent_mask()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.generate_square_subsequent_mask", "type": "torch.nn", "text": ["Generate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0)."]}, {"name": "torch.nn.TransformerDecoder", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder", "type": "torch.nn", "text": ["TransformerDecoder is a stack of N decoder layers", "Pass the inputs (and mask) through the decoder layer in turn.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerDecoder.forward()", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder.forward", "type": "torch.nn", "text": ["Pass the inputs (and mask) through the decoder layer in turn.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerDecoderLayer", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer", "type": "torch.nn", "text": ["TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.", "Pass the inputs (and mask) through the decoder layer.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerDecoderLayer.forward()", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer.forward", "type": "torch.nn", "text": ["Pass the inputs (and mask) through the decoder layer.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoder", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder", "type": "torch.nn", "text": ["TransformerEncoder is a stack of N encoder layers", "Pass the input through the encoder layers in turn.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoder.forward()", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder.forward", "type": "torch.nn", "text": ["Pass the input through the encoder layers in turn.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoderLayer", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer", "type": "torch.nn", "text": ["TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.", "Pass the input through the encoder layer.", "see the docs in Transformer class."]}, {"name": "torch.nn.TransformerEncoderLayer.forward()", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer.forward", "type": "torch.nn", "text": ["Pass the input through the encoder layer.", "see the docs in Transformer class."]}, {"name": "torch.nn.TripletMarginLoss", "path": "generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss", "type": "torch.nn", "text": ["Creates a criterion that measures the triplet loss given an input tensors x1x1 , x2x2 , x3x3  and a margin with a value greater than 00 . This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n (i.e., anchor, positive examples and negative examples respectively). The shapes of all input tensors should be (N,D)(N, D) .", "The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al.", "The loss function for each sample in the mini-batch is:", "where", "See also TripletMarginWithDistanceLoss, which computes the triplet margin loss for input tensors using a custom distance function.", "otherwise."]}, {"name": "torch.nn.TripletMarginWithDistanceLoss", "path": "generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss", "type": "torch.nn", "text": ["Creates a criterion that measures the triplet loss given input tensors aa , pp , and nn  (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d).", "The unreduced loss (i.e., with reduction set to 'none') can be described as:", "where NN  is the batch size; dd  is a nonnegative, real-valued function quantifying the closeness of two tensors, referred to as the distance_function; and marginmargin  is a nonnegative margin representing the minimum difference between the positive and negative distances that is required for the loss to be 0. The input tensors have NN  elements each and can be of any shape that the distance function can handle.", "If reduction is not 'none' (default 'mean'), then:", "See also TripletMarginLoss, which computes the triplet loss for input tensors using the lpl_p  distance as the distance function.", "Examples:", "V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses: http://www.bmva.org/bmvc/2016/papers/paper119/index.html"]}, {"name": "torch.nn.Unflatten", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten", "type": "torch.nn", "text": ["Unflattens a tensor dim expanding it to a desired shape. For use with Sequential.", "Adds a child module to the current module.", "The module can be accessed as an attribute using the given name.", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:", "Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module", "Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:", "Returns an iterator over immediate children modules.", "Module \u2013 a child module", "Moves all model parameters and buffers to the CPU.", "self", "Module", "Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Casts all floating point parameters and buffers to double datatype.", "self", "Module", "Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module", "Casts all floating point parameters and buffers to float datatype.", "self", "Module", "Casts all floating point parameters and buffers to half datatype.", "self", "Module", "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields", "Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:", "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:", "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:", "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:", "Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:", "Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:", "Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle", "Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name.", "Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module", "Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:", "Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:", "Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module", "Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module", "Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module", "Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.Unflatten.add_module()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.add_module", "type": "torch.nn", "text": ["Adds a child module to the current module.", "The module can be accessed as an attribute using the given name."]}, {"name": "torch.nn.Unflatten.apply()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.apply", "type": "torch.nn", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Module", "Example:"]}, {"name": "torch.nn.Unflatten.bfloat16()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.bfloat16", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to bfloat16 datatype.", "self", "Module"]}, {"name": "torch.nn.Unflatten.buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.buffers", "type": "torch.nn", "text": ["Returns an iterator over module buffers.", "recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.", "torch.Tensor \u2013 module buffer", "Example:"]}, {"name": "torch.nn.Unflatten.children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.children", "type": "torch.nn", "text": ["Returns an iterator over immediate children modules.", "Module \u2013 a child module"]}, {"name": "torch.nn.Unflatten.cpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cpu", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the CPU.", "self", "Module"]}, {"name": "torch.nn.Unflatten.cuda()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cuda", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the GPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Unflatten.double()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.double", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to double datatype.", "self", "Module"]}, {"name": "torch.nn.Unflatten.eval()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.eval", "type": "torch.nn", "text": ["Sets the module in evaluation mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "This is equivalent with self.train(False).", "self", "Module"]}, {"name": "torch.nn.Unflatten.float()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.float", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to float datatype.", "self", "Module"]}, {"name": "torch.nn.Unflatten.half()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.half", "type": "torch.nn", "text": ["Casts all floating point parameters and buffers to half datatype.", "self", "Module"]}, {"name": "torch.nn.Unflatten.load_state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.load_state_dict", "type": "torch.nn", "text": ["Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.", "NamedTuple with missing_keys and unexpected_keys fields"]}, {"name": "torch.nn.Unflatten.modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.modules", "type": "torch.nn", "text": ["Returns an iterator over all modules in the network.", "Module \u2013 a module in the network", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Unflatten.named_buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_buffers", "type": "torch.nn", "text": ["Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.", "(string, torch.Tensor) \u2013 Tuple containing the name and buffer", "Example:"]}, {"name": "torch.nn.Unflatten.named_children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_children", "type": "torch.nn", "text": ["Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple containing a name and child module", "Example:"]}, {"name": "torch.nn.Unflatten.named_modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_modules", "type": "torch.nn", "text": ["Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.", "(string, Module) \u2013 Tuple of name and module", "Note", "Duplicate modules are returned only once. In the following example, l will be returned only once.", "Example:"]}, {"name": "torch.nn.Unflatten.named_parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_parameters", "type": "torch.nn", "text": ["Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.", "(string, Parameter) \u2013 Tuple containing the name and parameter", "Example:"]}, {"name": "torch.nn.Unflatten.parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.parameters", "type": "torch.nn", "text": ["Returns an iterator over module parameters.", "This is typically passed to an optimizer.", "recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.", "Parameter \u2013 module parameter", "Example:"]}, {"name": "torch.nn.Unflatten.register_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook on the module.", "This function is deprecated in favor of nn.Module.register_full_backward_hook() and the behavior of this function will change in future versions.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Unflatten.register_buffer()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_buffer", "type": "torch.nn", "text": ["Adds a buffer to the module.", "This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict.", "Buffers can be accessed as attributes using given names.", "Example:"]}, {"name": "torch.nn.Unflatten.register_forward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_hook", "type": "torch.nn", "text": ["Registers a forward hook on the module.", "The hook will be called every time after forward() has computed an output. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Unflatten.register_forward_pre_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_pre_hook", "type": "torch.nn", "text": ["Registers a forward pre-hook on the module.", "The hook will be called every time before forward() is invoked. It should have the following signature:", "The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Unflatten.register_full_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_full_backward_hook", "type": "torch.nn", "text": ["Registers a backward hook on the module.", "The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:", "The grad_input and grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in grad_input and grad_output will be None for all non-Tensor arguments.", "Warning", "Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.", "a handle that can be used to remove the added hook by calling handle.remove()", "torch.utils.hooks.RemovableHandle"]}, {"name": "torch.nn.Unflatten.register_parameter()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_parameter", "type": "torch.nn", "text": ["Adds a parameter to the module.", "The parameter can be accessed as an attribute using given name."]}, {"name": "torch.nn.Unflatten.requires_grad_()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.requires_grad_", "type": "torch.nn", "text": ["Change if autograd should record operations on parameters in this module.", "This method sets the parameters\u2019 requires_grad attributes in-place.", "This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).", "requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.", "self", "Module"]}, {"name": "torch.nn.Unflatten.state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.state_dict", "type": "torch.nn", "text": ["Returns a dictionary containing a whole state of the module.", "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.", "a dictionary containing a whole state of the module", "dict", "Example:"]}, {"name": "torch.nn.Unflatten.to()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.to", "type": "torch.nn", "text": ["Moves and/or casts the parameters and buffers.", "This can be called as", "Its signature is similar to torch.Tensor.to(), but only accepts floating point or complex dtype`s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.", "See below for examples.", "Note", "This method modifies the module in-place.", "self", "Module", "Examples:"]}, {"name": "torch.nn.Unflatten.train()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.train", "type": "torch.nn", "text": ["Sets the module in training mode.", "This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.", "mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.", "self", "Module"]}, {"name": "torch.nn.Unflatten.type()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.type", "type": "torch.nn", "text": ["Casts all parameters and buffers to dst_type.", "dst_type (type or string) \u2013 the desired type", "self", "Module"]}, {"name": "torch.nn.Unflatten.xpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.xpu", "type": "torch.nn", "text": ["Moves all model parameters and buffers to the XPU.", "This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.", "device (int, optional) \u2013 if specified, all parameters will be copied to that device", "self", "Module"]}, {"name": "torch.nn.Unflatten.zero_grad()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.zero_grad", "type": "torch.nn", "text": ["Sets gradients of all model parameters to zero. See similar function under torch.optim.Optimizer for more context.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See torch.optim.Optimizer.zero_grad() for details."]}, {"name": "torch.nn.Unfold", "path": "generated/torch.nn.unfold#torch.nn.Unfold", "type": "torch.nn", "text": ["Extracts sliding local blocks from a batched input tensor.", "Consider a batched input tensor of shape (N,C,\u2217)(N, C, *) , where NN  is the batch dimension, CC  is the channel dimension, and \u2217*  represent arbitrary spatial dimensions. This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L) , where C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})  is the total number of values within each block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})  spatial locations each containing a CC -channeled vector), and LL  is the total number of such blocks:", "where spatial_size\\text{spatial\\_size}  is formed by the spatial dimensions of input (\u2217*  above), and dd  is over all spatial dimensions.", "Therefore, indexing output at the last dimension (column dimension) gives all values within a certain block.", "The padding, stride and dilation arguments specify how the sliding blocks are retrieved.", "Note", "Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other.", "In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters:", "Then for any (supported) input tensor the following equality holds:", "where divisor is a tensor that depends only on the shape and dtype of the input:", "When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).", "Warning", "Currently, only 4-D input tensors (batched image-like tensors) are supported.", "Examples:"]}, {"name": "torch.nn.Upsample", "path": "generated/torch.nn.upsample#torch.nn.Upsample", "type": "torch.nn", "text": ["Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.", "The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.", "The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.", "One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous)", "Warning", "With align_corners = True, the linearly interpolating modes (linear, bilinear, bicubic, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs.", "Note", "If you want downsampling/general resizing, you should use interpolate().", "Examples:"]}, {"name": "torch.nn.UpsamplingBilinear2d", "path": "generated/torch.nn.upsamplingbilinear2d#torch.nn.UpsamplingBilinear2d", "type": "torch.nn", "text": ["Applies a 2D bilinear upsampling to an input signal composed of several input channels.", "To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument.", "When size is given, it is the output size of the image (h, w).", "Warning", "This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True).", "Examples:"]}, {"name": "torch.nn.UpsamplingNearest2d", "path": "generated/torch.nn.upsamplingnearest2d#torch.nn.UpsamplingNearest2d", "type": "torch.nn", "text": ["Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.", "To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument.", "When size is given, it is the output size of the image (h, w).", "Warning", "This class is deprecated in favor of interpolate().", "Examples:"]}, {"name": "torch.nn.utils.clip_grad_norm_()", "path": "generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_", "type": "torch.nn", "text": ["Clips gradient norm of an iterable of parameters.", "The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.", "Total norm of the parameters (viewed as a single vector)."]}, {"name": "torch.nn.utils.clip_grad_value_()", "path": "generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_", "type": "torch.nn", "text": ["Clips gradient of an iterable of parameters at specified value.", "Gradients are modified in-place."]}, {"name": "torch.nn.utils.parameters_to_vector()", "path": "generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector", "type": "torch.nn", "text": ["Convert parameters to one vector", "parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.", "The parameters represented by a single vector"]}, {"name": "torch.nn.utils.prune.BasePruningMethod", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod", "type": "torch.nn", "text": ["Abstract base class for creation of new pruning techniques.", "Provides a skeleton for customization requiring the overriding of methods such as compute_mask() and apply().", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.BasePruningMethod.compute_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.compute_mask", "type": "torch.nn", "text": ["Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.BasePruningMethod.prune()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.BasePruningMethod.remove()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.CustomFromMask", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.CustomFromMask.apply()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.CustomFromMask.apply_mask()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.CustomFromMask.prune()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.CustomFromMask.remove()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.custom_from_mask()", "path": "generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask", "type": "torch.nn", "text": ["Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.global_unstructured()", "path": "generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured", "type": "torch.nn", "text": ["Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method. Modifies modules in place by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.", "TypeError \u2013 if PRUNING_TYPE != 'unstructured'", "Note", "Since global structured pruning doesn\u2019t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods."]}, {"name": "torch.nn.utils.prune.Identity", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity", "type": "torch.nn", "text": ["Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.Identity.apply()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.Identity.apply_mask()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.Identity.prune()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.Identity.remove()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.is_pruned()", "path": "generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned", "type": "torch.nn", "text": ["Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.", "module (nn.Module) \u2013 object that is either pruned or unpruned", "binary answer to whether module is pruned."]}, {"name": "torch.nn.utils.prune.L1Unstructured", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured", "type": "torch.nn", "text": ["Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.", "amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.L1Unstructured.apply()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.L1Unstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.L1Unstructured.prune()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.L1Unstructured.remove()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.l1_unstructured()", "path": "generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured", "type": "torch.nn", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.LnStructured", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured", "type": "torch.nn", "text": ["Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.LnStructured.apply()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.LnStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.LnStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.compute_mask", "type": "torch.nn", "text": ["Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)"]}, {"name": "torch.nn.utils.prune.LnStructured.prune()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.LnStructured.remove()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.ln_structured()", "path": "generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured", "type": "torch.nn", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.PruningContainer", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer", "type": "torch.nn", "text": ["Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls.", "Accepts as argument an instance of a BasePruningMethod or an iterable of them.", "Adds a child pruning method to the container.", "method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):", "new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).", "mask (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.PruningContainer.add_pruning_method()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.add_pruning_method", "type": "torch.nn", "text": ["Adds a child pruning method to the container.", "method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container."]}, {"name": "torch.nn.utils.prune.PruningContainer.apply()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.PruningContainer.apply_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.PruningContainer.compute_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.compute_mask", "type": "torch.nn", "text": ["Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):", "new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).", "mask (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.PruningContainer.prune()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.PruningContainer.remove()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomStructured", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured", "type": "torch.nn", "text": ["Prune entire (currently unpruned) channels in a tensor at random.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomStructured.apply()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.RandomStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.RandomStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.compute_mask", "type": "torch.nn", "text": ["Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.", "mask to apply to t, of same dims as t", "mask (torch.Tensor)", "IndexError \u2013 if self.dim >= len(t.shape)"]}, {"name": "torch.nn.utils.prune.RandomStructured.prune()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.RandomStructured.remove()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomUnstructured", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured", "type": "torch.nn", "text": ["Prune (currently unpruned) units in a tensor at random.", "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.", "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)", "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t.", "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply", "type": "torch.nn", "text": ["Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask."]}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply_mask", "type": "torch.nn", "text": ["Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.", "module (nn.Module) \u2013 module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"]}, {"name": "torch.nn.utils.prune.RandomUnstructured.prune()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.prune", "type": "torch.nn", "text": ["Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().", "pruned version of tensor t."]}, {"name": "torch.nn.utils.prune.RandomUnstructured.remove()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.prune.random_structured()", "path": "generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured", "type": "torch.nn", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.random_unstructured()", "path": "generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured", "type": "torch.nn", "text": ["Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.", "modified (i.e. pruned) version of the input module", "module (nn.Module)"]}, {"name": "torch.nn.utils.prune.remove()", "path": "generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove", "type": "torch.nn", "text": ["Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.", "Note", "Pruning itself is NOT undone or reversed!"]}, {"name": "torch.nn.utils.remove_spectral_norm()", "path": "generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm", "type": "torch.nn", "text": ["Removes the spectral normalization reparameterization from a module."]}, {"name": "torch.nn.utils.remove_weight_norm()", "path": "generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm", "type": "torch.nn", "text": ["Removes the weight normalization reparameterization from a module."]}, {"name": "torch.nn.utils.rnn.PackedSequence", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence", "type": "torch.nn", "text": ["Holds the data and list of batch_sizes of a packed sequence.", "All RNN modules accept packed sequences as inputs.", "Note", "Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence().", "Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence(). For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1].", "Note", "data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data.", "However, batch_sizes should always be a CPU torch.int64 tensor.", "This invariant is maintained throughout PackedSequence class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).", "Alias for field number 1", "Return number of occurrences of value.", "Alias for field number 0", "Return first index of value.", "Raises ValueError if the value is not present.", "Returns true if self.data stored on a gpu", "Returns true if self.data stored on in pinned memory", "Alias for field number 2", "Performs dtype and/or device conversion on self.data.", "It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.", "Note", "If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.", "Alias for field number 3"]}, {"name": "torch.nn.utils.rnn.PackedSequence.batch_sizes()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.batch_sizes", "type": "torch.nn", "text": ["Alias for field number 1"]}, {"name": "torch.nn.utils.rnn.PackedSequence.count()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.count", "type": "torch.nn", "text": ["Return number of occurrences of value."]}, {"name": "torch.nn.utils.rnn.PackedSequence.data()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.data", "type": "torch.nn", "text": ["Alias for field number 0"]}, {"name": "torch.nn.utils.rnn.PackedSequence.index()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.index", "type": "torch.nn", "text": ["Return first index of value.", "Raises ValueError if the value is not present."]}, {"name": "torch.nn.utils.rnn.PackedSequence.is_cuda()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_cuda", "type": "torch.nn", "text": ["Returns true if self.data stored on a gpu"]}, {"name": "torch.nn.utils.rnn.PackedSequence.is_pinned()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_pinned", "type": "torch.nn", "text": ["Returns true if self.data stored on in pinned memory"]}, {"name": "torch.nn.utils.rnn.PackedSequence.sorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.sorted_indices", "type": "torch.nn", "text": ["Alias for field number 2"]}, {"name": "torch.nn.utils.rnn.PackedSequence.to()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.to", "type": "torch.nn", "text": ["Performs dtype and/or device conversion on self.data.", "It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.", "Note", "If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration."]}, {"name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.unsorted_indices", "type": "torch.nn", "text": ["Alias for field number 3"]}, {"name": "torch.nn.utils.rnn.pack_padded_sequence()", "path": "generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence", "type": "torch.nn", "text": ["Packs a Tensor containing padded sequences of variable length.", "input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected.", "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.", "Note", "This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute.", "a PackedSequence object"]}, {"name": "torch.nn.utils.rnn.pack_sequence()", "path": "generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence", "type": "torch.nn", "text": ["Packs a list of variable length Tensors", "sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero.", "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted in the order of decreasing length. enforce_sorted = True is only necessary for ONNX export.", "a PackedSequence object"]}, {"name": "torch.nn.utils.rnn.pad_packed_sequence()", "path": "generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence", "type": "torch.nn", "text": ["Pads a packed batch of variable length sequences.", "It is an inverse operation to pack_padded_sequence().", "The returned Tensor\u2019s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format.", "Note", "total_length is useful to implement the pack sequence -> recurrent network -> unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details.", "Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to pack_padded_sequence or pack_sequence."]}, {"name": "torch.nn.utils.rnn.pad_sequence()", "path": "generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence", "type": "torch.nn", "text": ["Pad a list of variable length Tensors with padding_value", "pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise.", "B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none.", "Note", "This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.", "Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise"]}, {"name": "torch.nn.utils.spectral_norm()", "path": "generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm", "type": "torch.nn", "text": ["Applies spectral normalization to a parameter in the given module.", "Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \u03c3\\sigma  of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call.", "See Spectral Normalization for Generative Adversarial Networks .", "The original module with the spectral norm hook", "Example:"]}, {"name": "torch.nn.utils.vector_to_parameters()", "path": "generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters", "type": "torch.nn", "text": ["Convert one vector to the parameters"]}, {"name": "torch.nn.utils.weight_norm()", "path": "generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm", "type": "torch.nn", "text": ["Applies weight normalization to a parameter in the given module.", "Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call.", "By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None.", "See https://arxiv.org/abs/1602.07868", "The original module with the weight norm hook", "Example:"]}, {"name": "torch.nn.ZeroPad2d", "path": "generated/torch.nn.zeropad2d#torch.nn.ZeroPad2d", "type": "torch.nn", "text": ["Pads the input tensor boundaries with zero.", "For N-dimensional padding, use torch.nn.functional.pad().", "padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left} , padding_right\\text{padding\\_right} , padding_top\\text{padding\\_top} , padding_bottom\\text{padding\\_bottom} )", "Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})  where", "Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom} ", "Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right} ", "Examples:"]}, {"name": "torch.nonzero()", "path": "generated/torch.nonzero#torch.nonzero", "type": "torch", "text": ["Note", "torch.nonzero(..., as_tuple=False) (default) returns a 2-D tensor where each row is the index for a nonzero value.", "torch.nonzero(..., as_tuple=True) returns a tuple of 1-D index tensors, allowing for advanced indexing, so x[x.nonzero(as_tuple=True)] gives all nonzero values of tensor x. Of the returned tuple, each index tensor contains nonzero indices for a certain dimension.", "See below for more details on the two behaviors.", "When input is on CUDA, torch.nonzero() causes host-device synchronization.", "When as_tuple is ``False`` (default):", "Returns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style).", "If input has nn  dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n) , where zz  is the total number of non-zero elements in the input tensor.", "When as_tuple is ``True``:", "Returns a tuple of 1-D tensors, one for each dimension in input, each containing the indices (in that dimension) of all non-zero elements of input .", "If input has nn  dimensions, then the resulting tuple contains nn  tensors of size zz , where zz  is the total number of non-zero elements in the input tensor.", "As a special case, when input has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.", "input (Tensor) \u2013 the input tensor.", "out (LongTensor, optional) \u2013 the output tensor containing indices", "If as_tuple is False, the output tensor containing indices. If as_tuple is True, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.", "LongTensor or tuple of LongTensor", "Example:"]}, {"name": "torch.norm()", "path": "generated/torch.norm#torch.norm", "type": "torch", "text": ["Returns the matrix norm or vector norm of a given tensor.", "Warning", "torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm() instead, but note that torch.linalg.norm() has a different signature and slightly different behavior that is more consistent with NumPy\u2019s numpy.linalg.norm.", "p (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013 ", "the order of norm. Default: 'fro' The following norms can be calculated:", "ord", "matrix norm", "vector norm", "\u2019fro\u2019", "Frobenius norm", "\u2013", "\u2018nuc\u2019", "nuclear norm", "\u2013", "Number", "\u2013", "sum(abs(x)**ord)**(1./ord)", "The vector norm can be calculated across any number of dimensions. The corresponding dimensions of input are flattened into one dimension, and the norm is calculated on the flattened dimension.", "Frobenius norm produces the same result as p=2 in all cases except when dim is a list of three or more dims, in which case Frobenius norm throws an error.", "Nuclear norm can only be calculated across exactly two dimensions.", "Note", "Even though p='fro' supports any number of dimensions, the true mathematical definition of Frobenius norm only applies to tensors with exactly two dimensions. torch.linalg.norm() with ord='fro' aligns with the mathematical definition, since it can only be applied across exactly two dimensions.", "Example:"]}, {"name": "torch.normal()", "path": "generated/torch.normal#torch.normal", "type": "torch", "text": ["Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.", "The mean is a tensor with the mean of each output element\u2019s normal distribution", "The std is a tensor with the standard deviation of each output element\u2019s normal distribution", "The shapes of mean and std don\u2019t need to match, but the total number of elements in each tensor need to be the same.", "Note", "When the shapes do not match, the shape of mean is used as the shape for the returned output tensor", "Example:", "Similar to the function above, but the means are shared among all drawn elements.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Similar to the function above, but the standard-deviations are shared among all drawn elements.", "out (Tensor, optional) \u2013 the output tensor", "Example:", "Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by size.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.not_equal()", "path": "generated/torch.not_equal#torch.not_equal", "type": "torch", "text": ["Alias for torch.ne()."]}, {"name": "torch.no_grad", "path": "generated/torch.no_grad#torch.no_grad", "type": "torch", "text": ["Context-manager that disabled gradient calculation.", "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.", "In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator. (Make sure to instantiate with parenthesis.)", "Example:"]}, {"name": "torch.numel()", "path": "generated/torch.numel#torch.numel", "type": "torch", "text": ["Returns the total number of elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.ones()", "path": "generated/torch.ones#torch.ones", "type": "torch", "text": ["Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.ones_like()", "path": "generated/torch.ones_like#torch.ones_like", "type": "torch", "text": ["Returns a tensor filled with the scalar value 1, with the same size as input. torch.ones_like(input) is equivalent to torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "Warning", "As of 0.4, this function does not support an out keyword. As an alternative, the old torch.ones_like(input, out=output) is equivalent to torch.ones(input.size(), out=output).", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}, {"name": "torch.onnx", "path": "onnx", "type": "torch.onnx", "text": ["Indexing", "Adding support for operators", "Operator Export Type", "Here is a simple script which exports a pretrained AlexNet as defined in torchvision into ONNX. It runs a single round of inference and then saves the resulting traced model to alexnet.onnx:", "The resulting alexnet.onnx is a binary protobuf file which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The keyword argument verbose=True causes the exporter to print out a human-readable representation of the network:", "You can also verify the protobuf using the ONNX library. You can install ONNX with conda:", "Then, you can run:", "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions.", "Once these are installed, you can use the backend for Caffe2:", "You can also run the exported model with ONNX Runtime, you will need to install ONNX Runtime: please follow these instructions.", "Once these are installed, you can use the backend for ONNX Runtime:", "Here is another tutorial of exporting the SuperResolution model to ONNX..", "In the future, there will be backends for other frameworks as well.", "The ONNX exporter can be both trace-based and script-based exporter.", "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements of a part of a model. Checkout this example:", "With trace-based exporter, we get the result ONNX graph which unrolls the for loop:", "To utilize script-based exporter for capturing the dynamic loop, we can write the loop in script, and call it from the regular nn.Module:", "Now the exported ONNX graph becomes:", "The dynamic control flow is captured correctly. We can verify in backends with different loop range.", "To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please avoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers. E.g.:", "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model. For the trace-based exporter, tracing treats the numpy values as the constant node, therefore it calculates the wrong result if we change the input. So the PyTorch model need implement using torch operators. For example, do not use numpy operators on numpy tensors:", "do not convert to numpy types:", "Always use torch tensors and torch operators: torch.concat, etc. In addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,", "There are two ways to handle models which consist of named parameters or keyword arguments as inputs:", "For example, in the model:", "There are two ways of exporting the model:", "Not using a dictionary for the keyword arguments and passing all the inputs in the same order as required by the model", "Using a dictionary to represent the keyword arguments. This dictionary is always passed in addition to the non-keyword arguments and is always the last argument in the args tuple.", "For cases in which there are no keyword arguments, models can be exported with either an empty or no dictionary. For example,", "An exception to this rule are cases in which the last input is also of a dictionary type. In these cases it is mandatory to have an empty dictionary as the last argument in the args tuple. For example,", "Without the presence of the empty dictionary, the export call assumes that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this.", "Tensor indexing in PyTorch is very flexible and complicated. There are two categories of indexing. Both are largely supported in exporting today. If you are experiencing issues exporting indexing that belongs to the supported patterns below, please double check that you are exporting with the latest opset (opset_version=12).", "This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:", "Below is the list of supported patterns for RHS indexing.", "And below is the list of unsupported patterns for RHS indexing.", "In code, this type of indexing occurs on the LHS. Export is supported for ONNX opset version >= 11. E.g.:", "Below is the list of supported patterns for LHS indexing.", "And below is the list of unsupported patterns for LHS indexing.", "If you are experiencing issues exporting indexing that belongs to the above supported patterns, please double check that you are exporting with the latest opset (opset_version=12).", "All TorchVision models, except for quantized versions, are exportable to ONNX. More details can be found in TorchVision.", "The following operators are supported:", "The operator set above is sufficient to export the following models:", "Adding export support for operators is an advance usage.", "To achieve this, developers need to touch the source code of PyTorch. Please follow the instructions for installing PyTorch from source. If the wanted operator is standardized in ONNX, it should be easy to add support for exporting such operator (adding a symbolic function for the operator). To confirm whether the operator is standardized or not, please check the ONNX operator list.", "If the operator is an ATen operator, which means you can find the declaration of the function in torch/csrc/autograd/generated/VariableType.h (available in generated code in PyTorch install dir), you should add the symbolic function in torch/onnx/symbolic_opset<version>.py and follow the instructions listed as below:", "If the operator is a non-ATen operator, the symbolic function has to be added in the corresponding PyTorch Function class. Please read the following instructions:", "Symbolic functions should be implemented in Python. All of these functions interact with Python methods which are implemented via C++-Python bindings, but intuitively the interface they provide looks like this:", "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.", "Here is an example of handling missing symbolic function for elu operator. We try to export the model and see the error message as below:", "The export fails because PyTorch does not support exporting elu operator. We find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override; in VariableType.h. This means elu is an ATen operator. We check the ONNX operator list, and confirm that Elu is standardized in ONNX. We add the following lines to symbolic_opset9.py:", "Now PyTorch is able to export elu operator.", "There are more examples in symbolic_opset9.py, symbolic_opset10.py.", "The interface for specifying operator definitions is experimental; adventurous users should note that the APIs will probably change in a future interface.", "Following this tutorial Extending TorchScript with Custom C++ Operators, you can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:", "Depending on the custom operator, you can export it as one or a combination of existing ONNX ops. You can also export it as a custom op in ONNX as well. In that case, you can specify the custom domain and version (custom opset) using the custom_opsets dictionary at export. If not explicitly specified, the custom opset version is set to 1 by default. Using custom ONNX ops, you will need to extend the backend of your choice with matching custom ops implementation, e.g. Caffe2 custom ops, ONNX Runtime custom ops.", "Exporting models with unsupported ONNX operators can be achieved using the operator_export_type flag in export API. This flag is useful when users try to export ATen and non-ATen operators that are not registered and supported in ONNX.", "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode.", "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX.", "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly. In the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator.", "To export a raw ir.", "This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX. Exported falls through and exports the operator as is, as custom op. Exporting custom operators enables users to register and implement the operator as part of their runtime backend.", "Q: I have exported my lstm model, but its input size seems to be fixed?", "The tracer records the example inputs shape in the graph. In case the model should accept inputs of dynamic shape, you can utilize the parameter dynamic_axes in export api.", "Q: How to export models with loops in it?", "Please checkout Tracing vs Scripting.", "Q: Does ONNX support implicit scalar datatype casting?", "No, but the exporter will try to handle that part. Scalars are converted to constant tensors in ONNX. The exporter will try to figure out the right datatype for scalars. However for cases that it failed to do so, you will need to manually provide the datatype information. This often happens with scripted models, where the datatypes are not recorded. We are trying to improve the datatype propagation in the exporter such that manual changes are not required in the future.", "Q: Is tensor in-place indexed assignment like data[index] = new_data supported?", "Yes, this is supported for ONNX opset version >= 11. Please checkout Indexing.", "Q: Is tensor list exportable to ONNX?", "Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11. Similar to list, Sequence is a data type that contains arbitrary number of Tensors. Associated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace add operator. E.g.:", "use_external_data_format argument in export API enables export of models in ONNX external data format. With this option enabled, the exporter stores some model parameters in external binary files, rather than the ONNX file itself. These external binary files are stored in the same location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model.", "This argument enables export of large models to ONNX. Models larger than 2GB cannot be exported in one file because of the protobuf size limit. Users should set use_external_data_format to True to successfully export such models.", "Training argument in export API allows users to export models in a training-friendly mode. TrainingMode.TRAINING exports model in a training-friendly mode that avoids certain model optimizations which might interfere with model parameter training. TrainingMode.PRESERVE exports the model in inference mode if model.training is False. Otherwise, it exports the model in a training-friendly mode. The default mode for this argument is TrainingMode.EVAL which exports the model in inference mode.", "Export a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)", "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013 ", "a dictionary to specify the input to the corresponding named parameter: - KEY: str, named parameter - VALUE: corresponding input args can be structured either as:", "ONLY A TUPLE OF ARGUMENTS or torch.Tensor:", "The inputs to the model, e.g., such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args. If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.", "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:", "The inputs to the model are structured as a tuple consisting of non-keyword arguments and the last value of this tuple being a dictionary consisting of named parameters and the corresponding inputs as key-value pairs. If certain named argument is not present in the dictionary, it is assigned the default value, or None if default value is not provided.", "Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example.", "\u2026 return x", "m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)}", "In the previous iteration, the call to export API would look like", "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)", "This would work as intended. However, the export function would now assume that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this.", "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)", "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 ", "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph:", "is exported as:", "In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph:", "is exported as:", "In the above example, prim::ListConstruct is not supported, hence exporter falls through.", "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013 ", "a dictionary to specify dynamic axes of input/output, such that: - KEY: input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export.", "Example. if we have the following shape for inputs and outputs:", "Then dynamic axes can be defined either as:", "ONLY INDICES:", "INDICES WITH CORRESPONDING NAMES:", "MIXED MODE OF (1) and (2):", "keep_initializers_as_inputs (bool, default None) \u2013 ", "If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs.", "This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version < 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.", "A context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019, resetting it when we exit the with-block. A no-op if mode is None.", "In version 1.6 changed to this from set_training", "Check whether it\u2019s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread."]}, {"name": "torch.onnx.export()", "path": "onnx#torch.onnx.export", "type": "torch.onnx", "text": ["Export a model into ONNX format. This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)", "args (tuple of arguments or torch.Tensor, a dictionary consisting of named arguments (optional)) \u2013 ", "a dictionary to specify the input to the corresponding named parameter: - KEY: str, named parameter - VALUE: corresponding input args can be structured either as:", "ONLY A TUPLE OF ARGUMENTS or torch.Tensor:", "The inputs to the model, e.g., such that model(*args) is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args. If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.", "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:", "The inputs to the model are structured as a tuple consisting of non-keyword arguments and the last value of this tuple being a dictionary consisting of named parameters and the corresponding inputs as key-value pairs. If certain named argument is not present in the dictionary, it is assigned the default value, or None if default value is not provided.", "Cases in which an dictionary input is the last input of the args tuple would cause a conflict when a dictionary of named parameters is used. The model below provides such an example.", "\u2026 return x", "m = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)}", "In the previous iteration, the call to export API would look like", "torch.onnx.export(model, (k, x), \u2018test.onnx\u2019)", "This would work as intended. However, the export function would now assume that the \u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments. In order to prevent this from being an issue a constraint is placed to provide an empty dictionary as the last input in the tuple args in such cases. The new call would look like this.", "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)", "operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 ", "OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX namespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops (with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported in ONNX or its symbolic is missing, fall back on ATen op. Registered ops are exported to ONNX regularly. Example graph:", "is exported as:", "In the above example, aten::triu is not supported in ONNX, hence exporter falls back on this op. OperatorExportTypes.RAW: Export raw ir. OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall through and export the operator as is, as a custom ONNX op. Using this mode, the op can be exported and implemented by the user for their runtime backend. Example graph:", "is exported as:", "In the above example, prim::ListConstruct is not supported, hence exporter falls through.", "dynamic_axes (dict<string, dict<python:int, string>> or dict<string, list(int)>, default empty dict) \u2013 ", "a dictionary to specify dynamic axes of input/output, such that: - KEY: input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export.", "Example. if we have the following shape for inputs and outputs:", "Then dynamic axes can be defined either as:", "ONLY INDICES:", "INDICES WITH CORRESPONDING NAMES:", "MIXED MODE OF (1) and (2):", "keep_initializers_as_inputs (bool, default None) \u2013 ", "If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs.", "This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version < 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored."]}, {"name": "torch.onnx.export_to_pretty_string()", "path": "onnx#torch.onnx.export_to_pretty_string", "type": "torch.onnx", "text": []}, {"name": "torch.onnx.is_in_onnx_export()", "path": "onnx#torch.onnx.is_in_onnx_export", "type": "torch.onnx", "text": ["Check whether it\u2019s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread."]}, {"name": "torch.onnx.operators.shape_as_tensor()", "path": "onnx#torch.onnx.operators.shape_as_tensor", "type": "torch.onnx", "text": []}, {"name": "torch.onnx.register_custom_op_symbolic()", "path": "onnx#torch.onnx.register_custom_op_symbolic", "type": "torch.onnx", "text": []}, {"name": "torch.onnx.select_model_mode_for_export()", "path": "onnx#torch.onnx.select_model_mode_for_export", "type": "torch.onnx", "text": ["A context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019, resetting it when we exit the with-block. A no-op if mode is None.", "In version 1.6 changed to this from set_training"]}, {"name": "torch.optim", "path": "optim", "type": "torch.optim", "text": ["torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future.", "To use torch.optim you have to construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients.", "To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.", "Note", "If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.", "In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.", "Example:", "Optimizer s also support specifying per-parameter options. To do this, instead of passing an iterable of Variable s, pass in an iterable of dict s. Each of them will define a separate parameter group, and should contain a params key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.", "Note", "You can still pass options as keyword arguments. They will be used as defaults, in the groups that didn\u2019t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.", "For example, this is very useful when one wants to specify per-layer learning rates:", "This means that model.base\u2019s parameters will use the default learning rate of 1e-2, model.classifier\u2019s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters.", "All optimizers implement a step() method, that updates the parameters. It can be used in two ways:", "This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. backward().", "Example:", "Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.", "Example:", "Base class for all optimizers.", "Warning", "Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes.", "Performs a single optimization step (parameter update).", "closure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.", "Note", "Unless otherwise specified, this function should not modify the .grad field of the parameters.", "Sets the gradients of all optimized torch.Tensor s to zero.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).", "Implements Adadelta algorithm.", "It has been proposed in ADADELTA: An Adaptive Learning Rate Method.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements Adagrad algorithm.", "It has been proposed in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements Adam algorithm.", "It has been proposed in Adam: A Method for Stochastic Optimization. The implementation of the L2 penalty follows changes proposed in Decoupled Weight Decay Regularization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements AdamW algorithm.", "The original Adam algorithm was proposed in Adam: A Method for Stochastic Optimization. The AdamW variant was proposed in Decoupled Weight Decay Regularization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements lazy version of Adam algorithm suitable for sparse tensors.", "In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements Adamax algorithm (a variant of Adam based on infinity norm).", "It has been proposed in Adam: A Method for Stochastic Optimization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements Averaged Stochastic Gradient Descent.", "It has been proposed in Acceleration of stochastic approximation by averaging.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements L-BFGS algorithm, heavily inspired by minFunc <https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>.", "Warning", "This optimizer doesn\u2019t support per-parameter options and parameter groups (there can be only one).", "Warning", "Right now all parameters have to be on a single device. This will be improved in the future.", "Note", "This is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn\u2019t fit in memory try reducing the history size, or use a different algorithm.", "Performs a single optimization step.", "closure (callable) \u2013 A closure that reevaluates the model and returns the loss.", "Implements RMSprop algorithm.", "Proposed by G. Hinton in his course.", "The centered version first appears in Generating Sequences With Recurrent Neural Networks.", "The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon)  where \u03b1\\alpha  is the scheduled learning rate and vv  is the weighted moving average of the squared gradient.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements the resilient backpropagation algorithm.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "Implements stochastic gradient descent (optionally with momentum).", "Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.", "Note", "The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks.", "Considering the specific case of Momentum, the update can be written as", "where pp , gg , vv  and \u03bc\\mu  denote the parameters, gradient, velocity, and momentum respectively.", "This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form", "The Nesterov version is analogously modified.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.", "torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements.", "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you should write your code this way:", "Warning", "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling scheduler.step()) before the optimizer\u2019s update (calling optimizer.step()), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling scheduler.step() at the wrong time.", "Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.", "Loads the schedulers state.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.", "Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.", "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr and TcurT_{cur}  is the number of epochs since the last restart in SGDR:", "When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.", "Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced.", "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.", "Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This class has three built-in policies, as put forth in the paper:", "This implementation was adapted from the github repo: bckenstler/CLR", "Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index.", "If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum.", "Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.", "The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This scheduler is not chainable.", "Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):", "You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.", "The default behaviour of this scheduler follows the fastai implementation of 1cycle, which claims that \u201cunpublished work has shown even better results by using only two phases\u201d. To mimic the behaviour of the original paper instead, set three_phase=True.", "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr, TcurT_{cur}  is the number of epochs since the last restart and TiT_{i}  is the number of epochs between two warm restarts in SGDR:", "When Tcur=TiT_{cur}=T_{i} , set \u03b7t=\u03b7min\\eta_t = \\eta_{min} . When Tcur=0T_{cur}=0  after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max} .", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.", "Step could be called after every batch update", "This function can be called in an interleaved way.", "torch.optim.swa_utils implements Stochastic Weight Averaging (SWA). In particular, torch.optim.swa_utils.AveragedModel class implements SWA models, torch.optim.swa_utils.SWALR implements the SWA learning rate scheduler and torch.optim.swa_utils.update_bn() is a utility function used to update SWA batch normalization statistics at the end of training.", "SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization.", "AveragedModel class serves to compute the weights of the SWA model. You can create an averaged model by running:", "Here the model model can be an arbitrary torch.nn.Module object. swa_model will keep track of the running averages of the parameters of the model. To update these averages, you can use the update_parameters() function:", "Typically, in SWA the learning rate is set to a high constant value. SWALR is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:", "You can also use cosine annealing to a fixed value instead of linear annealing by setting anneal_strategy=\"cos\".", "update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader loader at the end of training:", "update_bn() applies the swa_model to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.", "Warning", "update_bn() assumes that each batch in the dataloader loader is either a tensors or a list of tensors where the first element is the tensor that the network swa_model should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the swa_model by doing a forward pass with the swa_model on each element of the dataset.", "By default, torch.optim.swa_utils.AveragedModel computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the avg_fn parameter. In the following example ema_model computes an exponential moving average.", "Example:", "In the example below, swa_model is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:"]}, {"name": "torch.optim.Adadelta", "path": "optim#torch.optim.Adadelta", "type": "torch.optim", "text": ["Implements Adadelta algorithm.", "It has been proposed in ADADELTA: An Adaptive Learning Rate Method.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adadelta.step()", "path": "optim#torch.optim.Adadelta.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adagrad", "path": "optim#torch.optim.Adagrad", "type": "torch.optim", "text": ["Implements Adagrad algorithm.", "It has been proposed in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adagrad.step()", "path": "optim#torch.optim.Adagrad.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adam", "path": "optim#torch.optim.Adam", "type": "torch.optim", "text": ["Implements Adam algorithm.", "It has been proposed in Adam: A Method for Stochastic Optimization. The implementation of the L2 penalty follows changes proposed in Decoupled Weight Decay Regularization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adam.step()", "path": "optim#torch.optim.Adam.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adamax", "path": "optim#torch.optim.Adamax", "type": "torch.optim", "text": ["Implements Adamax algorithm (a variant of Adam based on infinity norm).", "It has been proposed in Adam: A Method for Stochastic Optimization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Adamax.step()", "path": "optim#torch.optim.Adamax.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.AdamW", "path": "optim#torch.optim.AdamW", "type": "torch.optim", "text": ["Implements AdamW algorithm.", "The original Adam algorithm was proposed in Adam: A Method for Stochastic Optimization. The AdamW variant was proposed in Decoupled Weight Decay Regularization.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.AdamW.step()", "path": "optim#torch.optim.AdamW.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.ASGD", "path": "optim#torch.optim.ASGD", "type": "torch.optim", "text": ["Implements Averaged Stochastic Gradient Descent.", "It has been proposed in Acceleration of stochastic approximation by averaging.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.ASGD.step()", "path": "optim#torch.optim.ASGD.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.LBFGS", "path": "optim#torch.optim.LBFGS", "type": "torch.optim", "text": ["Implements L-BFGS algorithm, heavily inspired by minFunc <https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>.", "Warning", "This optimizer doesn\u2019t support per-parameter options and parameter groups (there can be only one).", "Warning", "Right now all parameters have to be on a single device. This will be improved in the future.", "Note", "This is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn\u2019t fit in memory try reducing the history size, or use a different algorithm.", "Performs a single optimization step.", "closure (callable) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.LBFGS.step()", "path": "optim#torch.optim.LBFGS.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingLR", "type": "torch.optim", "text": ["Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr and TcurT_{cur}  is the number of epochs since the last restart in SGDR:", "When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "type": "torch.optim", "text": ["Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}  is set to the initial lr, TcurT_{cur}  is the number of epochs since the last restart and TiT_{i}  is the number of epochs between two warm restarts in SGDR:", "When Tcur=TiT_{cur}=T_{i} , set \u03b7t=\u03b7min\\eta_t = \\eta_{min} . When Tcur=0T_{cur}=0  after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max} .", "It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.", "Step could be called after every batch update", "This function can be called in an interleaved way."]}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step()", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step", "type": "torch.optim", "text": ["Step could be called after every batch update", "This function can be called in an interleaved way."]}, {"name": "torch.optim.lr_scheduler.CyclicLR", "path": "optim#torch.optim.lr_scheduler.CyclicLR", "type": "torch.optim", "text": ["Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.", "Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This class has three built-in policies, as put forth in the paper:", "This implementation was adapted from the github repo: bckenstler/CLR", "Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index.", "If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum."]}, {"name": "torch.optim.lr_scheduler.CyclicLR.get_lr()", "path": "optim#torch.optim.lr_scheduler.CyclicLR.get_lr", "type": "torch.optim", "text": ["Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index.", "If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum."]}, {"name": "torch.optim.lr_scheduler.ExponentialLR", "path": "optim#torch.optim.lr_scheduler.ExponentialLR", "type": "torch.optim", "text": ["Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr."]}, {"name": "torch.optim.lr_scheduler.LambdaLR", "path": "optim#torch.optim.lr_scheduler.LambdaLR", "type": "torch.optim", "text": ["Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.", "Loads the schedulers state.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer."]}, {"name": "torch.optim.lr_scheduler.LambdaLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.load_state_dict", "type": "torch.optim", "text": ["Loads the schedulers state.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.LambdaLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.state_dict", "type": "torch.optim", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.", "When saving or loading the scheduler, please make sure to also save or load the state of the optimizer."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR", "type": "torch.optim", "text": ["Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.", "Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().", "Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict", "type": "torch.optim", "text": ["Loads the schedulers state.", "state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.state_dict", "type": "torch.optim", "text": ["Returns the state of the scheduler as a dict.", "It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas."]}, {"name": "torch.optim.lr_scheduler.MultiStepLR", "path": "optim#torch.optim.lr_scheduler.MultiStepLR", "type": "torch.optim", "text": ["Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr."]}, {"name": "torch.optim.lr_scheduler.OneCycleLR", "path": "optim#torch.optim.lr_scheduler.OneCycleLR", "type": "torch.optim", "text": ["Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.", "The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.", "This scheduler is not chainable.", "Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):", "You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.", "The default behaviour of this scheduler follows the fastai implementation of 1cycle, which claims that \u201cunpublished work has shown even better results by using only two phases\u201d. To mimic the behaviour of the original paper instead, set three_phase=True."]}, {"name": "torch.optim.lr_scheduler.ReduceLROnPlateau", "path": "optim#torch.optim.lr_scheduler.ReduceLROnPlateau", "type": "torch.optim", "text": ["Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced."]}, {"name": "torch.optim.lr_scheduler.StepLR", "path": "optim#torch.optim.lr_scheduler.StepLR", "type": "torch.optim", "text": ["Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr."]}, {"name": "torch.optim.Optimizer", "path": "optim#torch.optim.Optimizer", "type": "torch.optim", "text": ["Base class for all optimizers.", "Warning", "Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries.", "Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.", "Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().", "Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes.", "Performs a single optimization step (parameter update).", "closure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.", "Note", "Unless otherwise specified, this function should not modify the .grad field of the parameters.", "Sets the gradients of all optimized torch.Tensor s to zero.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.Optimizer.add_param_group()", "path": "optim#torch.optim.Optimizer.add_param_group", "type": "torch.optim", "text": ["Add a param group to the Optimizer s param_groups.", "This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses."]}, {"name": "torch.optim.Optimizer.load_state_dict()", "path": "optim#torch.optim.Optimizer.load_state_dict", "type": "torch.optim", "text": ["Loads the optimizer state.", "state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.optim.Optimizer.state_dict()", "path": "optim#torch.optim.Optimizer.state_dict", "type": "torch.optim", "text": ["Returns the state of the optimizer as a dict.", "It contains two entries:", "differs between optimizer classes."]}, {"name": "torch.optim.Optimizer.step()", "path": "optim#torch.optim.Optimizer.step", "type": "torch.optim", "text": ["Performs a single optimization step (parameter update).", "closure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.", "Note", "Unless otherwise specified, this function should not modify the .grad field of the parameters."]}, {"name": "torch.optim.Optimizer.zero_grad()", "path": "optim#torch.optim.Optimizer.zero_grad", "type": "torch.optim", "text": ["Sets the gradients of all optimized torch.Tensor s to zero.", "set_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests zero_grad(set_to_none=True) followed by a backward pass, .grads are guaranteed to be None for params that did not receive a gradient. 3. torch.optim optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether)."]}, {"name": "torch.optim.RMSprop", "path": "optim#torch.optim.RMSprop", "type": "torch.optim", "text": ["Implements RMSprop algorithm.", "Proposed by G. Hinton in his course.", "The centered version first appears in Generating Sequences With Recurrent Neural Networks.", "The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon)  where \u03b1\\alpha  is the scheduled learning rate and vv  is the weighted moving average of the squared gradient.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.RMSprop.step()", "path": "optim#torch.optim.RMSprop.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Rprop", "path": "optim#torch.optim.Rprop", "type": "torch.optim", "text": ["Implements the resilient backpropagation algorithm.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.Rprop.step()", "path": "optim#torch.optim.Rprop.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.SGD", "path": "optim#torch.optim.SGD", "type": "torch.optim", "text": ["Implements stochastic gradient descent (optionally with momentum).", "Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.", "Note", "The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks.", "Considering the specific case of Momentum, the update can be written as", "where pp , gg , vv  and \u03bc\\mu  denote the parameters, gradient, velocity, and momentum respectively.", "This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form", "The Nesterov version is analogously modified.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.SGD.step()", "path": "optim#torch.optim.SGD.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.SparseAdam", "path": "optim#torch.optim.SparseAdam", "type": "torch.optim", "text": ["Implements lazy version of Adam algorithm suitable for sparse tensors.", "In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.", "Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.optim.SparseAdam.step()", "path": "optim#torch.optim.SparseAdam.step", "type": "torch.optim", "text": ["Performs a single optimization step.", "closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss."]}, {"name": "torch.orgqr()", "path": "generated/torch.orgqr#torch.orgqr", "type": "torch", "text": ["Computes the orthogonal matrix Q of a QR factorization, from the (input, input2) tuple returned by torch.geqrf().", "This directly calls the underlying LAPACK function ?orgqr. See LAPACK documentation for orgqr for further details."]}, {"name": "torch.ormqr()", "path": "generated/torch.ormqr#torch.ormqr", "type": "torch", "text": ["Multiplies mat (given by input3) by the orthogonal Q matrix of the QR factorization formed by torch.geqrf() that is represented by (a, tau) (given by (input, input2)).", "This directly calls the underlying LAPACK function ?ormqr. See LAPACK documentation for ormqr for further details."]}, {"name": "torch.outer()", "path": "generated/torch.outer#torch.outer", "type": "torch", "text": ["Outer product of input and vec2. If input is a vector of size nn  and vec2 is a vector of size mm , then out must be a matrix of size (n\u00d7m)(n \\times m) .", "Note", "This function does not broadcast.", "out (Tensor, optional) \u2013 optional output matrix", "Example:"]}, {"name": "torch.overrides", "path": "torch.overrides", "type": "torch.overrides", "text": ["This module exposes various helper functions for the __torch_function__ protocol. See Extending torch for more detail on the __torch_function__ protocol.", "Return public functions that cannot be overridden by __torch_function__.", "A tuple of functions that are publicly available in the torch API but cannot be overridden with __torch_function__. Mostly this is because none of the arguments of these functions are tensors or tensor-likes.", "Set[Callable]", "List functions that are overridable via __torch_function__", "A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.", "Dict[Any, List[Callable]]", "Return a dict containing dummy overrides for all overridable functions", "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines __torch_function__.", "Dict[Callable, Callable]", "Implement a function with checks for __torch_function__ overrides.", "See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.", "Result from calling implementation or an __torch_function__ method, as appropriate.", "object", ":raises TypeError : if no implementation is found.:", "Check for __torch_function__ implementations in the elements of an iterable. Considers exact Tensor s and Parameter s non-dispatchable. :param relevant_args: Iterable or aguments to check for __torch_function__ methods. :type relevant_args: iterable", "True if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.", "bool", "See also", "Checks if something is a Tensor-like, including an exact Tensor.", "Returns True if the passed-in input is a Tensor-like.", "Currently, this occurs whenever there\u2019s a __torch_function__ attribute on the type of the input.", "A subclass of tensor is generally a Tensor-like.", "Built-in or user types aren\u2019t usually Tensor-like.", "But, they can be made Tensor-like by implementing __torch_function__.", "Returns True if the function passed in is a handler for a method or property belonging to torch.Tensor, as passed into __torch_function__.", "Note", "For properties, their __get__ method must be passed in.", "This may be needed, in particular, for the following reasons:", "Wraps a given function with __torch_function__ -related functionality.", "dispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.", "Note", "This decorator may reduce the performance of your code. Generally, it\u2019s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available."]}, {"name": "torch.overrides.get_ignored_functions()", "path": "torch.overrides#torch.overrides.get_ignored_functions", "type": "torch.overrides", "text": ["Return public functions that cannot be overridden by __torch_function__.", "A tuple of functions that are publicly available in the torch API but cannot be overridden with __torch_function__. Mostly this is because none of the arguments of these functions are tensors or tensor-likes.", "Set[Callable]"]}, {"name": "torch.overrides.get_overridable_functions()", "path": "torch.overrides#torch.overrides.get_overridable_functions", "type": "torch.overrides", "text": ["List functions that are overridable via __torch_function__", "A dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.", "Dict[Any, List[Callable]]"]}, {"name": "torch.overrides.get_testing_overrides()", "path": "torch.overrides#torch.overrides.get_testing_overrides", "type": "torch.overrides", "text": ["Return a dict containing dummy overrides for all overridable functions", "A dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines __torch_function__.", "Dict[Callable, Callable]"]}, {"name": "torch.overrides.handle_torch_function()", "path": "torch.overrides#torch.overrides.handle_torch_function", "type": "torch.overrides", "text": ["Implement a function with checks for __torch_function__ overrides.", "See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.", "Result from calling implementation or an __torch_function__ method, as appropriate.", "object", ":raises TypeError : if no implementation is found.:"]}, {"name": "torch.overrides.has_torch_function()", "path": "torch.overrides#torch.overrides.has_torch_function", "type": "torch.overrides", "text": ["Check for __torch_function__ implementations in the elements of an iterable. Considers exact Tensor s and Parameter s non-dispatchable. :param relevant_args: Iterable or aguments to check for __torch_function__ methods. :type relevant_args: iterable", "True if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.", "bool", "See also", "Checks if something is a Tensor-like, including an exact Tensor."]}, {"name": "torch.overrides.is_tensor_like()", "path": "torch.overrides#torch.overrides.is_tensor_like", "type": "torch.overrides", "text": ["Returns True if the passed-in input is a Tensor-like.", "Currently, this occurs whenever there\u2019s a __torch_function__ attribute on the type of the input.", "A subclass of tensor is generally a Tensor-like.", "Built-in or user types aren\u2019t usually Tensor-like.", "But, they can be made Tensor-like by implementing __torch_function__."]}, {"name": "torch.overrides.is_tensor_method_or_property()", "path": "torch.overrides#torch.overrides.is_tensor_method_or_property", "type": "torch.overrides", "text": ["Returns True if the function passed in is a handler for a method or property belonging to torch.Tensor, as passed into __torch_function__.", "Note", "For properties, their __get__ method must be passed in.", "This may be needed, in particular, for the following reasons:"]}, {"name": "torch.overrides.wrap_torch_function()", "path": "torch.overrides#torch.overrides.wrap_torch_function", "type": "torch.overrides", "text": ["Wraps a given function with __torch_function__ -related functionality.", "dispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.", "Note", "This decorator may reduce the performance of your code. Generally, it\u2019s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available."]}, {"name": "torch.pca_lowrank()", "path": "generated/torch.pca_lowrank#torch.pca_lowrank", "type": "torch", "text": ["Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.", "This function returns a namedtuple (U, S, V) which is the nearly optimal approximation of a singular value decomposition of a centered matrix AA  such that A=Udiag(S)VTA = U diag(S) V^T .", "Note", "The relation of (U, S, V) to PCA is as follows:", "Note", "Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:", "Note", "To obtain repeatable results, reset the seed for the pseudorandom number generator", "References:"]}, {"name": "torch.pinverse()", "path": "generated/torch.pinverse#torch.pinverse", "type": "torch", "text": ["Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at Moore-Penrose inverse for more details", "Note", "torch.pinverse() is deprecated. Please use torch.linalg.pinv() instead which includes new parameters hermitian and out.", "Note", "This method is implemented using the Singular Value Decomposition.", "Note", "The pseudo-inverse is not necessarily a continuous function in the elements of the matrix [1]. Therefore, derivatives are not always existent, and exist for a constant rank only [2]. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See svd() for more details.", "Note", "Supports real and complex inputs. Batched version for complex inputs is only supported on the CPU.", "The pseudo-inverse of input of dimensions (\u2217,n,m)(*, n, m) ", "Example:"]}, {"name": "torch.poisson()", "path": "generated/torch.poisson#torch.poisson", "type": "torch", "text": ["Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,", "input (Tensor) \u2013 the input tensor containing the rates of the Poisson distribution", "generator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling", "Example:"]}, {"name": "torch.polar()", "path": "generated/torch.polar#torch.polar", "type": "torch", "text": ["Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.", "out (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128."]}, {"name": "torch.polygamma()", "path": "generated/torch.polygamma#torch.polygamma", "type": "torch", "text": ["Computes the nthn^{th}  derivative of the digamma function on input. n\u22650n \\geq 0  is called the order of the polygamma function.", "Note", "This function is implemented only for nonnegative integers n\u22650n \\geq 0 .", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.pow()", "path": "generated/torch.pow#torch.pow", "type": "torch", "text": ["Takes the power of each element in input with exponent and returns a tensor with the result.", "exponent can be either a single float number or a Tensor with the same number of elements as input.", "When exponent is a scalar value, the operation applied is:", "When exponent is a tensor, the operation applied is:", "When exponent is a tensor, the shapes of input and exponent must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "self is a scalar float value, and exponent is a tensor. The returned tensor out is of the same shape as exponent", "The operation applied is:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.prod()", "path": "generated/torch.prod#torch.prod", "type": "torch", "text": ["Returns the product of all elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the product of each row of the input tensor in the given dimension dim.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:"]}, {"name": "torch.promote_types()", "path": "generated/torch.promote_types#torch.promote_types", "type": "torch", "text": ["Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2. See type promotion documentation for more information on the type promotion logic.", "Example:"]}, {"name": "torch.qr()", "path": "generated/torch.qr#torch.qr", "type": "torch", "text": ["Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices.", "If some is True, then this function returns the thin (reduced) QR factorization. Otherwise, if some is False, this function returns the complete QR factorization.", "Warning", "torch.qr is deprecated. Please use torch.linalg.qr() instead.", "Differences with torch.linalg.qr:", "torch.linalg.qr takes a string parameter mode instead of some:", "Warning", "If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first min\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))  columns of input are linearly independent. This behavior will propably change once QR supports pivoting.", "Note", "This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.", "some (bool, optional) \u2013 ", "Set to True for reduced QR decomposition and False for complete QR decomposition. If k = min(m, n) then:", "out (tuple, optional) \u2013 tuple of Q and R tensors. The dimensions of Q and R are detailed in the description of some above.", "Example:"]}, {"name": "torch.quantile()", "path": "generated/torch.quantile#torch.quantile", "type": "torch", "text": ["Returns the q-th quantiles of all elements in the input tensor, doing a linear interpolation when the q-th quantile lies between two data points.", "Example:", "Returns the q-th quantiles of each row of the input tensor along the dimension dim, doing a linear interpolation when the q-th quantile lies between two data points. By default, dim is None resulting in the input tensor being flattened before computation.", "If keepdim is True, the output dimensions are of the same size as input except in the dimensions being reduced (dim or all if dim is None) where they have size 1. Otherwise, the dimensions being reduced are squeezed (see torch.squeeze()). If q is a 1D tensor, an extra dimension is prepended to the output tensor with the same size as q which represents the quantiles.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.quantization", "path": "torch.quantization", "type": "torch.quantization", "text": ["This module implements the functions you call directly to convert your model from FP32 to quantized form. For example the prepare() is used in post training quantization to prepares your model for the calibration step and convert() actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu.", "Quantize the input float model with post training static quantization.", "First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.", "Quantized model.", "Converts a float model to dynamic (i.e. weights-only) quantized model.", "Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.", "For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.", "Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.", "qconfig_spec \u2013 ", "Either:", "Do quantization aware training and output a quantized model", "Quantized model.", "Prepares a copy of the model for quantization calibration or quantization-aware training.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.", "The model will be attached with observer or fake quant modules, and qconfig will be propagated.", "Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.", "Converts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.", "Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.", "Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.", "Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):", "my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8), weight=default_observer.with_args(dtype=torch.qint8))", "Describes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights.", "It\u2019s like QConfig, but for dynamic quantization.", "Note that QConfigDynamic needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers.", "Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):", "my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))", "Fuses a list of modules into a single module", "Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.", "model with fused modules. A new copy is created if inplace=True.", "Examples:", "Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.", "qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules", "Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert.", "A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.", "This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub.", "Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.", "Either the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.", "Add observer for the leaf child of the module.", "This function insert observer module to all leaf child module that has a valid qconfig attribute.", "None, module is modified inplace with added observer modules and forward_hooks", "Swaps the module if it has a quantized counterpart and it has an observer attached.", "The corresponding quantized module of mod", "Propagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module", "None, module is modified inplace with qconfig attached", "Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset", "Base observer Module. Any observer implementation should derive from this class.", "Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.", "dtype \u2013 Quantized data type", "Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances.", "Example:", "Observer module for computing the quantization parameters based on the running min and max values.", "This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "Given running min/max as xminx_\\text{min}  and xmaxx_\\text{max} , scale ss  and zero point zz  are computed as:", "The running minimum/maximum xmin/maxx_\\text{min/max}  is computed as:", "where XX  is the observed tensor.", "The scale ss  and zero point zz  are then computed as:", "where QminQ_\\text{min}  and QmaxQ_\\text{max}  are the minimum and maximum of the quantized data type.", "Warning", "Only works with torch.per_tensor_symmetric quantization scheme", "Warning", "dtype can only take torch.qint8 or torch.quint8.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.", "Observer module for computing the quantization parameters based on the moving average of the min and max values.", "This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The moving average min/max is computed as follows", "where xmin/maxx_\\text{min/max}  is the running average min/max, XX  is is the incoming tensor, and cc  is the averaging_constant.", "The scale and zero point are then computed as in MinMaxObserver.", "Note", "Only works with torch.per_tensor_affine quantization scheme.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.", "Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.", "Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.", "The module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.", "The scale and zero point are computed as follows:", "The histogram is computed continuously, and the ranges per bin change with every new tensor observed.", "The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.", "MinMaxObserver", "Simulate the quantize and dequantize operations in training time. The output of this module is given by", "x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale", "allowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype", "~FakeQuantize.observer (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.", "Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float().", "Primarily used for quantization to float16 which doesn\u2019t require determining ranges.", "Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers", "The module is mainly for debug and records the tensor values during runtime.", "nn.intrinsic"]}, {"name": "torch.quantization.add_observer_()", "path": "torch.quantization#torch.quantization.add_observer_", "type": "torch.quantization", "text": ["Add observer for the leaf child of the module.", "This function insert observer module to all leaf child module that has a valid qconfig attribute.", "None, module is modified inplace with added observer modules and forward_hooks"]}, {"name": "torch.quantization.add_quant_dequant()", "path": "torch.quantization#torch.quantization.add_quant_dequant", "type": "torch.quantization", "text": ["Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.", "Either the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it."]}, {"name": "torch.quantization.convert()", "path": "torch.quantization#torch.quantization.convert", "type": "torch.quantization", "text": ["Converts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True."]}, {"name": "torch.quantization.default_eval_fn()", "path": "torch.quantization#torch.quantization.default_eval_fn", "type": "torch.quantization", "text": ["Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset"]}, {"name": "torch.quantization.DeQuantStub", "path": "torch.quantization#torch.quantization.DeQuantStub", "type": "torch.quantization", "text": ["Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert."]}, {"name": "torch.quantization.FakeQuantize", "path": "torch.quantization#torch.quantization.FakeQuantize", "type": "torch.quantization", "text": ["Simulate the quantize and dequantize operations in training time. The output of this module is given by", "x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale", "allowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype", "~FakeQuantize.observer (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point."]}, {"name": "torch.quantization.fuse_modules()", "path": "torch.quantization#torch.quantization.fuse_modules", "type": "torch.quantization", "text": ["Fuses a list of modules into a single module", "Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.", "model with fused modules. A new copy is created if inplace=True.", "Examples:"]}, {"name": "torch.quantization.get_observer_dict()", "path": "torch.quantization#torch.quantization.get_observer_dict", "type": "torch.quantization", "text": ["Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers"]}, {"name": "torch.quantization.HistogramObserver", "path": "torch.quantization#torch.quantization.HistogramObserver", "type": "torch.quantization", "text": ["The module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.", "The scale and zero point are computed as follows:", "The histogram is computed continuously, and the ranges per bin change with every new tensor observed.", "The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.", "MinMaxObserver"]}, {"name": "torch.quantization.MinMaxObserver", "path": "torch.quantization#torch.quantization.MinMaxObserver", "type": "torch.quantization", "text": ["Observer module for computing the quantization parameters based on the running min and max values.", "This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "Given running min/max as xminx_\\text{min}  and xmaxx_\\text{max} , scale ss  and zero point zz  are computed as:", "The running minimum/maximum xmin/maxx_\\text{min/max}  is computed as:", "where XX  is the observed tensor.", "The scale ss  and zero point zz  are then computed as:", "where QminQ_\\text{min}  and QmaxQ_\\text{max}  are the minimum and maximum of the quantized data type.", "Warning", "Only works with torch.per_tensor_symmetric quantization scheme", "Warning", "dtype can only take torch.qint8 or torch.quint8.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0."]}, {"name": "torch.quantization.MovingAverageMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAverageMinMaxObserver", "type": "torch.quantization", "text": ["Observer module for computing the quantization parameters based on the moving average of the min and max values.", "This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The moving average min/max is computed as follows", "where xmin/maxx_\\text{min/max}  is the running average min/max, XX  is is the incoming tensor, and cc  is the averaging_constant.", "The scale and zero point are then computed as in MinMaxObserver.", "Note", "Only works with torch.per_tensor_affine quantization scheme.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0."]}, {"name": "torch.quantization.MovingAveragePerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAveragePerChannelMinMaxObserver", "type": "torch.quantization", "text": ["Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0."]}, {"name": "torch.quantization.NoopObserver", "path": "torch.quantization#torch.quantization.NoopObserver", "type": "torch.quantization", "text": ["Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float().", "Primarily used for quantization to float16 which doesn\u2019t require determining ranges."]}, {"name": "torch.quantization.ObserverBase", "path": "torch.quantization#torch.quantization.ObserverBase", "type": "torch.quantization", "text": ["Base observer Module. Any observer implementation should derive from this class.", "Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.", "dtype \u2013 Quantized data type", "Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances.", "Example:"]}, {"name": "torch.quantization.ObserverBase.with_args()", "path": "torch.quantization#torch.quantization.ObserverBase.with_args", "type": "torch.quantization", "text": ["Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances.", "Example:"]}, {"name": "torch.quantization.PerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.PerChannelMinMaxObserver", "type": "torch.quantization", "text": ["Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0."]}, {"name": "torch.quantization.prepare()", "path": "torch.quantization#torch.quantization.prepare", "type": "torch.quantization", "text": ["Prepares a copy of the model for quantization calibration or quantization-aware training.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.", "The model will be attached with observer or fake quant modules, and qconfig will be propagated."]}, {"name": "torch.quantization.prepare_qat()", "path": "torch.quantization#torch.quantization.prepare_qat", "type": "torch.quantization", "text": ["Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute."]}, {"name": "torch.quantization.propagate_qconfig_()", "path": "torch.quantization#torch.quantization.propagate_qconfig_", "type": "torch.quantization", "text": ["Propagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module", "None, module is modified inplace with qconfig attached"]}, {"name": "torch.quantization.QConfig", "path": "torch.quantization#torch.quantization.QConfig", "type": "torch.quantization", "text": ["Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.", "Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.", "Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):", "my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8), weight=default_observer.with_args(dtype=torch.qint8))"]}, {"name": "torch.quantization.QConfigDynamic", "path": "torch.quantization#torch.quantization.QConfigDynamic", "type": "torch.quantization", "text": ["Describes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights.", "It\u2019s like QConfig, but for dynamic quantization.", "Note that QConfigDynamic needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers.", "Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):", "my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))"]}, {"name": "torch.quantization.quantize()", "path": "torch.quantization#torch.quantization.quantize", "type": "torch.quantization", "text": ["Quantize the input float model with post training static quantization.", "First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.", "Quantized model."]}, {"name": "torch.quantization.quantize_dynamic()", "path": "torch.quantization#torch.quantization.quantize_dynamic", "type": "torch.quantization", "text": ["Converts a float model to dynamic (i.e. weights-only) quantized model.", "Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.", "For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.", "Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.", "qconfig_spec \u2013 ", "Either:"]}, {"name": "torch.quantization.quantize_qat()", "path": "torch.quantization#torch.quantization.quantize_qat", "type": "torch.quantization", "text": ["Do quantization aware training and output a quantized model", "Quantized model."]}, {"name": "torch.quantization.QuantStub", "path": "torch.quantization#torch.quantization.QuantStub", "type": "torch.quantization", "text": ["Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.", "qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules"]}, {"name": "torch.quantization.QuantWrapper", "path": "torch.quantization#torch.quantization.QuantWrapper", "type": "torch.quantization", "text": ["A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.", "This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub."]}, {"name": "torch.quantization.RecordingObserver", "path": "torch.quantization#torch.quantization.RecordingObserver", "type": "torch.quantization", "text": ["The module is mainly for debug and records the tensor values during runtime."]}, {"name": "torch.quantization.swap_module()", "path": "torch.quantization#torch.quantization.swap_module", "type": "torch.quantization", "text": ["Swaps the module if it has a quantized counterpart and it has an observer attached.", "The corresponding quantized module of mod"]}, {"name": "torch.quantize_per_channel()", "path": "generated/torch.quantize_per_channel#torch.quantize_per_channel", "type": "torch", "text": ["Converts a float tensor to a per-channel quantized tensor with given scales and zero points.", "A newly quantized tensor", "Tensor", "Example:"]}, {"name": "torch.quantize_per_tensor()", "path": "generated/torch.quantize_per_tensor#torch.quantize_per_tensor", "type": "torch", "text": ["Converts a float tensor to a quantized tensor with given scale and zero point.", "A newly quantized tensor", "Tensor", "Example:"]}, {"name": "torch.quasirandom.SobolEngine", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine", "type": "torch", "text": ["The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences. Sobol sequences are an example of low discrepancy quasi-random sequences.", "This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 21201. It uses direction numbers from https://web.maths.unsw.edu.au/~fkuo/sobol/ obtained using the search criterion D(6) up to the dimension 21201. This is the recommended choice by the authors.", "Examples:", "Function to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension) .", "Function to draw a sequence of 2**m points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (2\u2217\u2217m,dimension)(2**m, dimension) .", "Function to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.", "n (Int) \u2013 The number of steps to fast-forward by.", "Function to reset the SobolEngine to base state."]}, {"name": "torch.quasirandom.SobolEngine.draw()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw", "type": "torch", "text": ["Function to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension) ."]}, {"name": "torch.quasirandom.SobolEngine.draw_base2()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw_base2", "type": "torch", "text": ["Function to draw a sequence of 2**m points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (2\u2217\u2217m,dimension)(2**m, dimension) ."]}, {"name": "torch.quasirandom.SobolEngine.fast_forward()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.fast_forward", "type": "torch", "text": ["Function to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.", "n (Int) \u2013 The number of steps to fast-forward by."]}, {"name": "torch.quasirandom.SobolEngine.reset()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.reset", "type": "torch", "text": ["Function to reset the SobolEngine to base state."]}, {"name": "torch.rad2deg()", "path": "generated/torch.rad2deg#torch.rad2deg", "type": "torch", "text": ["Returns a new tensor with each of the elements of input converted from angles in radians to degrees.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.rand()", "path": "generated/torch.rand#torch.rand", "type": "torch", "text": ["Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) ", "The shape of the tensor is defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.randint()", "path": "generated/torch.randint#torch.randint", "type": "torch", "text": ["Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).", "The shape of the tensor is defined by the variable argument size.", "Note", "With the global dtype default (torch.float32), this function returns a tensor with dtype torch.int64.", "Example:"]}, {"name": "torch.randint_like()", "path": "generated/torch.randint_like#torch.randint_like", "type": "torch", "text": ["Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive)."]}, {"name": "torch.randn()", "path": "generated/torch.randn#torch.randn", "type": "torch", "text": ["Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).", "The shape of the tensor is defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.randn_like()", "path": "generated/torch.randn_like#torch.randn_like", "type": "torch", "text": ["Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like(input) is equivalent to torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "input (Tensor) \u2013 the size of input will determine size of the output tensor."]}, {"name": "torch.random", "path": "random", "type": "torch.random", "text": ["Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.", "Returns the random number generator state as a torch.ByteTensor.", "Returns the initial seed for generating random numbers as a Python long.", "Sets the seed for generating random numbers. Returns a torch.Generator object.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed.", "Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG.", "Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.random.fork_rng()", "path": "random#torch.random.fork_rng", "type": "torch.random", "text": ["Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in."]}, {"name": "torch.random.get_rng_state()", "path": "random#torch.random.get_rng_state", "type": "torch.random", "text": ["Returns the random number generator state as a torch.ByteTensor."]}, {"name": "torch.random.initial_seed()", "path": "random#torch.random.initial_seed", "type": "torch.random", "text": ["Returns the initial seed for generating random numbers as a Python long."]}, {"name": "torch.random.manual_seed()", "path": "random#torch.random.manual_seed", "type": "torch.random", "text": ["Sets the seed for generating random numbers. Returns a torch.Generator object.", "seed (int) \u2013 The desired seed. Value must be within the inclusive range [-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula 0xffff_ffff_ffff_ffff + seed."]}, {"name": "torch.random.seed()", "path": "random#torch.random.seed", "type": "torch.random", "text": ["Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG."]}, {"name": "torch.random.set_rng_state()", "path": "random#torch.random.set_rng_state", "type": "torch.random", "text": ["Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.randperm()", "path": "generated/torch.randperm#torch.randperm", "type": "torch", "text": ["Returns a random permutation of integers from 0 to n - 1.", "n (int) \u2013 the upper bound (exclusive)", "Example:"]}, {"name": "torch.rand_like()", "path": "generated/torch.rand_like#torch.rand_like", "type": "torch", "text": ["Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) . torch.rand_like(input) is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "input (Tensor) \u2013 the size of input will determine size of the output tensor."]}, {"name": "torch.range()", "path": "generated/torch.range#torch.range", "type": "torch", "text": ["Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1  with values from start to end with step step. Step is the gap between two values in the tensor.", "Warning", "This function is deprecated and will be removed in a future release because its behavior is inconsistent with Python\u2019s range builtin. Instead, use torch.arange(), which produces values in [start, end).", "Example:"]}, {"name": "torch.ravel()", "path": "generated/torch.ravel#torch.ravel", "type": "torch", "text": ["Return a contiguous flattened tensor. A copy is made only if needed.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.real()", "path": "generated/torch.real#torch.real", "type": "torch", "text": ["Returns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "real() is only supported for tensors with complex dtypes.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.reciprocal()", "path": "generated/torch.reciprocal#torch.reciprocal", "type": "torch", "text": ["Returns a new tensor with the reciprocal of the elements of input", "Note", "Unlike NumPy\u2019s reciprocal, torch.reciprocal supports integral inputs. Integral inputs to reciprocal are automatically promoted to the default scalar type.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.remainder()", "path": "generated/torch.remainder#torch.remainder", "type": "torch", "text": ["Computes the element-wise remainder of division.", "The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the divisor other.", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "Note", "Complex inputs are not supported. In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers. See torch.fmod() for how division by zero is handled.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "See also", "torch.fmod(), which computes the element-wise remainder of division equivalently to the C library function fmod()."]}, {"name": "torch.renorm()", "path": "generated/torch.renorm#torch.renorm", "type": "torch", "text": ["Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm", "Note", "If the norm of a row is lower than maxnorm, the row is unchanged", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.repeat_interleave()", "path": "generated/torch.repeat_interleave#torch.repeat_interleave", "type": "torch", "text": ["Repeat elements of a tensor.", "Warning", "This is different from torch.Tensor.repeat() but similar to numpy.repeat.", "Repeated tensor which has the same shape as input, except along the given axis.", "Tensor", "Example:", "If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be tensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times, 1 appears n2 times, 2 appears n3 times, etc."]}, {"name": "torch.reshape()", "path": "generated/torch.reshape#torch.reshape", "type": "torch", "text": ["Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.", "See torch.Tensor.view() on when it is possible to return a view.", "A single dimension may be -1, in which case it\u2019s inferred from the remaining dimensions and the number of elements in input.", "Example:"]}, {"name": "torch.result_type()", "path": "generated/torch.result_type#torch.result_type", "type": "torch", "text": ["Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors. See type promotion documentation for more information on the type promotion logic.", "Example:"]}, {"name": "torch.roll()", "path": "generated/torch.roll#torch.roll", "type": "torch", "text": ["Roll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.", "Example:"]}, {"name": "torch.rot90()", "path": "generated/torch.rot90#torch.rot90", "type": "torch", "text": ["Rotate a n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k > 0, and from the second towards the first for k < 0.", "Example:"]}, {"name": "torch.round()", "path": "generated/torch.round#torch.round", "type": "torch", "text": ["Returns a new tensor with each of the elements of input rounded to the closest integer.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.row_stack()", "path": "generated/torch.row_stack#torch.row_stack", "type": "torch", "text": ["Alias of torch.vstack()."]}, {"name": "torch.rsqrt()", "path": "generated/torch.rsqrt#torch.rsqrt", "type": "torch", "text": ["Returns a new tensor with the reciprocal of the square-root of each of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.save()", "path": "generated/torch.save#torch.save", "type": "torch", "text": ["Saves an object to a disk file.", "See also: saving-loading-tensors", "Note", "A common PyTorch convention is to save tensors using .pt file extension.", "Note", "PyTorch preserves storage sharing across serialization. See preserve-storage-sharing for more details.", "Note", "The 1.6 release of PyTorch switched torch.save to use a new zipfile-based file format. torch.load still retains the ability to load files in the old format. If for any reason you want torch.save to use the old format, pass the kwarg _use_new_zipfile_serialization=False."]}, {"name": "torch.scatter()", "path": "generated/torch.scatter#torch.scatter", "type": "torch", "text": ["Out-of-place version of torch.Tensor.scatter_()"]}, {"name": "torch.scatter_add()", "path": "generated/torch.scatter_add#torch.scatter_add", "type": "torch", "text": ["Out-of-place version of torch.Tensor.scatter_add_()"]}, {"name": "torch.searchsorted()", "path": "generated/torch.searchsorted#torch.searchsorted", "type": "torch", "text": ["Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, the order of the corresponding innermost dimension within sorted_sequence would be preserved. Return a new tensor with the same size as values. If right is False (default), then the left boundary of sorted_sequence is closed. More formally, the returned index satisfies the following rules:", "sorted_sequence", "right", "returned index satisfies", "1-D", "False", "sorted_sequence[i-1] < values[m][n]...[l][x] <= sorted_sequence[i]", "1-D", "True", "sorted_sequence[i-1] <= values[m][n]...[l][x] < sorted_sequence[i]", "N-D", "False", "sorted_sequence[m][n]...[l][i-1] < values[m][n]...[l][x] <= sorted_sequence[m][n]...[l][i]", "N-D", "True", "sorted_sequence[m][n]...[l][i-1] <= values[m][n]...[l][x] < sorted_sequence[m][n]...[l][i]", "Note", "If your use case is always 1-D sorted sequence, torch.bucketize() is preferred, because it has fewer dimension checks resulting in slightly better performance.", "Example:"]}, {"name": "torch.seed()", "path": "generated/torch.seed#torch.seed", "type": "torch", "text": ["Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG."]}, {"name": "torch.set_default_dtype()", "path": "generated/torch.set_default_dtype#torch.set_default_dtype", "type": "torch", "text": ["Sets the default floating point dtype to d. This dtype is:", "The default floating point dtype is initially torch.float32.", "d (torch.dtype) \u2013 the floating point dtype to make the default"]}, {"name": "torch.set_default_tensor_type()", "path": "generated/torch.set_default_tensor_type#torch.set_default_tensor_type", "type": "torch", "text": ["Sets the default torch.Tensor type to floating point tensor type t. This type will also be used as default floating point type for type inference in torch.tensor().", "The default floating point tensor type is initially torch.FloatTensor.", "t (type or string) \u2013 the floating point tensor type or its name", "Example:"]}, {"name": "torch.set_flush_denormal()", "path": "generated/torch.set_flush_denormal#torch.set_flush_denormal", "type": "torch", "text": ["Disables denormal floating numbers on CPU.", "Returns True if your system supports flushing denormal numbers and it successfully configures flush denormal mode. set_flush_denormal() is only supported on x86 architectures supporting SSE3.", "mode (bool) \u2013 Controls whether to enable flush denormal mode or not", "Example:"]}, {"name": "torch.set_grad_enabled", "path": "generated/torch.set_grad_enabled#torch.set_grad_enabled", "type": "torch", "text": ["Context-manager that sets gradient calculation to on or off.", "set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function.", "This context manager is thread local; it will not affect computation in other threads.", "mode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.", "Example:"]}, {"name": "torch.set_num_interop_threads()", "path": "generated/torch.set_num_interop_threads#torch.set_num_interop_threads", "type": "torch", "text": ["Sets the number of threads used for interop parallelism (e.g. in JIT interpreter) on CPU.", "Warning", "Can only be called once and before any inter-op parallel work is started (e.g. JIT execution)."]}, {"name": "torch.set_num_threads()", "path": "generated/torch.set_num_threads#torch.set_num_threads", "type": "torch", "text": ["Sets the number of threads used for intraop parallelism on CPU.", "Warning", "To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code."]}, {"name": "torch.set_printoptions()", "path": "generated/torch.set_printoptions#torch.set_printoptions", "type": "torch", "text": ["Set options for printing. Items shamelessly taken from NumPy"]}, {"name": "torch.set_rng_state()", "path": "generated/torch.set_rng_state#torch.set_rng_state", "type": "torch", "text": ["Sets the random number generator state.", "new_state (torch.ByteTensor) \u2013 The desired state"]}, {"name": "torch.sgn()", "path": "generated/torch.sgn#torch.sgn", "type": "torch", "text": ["For complex tensors, this function returns a new tensor whose elemants have the same angle as that of the elements of input and absolute value 1. For a non-complex tensor, this function returns the signs of the elements of input (see torch.sign()).", "outi=0\\text{out}_{i} = 0 , if \u2223inputi\u2223==0|{\\text{{input}}_i}| == 0  outi=inputi\u2223inputi\u2223\\text{out}_{i} = \\frac{{\\text{{input}}_i}}{|{\\text{{input}}_i}|} , otherwise", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sigmoid()", "path": "generated/torch.sigmoid#torch.sigmoid", "type": "torch", "text": ["Returns a new tensor with the sigmoid of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sign()", "path": "generated/torch.sign#torch.sign", "type": "torch", "text": ["Returns a new tensor with the signs of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.signbit()", "path": "generated/torch.signbit#torch.signbit", "type": "torch", "text": ["Tests if each element of input has its sign bit set (is less than zero) or not.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sin()", "path": "generated/torch.sin#torch.sin", "type": "torch", "text": ["Returns a new tensor with the sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sinc()", "path": "generated/torch.sinc#torch.sinc", "type": "torch", "text": ["Computes the normalized sinc of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sinh()", "path": "generated/torch.sinh#torch.sinh", "type": "torch", "text": ["Returns a new tensor with the hyperbolic sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "When input is on the CPU, the implementation of torch.sinh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details."]}, {"name": "torch.slogdet()", "path": "generated/torch.slogdet#torch.slogdet", "type": "torch", "text": ["Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.", "Note", "torch.slogdet() is deprecated. Please use torch.linalg.slogdet() instead.", "Note", "If input has zero determinant, this returns (0, -inf).", "Note", "Backward through slogdet() internally uses SVD results when input is not invertible. In this case, double backward through slogdet() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.", "input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.", "A namedtuple (sign, logabsdet) containing the sign of the determinant, and the log value of the absolute determinant.", "Example:"]}, {"name": "torch.smm()", "path": "sparse#torch.smm", "type": "torch.sparse", "text": ["Performs a matrix multiplication of the sparse matrix input with the dense matrix mat."]}, {"name": "torch.solve()", "path": "generated/torch.solve#torch.solve", "type": "torch", "text": ["This function returns the solution to the system of linear equations represented by AX=BAX = B  and the LU factorization of A, in order as a namedtuple solution, LU.", "LU contains L and U factors for LU factorization of A.", "torch.solve(B, A) can take in 2D inputs B, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs solution, LU.", "Supports real-valued and complex-valued inputs.", "Note", "Irrespective of the original strides, the returned matrices solution and LU will be transposed, i.e. with strides like B.contiguous().transpose(-1, -2).stride() and A.contiguous().transpose(-1, -2).stride() respectively.", "out ((Tensor, Tensor), optional) \u2013 optional output tuple.", "Example:"]}, {"name": "torch.sort()", "path": "generated/torch.sort#torch.sort", "type": "torch", "text": ["Sorts the elements of the input tensor along a given dimension in ascending order by value.", "If dim is not given, the last dimension of the input is chosen.", "If descending is True then the elements are sorted in descending order by value.", "A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.", "out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers", "Example:"]}, {"name": "torch.sparse", "path": "sparse", "type": "torch.sparse", "text": ["PyTorch provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type. By default, array elements are stored contiguously in memory leading to efficient implementations of various array processing algorithms that relay on the fast access to array elements. However, there exists an important class of multi-dimensional arrays, so-called sparse arrays, where the contiguous memory storage of array elements turns out to be suboptimal. Sparse arrays have a property of having a vast portion of elements being equal to zero which means that a lot of memory as well as processor resources can be spared if only the non-zero elements are stored or/and processed. Various sparse storage formats (such as COO, CSR/CSC, LIL, etc.) have been developed that are optimized for a particular structure of non-zero elements in sparse arrays as well as for specific operations on the arrays.", "Note", "When talking about storing only non-zero elements of a sparse array, the usage of adjective \u201cnon-zero\u201d is not strict: one is allowed to store also zeros in the sparse array data structure. Hence, in the following, we use \u201cspecified elements\u201d for those array elements that are actually stored. In addition, the unspecified elements are typically assumed to have zero value, but not only, hence we use the term \u201cfill value\u201d to denote such elements.", "Note", "Using a sparse storage format for storing sparse arrays can be advantageous only when the size and sparsity levels of arrays are high. Otherwise, for small-sized or low-sparsity arrays using the contiguous memory storage format is likely the most efficient approach.", "Warning", "The PyTorch API of sparse tensors is in beta and may change in the near future.", "Currently, PyTorch implements the so-called Coordinate format, or COO format, as the default sparse storage format for storing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular,", "where ndim is the dimensionality of the tensor and nse is the number of specified elements.", "Note", "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant overhead from storing other tensor data).", "The memory consumption of a strided tensor is at least product(<tensor shape>) * <size of element type in bytes>.", "For example, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least (2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor layout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using the default strided tensor layout. Notice the 200 fold memory saving from using the COO storage format.", "A sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function torch.sparse_coo_tensor().", "Suppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write:", "Note that the input i is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:", "An empty sparse COO tensor can be constructed by specifying its size only:", "Pytorch implements an extension of sparse tensors with scalar values to sparse tensors with (contiguous) tensor values. Such tensors are called hybrid tensors.", "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional tensor so that we have:", "Note", "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid sparse tensor, where M and K are the numbers of sparse and dense dimensions, respectively, such that M + K == N holds.", "Suppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write", "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following invariants:", "Note", "Dense dimensions always follow sparse dimensions, that is, mixing of dense and sparse dimensions is not supported.", "PyTorch sparse COO tensor format permits uncoalesced sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, 3 and 4, for the same index 1, that leads to an 1-D uncoalesced tensor:", "while the coalescing process will accumulate the multi-valued elements into a single value using summation:", "In general, the output of torch.Tensor.coalesce() method is a sparse tensor with the following properties:", "Note", "For the most part, you shouldn\u2019t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a coalesced or uncoalesced sparse tensor.", "However, some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors.", "For instance, addition of sparse COO tensors is implemented by simply concatenating the indices and values tensors:", "If you repeatedly perform an operation that can produce duplicate entries (e.g., torch.Tensor.add()), you should occasionally coalesce your sparse tensors to prevent them from growing too large.", "On the other hand, the lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products.", "Let\u2019s consider the following example:", "As mentioned above, a sparse COO tensor is a torch.Tensor instance and to distinguish it from the Tensor instances that use some other layout, on can use torch.Tensor.is_sparse or torch.Tensor.layout properties:", "The number of sparse and dense dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim(), respectively. For instance:", "If s is a sparse COO tensor then its COO format data can be acquired using methods torch.Tensor.indices() and torch.Tensor.values().", "Note", "Currently, one can acquire the COO format data only when the tensor instance is coalesced:", "For acquiring the COO format data of an uncoalesced tensor, use torch.Tensor._values() and torch.Tensor._indices():", "Constructing a new sparse COO tensor results a tensor that is not coalesced:", "but one can construct a coalesced copy of a sparse COO tensor using the torch.Tensor.coalesce() method:", "When working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on an uncoalesced sparse tensor could be implemented by multiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not hold in general.", "Slicing (with positive step) of a sparse COO tensor is supported only for dense dimensions. Indexing is supported for both sparse and dense dimensions:", "In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, torch.sparse.softmax() computes the softmax with the assumption that the fill value is negative infinity.", "The following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here T[layout] denotes a tensor with a given layout. Similarly, M[layout] denotes a matrix (2-D PyTorch tensor), and V[layout] denotes a vector (1-D PyTorch tensor). In addition, f denotes a scalar (float or 0-D PyTorch tensor), * is element-wise multiplication, and @ is matrix multiplication.", "PyTorch operation", "Sparse grad?", "Layout signature", "torch.mv()", "no", "M[sparse_coo] @ V[strided] -> V[strided]", "torch.matmul()", "no", "M[sparse_coo] @ M[strided] -> M[strided]", "torch.mm()", "no", "M[sparse_coo] @ M[strided] -> M[strided]", "torch.sparse.mm()", "yes", "M[sparse_coo] @ M[strided] -> M[strided]", "torch.smm()", "no", "M[sparse_coo] @ M[strided] -> M[sparse_coo]", "torch.hspmm()", "no", "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]", "torch.bmm()", "no", "T[sparse_coo] @ T[strided] -> T[strided]", "torch.addmm()", "no", "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]", "torch.sparse.addmm()", "yes", "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]", "torch.sspaddmm()", "no", "f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]", "torch.lobpcg()", "no", "GENEIG(M[sparse_coo]) -> M[strided], M[strided]", "torch.pca_lowrank()", "yes", "PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]", "torch.svd_lowrank()", "yes", "SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]", "where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports backward with respect to sparse matrix argument. All PyTorch operations, except torch.smm(), support backward with respect to strided matrix arguments.", "Note", "Currently, PyTorch does not support matrix multiplication with the layout signature M[strided] @ M[sparse_coo]. However, applications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().", "The following methods are specific to sparse tensors:", "Is True if the Tensor uses sparse storage layout, False otherwise.", "Return the number of dense dimensions in a sparse tensor self.", "Warning", "Throws an error if self is not a sparse tensor.", "See also Tensor.sparse_dim() and hybrid tensors.", "Return the number of sparse dimensions in a sparse tensor self.", "Warning", "Throws an error if self is not a sparse tensor.", "See also Tensor.dense_dim() and hybrid tensors.", "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.", "Note", "The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.", "mask (Tensor) \u2013 a sparse tensor whose indices are used as a filter", "Example:", "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions.", "Note", "If the number of specified elements in self is zero, then size, sparse_dim, and dense_dim can be any size and positive integers such that len(size) == sparse_dim +\ndense_dim.", "If self specifies one or more elements, however, then each dimension in size must not be smaller than the corresponding dimension of self, sparse_dim must equal the number of sparse dimensions in self, and dense_dim must equal the number of dense dimensions in self.", "Warning", "Throws an error if self is not a sparse tensor.", "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions.", "Creates a strided copy of self.", "Warning", "Throws an error if self is a strided tensor.", "Example:", "Returns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.", "sparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor", "Example:", "Returns a coalesced copy of self if self is an uncoalesced tensor.", "Returns self if self is a coalesced tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "Returns True if self is a sparse COO tensor that is coalesced, False otherwise.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See coalesce() and uncoalesced tensors.", "Return the indices tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.values().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.", "Return the values tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.indices().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.", "The following torch.Tensor methods support sparse COO tensors:", "add() add_() addmm() addmm_() any() asin() asin_() arcsin() arcsin_() bmm() clone() deg2rad() deg2rad_() detach() detach_() dim() div() div_() floor_divide() floor_divide_() get_device() index_select() isnan() log1p() log1p_() mm() mul() mul_() mv() narrow_copy() neg() neg_() negative() negative_() numel() rad2deg() rad2deg_() resize_as_() size() pow() sqrt() square() smm() sspaddmm() sub() sub_() t() t_() transpose() transpose_() zero_()", "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.", "Note", "This function returns an uncoalesced tensor.", "Example:", "Returns the sum of each row of the sparse tensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a dense tensor instead of a sparse tensor.", "All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input.", "During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.", "Example:", "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. mat1 need to have sparse_dim = 2. Note that the gradients of mat1 is a coalesced sparse tensor.", "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2. Similar to torch.mm(), If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, out will be a (n\u00d7p)(n \\times p)  tensor. mat1 need to have sparse_dim = 2. This function also supports backward for both matrices. Note that the gradients of mat1 is a coalesced sparse tensor.", "The format of the output tensor of this function follows: - sparse x sparse -> sparse - sparse x dense -> dense", "Example:", "Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.", "Note: This function is equivalent to torch.addmm(), except input and mat1 are sparse.", "Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2. The result is a (1 + 1)-dimensional hybrid COO matrix.", "{out} \u2013 ", "Performs a matrix multiplication of the sparse matrix input with the dense matrix mat.", "Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)} ", "where i,ji, j  run over sparse tensor indices and unspecified entries are ignores. This is equivalent to defining unspecified entries as negative infinity so that exp(xk)=0exp(x_k) = 0  when the entry with index kk  has not specified.", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1.", "Applies a softmax function followed by logarithm.", "See softmax for more details.", "The following torch functions support sparse COO tensors:", "cat() dstack() empty() empty_like() hstack() index_select() is_complex() is_floating_point() is_nonzero() is_same_size() is_signed() is_tensor() lobpcg() mm() native_norm() pca_lowrank() select() stack() svd_lowrank() unsqueeze() vstack() zeros() zeros_like()"]}, {"name": "torch.sparse.addmm()", "path": "sparse#torch.sparse.addmm", "type": "torch.sparse", "text": ["This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. mat1 need to have sparse_dim = 2. Note that the gradients of mat1 is a coalesced sparse tensor."]}, {"name": "torch.sparse.log_softmax()", "path": "sparse#torch.sparse.log_softmax", "type": "torch.sparse", "text": ["Applies a softmax function followed by logarithm.", "See softmax for more details."]}, {"name": "torch.sparse.mm()", "path": "sparse#torch.sparse.mm", "type": "torch.sparse", "text": ["Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2. Similar to torch.mm(), If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, out will be a (n\u00d7p)(n \\times p)  tensor. mat1 need to have sparse_dim = 2. This function also supports backward for both matrices. Note that the gradients of mat1 is a coalesced sparse tensor.", "The format of the output tensor of this function follows: - sparse x sparse -> sparse - sparse x dense -> dense", "Example:"]}, {"name": "torch.sparse.softmax()", "path": "sparse#torch.sparse.softmax", "type": "torch.sparse", "text": ["Applies a softmax function.", "Softmax is defined as:", "Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)} ", "where i,ji, j  run over sparse tensor indices and unspecified entries are ignores. This is equivalent to defining unspecified entries as negative infinity so that exp(xk)=0exp(x_k) = 0  when the entry with index kk  has not specified.", "It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1."]}, {"name": "torch.sparse.sum()", "path": "sparse#torch.sparse.sum", "type": "torch.sparse", "text": ["Returns the sum of each row of the sparse tensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a dense tensor instead of a sparse tensor.", "All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input.", "During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.", "Example:"]}, {"name": "torch.sparse_coo_tensor()", "path": "generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor", "type": "torch", "text": ["Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.", "Note", "This function returns an uncoalesced tensor.", "Example:"]}, {"name": "torch.split()", "path": "generated/torch.split#torch.split", "type": "torch", "text": ["Splits the tensor into chunks. Each chunk is a view of the original tensor.", "If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size.", "If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections."]}, {"name": "torch.sqrt()", "path": "generated/torch.sqrt#torch.sqrt", "type": "torch", "text": ["Returns a new tensor with the square-root of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.square()", "path": "generated/torch.square#torch.square", "type": "torch", "text": ["Returns a new tensor with the square of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.squeeze()", "path": "generated/torch.squeeze#torch.squeeze", "type": "torch", "text": ["Returns a tensor with all the dimensions of input of size 1 removed.", "For example, if input is of shape: (A\u00d71\u00d7B\u00d7C\u00d71\u00d7D)(A \\times 1 \\times B \\times C \\times 1 \\times D)  then the out tensor will be of shape: (A\u00d7B\u00d7C\u00d7D)(A \\times B \\times C \\times D) .", "When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B) , squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A\u00d7B)(A \\times B) .", "Note", "The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.", "Warning", "If the tensor has a batch dimension of size 1, then squeeze(input) will also remove the batch dimension, which can lead to unexpected errors.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.sspaddmm()", "path": "sparse#torch.sspaddmm", "type": "torch.sparse", "text": ["Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.", "Note: This function is equivalent to torch.addmm(), except input and mat1 are sparse."]}, {"name": "torch.stack()", "path": "generated/torch.stack#torch.stack", "type": "torch", "text": ["Concatenates a sequence of tensors along a new dimension.", "All tensors need to be of the same size.", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.std()", "path": "generated/torch.std#torch.std", "type": "torch", "text": ["Returns the standard-deviation of all elements in the input tensor.", "If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "Example:", "Returns the standard-deviation of each row of the input tensor in the dimension dim. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.std_mean()", "path": "generated/torch.std_mean#torch.std_mean", "type": "torch", "text": ["Returns the standard-deviation and mean of all elements in the input tensor.", "If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "Example:", "Returns the standard-deviation and mean of each row of the input tensor in the dimension dim. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "Example:"]}, {"name": "torch.stft()", "path": "generated/torch.stft#torch.stft", "type": "torch", "text": ["Short-time Fourier transform (STFT).", "Warning", "From version 1.8.0, return_complex must always be given explicitly for real inputs and return_complex=False has been deprecated. Strongly prefer return_complex=True as in a future pytorch release, this function will only return complex tensors.", "Note that torch.view_as_real() can be used to recover a real tensor with an extra last dimension for real and imaginary components.", "The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time. The interface of this function is modeled after the librosa stft function.", "Ignoring the optional batch dimension, this method computes the following expression:", "where mm  is the index of the sliding window, and \u03c9\\omega  is the frequency that 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft} . When onesided is the default value True,", "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)  if return_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N \\times T \\times 2) . Where \u2217*  is the optional batch size of input, NN  is the number of frequencies where STFT is applied and TT  is the total number of frames used.", "Warning", "This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.", "A tensor containing the STFT result with shape described above", "Tensor"]}, {"name": "torch.Storage", "path": "storage", "type": "torch.Storage", "text": ["A torch.Storage is a contiguous, one-dimensional array of a single data type.", "Every torch.Tensor has a corresponding storage of the same data type.", "Casts this storage to bfloat16 type", "Casts this storage to bool type", "Casts this storage to byte type", "Casts this storage to char type", "Returns a copy of this storage", "Casts this storage to complex double type", "Casts this storage to complex float type", "Returns a CPU copy of this storage if it\u2019s not already on the CPU", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "Casts this storage to double type", "Casts this storage to float type", "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file.", "size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.", "Casts this storage to half type", "Casts this storage to int type", "Casts this storage to long type", "Copies the storage to pinned memory, if it\u2019s not already pinned.", "Moves the storage to shared memory.", "This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.", "Returns: self", "Casts this storage to short type", "Returns a list containing the elements of this storage", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.sub()", "path": "generated/torch.sub#torch.sub", "type": "torch", "text": ["Subtracts other, scaled by alpha, from input.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.", "Example:"]}, {"name": "torch.subtract()", "path": "generated/torch.subtract#torch.subtract", "type": "torch", "text": ["Alias for torch.sub()."]}, {"name": "torch.sum()", "path": "generated/torch.sum#torch.sum", "type": "torch", "text": ["Returns the sum of all elements in the input tensor.", "input (Tensor) \u2013 the input tensor.", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:", "Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.", "Example:"]}, {"name": "torch.svd()", "path": "generated/torch.svd#torch.svd", "type": "torch", "text": ["Computes the singular value decomposition of either a matrix or batch of matrices input. The singular value decomposition is represented as a namedtuple (U,S,V), such that input = U diag(S) V\u1d34, where V\u1d34 is the transpose of V for the real-valued inputs, or the conjugate transpose of V for the complex-valued inputs. If input is a batch of tensors, then U, S, and V are also batched with the same batch dimensions as input.", "If some is True (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n, m) orthonormal columns.", "If compute_uv is False, the returned U and V will be zero-filled matrices of shape (m \u00d7 m) and (n \u00d7 n) respectively, and the same device as input. The some argument has no effect when compute_uv is False.", "Supports input of float, double, cfloat and cdouble data types. The dtypes of U and V are the same as input\u2019s. S will always be real-valued, even if input is complex.", "Warning", "torch.svd() is deprecated. Please use torch.linalg.svd() instead, which is similar to NumPy\u2019s numpy.linalg.svd.", "Note", "Differences with torch.linalg.svd():", "Note", "The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.", "Note", "The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the cuSOLVER routines gesvdj and gesvdjBatched on CUDA 10.1.243 and later, and uses the MAGMA routine gesdd on earlier versions of CUDA.", "Note", "The returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride().", "Note", "Gradients computed using U and V may be unstable if input is not full rank or has non-unique singular values.", "Note", "When some = False, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.", "Note", "The S tensor can only be used to compute gradients if compute_uv is True.", "Note", "With the complex-valued input the backward operation works correctly only for gauge invariant loss functions. Please look at Gauge problem in AD for more details.", "Note", "Since U and V of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor ei\u03d5e^{i \\phi}  while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different U and V tensors.", "out (tuple, optional) \u2013 the output tuple of tensors", "Example:"]}, {"name": "torch.svd_lowrank()", "path": "generated/torch.svd_lowrank#torch.svd_lowrank", "type": "torch", "text": ["Return the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix AA  such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T . In case MM  is given, then SVD is computed for the matrix A\u2212MA - M .", "Note", "The implementation is based on the Algorithm 5.1 from Halko et al, 2009.", "Note", "To obtain repeatable results, reset the seed for the pseudorandom number generator", "Note", "The input is assumed to be a low-rank matrix.", "Note", "In general, use the full-rank SVD implementation torch.svd for dense matrices due to its 10-fold higher performance characteristics. The low-rank SVD will be useful for huge sparse matrices that torch.svd cannot handle.", "A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n) ", "q (int, optional): a slightly overestimated rank of A.", "conduct; niter must be a nonnegative integer, and defaults to 2", "(\u2217,1,n)(*, 1, n) ."]}, {"name": "torch.swapaxes()", "path": "generated/torch.swapaxes#torch.swapaxes", "type": "torch", "text": ["Alias for torch.transpose().", "This function is equivalent to NumPy\u2019s swapaxes function.", "Examples:"]}, {"name": "torch.swapdims()", "path": "generated/torch.swapdims#torch.swapdims", "type": "torch", "text": ["Alias for torch.transpose().", "This function is equivalent to NumPy\u2019s swapaxes function.", "Examples:"]}, {"name": "torch.symeig()", "path": "generated/torch.symeig#torch.symeig", "type": "torch", "text": ["This function returns eigenvalues and eigenvectors of a real symmetric matrix input or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).", "This function calculates all eigenvalues (and vectors) of input such that input=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^T .", "The boolean argument eigenvectors defines computation of both eigenvectors and eigenvalues or eigenvalues only.", "If it is False, only eigenvalues are computed. If it is True, both eigenvalues and eigenvectors are computed.", "Since the input matrix input is supposed to be symmetric, only the upper triangular portion is used by default.", "If upper is False, then lower triangular portion is used.", "Note", "The eigenvalues are returned in ascending order. If input is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.", "Note", "Irrespective of the original strides, the returned matrix V will be transposed, i.e. with strides V.contiguous().transpose(-1, -2).stride().", "Warning", "Extra care needs to be taken when backward through outputs. Such operation is only stable when all eigenvalues are distinct and becomes less stable the smaller min\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|  is.", "out (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)", "A namedtuple (eigenvalues, eigenvectors) containing", "(Tensor, Tensor)", "Examples:"]}, {"name": "torch.t()", "path": "generated/torch.t#torch.t", "type": "torch", "text": ["Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.", "0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.take()", "path": "generated/torch.take#torch.take", "type": "torch", "text": ["Returns a new tensor with the elements of input at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.", "Example:"]}, {"name": "torch.tan()", "path": "generated/torch.tan#torch.tan", "type": "torch", "text": ["Returns a new tensor with the tangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.tanh()", "path": "generated/torch.tanh#torch.tanh", "type": "torch", "text": ["Returns a new tensor with the hyperbolic tangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.Tensor", "path": "tensors", "type": "torch.Tensor", "text": ["A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.", "Torch defines 10 tensor types with CPU and GPU variants which are as follows:", "Data type", "dtype", "CPU tensor", "GPU tensor", "32-bit floating point", "torch.float32 or torch.float", "torch.FloatTensor", "torch.cuda.FloatTensor", "64-bit floating point", "torch.float64 or torch.double", "torch.DoubleTensor", "torch.cuda.DoubleTensor", "16-bit floating point 1", "torch.float16 or torch.half", "torch.HalfTensor", "torch.cuda.HalfTensor", "16-bit floating point 2", "torch.bfloat16", "torch.BFloat16Tensor", "torch.cuda.BFloat16Tensor", "32-bit complex", "torch.complex32", "64-bit complex", "torch.complex64", "128-bit complex", "torch.complex128 or torch.cdouble", "8-bit integer (unsigned)", "torch.uint8", "torch.ByteTensor", "torch.cuda.ByteTensor", "8-bit integer (signed)", "torch.int8", "torch.CharTensor", "torch.cuda.CharTensor", "16-bit integer (signed)", "torch.int16 or torch.short", "torch.ShortTensor", "torch.cuda.ShortTensor", "32-bit integer (signed)", "torch.int32 or torch.int", "torch.IntTensor", "torch.cuda.IntTensor", "64-bit integer (signed)", "torch.int64 or torch.long", "torch.LongTensor", "torch.cuda.LongTensor", "Boolean", "torch.bool", "torch.BoolTensor", "torch.cuda.BoolTensor", "Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.", "Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as float32", "torch.Tensor is an alias for the default tensor type (torch.FloatTensor).", "A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor:", "Warning", "torch.tensor() always copies data. If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach() to avoid a copy. If you have a numpy array and want to avoid a copy, use torch.as_tensor().", "A tensor of specific data type can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor creation op:", "The contents of a tensor can be accessed and modified using Python\u2019s indexing and slicing notation:", "Use torch.Tensor.item() to get a Python number from a tensor containing a single value:", "A tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation.", "Each tensor has an associated torch.Storage, which holds its data. The tensor class also provides multi-dimensional, strided view of a storage and defines numeric operations on it.", "Note", "For more information on tensor views, see Tensor Views.", "Note", "For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor, see Tensor Attributes.", "Note", "Methods which mutate a tensor are marked with an underscore suffix. For example, torch.FloatTensor.abs_() computes the absolute value in-place and returns the modified tensor, while torch.FloatTensor.abs() computes the result in a new tensor.", "Note", "To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using to() method on the tensor.", "Warning", "Current implementation of torch.Tensor introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.", "There are a few main ways to create a tensor, depending on your use case.", "Returns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Warning", "new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().", "Warning", "When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.", "Example:", "Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:", "Returns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:", "Returns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:", "Returns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:", "Is True if the Tensor is stored on the GPU, False otherwise.", "Is True if the Tensor is quantized, False otherwise.", "Is True if the Tensor is a meta tensor, False otherwise. Meta tensors are like normal tensors, but they carry no data.", "Is the torch.device where this Tensor is.", "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it.", "Alias for dim()", "Is this Tensor with its dimensions reversed.", "If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0).", "Returns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "real() is only supported for tensors with complex dtypes.", "Returns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "imag() is only supported for tensors with complex dtypes.", "See torch.abs()", "In-place version of abs()", "Alias for abs()", "In-place version of absolute() Alias for abs_()", "See torch.acos()", "In-place version of acos()", "See torch.arccos()", "In-place version of arccos()", "Add a scalar or tensor to self tensor. If both alpha and other are specified, each element of other is scaled by alpha before being used.", "When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor", "See torch.add()", "In-place version of add()", "See torch.addbmm()", "In-place version of addbmm()", "See torch.addcdiv()", "In-place version of addcdiv()", "See torch.addcmul()", "In-place version of addcmul()", "See torch.addmm()", "In-place version of addmm()", "See torch.sspaddmm()", "See torch.addmv()", "In-place version of addmv()", "See torch.addr()", "In-place version of addr()", "See torch.allclose()", "See torch.amax()", "See torch.amin()", "See torch.angle()", "Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.", "Note", "This function only works with CPU tensors and should not be used in code sections that require high performance.", "See torch.argmax()", "See torch.argmin()", "See torch.argsort()", "See torch.asin()", "In-place version of asin()", "See torch.arcsin()", "In-place version of arcsin()", "See torch.as_strided()", "See torch.atan()", "In-place version of atan()", "See torch.arctan()", "In-place version of arctan()", "See torch.atan2()", "In-place version of atan2()", "See torch.all()", "See torch.any()", "Computes the gradient of current tensor w.r.t. graph leaves.", "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "See torch.baddbmm()", "In-place version of baddbmm()", "Returns a result tensor where each result[i]\\texttt{result[i]}  is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . self must have floating point dtype, and the result will have the same dtype.", "See torch.bernoulli()", "Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}) . self can have integral dtype.", "p_tensor should be a tensor containing probabilities to be used for drawing the binary random number.", "The ith\\text{i}^{th}  element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]}) .", "self can have integral dtype, but p_tensor must have floating point dtype.", "See also bernoulli() and torch.bernoulli()", "self.bfloat16() is equivalent to self.to(torch.bfloat16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.bincount()", "See torch.bitwise_not()", "In-place version of bitwise_not()", "See torch.bitwise_and()", "In-place version of bitwise_and()", "See torch.bitwise_or()", "In-place version of bitwise_or()", "See torch.bitwise_xor()", "In-place version of bitwise_xor()", "See torch.bmm()", "self.bool() is equivalent to self.to(torch.bool). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "self.byte() is equivalent to self.to(torch.uint8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.broadcast_to().", "Fills the tensor with numbers drawn from the Cauchy distribution:", "See torch.ceil()", "In-place version of ceil()", "self.char() is equivalent to self.to(torch.int8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.cholesky()", "See torch.cholesky_inverse()", "See torch.cholesky_solve()", "See torch.chunk()", "See torch.clamp()", "In-place version of clamp()", "Alias for clamp().", "Alias for clamp_().", "See torch.clone()", "Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format.", "Copies the elements from src into self tensor and returns self.", "The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device.", "See torch.conj()", "See torch.copysign()", "In-place version of copysign()", "See torch.cos()", "In-place version of cos()", "See torch.cosh()", "In-place version of cosh()", "See torch.count_nonzero()", "See torch.acosh()", "In-place version of acosh()", "acosh() -> Tensor", "See torch.arccosh()", "acosh_() -> Tensor", "In-place version of arccosh()", "Returns a copy of this object in CPU memory.", "If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.cross()", "Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.", "See torch.logcumsumexp()", "See torch.cummax()", "See torch.cummin()", "See torch.cumprod()", "In-place version of cumprod()", "See torch.cumsum()", "In-place version of cumsum()", "Returns the address of the first element of self tensor.", "See torch.deg2rad()", "Given a quantized Tensor, dequantize it and return the dequantized float Tensor.", "See torch.det()", "Return the number of dense dimensions in a sparse tensor self.", "Warning", "Throws an error if self is not a sparse tensor.", "See also Tensor.sparse_dim() and hybrid tensors.", "Returns a new Tensor, detached from the current graph.", "The result will never require gradient.", "Note", "Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.", "Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.", "See torch.diag()", "See torch.diag_embed()", "See torch.diagflat()", "See torch.diagonal()", "Fill the main diagonal of a tensor that has at least 2-dimensions. When dims>2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.", "Example:", "See torch.fmax()", "See torch.fmin()", "See torch.diff()", "See torch.digamma()", "In-place version of digamma()", "Returns the number of dimensions of self tensor.", "See torch.dist()", "See torch.div()", "In-place version of div()", "See torch.divide()", "In-place version of divide()", "See torch.dot()", "self.double() is equivalent to self.to(torch.float64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.eig()", "Returns the size in bytes of an individual element.", "Example:", "See torch.eq()", "In-place version of eq()", "See torch.equal()", "See torch.erf()", "In-place version of erf()", "See torch.erfc()", "In-place version of erfc()", "See torch.erfinv()", "In-place version of erfinv()", "See torch.exp()", "In-place version of exp()", "See torch.expm1()", "In-place version of expm1()", "Returns a new view of the self tensor with singleton dimensions expanded to a larger size.", "Passing -1 as the size for a dimension means not changing the size of that dimension.", "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.", "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.", "*sizes (torch.Size or int...) \u2013 the desired expanded size", "Warning", "More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:", "Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()).", "Please see expand() for more information about expand.", "other (torch.Tensor) \u2013 The result tensor has the same size as other.", "Fills self tensor with elements drawn from the exponential distribution:", "See torch.fix().", "In-place version of fix()", "Fills self tensor with the specified value.", "see torch.flatten()", "See torch.flip()", "See torch.fliplr()", "See torch.flipud()", "self.float() is equivalent to self.to(torch.float32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.float_power()", "In-place version of float_power()", "See torch.floor()", "In-place version of floor()", "See torch.floor_divide()", "In-place version of floor_divide()", "See torch.fmod()", "In-place version of fmod()", "See torch.frac()", "In-place version of frac()", "See torch.gather()", "See torch.gcd()", "In-place version of gcd()", "See torch.ge().", "In-place version of ge().", "See torch.greater_equal().", "In-place version of greater_equal().", "Fills self tensor with elements drawn from the geometric distribution:", "See torch.geqrf()", "See torch.ger()", "For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.", "Example:", "See torch.gt().", "In-place version of gt().", "See torch.greater().", "In-place version of greater().", "self.half() is equivalent to self.to(torch.float16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.nn.functional.hardshrink()", "See torch.heaviside()", "See torch.histc()", "See torch.hypot()", "In-place version of hypot()", "See torch.i0()", "In-place version of i0()", "See torch.igamma()", "In-place version of igamma()", "See torch.igammac()", "In-place version of igammac()", "Accumulate the elements of tensor into the self tensor by adding to the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is added to the jth row of self.", "The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Example:", "Out-of-place version of torch.Tensor.index_add_(). tensor1 corresponds to self in torch.Tensor.index_add_().", "Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self.", "The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "Note", "If index contains duplicate entries, multiple elements from tensor will be copied to the same index of self. The result is nondeterministic since it depends on which copy occurs last.", "Example:", "Out-of-place version of torch.Tensor.index_copy_(). tensor1 corresponds to self in torch.Tensor.index_copy_().", "Fills the elements of the self tensor with value val by selecting the indices in the order given in index.", "Out-of-place version of torch.Tensor.index_fill_(). tensor1 corresponds to self in torch.Tensor.index_fill_().", "Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, values) is equivalent to tensor[indices] = values. Returns self.", "If accumulate is True, the elements in values are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.", "Out-place version of index_put_(). tensor1 corresponds to self in torch.Tensor.index_put_().", "See torch.index_select()", "Return the indices tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.values().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.", "See torch.inner().", "self.int() is equivalent to self.to(torch.int32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.", "See torch.inverse()", "See torch.isclose()", "See torch.isfinite()", "See torch.isinf()", "See torch.isposinf()", "See torch.isneginf()", "See torch.isnan()", "Returns True if self tensor is contiguous in memory in the order specified by memory format.", "memory_format (torch.memory_format, optional) \u2013 Specifies memory allocation order. Default: torch.contiguous_format.", "Returns True if the data type of self is a complex data type.", "Returns True if the data type of self is a floating point data type.", "All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.", "Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad().", "Example:", "Returns true if this tensor resides in pinned memory.", "Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).", "Checks if tensor is in shared memory.", "This is always True for CUDA tensors.", "Returns True if the data type of self is a signed data type.", "Is True if the Tensor uses sparse storage layout, False otherwise.", "See torch.istft()", "See torch.isreal()", "Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist().", "This operation is not differentiable.", "Example:", "See torch.kthvalue()", "See torch.lcm()", "In-place version of lcm()", "See torch.ldexp()", "In-place version of ldexp()", "See torch.le().", "In-place version of le().", "See torch.less_equal().", "In-place version of less_equal().", "See torch.lerp()", "In-place version of lerp()", "See torch.lgamma()", "In-place version of lgamma()", "See torch.log()", "In-place version of log()", "See torch.logdet()", "See torch.log10()", "In-place version of log10()", "See torch.log1p()", "In-place version of log1p()", "See torch.log2()", "In-place version of log2()", "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu  and standard deviation \u03c3\\sigma . Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:", "See torch.logaddexp()", "See torch.logaddexp2()", "See torch.logsumexp()", "See torch.logical_and()", "In-place version of logical_and()", "See torch.logical_not()", "In-place version of logical_not()", "See torch.logical_or()", "In-place version of logical_or()", "See torch.logical_xor()", "In-place version of logical_xor()", "See torch.logit()", "In-place version of logit()", "self.long() is equivalent to self.to(torch.int64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.lstsq()", "See torch.lt().", "In-place version of lt().", "lt(other) -> Tensor", "See torch.less().", "In-place version of less().", "See torch.lu()", "See torch.lu_solve()", "Makes a cls instance with the same data pointer as self. Changes in the output mirror changes in self, and the output stays attached to the autograd graph. cls must be a subclass of Tensor.", "Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable.", "The callable should have the signature:", "Copies elements from source into self tensor at positions where the mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask", "Note", "The mask operates on the self tensor, not on the given source tensor.", "Out-of-place version of torch.Tensor.masked_scatter_()", "Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor.", "Out-of-place version of torch.Tensor.masked_fill_()", "See torch.masked_select()", "See torch.matmul()", "See torch.matrix_power()", "See torch.matrix_exp()", "See torch.max()", "See torch.maximum()", "See torch.mean()", "See torch.median()", "See torch.nanmedian()", "See torch.min()", "See torch.minimum()", "See torch.mm()", "See torch.smm()", "See torch.mode()", "See torch.movedim()", "See torch.moveaxis()", "See torch.msort()", "See torch.mul().", "In-place version of mul().", "See torch.multiply().", "In-place version of multiply().", "See torch.multinomial()", "See torch.mv()", "See torch.mvlgamma()", "In-place version of mvlgamma()", "See torch.nansum()", "See torch.narrow()", "Example:", "Same as Tensor.narrow() except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling `narrow_copy with `dimemsion > self.sparse_dim()` will return a copy with the relevant dense dimension narrowed, and `self.shape` updated accordingly.", "Alias for dim()", "See torch.nan_to_num().", "In-place version of nan_to_num().", "See torch.ne().", "In-place version of ne().", "See torch.not_equal().", "In-place version of not_equal().", "See torch.neg()", "In-place version of neg()", "See torch.negative()", "In-place version of negative()", "Alias for numel()", "See torch.nextafter()", "In-place version of nextafter()", "See torch.nonzero()", "See torch.norm()", "Fills self tensor with elements samples from the normal distribution parameterized by mean and std.", "See torch.numel()", "Returns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage. Changes to self tensor will be reflected in the ndarray and vice versa.", "See torch.orgqr()", "See torch.ormqr()", "See torch.outer().", "Returns a view of the original tensor with its dimensions permuted.", "*dims (int...) \u2013 The desired ordering of dimensions", "Copies the tensor to pinned memory, if it\u2019s not already pinned.", "See torch.pinverse()", "See torch.polygamma()", "In-place version of polygamma()", "See torch.pow()", "In-place version of pow()", "See torch.prod()", "Copies the elements from tensor into the positions specified by indices. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.", "If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.", "Example:", "See torch.qr()", "Returns the quantization scheme of a given QTensor.", "See torch.quantile()", "See torch.nanquantile()", "Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().", "Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().", "Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.", "Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.", "Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.", "See torch.rad2deg()", "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53].", "see torch.ravel()", "See torch.reciprocal()", "In-place version of reciprocal()", "Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.", "Note", "The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.", "Registers a backward hook.", "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Example:", "See torch.remainder()", "In-place version of remainder()", "See torch.renorm()", "In-place version of renorm()", "Repeats this tensor along the specified dimensions.", "Unlike expand(), this function copies the tensor\u2019s data.", "Warning", "repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().", "sizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along each dimension", "Example:", "See torch.repeat_interleave().", "Is True if gradients need to be computed for this Tensor, False otherwise.", "Note", "The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.", "Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor.", "requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.", "requires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.", "Example:", "Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "See torch.reshape()", "shape (tuple of python:ints or int...) \u2013 the desired shape", "Returns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "Please see reshape() for more information about reshape.", "other (torch.Tensor) \u2013 The result tensor has the same shape as other.", "Resizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.", "Warning", "This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().", "Example:", "Resizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches tensor.size().", "Enables .grad attribute for non-leaf Tensors.", "See torch.roll()", "See torch.rot90()", "See torch.round()", "In-place version of round()", "See torch.rsqrt()", "In-place version of rsqrt()", "Out-of-place version of torch.Tensor.scatter_()", "Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "This is the reverse operation of the manner described in gather().", "self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.", "Warning", "When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indicies specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "Given a 3-D tensor and reduction using the multiplication operation, self is updated as:", "Reducing with the addition operation is the same as using scatter_add_().", "Example:", "Adds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in src, it is added to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Example:", "Out-of-place version of torch.Tensor.scatter_add_()", "Slices the self tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.", "Note", "select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index].", "Sets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other.", "If source is a Storage, the method sets the underlying storage, offset, size, and stride.", "Moves the underlying storage to shared memory.", "This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.", "self.short() is equivalent to self.to(torch.int16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.", "See torch.sigmoid()", "In-place version of sigmoid()", "See torch.sign()", "In-place version of sign()", "See torch.signbit()", "See torch.sgn()", "In-place version of sgn()", "See torch.sin()", "In-place version of sin()", "See torch.sinc()", "In-place version of sinc()", "See torch.sinh()", "In-place version of sinh()", "See torch.asinh()", "In-place version of asinh()", "See torch.arcsinh()", "In-place version of arcsinh()", "Returns the size of the self tensor. The returned value is a subclass of tuple.", "Example:", "See torch.slogdet()", "See torch.solve()", "See torch.sort()", "See torch.split()", "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.", "Note", "The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.", "mask (Tensor) \u2013 a sparse tensor whose indices are used as a filter", "Example:", "Return the number of sparse dimensions in a sparse tensor self.", "Warning", "Throws an error if self is not a sparse tensor.", "See also Tensor.dense_dim() and hybrid tensors.", "See torch.sqrt()", "In-place version of sqrt()", "See torch.square()", "In-place version of square()", "See torch.squeeze()", "In-place version of squeeze()", "See torch.std()", "See torch.stft()", "Warning", "This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.", "Returns the underlying storage.", "Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes).", "Example:", "Returns the type of the underlying storage.", "Returns the stride of self tensor.", "Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.", "dim (int, optional) \u2013 the desired dimension in which stride is required", "Example:", "See torch.sub().", "In-place version of sub()", "See torch.subtract().", "In-place version of subtract().", "See torch.sum()", "Sum this tensor to size. size must be broadcastable to this tensor size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor.", "See torch.svd()", "See torch.swapaxes()", "See torch.swapdims()", "See torch.symeig()", "See torch.t()", "In-place version of t()", "See torch.tensor_split()", "See torch.tile()", "Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).", "Note", "If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.", "Here are the ways to call to:", "Returns a Tensor with the specified dtype", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "Example:", "Returns a copy of the tensor in torch.mkldnn layout.", "See torch.take()", "See torch.tan()", "In-place version of tan()", "See torch.tanh()", "In-place version of tanh()", "See torch.atanh()", "In-place version of atanh()", "See torch.arctanh()", "In-place version of arctanh()", "Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary.", "This operation is not differentiable.", "Examples:", "See torch.topk()", "Returns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.", "sparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor", "Example:", "See torch.trace()", "See torch.transpose()", "In-place version of transpose()", "See torch.triangular_solve()", "See torch.tril()", "In-place version of tril()", "See torch.triu()", "In-place version of triu()", "See torch.true_divide()", "In-place version of true_divide_()", "See torch.trunc()", "In-place version of trunc()", "Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned.", "Returns this tensor cast to the type of the given tensor.", "This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())", "tensor (Tensor) \u2013 the tensor which has the desired type", "See torch.unbind()", "Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.", "Step between two slices is given by step.", "If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1.", "An additional dimension of size size is appended in the returned tensor.", "Example:", "Fills self tensor with numbers sampled from the continuous uniform distribution:", "Returns the unique elements of the input tensor.", "See torch.unique()", "Eliminates all but the first element from every consecutive group of equivalent elements.", "See torch.unique_consecutive()", "See torch.unsqueeze()", "In-place version of unsqueeze()", "Return the values tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.indices().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.", "See torch.var()", "See torch.vdot()", "Returns a new tensor with the same data as the self tensor but of a different shape.", "The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k  that satisfy the following contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots, d+k-1 ,", "Otherwise, it will not be possible to view self tensor as shape without copying it (e.g., via contiguous()). When it is unclear whether a view() can be performed, it is advisable to use reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.", "shape (torch.Size or int...) \u2013 the desired size", "Example:", "Returns a new tensor with the same data as the self tensor but of a different dtype. dtype must have the same number of bytes per element as self\u2019s dtype.", "Warning", "This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.", "dtype (torch.dtype) \u2013 the desired dtype", "Example:", "View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()).", "Please see view() for more information about view.", "other (torch.Tensor) \u2013 The result tensor has the same size as other.", "self.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where()", "See torch.xlogy()", "In-place version of xlogy()", "Fills self tensor with zeros."]}, {"name": "torch.tensor()", "path": "generated/torch.tensor#torch.tensor", "type": "torch", "text": ["Constructs a tensor with data.", "Warning", "torch.tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a NumPy ndarray and want to avoid a copy, use torch.as_tensor().", "Warning", "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach() and torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.", "data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.", "Example:"]}, {"name": "torch.Tensor.abs()", "path": "tensors#torch.Tensor.abs", "type": "torch.Tensor", "text": ["See torch.abs()"]}, {"name": "torch.Tensor.absolute()", "path": "tensors#torch.Tensor.absolute", "type": "torch.Tensor", "text": ["Alias for abs()"]}, {"name": "torch.Tensor.absolute_()", "path": "tensors#torch.Tensor.absolute_", "type": "torch.Tensor", "text": ["In-place version of absolute() Alias for abs_()"]}, {"name": "torch.Tensor.abs_()", "path": "tensors#torch.Tensor.abs_", "type": "torch.Tensor", "text": ["In-place version of abs()"]}, {"name": "torch.Tensor.acos()", "path": "tensors#torch.Tensor.acos", "type": "torch.Tensor", "text": ["See torch.acos()"]}, {"name": "torch.Tensor.acosh()", "path": "tensors#torch.Tensor.acosh", "type": "torch.Tensor", "text": ["See torch.acosh()"]}, {"name": "torch.Tensor.acosh_()", "path": "tensors#torch.Tensor.acosh_", "type": "torch.Tensor", "text": ["In-place version of acosh()"]}, {"name": "torch.Tensor.acos_()", "path": "tensors#torch.Tensor.acos_", "type": "torch.Tensor", "text": ["In-place version of acos()"]}, {"name": "torch.Tensor.add()", "path": "tensors#torch.Tensor.add", "type": "torch.Tensor", "text": ["Add a scalar or tensor to self tensor. If both alpha and other are specified, each element of other is scaled by alpha before being used.", "When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor", "See torch.add()"]}, {"name": "torch.Tensor.addbmm()", "path": "tensors#torch.Tensor.addbmm", "type": "torch.Tensor", "text": ["See torch.addbmm()"]}, {"name": "torch.Tensor.addbmm_()", "path": "tensors#torch.Tensor.addbmm_", "type": "torch.Tensor", "text": ["In-place version of addbmm()"]}, {"name": "torch.Tensor.addcdiv()", "path": "tensors#torch.Tensor.addcdiv", "type": "torch.Tensor", "text": ["See torch.addcdiv()"]}, {"name": "torch.Tensor.addcdiv_()", "path": "tensors#torch.Tensor.addcdiv_", "type": "torch.Tensor", "text": ["In-place version of addcdiv()"]}, {"name": "torch.Tensor.addcmul()", "path": "tensors#torch.Tensor.addcmul", "type": "torch.Tensor", "text": ["See torch.addcmul()"]}, {"name": "torch.Tensor.addcmul_()", "path": "tensors#torch.Tensor.addcmul_", "type": "torch.Tensor", "text": ["In-place version of addcmul()"]}, {"name": "torch.Tensor.addmm()", "path": "tensors#torch.Tensor.addmm", "type": "torch.Tensor", "text": ["See torch.addmm()"]}, {"name": "torch.Tensor.addmm_()", "path": "tensors#torch.Tensor.addmm_", "type": "torch.Tensor", "text": ["In-place version of addmm()"]}, {"name": "torch.Tensor.addmv()", "path": "tensors#torch.Tensor.addmv", "type": "torch.Tensor", "text": ["See torch.addmv()"]}, {"name": "torch.Tensor.addmv_()", "path": "tensors#torch.Tensor.addmv_", "type": "torch.Tensor", "text": ["In-place version of addmv()"]}, {"name": "torch.Tensor.addr()", "path": "tensors#torch.Tensor.addr", "type": "torch.Tensor", "text": ["See torch.addr()"]}, {"name": "torch.Tensor.addr_()", "path": "tensors#torch.Tensor.addr_", "type": "torch.Tensor", "text": ["In-place version of addr()"]}, {"name": "torch.Tensor.add_()", "path": "tensors#torch.Tensor.add_", "type": "torch.Tensor", "text": ["In-place version of add()"]}, {"name": "torch.Tensor.align_as()", "path": "named_tensor#torch.Tensor.align_as", "type": "Named Tensors", "text": ["Permutes the dimensions of the self tensor to match the dimension order in the other tensor, adding size-one dims for any new names.", "This operation is useful for explicit broadcasting by names (see examples).", "All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor.", "All dimension names of self must be present in other.names. other may contain named dimensions that are not in self.names; the output tensor has a size-one dimension for each of those new names.", "To align a tensor to a specific order, use align_to().", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.align_to()", "path": "named_tensor#torch.Tensor.align_to", "type": "Named Tensors", "text": ["Permutes the dimensions of the self tensor to match the order specified in names, adding size-one dims for any new names.", "All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor.", "All dimension names of self must be present in names. names may contain additional names that are not in self.names; the output tensor has a size-one dimension for each of those new names.", "names may contain up to one Ellipsis (...). The Ellipsis is expanded to be equal to all dimension names of self that are not mentioned in names, in the order that they appear in self.", "Python 2 does not support Ellipsis but one may use a string literal instead ('...').", "names (iterable of str) \u2013 The desired dimension ordering of the output tensor. May contain up to one Ellipsis that is expanded to all unmentioned dim names of self.", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.all()", "path": "tensors#torch.Tensor.all", "type": "torch.Tensor", "text": ["See torch.all()"]}, {"name": "torch.Tensor.allclose()", "path": "tensors#torch.Tensor.allclose", "type": "torch.Tensor", "text": ["See torch.allclose()"]}, {"name": "torch.Tensor.amax()", "path": "tensors#torch.Tensor.amax", "type": "torch.Tensor", "text": ["See torch.amax()"]}, {"name": "torch.Tensor.amin()", "path": "tensors#torch.Tensor.amin", "type": "torch.Tensor", "text": ["See torch.amin()"]}, {"name": "torch.Tensor.angle()", "path": "tensors#torch.Tensor.angle", "type": "torch.Tensor", "text": ["See torch.angle()"]}, {"name": "torch.Tensor.any()", "path": "tensors#torch.Tensor.any", "type": "torch.Tensor", "text": ["See torch.any()"]}, {"name": "torch.Tensor.apply_()", "path": "tensors#torch.Tensor.apply_", "type": "torch.Tensor", "text": ["Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.", "Note", "This function only works with CPU tensors and should not be used in code sections that require high performance."]}, {"name": "torch.Tensor.arccos()", "path": "tensors#torch.Tensor.arccos", "type": "torch.Tensor", "text": ["See torch.arccos()"]}, {"name": "torch.Tensor.arccosh()", "path": "tensors#torch.Tensor.arccosh", "type": "torch.Tensor", "text": ["acosh() -> Tensor", "See torch.arccosh()"]}, {"name": "torch.Tensor.arccosh_()", "path": "tensors#torch.Tensor.arccosh_", "type": "torch.Tensor", "text": ["acosh_() -> Tensor", "In-place version of arccosh()"]}, {"name": "torch.Tensor.arccos_()", "path": "tensors#torch.Tensor.arccos_", "type": "torch.Tensor", "text": ["In-place version of arccos()"]}, {"name": "torch.Tensor.arcsin()", "path": "tensors#torch.Tensor.arcsin", "type": "torch.Tensor", "text": ["See torch.arcsin()"]}, {"name": "torch.Tensor.arcsinh()", "path": "tensors#torch.Tensor.arcsinh", "type": "torch.Tensor", "text": ["See torch.arcsinh()"]}, {"name": "torch.Tensor.arcsinh_()", "path": "tensors#torch.Tensor.arcsinh_", "type": "torch.Tensor", "text": ["In-place version of arcsinh()"]}, {"name": "torch.Tensor.arcsin_()", "path": "tensors#torch.Tensor.arcsin_", "type": "torch.Tensor", "text": ["In-place version of arcsin()"]}, {"name": "torch.Tensor.arctan()", "path": "tensors#torch.Tensor.arctan", "type": "torch.Tensor", "text": ["See torch.arctan()"]}, {"name": "torch.Tensor.arctanh()", "path": "tensors#torch.Tensor.arctanh", "type": "torch.Tensor", "text": ["See torch.arctanh()"]}, {"name": "torch.Tensor.arctanh_()", "path": "tensors#torch.Tensor.arctanh_", "type": "torch.Tensor", "text": ["In-place version of arctanh()"]}, {"name": "torch.Tensor.arctan_()", "path": "tensors#torch.Tensor.arctan_", "type": "torch.Tensor", "text": ["In-place version of arctan()"]}, {"name": "torch.Tensor.argmax()", "path": "tensors#torch.Tensor.argmax", "type": "torch.Tensor", "text": ["See torch.argmax()"]}, {"name": "torch.Tensor.argmin()", "path": "tensors#torch.Tensor.argmin", "type": "torch.Tensor", "text": ["See torch.argmin()"]}, {"name": "torch.Tensor.argsort()", "path": "tensors#torch.Tensor.argsort", "type": "torch.Tensor", "text": ["See torch.argsort()"]}, {"name": "torch.Tensor.asin()", "path": "tensors#torch.Tensor.asin", "type": "torch.Tensor", "text": ["See torch.asin()"]}, {"name": "torch.Tensor.asinh()", "path": "tensors#torch.Tensor.asinh", "type": "torch.Tensor", "text": ["See torch.asinh()"]}, {"name": "torch.Tensor.asinh_()", "path": "tensors#torch.Tensor.asinh_", "type": "torch.Tensor", "text": ["In-place version of asinh()"]}, {"name": "torch.Tensor.asin_()", "path": "tensors#torch.Tensor.asin_", "type": "torch.Tensor", "text": ["In-place version of asin()"]}, {"name": "torch.Tensor.as_strided()", "path": "tensors#torch.Tensor.as_strided", "type": "torch.Tensor", "text": ["See torch.as_strided()"]}, {"name": "torch.Tensor.as_subclass()", "path": "tensors#torch.Tensor.as_subclass", "type": "torch.Tensor", "text": ["Makes a cls instance with the same data pointer as self. Changes in the output mirror changes in self, and the output stays attached to the autograd graph. cls must be a subclass of Tensor."]}, {"name": "torch.Tensor.atan()", "path": "tensors#torch.Tensor.atan", "type": "torch.Tensor", "text": ["See torch.atan()"]}, {"name": "torch.Tensor.atan2()", "path": "tensors#torch.Tensor.atan2", "type": "torch.Tensor", "text": ["See torch.atan2()"]}, {"name": "torch.Tensor.atan2_()", "path": "tensors#torch.Tensor.atan2_", "type": "torch.Tensor", "text": ["In-place version of atan2()"]}, {"name": "torch.Tensor.atanh()", "path": "tensors#torch.Tensor.atanh", "type": "torch.Tensor", "text": ["See torch.atanh()"]}, {"name": "torch.Tensor.atanh_()", "path": "tensors#torch.Tensor.atanh_", "type": "torch.Tensor", "text": ["In-place version of atanh()"]}, {"name": "torch.Tensor.atan_()", "path": "tensors#torch.Tensor.atan_", "type": "torch.Tensor", "text": ["In-place version of atan()"]}, {"name": "torch.Tensor.backward()", "path": "autograd#torch.Tensor.backward", "type": "torch.autograd", "text": ["Computes the gradient of current tensor w.r.t. graph leaves.", "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes."]}, {"name": "torch.Tensor.baddbmm()", "path": "tensors#torch.Tensor.baddbmm", "type": "torch.Tensor", "text": ["See torch.baddbmm()"]}, {"name": "torch.Tensor.baddbmm_()", "path": "tensors#torch.Tensor.baddbmm_", "type": "torch.Tensor", "text": ["In-place version of baddbmm()"]}, {"name": "torch.Tensor.bernoulli()", "path": "tensors#torch.Tensor.bernoulli", "type": "torch.Tensor", "text": ["Returns a result tensor where each result[i]\\texttt{result[i]}  is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . self must have floating point dtype, and the result will have the same dtype.", "See torch.bernoulli()"]}, {"name": "torch.Tensor.bernoulli_()", "path": "tensors#torch.Tensor.bernoulli_", "type": "torch.Tensor", "text": ["Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p}) . self can have integral dtype.", "p_tensor should be a tensor containing probabilities to be used for drawing the binary random number.", "The ith\\text{i}^{th}  element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]}) .", "self can have integral dtype, but p_tensor must have floating point dtype.", "See also bernoulli() and torch.bernoulli()"]}, {"name": "torch.Tensor.bfloat16()", "path": "tensors#torch.Tensor.bfloat16", "type": "torch.Tensor", "text": ["self.bfloat16() is equivalent to self.to(torch.bfloat16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.bincount()", "path": "tensors#torch.Tensor.bincount", "type": "torch.Tensor", "text": ["See torch.bincount()"]}, {"name": "torch.Tensor.bitwise_and()", "path": "tensors#torch.Tensor.bitwise_and", "type": "torch.Tensor", "text": ["See torch.bitwise_and()"]}, {"name": "torch.Tensor.bitwise_and_()", "path": "tensors#torch.Tensor.bitwise_and_", "type": "torch.Tensor", "text": ["In-place version of bitwise_and()"]}, {"name": "torch.Tensor.bitwise_not()", "path": "tensors#torch.Tensor.bitwise_not", "type": "torch.Tensor", "text": ["See torch.bitwise_not()"]}, {"name": "torch.Tensor.bitwise_not_()", "path": "tensors#torch.Tensor.bitwise_not_", "type": "torch.Tensor", "text": ["In-place version of bitwise_not()"]}, {"name": "torch.Tensor.bitwise_or()", "path": "tensors#torch.Tensor.bitwise_or", "type": "torch.Tensor", "text": ["See torch.bitwise_or()"]}, {"name": "torch.Tensor.bitwise_or_()", "path": "tensors#torch.Tensor.bitwise_or_", "type": "torch.Tensor", "text": ["In-place version of bitwise_or()"]}, {"name": "torch.Tensor.bitwise_xor()", "path": "tensors#torch.Tensor.bitwise_xor", "type": "torch.Tensor", "text": ["See torch.bitwise_xor()"]}, {"name": "torch.Tensor.bitwise_xor_()", "path": "tensors#torch.Tensor.bitwise_xor_", "type": "torch.Tensor", "text": ["In-place version of bitwise_xor()"]}, {"name": "torch.Tensor.bmm()", "path": "tensors#torch.Tensor.bmm", "type": "torch.Tensor", "text": ["See torch.bmm()"]}, {"name": "torch.Tensor.bool()", "path": "tensors#torch.Tensor.bool", "type": "torch.Tensor", "text": ["self.bool() is equivalent to self.to(torch.bool). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.broadcast_to()", "path": "tensors#torch.Tensor.broadcast_to", "type": "torch.Tensor", "text": ["See torch.broadcast_to()."]}, {"name": "torch.Tensor.byte()", "path": "tensors#torch.Tensor.byte", "type": "torch.Tensor", "text": ["self.byte() is equivalent to self.to(torch.uint8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.cauchy_()", "path": "tensors#torch.Tensor.cauchy_", "type": "torch.Tensor", "text": ["Fills the tensor with numbers drawn from the Cauchy distribution:"]}, {"name": "torch.Tensor.ceil()", "path": "tensors#torch.Tensor.ceil", "type": "torch.Tensor", "text": ["See torch.ceil()"]}, {"name": "torch.Tensor.ceil_()", "path": "tensors#torch.Tensor.ceil_", "type": "torch.Tensor", "text": ["In-place version of ceil()"]}, {"name": "torch.Tensor.char()", "path": "tensors#torch.Tensor.char", "type": "torch.Tensor", "text": ["self.char() is equivalent to self.to(torch.int8). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.cholesky()", "path": "tensors#torch.Tensor.cholesky", "type": "torch.Tensor", "text": ["See torch.cholesky()"]}, {"name": "torch.Tensor.cholesky_inverse()", "path": "tensors#torch.Tensor.cholesky_inverse", "type": "torch.Tensor", "text": ["See torch.cholesky_inverse()"]}, {"name": "torch.Tensor.cholesky_solve()", "path": "tensors#torch.Tensor.cholesky_solve", "type": "torch.Tensor", "text": ["See torch.cholesky_solve()"]}, {"name": "torch.Tensor.chunk()", "path": "tensors#torch.Tensor.chunk", "type": "torch.Tensor", "text": ["See torch.chunk()"]}, {"name": "torch.Tensor.clamp()", "path": "tensors#torch.Tensor.clamp", "type": "torch.Tensor", "text": ["See torch.clamp()"]}, {"name": "torch.Tensor.clamp_()", "path": "tensors#torch.Tensor.clamp_", "type": "torch.Tensor", "text": ["In-place version of clamp()"]}, {"name": "torch.Tensor.clip()", "path": "tensors#torch.Tensor.clip", "type": "torch.Tensor", "text": ["Alias for clamp()."]}, {"name": "torch.Tensor.clip_()", "path": "tensors#torch.Tensor.clip_", "type": "torch.Tensor", "text": ["Alias for clamp_()."]}, {"name": "torch.Tensor.clone()", "path": "tensors#torch.Tensor.clone", "type": "torch.Tensor", "text": ["See torch.clone()"]}, {"name": "torch.Tensor.coalesce()", "path": "sparse#torch.Tensor.coalesce", "type": "torch.sparse", "text": ["Returns a coalesced copy of self if self is an uncoalesced tensor.", "Returns self if self is a coalesced tensor.", "Warning", "Throws an error if self is not a sparse COO tensor."]}, {"name": "torch.Tensor.conj()", "path": "tensors#torch.Tensor.conj", "type": "torch.Tensor", "text": ["See torch.conj()"]}, {"name": "torch.Tensor.contiguous()", "path": "tensors#torch.Tensor.contiguous", "type": "torch.Tensor", "text": ["Returns a contiguous in memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format."]}, {"name": "torch.Tensor.copysign()", "path": "tensors#torch.Tensor.copysign", "type": "torch.Tensor", "text": ["See torch.copysign()"]}, {"name": "torch.Tensor.copysign_()", "path": "tensors#torch.Tensor.copysign_", "type": "torch.Tensor", "text": ["In-place version of copysign()"]}, {"name": "torch.Tensor.copy_()", "path": "tensors#torch.Tensor.copy_", "type": "torch.Tensor", "text": ["Copies the elements from src into self tensor and returns self.", "The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device."]}, {"name": "torch.Tensor.cos()", "path": "tensors#torch.Tensor.cos", "type": "torch.Tensor", "text": ["See torch.cos()"]}, {"name": "torch.Tensor.cosh()", "path": "tensors#torch.Tensor.cosh", "type": "torch.Tensor", "text": ["See torch.cosh()"]}, {"name": "torch.Tensor.cosh_()", "path": "tensors#torch.Tensor.cosh_", "type": "torch.Tensor", "text": ["In-place version of cosh()"]}, {"name": "torch.Tensor.cos_()", "path": "tensors#torch.Tensor.cos_", "type": "torch.Tensor", "text": ["In-place version of cos()"]}, {"name": "torch.Tensor.count_nonzero()", "path": "tensors#torch.Tensor.count_nonzero", "type": "torch.Tensor", "text": ["See torch.count_nonzero()"]}, {"name": "torch.Tensor.cpu()", "path": "tensors#torch.Tensor.cpu", "type": "torch.Tensor", "text": ["Returns a copy of this object in CPU memory.", "If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.cross()", "path": "tensors#torch.Tensor.cross", "type": "torch.Tensor", "text": ["See torch.cross()"]}, {"name": "torch.Tensor.cuda()", "path": "tensors#torch.Tensor.cuda", "type": "torch.Tensor", "text": ["Returns a copy of this object in CUDA memory.", "If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned."]}, {"name": "torch.Tensor.cummax()", "path": "tensors#torch.Tensor.cummax", "type": "torch.Tensor", "text": ["See torch.cummax()"]}, {"name": "torch.Tensor.cummin()", "path": "tensors#torch.Tensor.cummin", "type": "torch.Tensor", "text": ["See torch.cummin()"]}, {"name": "torch.Tensor.cumprod()", "path": "tensors#torch.Tensor.cumprod", "type": "torch.Tensor", "text": ["See torch.cumprod()"]}, {"name": "torch.Tensor.cumprod_()", "path": "tensors#torch.Tensor.cumprod_", "type": "torch.Tensor", "text": ["In-place version of cumprod()"]}, {"name": "torch.Tensor.cumsum()", "path": "tensors#torch.Tensor.cumsum", "type": "torch.Tensor", "text": ["See torch.cumsum()"]}, {"name": "torch.Tensor.cumsum_()", "path": "tensors#torch.Tensor.cumsum_", "type": "torch.Tensor", "text": ["In-place version of cumsum()"]}, {"name": "torch.Tensor.data_ptr()", "path": "tensors#torch.Tensor.data_ptr", "type": "torch.Tensor", "text": ["Returns the address of the first element of self tensor."]}, {"name": "torch.Tensor.deg2rad()", "path": "tensors#torch.Tensor.deg2rad", "type": "torch.Tensor", "text": ["See torch.deg2rad()"]}, {"name": "torch.Tensor.dense_dim()", "path": "sparse#torch.Tensor.dense_dim", "type": "torch.sparse", "text": ["Return the number of dense dimensions in a sparse tensor self.", "Warning", "Throws an error if self is not a sparse tensor.", "See also Tensor.sparse_dim() and hybrid tensors."]}, {"name": "torch.Tensor.dequantize()", "path": "tensors#torch.Tensor.dequantize", "type": "torch.Tensor", "text": ["Given a quantized Tensor, dequantize it and return the dequantized float Tensor."]}, {"name": "torch.Tensor.det()", "path": "tensors#torch.Tensor.det", "type": "torch.Tensor", "text": ["See torch.det()"]}, {"name": "torch.Tensor.detach()", "path": "autograd#torch.Tensor.detach", "type": "torch.autograd", "text": ["Returns a new Tensor, detached from the current graph.", "The result will never require gradient.", "Note", "Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error."]}, {"name": "torch.Tensor.detach_()", "path": "autograd#torch.Tensor.detach_", "type": "torch.autograd", "text": ["Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place."]}, {"name": "torch.Tensor.device", "path": "tensors#torch.Tensor.device", "type": "torch.Tensor", "text": ["Is the torch.device where this Tensor is."]}, {"name": "torch.Tensor.diag()", "path": "tensors#torch.Tensor.diag", "type": "torch.Tensor", "text": ["See torch.diag()"]}, {"name": "torch.Tensor.diagflat()", "path": "tensors#torch.Tensor.diagflat", "type": "torch.Tensor", "text": ["See torch.diagflat()"]}, {"name": "torch.Tensor.diagonal()", "path": "tensors#torch.Tensor.diagonal", "type": "torch.Tensor", "text": ["See torch.diagonal()"]}, {"name": "torch.Tensor.diag_embed()", "path": "tensors#torch.Tensor.diag_embed", "type": "torch.Tensor", "text": ["See torch.diag_embed()"]}, {"name": "torch.Tensor.diff()", "path": "tensors#torch.Tensor.diff", "type": "torch.Tensor", "text": ["See torch.diff()"]}, {"name": "torch.Tensor.digamma()", "path": "tensors#torch.Tensor.digamma", "type": "torch.Tensor", "text": ["See torch.digamma()"]}, {"name": "torch.Tensor.digamma_()", "path": "tensors#torch.Tensor.digamma_", "type": "torch.Tensor", "text": ["In-place version of digamma()"]}, {"name": "torch.Tensor.dim()", "path": "tensors#torch.Tensor.dim", "type": "torch.Tensor", "text": ["Returns the number of dimensions of self tensor."]}, {"name": "torch.Tensor.dist()", "path": "tensors#torch.Tensor.dist", "type": "torch.Tensor", "text": ["See torch.dist()"]}, {"name": "torch.Tensor.div()", "path": "tensors#torch.Tensor.div", "type": "torch.Tensor", "text": ["See torch.div()"]}, {"name": "torch.Tensor.divide()", "path": "tensors#torch.Tensor.divide", "type": "torch.Tensor", "text": ["See torch.divide()"]}, {"name": "torch.Tensor.divide_()", "path": "tensors#torch.Tensor.divide_", "type": "torch.Tensor", "text": ["In-place version of divide()"]}, {"name": "torch.Tensor.div_()", "path": "tensors#torch.Tensor.div_", "type": "torch.Tensor", "text": ["In-place version of div()"]}, {"name": "torch.Tensor.dot()", "path": "tensors#torch.Tensor.dot", "type": "torch.Tensor", "text": ["See torch.dot()"]}, {"name": "torch.Tensor.double()", "path": "tensors#torch.Tensor.double", "type": "torch.Tensor", "text": ["self.double() is equivalent to self.to(torch.float64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.eig()", "path": "tensors#torch.Tensor.eig", "type": "torch.Tensor", "text": ["See torch.eig()"]}, {"name": "torch.Tensor.element_size()", "path": "tensors#torch.Tensor.element_size", "type": "torch.Tensor", "text": ["Returns the size in bytes of an individual element.", "Example:"]}, {"name": "torch.Tensor.eq()", "path": "tensors#torch.Tensor.eq", "type": "torch.Tensor", "text": ["See torch.eq()"]}, {"name": "torch.Tensor.equal()", "path": "tensors#torch.Tensor.equal", "type": "torch.Tensor", "text": ["See torch.equal()"]}, {"name": "torch.Tensor.eq_()", "path": "tensors#torch.Tensor.eq_", "type": "torch.Tensor", "text": ["In-place version of eq()"]}, {"name": "torch.Tensor.erf()", "path": "tensors#torch.Tensor.erf", "type": "torch.Tensor", "text": ["See torch.erf()"]}, {"name": "torch.Tensor.erfc()", "path": "tensors#torch.Tensor.erfc", "type": "torch.Tensor", "text": ["See torch.erfc()"]}, {"name": "torch.Tensor.erfc_()", "path": "tensors#torch.Tensor.erfc_", "type": "torch.Tensor", "text": ["In-place version of erfc()"]}, {"name": "torch.Tensor.erfinv()", "path": "tensors#torch.Tensor.erfinv", "type": "torch.Tensor", "text": ["See torch.erfinv()"]}, {"name": "torch.Tensor.erfinv_()", "path": "tensors#torch.Tensor.erfinv_", "type": "torch.Tensor", "text": ["In-place version of erfinv()"]}, {"name": "torch.Tensor.erf_()", "path": "tensors#torch.Tensor.erf_", "type": "torch.Tensor", "text": ["In-place version of erf()"]}, {"name": "torch.Tensor.exp()", "path": "tensors#torch.Tensor.exp", "type": "torch.Tensor", "text": ["See torch.exp()"]}, {"name": "torch.Tensor.expand()", "path": "tensors#torch.Tensor.expand", "type": "torch.Tensor", "text": ["Returns a new view of the self tensor with singleton dimensions expanded to a larger size.", "Passing -1 as the size for a dimension means not changing the size of that dimension.", "Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.", "Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.", "*sizes (torch.Size or int...) \u2013 the desired expanded size", "Warning", "More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.Tensor.expand_as()", "path": "tensors#torch.Tensor.expand_as", "type": "torch.Tensor", "text": ["Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()).", "Please see expand() for more information about expand.", "other (torch.Tensor) \u2013 The result tensor has the same size as other."]}, {"name": "torch.Tensor.expm1()", "path": "tensors#torch.Tensor.expm1", "type": "torch.Tensor", "text": ["See torch.expm1()"]}, {"name": "torch.Tensor.expm1_()", "path": "tensors#torch.Tensor.expm1_", "type": "torch.Tensor", "text": ["In-place version of expm1()"]}, {"name": "torch.Tensor.exponential_()", "path": "tensors#torch.Tensor.exponential_", "type": "torch.Tensor", "text": ["Fills self tensor with elements drawn from the exponential distribution:"]}, {"name": "torch.Tensor.exp_()", "path": "tensors#torch.Tensor.exp_", "type": "torch.Tensor", "text": ["In-place version of exp()"]}, {"name": "torch.Tensor.fill_()", "path": "tensors#torch.Tensor.fill_", "type": "torch.Tensor", "text": ["Fills self tensor with the specified value."]}, {"name": "torch.Tensor.fill_diagonal_()", "path": "tensors#torch.Tensor.fill_diagonal_", "type": "torch.Tensor", "text": ["Fill the main diagonal of a tensor that has at least 2-dimensions. When dims>2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.", "Example:"]}, {"name": "torch.Tensor.fix()", "path": "tensors#torch.Tensor.fix", "type": "torch.Tensor", "text": ["See torch.fix()."]}, {"name": "torch.Tensor.fix_()", "path": "tensors#torch.Tensor.fix_", "type": "torch.Tensor", "text": ["In-place version of fix()"]}, {"name": "torch.Tensor.flatten()", "path": "tensors#torch.Tensor.flatten", "type": "torch.Tensor", "text": ["see torch.flatten()"]}, {"name": "torch.Tensor.flip()", "path": "tensors#torch.Tensor.flip", "type": "torch.Tensor", "text": ["See torch.flip()"]}, {"name": "torch.Tensor.fliplr()", "path": "tensors#torch.Tensor.fliplr", "type": "torch.Tensor", "text": ["See torch.fliplr()"]}, {"name": "torch.Tensor.flipud()", "path": "tensors#torch.Tensor.flipud", "type": "torch.Tensor", "text": ["See torch.flipud()"]}, {"name": "torch.Tensor.float()", "path": "tensors#torch.Tensor.float", "type": "torch.Tensor", "text": ["self.float() is equivalent to self.to(torch.float32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.float_power()", "path": "tensors#torch.Tensor.float_power", "type": "torch.Tensor", "text": ["See torch.float_power()"]}, {"name": "torch.Tensor.float_power_()", "path": "tensors#torch.Tensor.float_power_", "type": "torch.Tensor", "text": ["In-place version of float_power()"]}, {"name": "torch.Tensor.floor()", "path": "tensors#torch.Tensor.floor", "type": "torch.Tensor", "text": ["See torch.floor()"]}, {"name": "torch.Tensor.floor_()", "path": "tensors#torch.Tensor.floor_", "type": "torch.Tensor", "text": ["In-place version of floor()"]}, {"name": "torch.Tensor.floor_divide()", "path": "tensors#torch.Tensor.floor_divide", "type": "torch.Tensor", "text": ["See torch.floor_divide()"]}, {"name": "torch.Tensor.floor_divide_()", "path": "tensors#torch.Tensor.floor_divide_", "type": "torch.Tensor", "text": ["In-place version of floor_divide()"]}, {"name": "torch.Tensor.fmax()", "path": "tensors#torch.Tensor.fmax", "type": "torch.Tensor", "text": ["See torch.fmax()"]}, {"name": "torch.Tensor.fmin()", "path": "tensors#torch.Tensor.fmin", "type": "torch.Tensor", "text": ["See torch.fmin()"]}, {"name": "torch.Tensor.fmod()", "path": "tensors#torch.Tensor.fmod", "type": "torch.Tensor", "text": ["See torch.fmod()"]}, {"name": "torch.Tensor.fmod_()", "path": "tensors#torch.Tensor.fmod_", "type": "torch.Tensor", "text": ["In-place version of fmod()"]}, {"name": "torch.Tensor.frac()", "path": "tensors#torch.Tensor.frac", "type": "torch.Tensor", "text": ["See torch.frac()"]}, {"name": "torch.Tensor.frac_()", "path": "tensors#torch.Tensor.frac_", "type": "torch.Tensor", "text": ["In-place version of frac()"]}, {"name": "torch.Tensor.gather()", "path": "tensors#torch.Tensor.gather", "type": "torch.Tensor", "text": ["See torch.gather()"]}, {"name": "torch.Tensor.gcd()", "path": "tensors#torch.Tensor.gcd", "type": "torch.Tensor", "text": ["See torch.gcd()"]}, {"name": "torch.Tensor.gcd_()", "path": "tensors#torch.Tensor.gcd_", "type": "torch.Tensor", "text": ["In-place version of gcd()"]}, {"name": "torch.Tensor.ge()", "path": "tensors#torch.Tensor.ge", "type": "torch.Tensor", "text": ["See torch.ge()."]}, {"name": "torch.Tensor.geometric_()", "path": "tensors#torch.Tensor.geometric_", "type": "torch.Tensor", "text": ["Fills self tensor with elements drawn from the geometric distribution:"]}, {"name": "torch.Tensor.geqrf()", "path": "tensors#torch.Tensor.geqrf", "type": "torch.Tensor", "text": ["See torch.geqrf()"]}, {"name": "torch.Tensor.ger()", "path": "tensors#torch.Tensor.ger", "type": "torch.Tensor", "text": ["See torch.ger()"]}, {"name": "torch.Tensor.get_device()", "path": "tensors#torch.Tensor.get_device", "type": "torch.Tensor", "text": ["For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.", "Example:"]}, {"name": "torch.Tensor.ge_()", "path": "tensors#torch.Tensor.ge_", "type": "torch.Tensor", "text": ["In-place version of ge()."]}, {"name": "torch.Tensor.grad", "path": "autograd#torch.Tensor.grad", "type": "torch.autograd", "text": ["This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it."]}, {"name": "torch.Tensor.greater()", "path": "tensors#torch.Tensor.greater", "type": "torch.Tensor", "text": ["See torch.greater()."]}, {"name": "torch.Tensor.greater_()", "path": "tensors#torch.Tensor.greater_", "type": "torch.Tensor", "text": ["In-place version of greater()."]}, {"name": "torch.Tensor.greater_equal()", "path": "tensors#torch.Tensor.greater_equal", "type": "torch.Tensor", "text": ["See torch.greater_equal()."]}, {"name": "torch.Tensor.greater_equal_()", "path": "tensors#torch.Tensor.greater_equal_", "type": "torch.Tensor", "text": ["In-place version of greater_equal()."]}, {"name": "torch.Tensor.gt()", "path": "tensors#torch.Tensor.gt", "type": "torch.Tensor", "text": ["See torch.gt()."]}, {"name": "torch.Tensor.gt_()", "path": "tensors#torch.Tensor.gt_", "type": "torch.Tensor", "text": ["In-place version of gt()."]}, {"name": "torch.Tensor.half()", "path": "tensors#torch.Tensor.half", "type": "torch.Tensor", "text": ["self.half() is equivalent to self.to(torch.float16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.hardshrink()", "path": "tensors#torch.Tensor.hardshrink", "type": "torch.Tensor", "text": ["See torch.nn.functional.hardshrink()"]}, {"name": "torch.Tensor.heaviside()", "path": "tensors#torch.Tensor.heaviside", "type": "torch.Tensor", "text": ["See torch.heaviside()"]}, {"name": "torch.Tensor.histc()", "path": "tensors#torch.Tensor.histc", "type": "torch.Tensor", "text": ["See torch.histc()"]}, {"name": "torch.Tensor.hypot()", "path": "tensors#torch.Tensor.hypot", "type": "torch.Tensor", "text": ["See torch.hypot()"]}, {"name": "torch.Tensor.hypot_()", "path": "tensors#torch.Tensor.hypot_", "type": "torch.Tensor", "text": ["In-place version of hypot()"]}, {"name": "torch.Tensor.i0()", "path": "tensors#torch.Tensor.i0", "type": "torch.Tensor", "text": ["See torch.i0()"]}, {"name": "torch.Tensor.i0_()", "path": "tensors#torch.Tensor.i0_", "type": "torch.Tensor", "text": ["In-place version of i0()"]}, {"name": "torch.Tensor.igamma()", "path": "tensors#torch.Tensor.igamma", "type": "torch.Tensor", "text": ["See torch.igamma()"]}, {"name": "torch.Tensor.igammac()", "path": "tensors#torch.Tensor.igammac", "type": "torch.Tensor", "text": ["See torch.igammac()"]}, {"name": "torch.Tensor.igammac_()", "path": "tensors#torch.Tensor.igammac_", "type": "torch.Tensor", "text": ["In-place version of igammac()"]}, {"name": "torch.Tensor.igamma_()", "path": "tensors#torch.Tensor.igamma_", "type": "torch.Tensor", "text": ["In-place version of igamma()"]}, {"name": "torch.Tensor.imag", "path": "tensors#torch.Tensor.imag", "type": "torch.Tensor", "text": ["Returns a new tensor containing imaginary values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "imag() is only supported for tensors with complex dtypes."]}, {"name": "torch.Tensor.index_add()", "path": "tensors#torch.Tensor.index_add", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.index_add_(). tensor1 corresponds to self in torch.Tensor.index_add_()."]}, {"name": "torch.Tensor.index_add_()", "path": "tensors#torch.Tensor.index_add_", "type": "torch.Tensor", "text": ["Accumulate the elements of tensor into the self tensor by adding to the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is added to the jth row of self.", "The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Example:"]}, {"name": "torch.Tensor.index_copy()", "path": "tensors#torch.Tensor.index_copy", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.index_copy_(). tensor1 corresponds to self in torch.Tensor.index_copy_()."]}, {"name": "torch.Tensor.index_copy_()", "path": "tensors#torch.Tensor.index_copy_", "type": "torch.Tensor", "text": ["Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self.", "The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.", "Note", "If index contains duplicate entries, multiple elements from tensor will be copied to the same index of self. The result is nondeterministic since it depends on which copy occurs last.", "Example:"]}, {"name": "torch.Tensor.index_fill()", "path": "tensors#torch.Tensor.index_fill", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.index_fill_(). tensor1 corresponds to self in torch.Tensor.index_fill_()."]}, {"name": "torch.Tensor.index_fill_()", "path": "tensors#torch.Tensor.index_fill_", "type": "torch.Tensor", "text": ["Fills the elements of the self tensor with value val by selecting the indices in the order given in index."]}, {"name": "torch.Tensor.index_put()", "path": "tensors#torch.Tensor.index_put", "type": "torch.Tensor", "text": ["Out-place version of index_put_(). tensor1 corresponds to self in torch.Tensor.index_put_()."]}, {"name": "torch.Tensor.index_put_()", "path": "tensors#torch.Tensor.index_put_", "type": "torch.Tensor", "text": ["Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, values) is equivalent to tensor[indices] = values. Returns self.", "If accumulate is True, the elements in values are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements."]}, {"name": "torch.Tensor.index_select()", "path": "tensors#torch.Tensor.index_select", "type": "torch.Tensor", "text": ["See torch.index_select()"]}, {"name": "torch.Tensor.indices()", "path": "sparse#torch.Tensor.indices", "type": "torch.sparse", "text": ["Return the indices tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.values().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details."]}, {"name": "torch.Tensor.inner()", "path": "tensors#torch.Tensor.inner", "type": "torch.Tensor", "text": ["See torch.inner()."]}, {"name": "torch.Tensor.int()", "path": "tensors#torch.Tensor.int", "type": "torch.Tensor", "text": ["self.int() is equivalent to self.to(torch.int32). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.int_repr()", "path": "tensors#torch.Tensor.int_repr", "type": "torch.Tensor", "text": ["Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."]}, {"name": "torch.Tensor.inverse()", "path": "tensors#torch.Tensor.inverse", "type": "torch.Tensor", "text": ["See torch.inverse()"]}, {"name": "torch.Tensor.isclose()", "path": "tensors#torch.Tensor.isclose", "type": "torch.Tensor", "text": ["See torch.isclose()"]}, {"name": "torch.Tensor.isfinite()", "path": "tensors#torch.Tensor.isfinite", "type": "torch.Tensor", "text": ["See torch.isfinite()"]}, {"name": "torch.Tensor.isinf()", "path": "tensors#torch.Tensor.isinf", "type": "torch.Tensor", "text": ["See torch.isinf()"]}, {"name": "torch.Tensor.isnan()", "path": "tensors#torch.Tensor.isnan", "type": "torch.Tensor", "text": ["See torch.isnan()"]}, {"name": "torch.Tensor.isneginf()", "path": "tensors#torch.Tensor.isneginf", "type": "torch.Tensor", "text": ["See torch.isneginf()"]}, {"name": "torch.Tensor.isposinf()", "path": "tensors#torch.Tensor.isposinf", "type": "torch.Tensor", "text": ["See torch.isposinf()"]}, {"name": "torch.Tensor.isreal()", "path": "tensors#torch.Tensor.isreal", "type": "torch.Tensor", "text": ["See torch.isreal()"]}, {"name": "torch.Tensor.istft()", "path": "tensors#torch.Tensor.istft", "type": "torch.Tensor", "text": ["See torch.istft()"]}, {"name": "torch.Tensor.is_coalesced()", "path": "sparse#torch.Tensor.is_coalesced", "type": "torch.sparse", "text": ["Returns True if self is a sparse COO tensor that is coalesced, False otherwise.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See coalesce() and uncoalesced tensors."]}, {"name": "torch.Tensor.is_complex()", "path": "tensors#torch.Tensor.is_complex", "type": "torch.Tensor", "text": ["Returns True if the data type of self is a complex data type."]}, {"name": "torch.Tensor.is_contiguous()", "path": "tensors#torch.Tensor.is_contiguous", "type": "torch.Tensor", "text": ["Returns True if self tensor is contiguous in memory in the order specified by memory format.", "memory_format (torch.memory_format, optional) \u2013 Specifies memory allocation order. Default: torch.contiguous_format."]}, {"name": "torch.Tensor.is_cuda", "path": "tensors#torch.Tensor.is_cuda", "type": "torch.Tensor", "text": ["Is True if the Tensor is stored on the GPU, False otherwise."]}, {"name": "torch.Tensor.is_floating_point()", "path": "tensors#torch.Tensor.is_floating_point", "type": "torch.Tensor", "text": ["Returns True if the data type of self is a floating point data type."]}, {"name": "torch.Tensor.is_leaf", "path": "autograd#torch.Tensor.is_leaf", "type": "torch.autograd", "text": ["All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None.", "Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad().", "Example:"]}, {"name": "torch.Tensor.is_meta", "path": "tensors#torch.Tensor.is_meta", "type": "torch.Tensor", "text": ["Is True if the Tensor is a meta tensor, False otherwise. Meta tensors are like normal tensors, but they carry no data."]}, {"name": "torch.Tensor.is_pinned()", "path": "tensors#torch.Tensor.is_pinned", "type": "torch.Tensor", "text": ["Returns true if this tensor resides in pinned memory."]}, {"name": "torch.Tensor.is_quantized", "path": "tensors#torch.Tensor.is_quantized", "type": "torch.Tensor", "text": ["Is True if the Tensor is quantized, False otherwise."]}, {"name": "torch.Tensor.is_set_to()", "path": "tensors#torch.Tensor.is_set_to", "type": "torch.Tensor", "text": ["Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride)."]}, {"name": "torch.Tensor.is_shared()", "path": "tensors#torch.Tensor.is_shared", "type": "torch.Tensor", "text": ["Checks if tensor is in shared memory.", "This is always True for CUDA tensors."]}, {"name": "torch.Tensor.is_signed()", "path": "tensors#torch.Tensor.is_signed", "type": "torch.Tensor", "text": ["Returns True if the data type of self is a signed data type."]}, {"name": "torch.Tensor.is_sparse", "path": "sparse#torch.Tensor.is_sparse", "type": "torch.sparse", "text": ["Is True if the Tensor uses sparse storage layout, False otherwise."]}, {"name": "torch.Tensor.item()", "path": "tensors#torch.Tensor.item", "type": "torch.Tensor", "text": ["Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist().", "This operation is not differentiable.", "Example:"]}, {"name": "torch.Tensor.kthvalue()", "path": "tensors#torch.Tensor.kthvalue", "type": "torch.Tensor", "text": ["See torch.kthvalue()"]}, {"name": "torch.Tensor.lcm()", "path": "tensors#torch.Tensor.lcm", "type": "torch.Tensor", "text": ["See torch.lcm()"]}, {"name": "torch.Tensor.lcm_()", "path": "tensors#torch.Tensor.lcm_", "type": "torch.Tensor", "text": ["In-place version of lcm()"]}, {"name": "torch.Tensor.ldexp()", "path": "tensors#torch.Tensor.ldexp", "type": "torch.Tensor", "text": ["See torch.ldexp()"]}, {"name": "torch.Tensor.ldexp_()", "path": "tensors#torch.Tensor.ldexp_", "type": "torch.Tensor", "text": ["In-place version of ldexp()"]}, {"name": "torch.Tensor.le()", "path": "tensors#torch.Tensor.le", "type": "torch.Tensor", "text": ["See torch.le()."]}, {"name": "torch.Tensor.lerp()", "path": "tensors#torch.Tensor.lerp", "type": "torch.Tensor", "text": ["See torch.lerp()"]}, {"name": "torch.Tensor.lerp_()", "path": "tensors#torch.Tensor.lerp_", "type": "torch.Tensor", "text": ["In-place version of lerp()"]}, {"name": "torch.Tensor.less()", "path": "tensors#torch.Tensor.less", "type": "torch.Tensor", "text": ["lt(other) -> Tensor", "See torch.less()."]}, {"name": "torch.Tensor.less_()", "path": "tensors#torch.Tensor.less_", "type": "torch.Tensor", "text": ["In-place version of less()."]}, {"name": "torch.Tensor.less_equal()", "path": "tensors#torch.Tensor.less_equal", "type": "torch.Tensor", "text": ["See torch.less_equal()."]}, {"name": "torch.Tensor.less_equal_()", "path": "tensors#torch.Tensor.less_equal_", "type": "torch.Tensor", "text": ["In-place version of less_equal()."]}, {"name": "torch.Tensor.le_()", "path": "tensors#torch.Tensor.le_", "type": "torch.Tensor", "text": ["In-place version of le()."]}, {"name": "torch.Tensor.lgamma()", "path": "tensors#torch.Tensor.lgamma", "type": "torch.Tensor", "text": ["See torch.lgamma()"]}, {"name": "torch.Tensor.lgamma_()", "path": "tensors#torch.Tensor.lgamma_", "type": "torch.Tensor", "text": ["In-place version of lgamma()"]}, {"name": "torch.Tensor.log()", "path": "tensors#torch.Tensor.log", "type": "torch.Tensor", "text": ["See torch.log()"]}, {"name": "torch.Tensor.log10()", "path": "tensors#torch.Tensor.log10", "type": "torch.Tensor", "text": ["See torch.log10()"]}, {"name": "torch.Tensor.log10_()", "path": "tensors#torch.Tensor.log10_", "type": "torch.Tensor", "text": ["In-place version of log10()"]}, {"name": "torch.Tensor.log1p()", "path": "tensors#torch.Tensor.log1p", "type": "torch.Tensor", "text": ["See torch.log1p()"]}, {"name": "torch.Tensor.log1p_()", "path": "tensors#torch.Tensor.log1p_", "type": "torch.Tensor", "text": ["In-place version of log1p()"]}, {"name": "torch.Tensor.log2()", "path": "tensors#torch.Tensor.log2", "type": "torch.Tensor", "text": ["See torch.log2()"]}, {"name": "torch.Tensor.log2_()", "path": "tensors#torch.Tensor.log2_", "type": "torch.Tensor", "text": ["In-place version of log2()"]}, {"name": "torch.Tensor.logaddexp()", "path": "tensors#torch.Tensor.logaddexp", "type": "torch.Tensor", "text": ["See torch.logaddexp()"]}, {"name": "torch.Tensor.logaddexp2()", "path": "tensors#torch.Tensor.logaddexp2", "type": "torch.Tensor", "text": ["See torch.logaddexp2()"]}, {"name": "torch.Tensor.logcumsumexp()", "path": "tensors#torch.Tensor.logcumsumexp", "type": "torch.Tensor", "text": ["See torch.logcumsumexp()"]}, {"name": "torch.Tensor.logdet()", "path": "tensors#torch.Tensor.logdet", "type": "torch.Tensor", "text": ["See torch.logdet()"]}, {"name": "torch.Tensor.logical_and()", "path": "tensors#torch.Tensor.logical_and", "type": "torch.Tensor", "text": ["See torch.logical_and()"]}, {"name": "torch.Tensor.logical_and_()", "path": "tensors#torch.Tensor.logical_and_", "type": "torch.Tensor", "text": ["In-place version of logical_and()"]}, {"name": "torch.Tensor.logical_not()", "path": "tensors#torch.Tensor.logical_not", "type": "torch.Tensor", "text": ["See torch.logical_not()"]}, {"name": "torch.Tensor.logical_not_()", "path": "tensors#torch.Tensor.logical_not_", "type": "torch.Tensor", "text": ["In-place version of logical_not()"]}, {"name": "torch.Tensor.logical_or()", "path": "tensors#torch.Tensor.logical_or", "type": "torch.Tensor", "text": ["See torch.logical_or()"]}, {"name": "torch.Tensor.logical_or_()", "path": "tensors#torch.Tensor.logical_or_", "type": "torch.Tensor", "text": ["In-place version of logical_or()"]}, {"name": "torch.Tensor.logical_xor()", "path": "tensors#torch.Tensor.logical_xor", "type": "torch.Tensor", "text": ["See torch.logical_xor()"]}, {"name": "torch.Tensor.logical_xor_()", "path": "tensors#torch.Tensor.logical_xor_", "type": "torch.Tensor", "text": ["In-place version of logical_xor()"]}, {"name": "torch.Tensor.logit()", "path": "tensors#torch.Tensor.logit", "type": "torch.Tensor", "text": ["See torch.logit()"]}, {"name": "torch.Tensor.logit_()", "path": "tensors#torch.Tensor.logit_", "type": "torch.Tensor", "text": ["In-place version of logit()"]}, {"name": "torch.Tensor.logsumexp()", "path": "tensors#torch.Tensor.logsumexp", "type": "torch.Tensor", "text": ["See torch.logsumexp()"]}, {"name": "torch.Tensor.log_()", "path": "tensors#torch.Tensor.log_", "type": "torch.Tensor", "text": ["In-place version of log()"]}, {"name": "torch.Tensor.log_normal_()", "path": "tensors#torch.Tensor.log_normal_", "type": "torch.Tensor", "text": ["Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu  and standard deviation \u03c3\\sigma . Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:"]}, {"name": "torch.Tensor.long()", "path": "tensors#torch.Tensor.long", "type": "torch.Tensor", "text": ["self.long() is equivalent to self.to(torch.int64). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.lstsq()", "path": "tensors#torch.Tensor.lstsq", "type": "torch.Tensor", "text": ["See torch.lstsq()"]}, {"name": "torch.Tensor.lt()", "path": "tensors#torch.Tensor.lt", "type": "torch.Tensor", "text": ["See torch.lt()."]}, {"name": "torch.Tensor.lt_()", "path": "tensors#torch.Tensor.lt_", "type": "torch.Tensor", "text": ["In-place version of lt()."]}, {"name": "torch.Tensor.lu()", "path": "tensors#torch.Tensor.lu", "type": "torch.Tensor", "text": ["See torch.lu()"]}, {"name": "torch.Tensor.lu_solve()", "path": "tensors#torch.Tensor.lu_solve", "type": "torch.Tensor", "text": ["See torch.lu_solve()"]}, {"name": "torch.Tensor.map_()", "path": "tensors#torch.Tensor.map_", "type": "torch.Tensor", "text": ["Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable.", "The callable should have the signature:"]}, {"name": "torch.Tensor.masked_fill()", "path": "tensors#torch.Tensor.masked_fill", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.masked_fill_()"]}, {"name": "torch.Tensor.masked_fill_()", "path": "tensors#torch.Tensor.masked_fill_", "type": "torch.Tensor", "text": ["Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor."]}, {"name": "torch.Tensor.masked_scatter()", "path": "tensors#torch.Tensor.masked_scatter", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.masked_scatter_()"]}, {"name": "torch.Tensor.masked_scatter_()", "path": "tensors#torch.Tensor.masked_scatter_", "type": "torch.Tensor", "text": ["Copies elements from source into self tensor at positions where the mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask", "Note", "The mask operates on the self tensor, not on the given source tensor."]}, {"name": "torch.Tensor.masked_select()", "path": "tensors#torch.Tensor.masked_select", "type": "torch.Tensor", "text": ["See torch.masked_select()"]}, {"name": "torch.Tensor.matmul()", "path": "tensors#torch.Tensor.matmul", "type": "torch.Tensor", "text": ["See torch.matmul()"]}, {"name": "torch.Tensor.matrix_exp()", "path": "tensors#torch.Tensor.matrix_exp", "type": "torch.Tensor", "text": ["See torch.matrix_exp()"]}, {"name": "torch.Tensor.matrix_power()", "path": "tensors#torch.Tensor.matrix_power", "type": "torch.Tensor", "text": ["See torch.matrix_power()"]}, {"name": "torch.Tensor.max()", "path": "tensors#torch.Tensor.max", "type": "torch.Tensor", "text": ["See torch.max()"]}, {"name": "torch.Tensor.maximum()", "path": "tensors#torch.Tensor.maximum", "type": "torch.Tensor", "text": ["See torch.maximum()"]}, {"name": "torch.Tensor.mean()", "path": "tensors#torch.Tensor.mean", "type": "torch.Tensor", "text": ["See torch.mean()"]}, {"name": "torch.Tensor.median()", "path": "tensors#torch.Tensor.median", "type": "torch.Tensor", "text": ["See torch.median()"]}, {"name": "torch.Tensor.min()", "path": "tensors#torch.Tensor.min", "type": "torch.Tensor", "text": ["See torch.min()"]}, {"name": "torch.Tensor.minimum()", "path": "tensors#torch.Tensor.minimum", "type": "torch.Tensor", "text": ["See torch.minimum()"]}, {"name": "torch.Tensor.mm()", "path": "tensors#torch.Tensor.mm", "type": "torch.Tensor", "text": ["See torch.mm()"]}, {"name": "torch.Tensor.mode()", "path": "tensors#torch.Tensor.mode", "type": "torch.Tensor", "text": ["See torch.mode()"]}, {"name": "torch.Tensor.moveaxis()", "path": "tensors#torch.Tensor.moveaxis", "type": "torch.Tensor", "text": ["See torch.moveaxis()"]}, {"name": "torch.Tensor.movedim()", "path": "tensors#torch.Tensor.movedim", "type": "torch.Tensor", "text": ["See torch.movedim()"]}, {"name": "torch.Tensor.msort()", "path": "tensors#torch.Tensor.msort", "type": "torch.Tensor", "text": ["See torch.msort()"]}, {"name": "torch.Tensor.mul()", "path": "tensors#torch.Tensor.mul", "type": "torch.Tensor", "text": ["See torch.mul()."]}, {"name": "torch.Tensor.multinomial()", "path": "tensors#torch.Tensor.multinomial", "type": "torch.Tensor", "text": ["See torch.multinomial()"]}, {"name": "torch.Tensor.multiply()", "path": "tensors#torch.Tensor.multiply", "type": "torch.Tensor", "text": ["See torch.multiply()."]}, {"name": "torch.Tensor.multiply_()", "path": "tensors#torch.Tensor.multiply_", "type": "torch.Tensor", "text": ["In-place version of multiply()."]}, {"name": "torch.Tensor.mul_()", "path": "tensors#torch.Tensor.mul_", "type": "torch.Tensor", "text": ["In-place version of mul()."]}, {"name": "torch.Tensor.mv()", "path": "tensors#torch.Tensor.mv", "type": "torch.Tensor", "text": ["See torch.mv()"]}, {"name": "torch.Tensor.mvlgamma()", "path": "tensors#torch.Tensor.mvlgamma", "type": "torch.Tensor", "text": ["See torch.mvlgamma()"]}, {"name": "torch.Tensor.mvlgamma_()", "path": "tensors#torch.Tensor.mvlgamma_", "type": "torch.Tensor", "text": ["In-place version of mvlgamma()"]}, {"name": "torch.Tensor.names", "path": "named_tensor#torch.Tensor.names", "type": "Named Tensors", "text": ["Stores names for each of this tensor\u2019s dimensions.", "names[idx] corresponds to the name of tensor dimension idx. Names are either a string if the dimension is named or None if the dimension is unnamed.", "Dimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).", "Tensors may not have two named dimensions with the same name.", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.nanmedian()", "path": "tensors#torch.Tensor.nanmedian", "type": "torch.Tensor", "text": ["See torch.nanmedian()"]}, {"name": "torch.Tensor.nanquantile()", "path": "tensors#torch.Tensor.nanquantile", "type": "torch.Tensor", "text": ["See torch.nanquantile()"]}, {"name": "torch.Tensor.nansum()", "path": "tensors#torch.Tensor.nansum", "type": "torch.Tensor", "text": ["See torch.nansum()"]}, {"name": "torch.Tensor.nan_to_num()", "path": "tensors#torch.Tensor.nan_to_num", "type": "torch.Tensor", "text": ["See torch.nan_to_num()."]}, {"name": "torch.Tensor.nan_to_num_()", "path": "tensors#torch.Tensor.nan_to_num_", "type": "torch.Tensor", "text": ["In-place version of nan_to_num()."]}, {"name": "torch.Tensor.narrow()", "path": "tensors#torch.Tensor.narrow", "type": "torch.Tensor", "text": ["See torch.narrow()", "Example:"]}, {"name": "torch.Tensor.narrow_copy()", "path": "tensors#torch.Tensor.narrow_copy", "type": "torch.Tensor", "text": ["Same as Tensor.narrow() except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling `narrow_copy with `dimemsion > self.sparse_dim()` will return a copy with the relevant dense dimension narrowed, and `self.shape` updated accordingly."]}, {"name": "torch.Tensor.ndim", "path": "tensors#torch.Tensor.ndim", "type": "torch.Tensor", "text": ["Alias for dim()"]}, {"name": "torch.Tensor.ndimension()", "path": "tensors#torch.Tensor.ndimension", "type": "torch.Tensor", "text": ["Alias for dim()"]}, {"name": "torch.Tensor.ne()", "path": "tensors#torch.Tensor.ne", "type": "torch.Tensor", "text": ["See torch.ne()."]}, {"name": "torch.Tensor.neg()", "path": "tensors#torch.Tensor.neg", "type": "torch.Tensor", "text": ["See torch.neg()"]}, {"name": "torch.Tensor.negative()", "path": "tensors#torch.Tensor.negative", "type": "torch.Tensor", "text": ["See torch.negative()"]}, {"name": "torch.Tensor.negative_()", "path": "tensors#torch.Tensor.negative_", "type": "torch.Tensor", "text": ["In-place version of negative()"]}, {"name": "torch.Tensor.neg_()", "path": "tensors#torch.Tensor.neg_", "type": "torch.Tensor", "text": ["In-place version of neg()"]}, {"name": "torch.Tensor.nelement()", "path": "tensors#torch.Tensor.nelement", "type": "torch.Tensor", "text": ["Alias for numel()"]}, {"name": "torch.Tensor.new_empty()", "path": "tensors#torch.Tensor.new_empty", "type": "torch.Tensor", "text": ["Returns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:"]}, {"name": "torch.Tensor.new_full()", "path": "tensors#torch.Tensor.new_full", "type": "torch.Tensor", "text": ["Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:"]}, {"name": "torch.Tensor.new_ones()", "path": "tensors#torch.Tensor.new_ones", "type": "torch.Tensor", "text": ["Returns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:"]}, {"name": "torch.Tensor.new_tensor()", "path": "tensors#torch.Tensor.new_tensor", "type": "torch.Tensor", "text": ["Returns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Warning", "new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().", "Warning", "When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.", "Example:"]}, {"name": "torch.Tensor.new_zeros()", "path": "tensors#torch.Tensor.new_zeros", "type": "torch.Tensor", "text": ["Returns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.", "Example:"]}, {"name": "torch.Tensor.nextafter()", "path": "tensors#torch.Tensor.nextafter", "type": "torch.Tensor", "text": ["See torch.nextafter()"]}, {"name": "torch.Tensor.nextafter_()", "path": "tensors#torch.Tensor.nextafter_", "type": "torch.Tensor", "text": ["In-place version of nextafter()"]}, {"name": "torch.Tensor.ne_()", "path": "tensors#torch.Tensor.ne_", "type": "torch.Tensor", "text": ["In-place version of ne()."]}, {"name": "torch.Tensor.nonzero()", "path": "tensors#torch.Tensor.nonzero", "type": "torch.Tensor", "text": ["See torch.nonzero()"]}, {"name": "torch.Tensor.norm()", "path": "tensors#torch.Tensor.norm", "type": "torch.Tensor", "text": ["See torch.norm()"]}, {"name": "torch.Tensor.normal_()", "path": "tensors#torch.Tensor.normal_", "type": "torch.Tensor", "text": ["Fills self tensor with elements samples from the normal distribution parameterized by mean and std."]}, {"name": "torch.Tensor.not_equal()", "path": "tensors#torch.Tensor.not_equal", "type": "torch.Tensor", "text": ["See torch.not_equal()."]}, {"name": "torch.Tensor.not_equal_()", "path": "tensors#torch.Tensor.not_equal_", "type": "torch.Tensor", "text": ["In-place version of not_equal()."]}, {"name": "torch.Tensor.numel()", "path": "tensors#torch.Tensor.numel", "type": "torch.Tensor", "text": ["See torch.numel()"]}, {"name": "torch.Tensor.numpy()", "path": "tensors#torch.Tensor.numpy", "type": "torch.Tensor", "text": ["Returns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage. Changes to self tensor will be reflected in the ndarray and vice versa."]}, {"name": "torch.Tensor.orgqr()", "path": "tensors#torch.Tensor.orgqr", "type": "torch.Tensor", "text": ["See torch.orgqr()"]}, {"name": "torch.Tensor.ormqr()", "path": "tensors#torch.Tensor.ormqr", "type": "torch.Tensor", "text": ["See torch.ormqr()"]}, {"name": "torch.Tensor.outer()", "path": "tensors#torch.Tensor.outer", "type": "torch.Tensor", "text": ["See torch.outer()."]}, {"name": "torch.Tensor.permute()", "path": "tensors#torch.Tensor.permute", "type": "torch.Tensor", "text": ["Returns a view of the original tensor with its dimensions permuted.", "*dims (int...) \u2013 The desired ordering of dimensions"]}, {"name": "torch.Tensor.pinverse()", "path": "tensors#torch.Tensor.pinverse", "type": "torch.Tensor", "text": ["See torch.pinverse()"]}, {"name": "torch.Tensor.pin_memory()", "path": "tensors#torch.Tensor.pin_memory", "type": "torch.Tensor", "text": ["Copies the tensor to pinned memory, if it\u2019s not already pinned."]}, {"name": "torch.Tensor.polygamma()", "path": "tensors#torch.Tensor.polygamma", "type": "torch.Tensor", "text": ["See torch.polygamma()"]}, {"name": "torch.Tensor.polygamma_()", "path": "tensors#torch.Tensor.polygamma_", "type": "torch.Tensor", "text": ["In-place version of polygamma()"]}, {"name": "torch.Tensor.pow()", "path": "tensors#torch.Tensor.pow", "type": "torch.Tensor", "text": ["See torch.pow()"]}, {"name": "torch.Tensor.pow_()", "path": "tensors#torch.Tensor.pow_", "type": "torch.Tensor", "text": ["In-place version of pow()"]}, {"name": "torch.Tensor.prod()", "path": "tensors#torch.Tensor.prod", "type": "torch.Tensor", "text": ["See torch.prod()"]}, {"name": "torch.Tensor.put_()", "path": "tensors#torch.Tensor.put_", "type": "torch.Tensor", "text": ["Copies the elements from tensor into the positions specified by indices. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor.", "If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.", "Example:"]}, {"name": "torch.Tensor.qr()", "path": "tensors#torch.Tensor.qr", "type": "torch.Tensor", "text": ["See torch.qr()"]}, {"name": "torch.Tensor.qscheme()", "path": "tensors#torch.Tensor.qscheme", "type": "torch.Tensor", "text": ["Returns the quantization scheme of a given QTensor."]}, {"name": "torch.Tensor.quantile()", "path": "tensors#torch.Tensor.quantile", "type": "torch.Tensor", "text": ["See torch.quantile()"]}, {"name": "torch.Tensor.q_per_channel_axis()", "path": "tensors#torch.Tensor.q_per_channel_axis", "type": "torch.Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."]}, {"name": "torch.Tensor.q_per_channel_scales()", "path": "tensors#torch.Tensor.q_per_channel_scales", "type": "torch.Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor."]}, {"name": "torch.Tensor.q_per_channel_zero_points()", "path": "tensors#torch.Tensor.q_per_channel_zero_points", "type": "torch.Tensor", "text": ["Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor."]}, {"name": "torch.Tensor.q_scale()", "path": "tensors#torch.Tensor.q_scale", "type": "torch.Tensor", "text": ["Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer()."]}, {"name": "torch.Tensor.q_zero_point()", "path": "tensors#torch.Tensor.q_zero_point", "type": "torch.Tensor", "text": ["Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."]}, {"name": "torch.Tensor.rad2deg()", "path": "tensors#torch.Tensor.rad2deg", "type": "torch.Tensor", "text": ["See torch.rad2deg()"]}, {"name": "torch.Tensor.random_()", "path": "tensors#torch.Tensor.random_", "type": "torch.Tensor", "text": ["Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53]."]}, {"name": "torch.Tensor.ravel()", "path": "tensors#torch.Tensor.ravel", "type": "torch.Tensor", "text": ["see torch.ravel()"]}, {"name": "torch.Tensor.real", "path": "tensors#torch.Tensor.real", "type": "torch.Tensor", "text": ["Returns a new tensor containing real values of the self tensor. The returned tensor and self share the same underlying storage.", "Warning", "real() is only supported for tensors with complex dtypes."]}, {"name": "torch.Tensor.reciprocal()", "path": "tensors#torch.Tensor.reciprocal", "type": "torch.Tensor", "text": ["See torch.reciprocal()"]}, {"name": "torch.Tensor.reciprocal_()", "path": "tensors#torch.Tensor.reciprocal_", "type": "torch.Tensor", "text": ["In-place version of reciprocal()"]}, {"name": "torch.Tensor.record_stream()", "path": "tensors#torch.Tensor.record_stream", "type": "torch.Tensor", "text": ["Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.", "Note", "The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor."]}, {"name": "torch.Tensor.refine_names()", "path": "named_tensor#torch.Tensor.refine_names", "type": "Named Tensors", "text": ["Refines the dimension names of self according to names.", "Refining is a special case of renaming that \u201clifts\u201d unnamed dimensions. A None dim can be refined to have any name; a named dim can only be refined to have the same name.", "Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.", "names may contain up to one Ellipsis (...). The Ellipsis is expanded greedily; it is expanded in-place to fill names to the same length as self.dim() using names from the corresponding indices of self.names.", "Python 2 does not support Ellipsis but one may use a string literal instead ('...').", "names (iterable of str) \u2013 The desired names of the output tensor. May contain up to one Ellipsis.", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.register_hook()", "path": "autograd#torch.Tensor.register_hook", "type": "torch.autograd", "text": ["Registers a backward hook.", "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Example:"]}, {"name": "torch.Tensor.remainder()", "path": "tensors#torch.Tensor.remainder", "type": "torch.Tensor", "text": ["See torch.remainder()"]}, {"name": "torch.Tensor.remainder_()", "path": "tensors#torch.Tensor.remainder_", "type": "torch.Tensor", "text": ["In-place version of remainder()"]}, {"name": "torch.Tensor.rename()", "path": "named_tensor#torch.Tensor.rename", "type": "Named Tensors", "text": ["Renames dimension names of self.", "There are two main usages:", "self.rename(**rename_map) returns a view on tensor that has dims renamed as specified in the mapping rename_map.", "self.rename(*names) returns a view on tensor, renaming all dimensions positionally using names. Use self.rename(None) to drop names on a tensor.", "One cannot specify both positional args names and keyword args rename_map.", "Examples:", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.rename_()", "path": "named_tensor#torch.Tensor.rename_", "type": "Named Tensors", "text": ["In-place version of rename()."]}, {"name": "torch.Tensor.renorm()", "path": "tensors#torch.Tensor.renorm", "type": "torch.Tensor", "text": ["See torch.renorm()"]}, {"name": "torch.Tensor.renorm_()", "path": "tensors#torch.Tensor.renorm_", "type": "torch.Tensor", "text": ["In-place version of renorm()"]}, {"name": "torch.Tensor.repeat()", "path": "tensors#torch.Tensor.repeat", "type": "torch.Tensor", "text": ["Repeats this tensor along the specified dimensions.", "Unlike expand(), this function copies the tensor\u2019s data.", "Warning", "repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().", "sizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along each dimension", "Example:"]}, {"name": "torch.Tensor.repeat_interleave()", "path": "tensors#torch.Tensor.repeat_interleave", "type": "torch.Tensor", "text": ["See torch.repeat_interleave()."]}, {"name": "torch.Tensor.requires_grad", "path": "autograd#torch.Tensor.requires_grad", "type": "torch.autograd", "text": ["Is True if gradients need to be computed for this Tensor, False otherwise.", "Note", "The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details."]}, {"name": "torch.Tensor.requires_grad_()", "path": "tensors#torch.Tensor.requires_grad_", "type": "torch.Tensor", "text": ["Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor.", "requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.", "requires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.", "Example:"]}, {"name": "torch.Tensor.reshape()", "path": "tensors#torch.Tensor.reshape", "type": "torch.Tensor", "text": ["Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "See torch.reshape()", "shape (tuple of python:ints or int...) \u2013 the desired shape"]}, {"name": "torch.Tensor.reshape_as()", "path": "tensors#torch.Tensor.reshape_as", "type": "torch.Tensor", "text": ["Returns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view.", "Please see reshape() for more information about reshape.", "other (torch.Tensor) \u2013 The result tensor has the same shape as other."]}, {"name": "torch.Tensor.resize_()", "path": "tensors#torch.Tensor.resize_", "type": "torch.Tensor", "text": ["Resizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.", "Warning", "This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().", "Example:"]}, {"name": "torch.Tensor.resize_as_()", "path": "tensors#torch.Tensor.resize_as_", "type": "torch.Tensor", "text": ["Resizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of Tensor. Default: torch.contiguous_format. Note that memory format of self is going to be unaffected if self.size() matches tensor.size()."]}, {"name": "torch.Tensor.retain_grad()", "path": "autograd#torch.Tensor.retain_grad", "type": "torch.autograd", "text": ["Enables .grad attribute for non-leaf Tensors."]}, {"name": "torch.Tensor.roll()", "path": "tensors#torch.Tensor.roll", "type": "torch.Tensor", "text": ["See torch.roll()"]}, {"name": "torch.Tensor.rot90()", "path": "tensors#torch.Tensor.rot90", "type": "torch.Tensor", "text": ["See torch.rot90()"]}, {"name": "torch.Tensor.round()", "path": "tensors#torch.Tensor.round", "type": "torch.Tensor", "text": ["See torch.round()"]}, {"name": "torch.Tensor.round_()", "path": "tensors#torch.Tensor.round_", "type": "torch.Tensor", "text": ["In-place version of round()"]}, {"name": "torch.Tensor.rsqrt()", "path": "tensors#torch.Tensor.rsqrt", "type": "torch.Tensor", "text": ["See torch.rsqrt()"]}, {"name": "torch.Tensor.rsqrt_()", "path": "tensors#torch.Tensor.rsqrt_", "type": "torch.Tensor", "text": ["In-place version of rsqrt()"]}, {"name": "torch.Tensor.scatter()", "path": "tensors#torch.Tensor.scatter", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_()"]}, {"name": "torch.Tensor.scatter_()", "path": "tensors#torch.Tensor.scatter_", "type": "torch.Tensor", "text": ["Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "This is the reverse operation of the manner described in gather().", "self, index and src (if it is a Tensor) should all have the same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive.", "Warning", "When indices are not unique, the behavior is non-deterministic (one of the values from src will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Additionally accepts an optional reduce argument that allows specification of an optional reduction operation, which is applied to all values in the tensor src into self at the indicies specified in the index. For each value in src, the reduction operation is applied to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "Given a 3-D tensor and reduction using the multiplication operation, self is updated as:", "Reducing with the addition operation is the same as using scatter_add_().", "Example:"]}, {"name": "torch.Tensor.scatter_add()", "path": "tensors#torch.Tensor.scatter_add", "type": "torch.Tensor", "text": ["Out-of-place version of torch.Tensor.scatter_add_()"]}, {"name": "torch.Tensor.scatter_add_()", "path": "tensors#torch.Tensor.scatter_add_", "type": "torch.Tensor", "text": ["Adds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in src, it is added to an index in self which is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.", "For a 3-D tensor, self is updated as:", "self, index and src should have same number of dimensions. It is also required that index.size(d) <= src.size(d) for all dimensions d, and that index.size(d) <= self.size(d) for all dimensions d != dim. Note that index and src do not broadcast.", "Note", "This operation may behave nondeterministically when given tensors on a CUDA device. See Reproducibility for more information.", "Note", "The backward pass is implemented only for src.shape == index.shape.", "Example:"]}, {"name": "torch.Tensor.select()", "path": "tensors#torch.Tensor.select", "type": "torch.Tensor", "text": ["Slices the self tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.", "Note", "select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index]."]}, {"name": "torch.Tensor.set_()", "path": "tensors#torch.Tensor.set_", "type": "torch.Tensor", "text": ["Sets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other.", "If source is a Storage, the method sets the underlying storage, offset, size, and stride."]}, {"name": "torch.Tensor.sgn()", "path": "tensors#torch.Tensor.sgn", "type": "torch.Tensor", "text": ["See torch.sgn()"]}, {"name": "torch.Tensor.sgn_()", "path": "tensors#torch.Tensor.sgn_", "type": "torch.Tensor", "text": ["In-place version of sgn()"]}, {"name": "torch.Tensor.share_memory_()", "path": "tensors#torch.Tensor.share_memory_", "type": "torch.Tensor", "text": ["Moves the underlying storage to shared memory.", "This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized."]}, {"name": "torch.Tensor.short()", "path": "tensors#torch.Tensor.short", "type": "torch.Tensor", "text": ["self.short() is equivalent to self.to(torch.int16). See to().", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format."]}, {"name": "torch.Tensor.sigmoid()", "path": "tensors#torch.Tensor.sigmoid", "type": "torch.Tensor", "text": ["See torch.sigmoid()"]}, {"name": "torch.Tensor.sigmoid_()", "path": "tensors#torch.Tensor.sigmoid_", "type": "torch.Tensor", "text": ["In-place version of sigmoid()"]}, {"name": "torch.Tensor.sign()", "path": "tensors#torch.Tensor.sign", "type": "torch.Tensor", "text": ["See torch.sign()"]}, {"name": "torch.Tensor.signbit()", "path": "tensors#torch.Tensor.signbit", "type": "torch.Tensor", "text": ["See torch.signbit()"]}, {"name": "torch.Tensor.sign_()", "path": "tensors#torch.Tensor.sign_", "type": "torch.Tensor", "text": ["In-place version of sign()"]}, {"name": "torch.Tensor.sin()", "path": "tensors#torch.Tensor.sin", "type": "torch.Tensor", "text": ["See torch.sin()"]}, {"name": "torch.Tensor.sinc()", "path": "tensors#torch.Tensor.sinc", "type": "torch.Tensor", "text": ["See torch.sinc()"]}, {"name": "torch.Tensor.sinc_()", "path": "tensors#torch.Tensor.sinc_", "type": "torch.Tensor", "text": ["In-place version of sinc()"]}, {"name": "torch.Tensor.sinh()", "path": "tensors#torch.Tensor.sinh", "type": "torch.Tensor", "text": ["See torch.sinh()"]}, {"name": "torch.Tensor.sinh_()", "path": "tensors#torch.Tensor.sinh_", "type": "torch.Tensor", "text": ["In-place version of sinh()"]}, {"name": "torch.Tensor.sin_()", "path": "tensors#torch.Tensor.sin_", "type": "torch.Tensor", "text": ["In-place version of sin()"]}, {"name": "torch.Tensor.size()", "path": "tensors#torch.Tensor.size", "type": "torch.Tensor", "text": ["Returns the size of the self tensor. The returned value is a subclass of tuple.", "Example:"]}, {"name": "torch.Tensor.slogdet()", "path": "tensors#torch.Tensor.slogdet", "type": "torch.Tensor", "text": ["See torch.slogdet()"]}, {"name": "torch.Tensor.solve()", "path": "tensors#torch.Tensor.solve", "type": "torch.Tensor", "text": ["See torch.solve()"]}, {"name": "torch.Tensor.sort()", "path": "tensors#torch.Tensor.sort", "type": "torch.Tensor", "text": ["See torch.sort()"]}, {"name": "torch.Tensor.sparse_dim()", "path": "sparse#torch.Tensor.sparse_dim", "type": "torch.sparse", "text": ["Return the number of sparse dimensions in a sparse tensor self.", "Warning", "Throws an error if self is not a sparse tensor.", "See also Tensor.dense_dim() and hybrid tensors."]}, {"name": "torch.Tensor.sparse_mask()", "path": "sparse#torch.Tensor.sparse_mask", "type": "torch.sparse", "text": ["Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. The values of mask sparse tensor are ignored. self and mask tensors must have the same shape.", "Note", "The returned sparse tensor has the same indices as the sparse tensor mask, even when the corresponding values in self are zeros.", "mask (Tensor) \u2013 a sparse tensor whose indices are used as a filter", "Example:"]}, {"name": "torch.Tensor.sparse_resize_()", "path": "sparse#torch.Tensor.sparse_resize_", "type": "torch.sparse", "text": ["Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions.", "Note", "If the number of specified elements in self is zero, then size, sparse_dim, and dense_dim can be any size and positive integers such that len(size) == sparse_dim +\ndense_dim.", "If self specifies one or more elements, however, then each dimension in size must not be smaller than the corresponding dimension of self, sparse_dim must equal the number of sparse dimensions in self, and dense_dim must equal the number of dense dimensions in self.", "Warning", "Throws an error if self is not a sparse tensor."]}, {"name": "torch.Tensor.sparse_resize_and_clear_()", "path": "sparse#torch.Tensor.sparse_resize_and_clear_", "type": "torch.sparse", "text": ["Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions."]}, {"name": "torch.Tensor.split()", "path": "tensors#torch.Tensor.split", "type": "torch.Tensor", "text": ["See torch.split()"]}, {"name": "torch.Tensor.sqrt()", "path": "tensors#torch.Tensor.sqrt", "type": "torch.Tensor", "text": ["See torch.sqrt()"]}, {"name": "torch.Tensor.sqrt_()", "path": "tensors#torch.Tensor.sqrt_", "type": "torch.Tensor", "text": ["In-place version of sqrt()"]}, {"name": "torch.Tensor.square()", "path": "tensors#torch.Tensor.square", "type": "torch.Tensor", "text": ["See torch.square()"]}, {"name": "torch.Tensor.square_()", "path": "tensors#torch.Tensor.square_", "type": "torch.Tensor", "text": ["In-place version of square()"]}, {"name": "torch.Tensor.squeeze()", "path": "tensors#torch.Tensor.squeeze", "type": "torch.Tensor", "text": ["See torch.squeeze()"]}, {"name": "torch.Tensor.squeeze_()", "path": "tensors#torch.Tensor.squeeze_", "type": "torch.Tensor", "text": ["In-place version of squeeze()"]}, {"name": "torch.Tensor.std()", "path": "tensors#torch.Tensor.std", "type": "torch.Tensor", "text": ["See torch.std()"]}, {"name": "torch.Tensor.stft()", "path": "tensors#torch.Tensor.stft", "type": "torch.Tensor", "text": ["See torch.stft()", "Warning", "This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result."]}, {"name": "torch.Tensor.storage()", "path": "tensors#torch.Tensor.storage", "type": "torch.Tensor", "text": ["Returns the underlying storage."]}, {"name": "torch.Tensor.storage_offset()", "path": "tensors#torch.Tensor.storage_offset", "type": "torch.Tensor", "text": ["Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes).", "Example:"]}, {"name": "torch.Tensor.storage_type()", "path": "tensors#torch.Tensor.storage_type", "type": "torch.Tensor", "text": ["Returns the type of the underlying storage."]}, {"name": "torch.Tensor.stride()", "path": "tensors#torch.Tensor.stride", "type": "torch.Tensor", "text": ["Returns the stride of self tensor.", "Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.", "dim (int, optional) \u2013 the desired dimension in which stride is required", "Example:"]}, {"name": "torch.Tensor.sub()", "path": "tensors#torch.Tensor.sub", "type": "torch.Tensor", "text": ["See torch.sub()."]}, {"name": "torch.Tensor.subtract()", "path": "tensors#torch.Tensor.subtract", "type": "torch.Tensor", "text": ["See torch.subtract()."]}, {"name": "torch.Tensor.subtract_()", "path": "tensors#torch.Tensor.subtract_", "type": "torch.Tensor", "text": ["In-place version of subtract()."]}, {"name": "torch.Tensor.sub_()", "path": "tensors#torch.Tensor.sub_", "type": "torch.Tensor", "text": ["In-place version of sub()"]}, {"name": "torch.Tensor.sum()", "path": "tensors#torch.Tensor.sum", "type": "torch.Tensor", "text": ["See torch.sum()"]}, {"name": "torch.Tensor.sum_to_size()", "path": "tensors#torch.Tensor.sum_to_size", "type": "torch.Tensor", "text": ["Sum this tensor to size. size must be broadcastable to this tensor size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor."]}, {"name": "torch.Tensor.svd()", "path": "tensors#torch.Tensor.svd", "type": "torch.Tensor", "text": ["See torch.svd()"]}, {"name": "torch.Tensor.swapaxes()", "path": "tensors#torch.Tensor.swapaxes", "type": "torch.Tensor", "text": ["See torch.swapaxes()"]}, {"name": "torch.Tensor.swapdims()", "path": "tensors#torch.Tensor.swapdims", "type": "torch.Tensor", "text": ["See torch.swapdims()"]}, {"name": "torch.Tensor.symeig()", "path": "tensors#torch.Tensor.symeig", "type": "torch.Tensor", "text": ["See torch.symeig()"]}, {"name": "torch.Tensor.T", "path": "tensors#torch.Tensor.T", "type": "torch.Tensor", "text": ["Is this Tensor with its dimensions reversed.", "If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0)."]}, {"name": "torch.Tensor.t()", "path": "tensors#torch.Tensor.t", "type": "torch.Tensor", "text": ["See torch.t()"]}, {"name": "torch.Tensor.take()", "path": "tensors#torch.Tensor.take", "type": "torch.Tensor", "text": ["See torch.take()"]}, {"name": "torch.Tensor.tan()", "path": "tensors#torch.Tensor.tan", "type": "torch.Tensor", "text": ["See torch.tan()"]}, {"name": "torch.Tensor.tanh()", "path": "tensors#torch.Tensor.tanh", "type": "torch.Tensor", "text": ["See torch.tanh()"]}, {"name": "torch.Tensor.tanh_()", "path": "tensors#torch.Tensor.tanh_", "type": "torch.Tensor", "text": ["In-place version of tanh()"]}, {"name": "torch.Tensor.tan_()", "path": "tensors#torch.Tensor.tan_", "type": "torch.Tensor", "text": ["In-place version of tan()"]}, {"name": "torch.Tensor.tensor_split()", "path": "tensors#torch.Tensor.tensor_split", "type": "torch.Tensor", "text": ["See torch.tensor_split()"]}, {"name": "torch.Tensor.tile()", "path": "tensors#torch.Tensor.tile", "type": "torch.Tensor", "text": ["See torch.tile()"]}, {"name": "torch.Tensor.to()", "path": "tensors#torch.Tensor.to", "type": "torch.Tensor", "text": ["Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).", "Note", "If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.", "Here are the ways to call to:", "Returns a Tensor with the specified dtype", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "memory_format (torch.memory_format, optional): the desired memory format of returned Tensor. Default: torch.preserve_format.", "Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.", "Example:"]}, {"name": "torch.Tensor.tolist()", "path": "tensors#torch.Tensor.tolist", "type": "torch.Tensor", "text": ["Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary.", "This operation is not differentiable.", "Examples:"]}, {"name": "torch.Tensor.topk()", "path": "tensors#torch.Tensor.topk", "type": "torch.Tensor", "text": ["See torch.topk()"]}, {"name": "torch.Tensor.to_dense()", "path": "sparse#torch.Tensor.to_dense", "type": "torch.sparse", "text": ["Creates a strided copy of self.", "Warning", "Throws an error if self is a strided tensor.", "Example:"]}, {"name": "torch.Tensor.to_mkldnn()", "path": "tensors#torch.Tensor.to_mkldnn", "type": "torch.Tensor", "text": ["Returns a copy of the tensor in torch.mkldnn layout."]}, {"name": "torch.Tensor.to_sparse()", "path": "sparse#torch.Tensor.to_sparse", "type": "torch.sparse", "text": ["Returns a sparse copy of the tensor. PyTorch supports sparse tensors in coordinate format.", "sparseDims (int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor", "Example:"]}, {"name": "torch.Tensor.trace()", "path": "tensors#torch.Tensor.trace", "type": "torch.Tensor", "text": ["See torch.trace()"]}, {"name": "torch.Tensor.transpose()", "path": "tensors#torch.Tensor.transpose", "type": "torch.Tensor", "text": ["See torch.transpose()"]}, {"name": "torch.Tensor.transpose_()", "path": "tensors#torch.Tensor.transpose_", "type": "torch.Tensor", "text": ["In-place version of transpose()"]}, {"name": "torch.Tensor.triangular_solve()", "path": "tensors#torch.Tensor.triangular_solve", "type": "torch.Tensor", "text": ["See torch.triangular_solve()"]}, {"name": "torch.Tensor.tril()", "path": "tensors#torch.Tensor.tril", "type": "torch.Tensor", "text": ["See torch.tril()"]}, {"name": "torch.Tensor.tril_()", "path": "tensors#torch.Tensor.tril_", "type": "torch.Tensor", "text": ["In-place version of tril()"]}, {"name": "torch.Tensor.triu()", "path": "tensors#torch.Tensor.triu", "type": "torch.Tensor", "text": ["See torch.triu()"]}, {"name": "torch.Tensor.triu_()", "path": "tensors#torch.Tensor.triu_", "type": "torch.Tensor", "text": ["In-place version of triu()"]}, {"name": "torch.Tensor.true_divide()", "path": "tensors#torch.Tensor.true_divide", "type": "torch.Tensor", "text": ["See torch.true_divide()"]}, {"name": "torch.Tensor.true_divide_()", "path": "tensors#torch.Tensor.true_divide_", "type": "torch.Tensor", "text": ["In-place version of true_divide_()"]}, {"name": "torch.Tensor.trunc()", "path": "tensors#torch.Tensor.trunc", "type": "torch.Tensor", "text": ["See torch.trunc()"]}, {"name": "torch.Tensor.trunc_()", "path": "tensors#torch.Tensor.trunc_", "type": "torch.Tensor", "text": ["In-place version of trunc()"]}, {"name": "torch.Tensor.type()", "path": "tensors#torch.Tensor.type", "type": "torch.Tensor", "text": ["Returns the type if dtype is not provided, else casts this object to the specified type.", "If this is already of the correct type, no copy is performed and the original object is returned."]}, {"name": "torch.Tensor.type_as()", "path": "tensors#torch.Tensor.type_as", "type": "torch.Tensor", "text": ["Returns this tensor cast to the type of the given tensor.", "This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())", "tensor (Tensor) \u2013 the tensor which has the desired type"]}, {"name": "torch.Tensor.t_()", "path": "tensors#torch.Tensor.t_", "type": "torch.Tensor", "text": ["In-place version of t()"]}, {"name": "torch.Tensor.unbind()", "path": "tensors#torch.Tensor.unbind", "type": "torch.Tensor", "text": ["See torch.unbind()"]}, {"name": "torch.Tensor.unflatten()", "path": "named_tensor#torch.Tensor.unflatten", "type": "Named Tensors", "text": ["Expands the dimension dim of the self tensor over multiple dimensions of sizes given by sizes.", "[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(\u2018A\u2019, \u2018B1\u2019, \u2018B2\u2019))", "Warning", "The named tensor API is experimental and subject to change."]}, {"name": "torch.Tensor.unfold()", "path": "tensors#torch.Tensor.unfold", "type": "torch.Tensor", "text": ["Returns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.", "Step between two slices is given by step.", "If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1.", "An additional dimension of size size is appended in the returned tensor.", "Example:"]}, {"name": "torch.Tensor.uniform_()", "path": "tensors#torch.Tensor.uniform_", "type": "torch.Tensor", "text": ["Fills self tensor with numbers sampled from the continuous uniform distribution:"]}, {"name": "torch.Tensor.unique()", "path": "tensors#torch.Tensor.unique", "type": "torch.Tensor", "text": ["Returns the unique elements of the input tensor.", "See torch.unique()"]}, {"name": "torch.Tensor.unique_consecutive()", "path": "tensors#torch.Tensor.unique_consecutive", "type": "torch.Tensor", "text": ["Eliminates all but the first element from every consecutive group of equivalent elements.", "See torch.unique_consecutive()"]}, {"name": "torch.Tensor.unsqueeze()", "path": "tensors#torch.Tensor.unsqueeze", "type": "torch.Tensor", "text": ["See torch.unsqueeze()"]}, {"name": "torch.Tensor.unsqueeze_()", "path": "tensors#torch.Tensor.unsqueeze_", "type": "torch.Tensor", "text": ["In-place version of unsqueeze()"]}, {"name": "torch.Tensor.values()", "path": "sparse#torch.Tensor.values", "type": "torch.sparse", "text": ["Return the values tensor of a sparse COO tensor.", "Warning", "Throws an error if self is not a sparse COO tensor.", "See also Tensor.indices().", "Note", "This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details."]}, {"name": "torch.Tensor.var()", "path": "tensors#torch.Tensor.var", "type": "torch.Tensor", "text": ["See torch.var()"]}, {"name": "torch.Tensor.vdot()", "path": "tensors#torch.Tensor.vdot", "type": "torch.Tensor", "text": ["See torch.vdot()"]}, {"name": "torch.Tensor.view()", "path": "tensors#torch.Tensor.view", "type": "torch.Tensor", "text": ["Returns a new tensor with the same data as the self tensor but of a different shape.", "The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k  that satisfy the following contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots, d+k-1 ,", "Otherwise, it will not be possible to view self tensor as shape without copying it (e.g., via contiguous()). When it is unclear whether a view() can be performed, it is advisable to use reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.", "shape (torch.Size or int...) \u2013 the desired size", "Example:", "Returns a new tensor with the same data as the self tensor but of a different dtype. dtype must have the same number of bytes per element as self\u2019s dtype.", "Warning", "This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.", "dtype (torch.dtype) \u2013 the desired dtype", "Example:"]}, {"name": "torch.Tensor.view_as()", "path": "tensors#torch.Tensor.view_as", "type": "torch.Tensor", "text": ["View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()).", "Please see view() for more information about view.", "other (torch.Tensor) \u2013 The result tensor has the same size as other."]}, {"name": "torch.Tensor.where()", "path": "tensors#torch.Tensor.where", "type": "torch.Tensor", "text": ["self.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where()"]}, {"name": "torch.Tensor.xlogy()", "path": "tensors#torch.Tensor.xlogy", "type": "torch.Tensor", "text": ["See torch.xlogy()"]}, {"name": "torch.Tensor.xlogy_()", "path": "tensors#torch.Tensor.xlogy_", "type": "torch.Tensor", "text": ["In-place version of xlogy()"]}, {"name": "torch.Tensor.zero_()", "path": "tensors#torch.Tensor.zero_", "type": "torch.Tensor", "text": ["Fills self tensor with zeros."]}, {"name": "torch.tensordot()", "path": "generated/torch.tensordot#torch.tensordot", "type": "torch", "text": ["Returns a contraction of a and b over multiple dimensions.", "tensordot implements a generalized matrix product.", "When called with a non-negative integer argument dims = dd , and the number of dimensions of a and b is mm  and nn , respectively, tensordot() computes", "When called with dims of the list form, the given dimensions will be contracted in place of the last dd  of a and the first dd  of bb . The sizes in these dimensions must match, but tensordot() will deal with broadcasted dimensions.", "Examples:"]}, {"name": "torch.tensor_split()", "path": "generated/torch.tensor_split#torch.tensor_split", "type": "torch", "text": ["Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections. This function is based on NumPy\u2019s numpy.array_split().", "indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013 ", "If indices_or_sections is an integer n or a zero dimensional long tensor with value n, input is split into n sections along dimension dim. If input is divisible by n along dimension dim, each section will be of equal size, input.size(dim) / n. If input is not divisible by n, the sizes of the first int(input.size(dim) % n) sections will have size int(input.size(dim) / n) + 1, and the rest will have size int(input.size(dim) / n).", "If indices_or_sections is a list or tuple of ints, or a one-dimensional long tensor, then input is split along dimension dim at each of the indices in the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0 would result in the tensors input[:2], input[2:3], and input[3:].", "If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional long tensor on the CPU."]}, {"name": "torch.tile()", "path": "generated/torch.tile#torch.tile", "type": "torch", "text": ["Constructs a tensor by repeating the elements of input. The reps argument specifies the number of repetitions in each dimension.", "If reps specifies fewer dimensions than input has, then ones are prepended to reps until all dimensions are specified. For example, if input has shape (8, 6, 4, 2) and reps is (2, 2), then reps is treated as (1, 1, 2, 2).", "Analogously, if input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has as many dimensions as reps specifies. For example, if input has shape (4, 2) and reps is (3, 3, 2, 2), then input is treated as if it had the shape (1, 1, 4, 2).", "Note", "This function is similar to NumPy\u2019s tile function.", "Example:"]}, {"name": "torch.topk()", "path": "generated/torch.topk#torch.topk", "type": "torch", "text": ["Returns the k largest elements of the given input tensor along a given dimension.", "If dim is not given, the last dimension of the input is chosen.", "If largest is False then the k smallest elements are returned.", "A namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor.", "The boolean option sorted if True, will make sure that the returned k elements are themselves sorted", "out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers", "Example:"]}, {"name": "torch.torch.default_generator", "path": "torch#torch.torch.default_generator", "type": "torch", "text": []}, {"name": "torch.torch.device", "path": "tensor_attributes#torch.torch.device", "type": "Tensor Attributes", "text": []}, {"name": "torch.torch.dtype", "path": "tensor_attributes#torch.torch.dtype", "type": "Tensor Attributes", "text": []}, {"name": "torch.torch.finfo", "path": "type_info#torch.torch.finfo", "type": "Type Info", "text": []}, {"name": "torch.torch.iinfo", "path": "type_info#torch.torch.iinfo", "type": "Type Info", "text": []}, {"name": "torch.torch.layout", "path": "tensor_attributes#torch.torch.layout", "type": "Tensor Attributes", "text": []}, {"name": "torch.torch.memory_format", "path": "tensor_attributes#torch.torch.memory_format", "type": "Tensor Attributes", "text": []}, {"name": "torch.trace()", "path": "generated/torch.trace#torch.trace", "type": "torch", "text": ["Returns the sum of the elements of the diagonal of the input 2-D matrix.", "Example:"]}, {"name": "torch.transpose()", "path": "generated/torch.transpose#torch.transpose", "type": "torch", "text": ["Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.", "The resulting out tensor shares its underlying storage with the input tensor, so changing the content of one would change the content of the other.", "Example:"]}, {"name": "torch.trapz()", "path": "generated/torch.trapz#torch.trapz", "type": "torch", "text": ["Estimate \u222bydx\\int y\\,dx  along dim, using the trapezoid rule.", "A Tensor with the same shape as the input, except with dim removed. Each element of the returned tensor represents the estimated integral \u222bydx\\int y\\,dx  along dim.", "Example:", "As above, but the sample points are spaced uniformly at a distance of dx.", "y (Tensor) \u2013 The values of the function to integrate", "A Tensor with the same shape as the input, except with dim removed. Each element of the returned tensor represents the estimated integral \u222bydx\\int y\\,dx  along dim."]}, {"name": "torch.triangular_solve()", "path": "generated/torch.triangular_solve#torch.triangular_solve", "type": "torch", "text": ["Solves a system of equations with a triangular coefficient matrix AA  and multiple right-hand sides bb .", "In particular, solves AX=bAX = b  and assumes AA  is upper-triangular with the default keyword arguments.", "torch.triangular_solve(b, A) can take in 2D inputs b, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs X", "Supports real-valued and complex-valued inputs.", "A namedtuple (solution, cloned_coefficient) where cloned_coefficient is a clone of AA  and solution is the solution XX  to AX=bAX = b  (or whatever variant of the system of equations, depending on the keyword arguments.)", "Examples:"]}, {"name": "torch.tril()", "path": "generated/torch.tril#torch.tril", "type": "torch", "text": ["Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.", "The lower triangular part of the matrix is defined as the elements on and below the diagonal.", "The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.tril_indices()", "path": "generated/torch.tril_indices#torch.tril_indices", "type": "torch", "text": ["Returns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.", "The lower triangular part of the matrix is defined as the elements on and below the diagonal.", "The argument offset controls which diagonal to consider. If offset = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.", "Note", "When running on CUDA, row * col must be less than 2592^{59}  to prevent overflow during calculation."]}, {"name": "torch.triu()", "path": "generated/torch.triu#torch.triu", "type": "torch", "text": ["Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.", "The upper triangular part of the matrix is defined as the elements on and above the diagonal.", "The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.triu_indices()", "path": "generated/torch.triu_indices#torch.triu_indices", "type": "torch", "text": ["Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns.", "The upper triangular part of the matrix is defined as the elements on and above the diagonal.", "The argument offset controls which diagonal to consider. If offset = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace  for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]  where d1,d2d_{1}, d_{2}  are the dimensions of the matrix.", "Note", "When running on CUDA, row * col must be less than 2592^{59}  to prevent overflow during calculation."]}, {"name": "torch.true_divide()", "path": "generated/torch.true_divide#torch.true_divide", "type": "torch", "text": ["Alias for torch.div() with rounding_mode=None."]}, {"name": "torch.trunc()", "path": "generated/torch.trunc#torch.trunc", "type": "torch", "text": ["Returns a new tensor with the truncated integer values of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.unbind()", "path": "generated/torch.unbind#torch.unbind", "type": "torch", "text": ["Removes a tensor dimension.", "Returns a tuple of all slices along a given dimension, already without it.", "Example:"]}, {"name": "torch.unique()", "path": "generated/torch.unique#torch.unique", "type": "torch", "text": ["Returns the unique elements of the input tensor.", "Note", "This function is different from torch.unique_consecutive() in the sense that this function also eliminates non-consecutive duplicate values.", "Note", "Currently in the CUDA implementation and the CPU implementation when dim is specified, torch.unique always sort the tensor at the beginning regardless of the sort argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use torch.unique_consecutive() which avoids the sorting.", "A tensor or a tuple of tensors containing", "(Tensor, Tensor (optional), Tensor (optional))", "Example:"]}, {"name": "torch.unique_consecutive()", "path": "generated/torch.unique_consecutive#torch.unique_consecutive", "type": "torch", "text": ["Eliminates all but the first element from every consecutive group of equivalent elements.", "Note", "This function is different from torch.unique() in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to std::unique in C++.", "A tensor or a tuple of tensors containing", "(Tensor, Tensor (optional), Tensor (optional))", "Example:"]}, {"name": "torch.unsqueeze()", "path": "generated/torch.unsqueeze#torch.unsqueeze", "type": "torch", "text": ["Returns a new tensor with a dimension of size one inserted at the specified position.", "The returned tensor shares the same underlying data with this tensor.", "A dim value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at dim = dim + input.dim() + 1.", "Example:"]}, {"name": "torch.use_deterministic_algorithms()", "path": "generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms", "type": "torch", "text": ["Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When True, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a :class:RuntimeError when called.", "Warning", "This feature is in beta, and its design and implementation may change in the future.", "The following normally-nondeterministic operations will act deterministically when d=True:", "The following normally-nondeterministic operations will throw a RuntimeError when d=True:", "torch.nn.functional.interpolate() when called on a CUDA tensor that requires grad and one of the following modes is used:", "A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more details: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility If one of these environment variable configurations is not set, a RuntimeError will be raised from these operations when called with CUDA tensors:", "Note that deterministic operations tend to have worse performance than non-deterministic operations.", "d (bool) \u2013 If True, force operations to be deterministic. If False, allow non-deterministic operations."]}, {"name": "torch.utils.benchmark", "path": "benchmark_utils", "type": "torch.utils.benchmark", "text": ["Helper class for measuring execution time of PyTorch statements.", "For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html", "The PyTorch Timer is based on timeit.Timer (and in fact uses timeit.Timer internally), but with several key differences:", "Timer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.", "When measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the timeit API by conceptually merging timeit.Timer.repeat and timeit.Timer.autorange. (Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not desired.", "When defining a Timer, one can optionally specify label, sub_label, description, and env. (Defined later) These fields are included in the representation of result object and by the Compare class to group and display results for comparison.", "In addition to wall times, Timer can run a statement under Callgrind and report instructions executed.", "Directly analogous to timeit.Timer constructor arguments:", "stmt, setup, timer, globals", "PyTorch Timer specific constructor arguments:", "label, sub_label, description, env, num_threads", "sub_label \u2013 ", "Provide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d", "\u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using Compare.", "description \u2013 ", "String to distinguish measurements with identical label and sub_label. The principal use of description is to signal to Compare the columns of data. For instance one might set it based on the input size to create a table of the form:", "using Compare. It is also included when printing a Measurement.", "Measure many replicates while keeping timer overhead to a minimum.", "At a high level, blocked_autorange executes the following pseudo-code:", "Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:", "blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.", "A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)", "Collect instruction counts using Callgrind.", "Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.", "In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed.", "Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly.", "By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.", "A CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.", "Mirrors the semantics of timeit.Timer.timeit().", "Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit", "The result of a Timer measurement.", "This class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers.", "Convenience method for merging replicates.", "Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates)", "Approximate significant figure estimate.", "This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t.", "The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare.", "Top level container for Callgrind results collected by Timer.", "Manipulation is generally done using the FunctionCounts class, which is obtained by calling CallgrindStats.stats(\u2026). Several convenience methods are provided as well; the most significant is CallgrindStats.as_standardized().", "Strip library names and some prefixes from function strings.", "When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:", "Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing.", "Returns the total number of instructions executed.", "See FunctionCounts.denoise() for an explation of the denoise arg.", "Diff two sets of counts.", "One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. The subtract_baselines argument allows one to disable baseline correction, though in most cases it shouldn\u2019t matter as the baselines are expected to more or less cancel out.", "Returns detailed function counts.", "Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.", "inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details)", "Container for manipulating Callgrind results.", "Remove known noisy instructions.", "Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception.", "Keep only the elements where filter_fn applied to function name returns True.", "Apply map_fn to all of the function names.", "This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc."]}, {"name": "torch.utils.benchmark.CallgrindStats", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats", "type": "torch.utils.benchmark", "text": ["Top level container for Callgrind results collected by Timer.", "Manipulation is generally done using the FunctionCounts class, which is obtained by calling CallgrindStats.stats(\u2026). Several convenience methods are provided as well; the most significant is CallgrindStats.as_standardized().", "Strip library names and some prefixes from function strings.", "When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:", "Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing.", "Returns the total number of instructions executed.", "See FunctionCounts.denoise() for an explation of the denoise arg.", "Diff two sets of counts.", "One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. The subtract_baselines argument allows one to disable baseline correction, though in most cases it shouldn\u2019t matter as the baselines are expected to more or less cancel out.", "Returns detailed function counts.", "Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.", "inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details)"]}, {"name": "torch.utils.benchmark.CallgrindStats.as_standardized()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.as_standardized", "type": "torch.utils.benchmark", "text": ["Strip library names and some prefixes from function strings.", "When comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:", "Stripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivilent call sites when diffing."]}, {"name": "torch.utils.benchmark.CallgrindStats.counts()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.counts", "type": "torch.utils.benchmark", "text": ["Returns the total number of instructions executed.", "See FunctionCounts.denoise() for an explation of the denoise arg."]}, {"name": "torch.utils.benchmark.CallgrindStats.delta()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.delta", "type": "torch.utils.benchmark", "text": ["Diff two sets of counts.", "One common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is \u201cwhy\u201d. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis. The subtract_baselines argument allows one to disable baseline correction, though in most cases it shouldn\u2019t matter as the baselines are expected to more or less cancel out."]}, {"name": "torch.utils.benchmark.CallgrindStats.stats()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.stats", "type": "torch.utils.benchmark", "text": ["Returns detailed function counts.", "Conceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.", "inclusive matches the semantics of callgrind. If True, the counts include instructions executed by children. inclusive=True is useful for identifying hot spots in code; inclusive=False is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(\u2026) for more details)"]}, {"name": "torch.utils.benchmark.FunctionCounts", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts", "type": "torch.utils.benchmark", "text": ["Container for manipulating Callgrind results.", "Remove known noisy instructions.", "Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception.", "Keep only the elements where filter_fn applied to function name returns True.", "Apply map_fn to all of the function names.", "This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc."]}, {"name": "torch.utils.benchmark.FunctionCounts.denoise()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.denoise", "type": "torch.utils.benchmark", "text": ["Remove known noisy instructions.", "Several instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception."]}, {"name": "torch.utils.benchmark.FunctionCounts.filter()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.filter", "type": "torch.utils.benchmark", "text": ["Keep only the elements where filter_fn applied to function name returns True."]}, {"name": "torch.utils.benchmark.FunctionCounts.transform()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.transform", "type": "torch.utils.benchmark", "text": ["Apply map_fn to all of the function names.", "This can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc."]}, {"name": "torch.utils.benchmark.Measurement", "path": "benchmark_utils#torch.utils.benchmark.Measurement", "type": "torch.utils.benchmark", "text": ["The result of a Timer measurement.", "This class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers.", "Convenience method for merging replicates.", "Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates)", "Approximate significant figure estimate.", "This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t.", "The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare."]}, {"name": "torch.utils.benchmark.Measurement.merge()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.merge", "type": "torch.utils.benchmark", "text": ["Convenience method for merging replicates.", "Merge will extrapolate times to number_per_run=1 and will not transfer any metadata. (Since it might differ between replicates)"]}, {"name": "torch.utils.benchmark.Measurement.significant_figures()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.significant_figures", "type": "torch.utils.benchmark", "text": ["Approximate significant figure estimate.", "This property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of n, so z can approximate t.", "The significant figure estimation used in conjunction with the trim_sigfig method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for Compare."]}, {"name": "torch.utils.benchmark.Timer", "path": "benchmark_utils#torch.utils.benchmark.Timer", "type": "torch.utils.benchmark", "text": ["Helper class for measuring execution time of PyTorch statements.", "For a full tutorial on how to use this class, see: https://pytorch.org/tutorials/recipes/recipes/benchmark.html", "The PyTorch Timer is based on timeit.Timer (and in fact uses timeit.Timer internally), but with several key differences:", "Timer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.", "When measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the timeit API by conceptually merging timeit.Timer.repeat and timeit.Timer.autorange. (Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not desired.", "When defining a Timer, one can optionally specify label, sub_label, description, and env. (Defined later) These fields are included in the representation of result object and by the Compare class to group and display results for comparison.", "In addition to wall times, Timer can run a statement under Callgrind and report instructions executed.", "Directly analogous to timeit.Timer constructor arguments:", "stmt, setup, timer, globals", "PyTorch Timer specific constructor arguments:", "label, sub_label, description, env, num_threads", "sub_label \u2013 ", "Provide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d", "\u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using Compare.", "description \u2013 ", "String to distinguish measurements with identical label and sub_label. The principal use of description is to signal to Compare the columns of data. For instance one might set it based on the input size to create a table of the form:", "using Compare. It is also included when printing a Measurement.", "Measure many replicates while keeping timer overhead to a minimum.", "At a high level, blocked_autorange executes the following pseudo-code:", "Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:", "blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.", "A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)", "Collect instruction counts using Callgrind.", "Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.", "In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed.", "Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly.", "By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.", "A CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results.", "Mirrors the semantics of timeit.Timer.timeit().", "Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit"]}, {"name": "torch.utils.benchmark.Timer.blocked_autorange()", "path": "benchmark_utils#torch.utils.benchmark.Timer.blocked_autorange", "type": "torch.utils.benchmark", "text": ["Measure many replicates while keeping timer overhead to a minimum.", "At a high level, blocked_autorange executes the following pseudo-code:", "Note the variable block_size in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:", "blocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.", "A Measurement object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)"]}, {"name": "torch.utils.benchmark.Timer.collect_callgrind()", "path": "benchmark_utils#torch.utils.benchmark.Timer.collect_callgrind", "type": "torch.utils.benchmark", "text": ["Collect instruction counts using Callgrind.", "Unlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs stmt in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, howevever this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.", "In order to to use this method valgrind, callgrind_control, and callgrind_annotate must be installed.", "Because there is a process boundary between the caller (this process) and the stmt execution, globals cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, nn.Modules\u2019s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The GlobalsBridge class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to setup for them to transfer properly.", "By default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives stmt.", "A CallgrindStats object which provides instruction counts and some basic facilities for analyzing and manipulating results."]}, {"name": "torch.utils.benchmark.Timer.timeit()", "path": "benchmark_utils#torch.utils.benchmark.Timer.timeit", "type": "torch.utils.benchmark", "text": ["Mirrors the semantics of timeit.Timer.timeit().", "Execute the main statement (stmt) number times. https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit"]}, {"name": "torch.utils.bottleneck", "path": "bottleneck", "type": "torch.utils.bottleneck", "text": ["torch.utils.bottleneck is a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch\u2019s autograd profiler.", "Run it on the command line with", "where [args] are any number of arguments to script.py, or run python -m torch.utils.bottleneck -h for more usage instructions.", "Warning", "Because your script will be profiled, please ensure that it exits in a finite amount of time.", "Warning", "Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.", "Note", "To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (\u201cCPU total time is much greater than CUDA total time\u201d). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.", "Of course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you\u2019re evaluating. If the profiler outputs don\u2019t help, you could try looking at the result of torch.autograd.profiler.emit_nvtx() with nvprof. However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline.", "Warning", "If you are profiling CUDA code, the first profiler that bottleneck runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.", "For more complicated uses of the profilers (like in a multi-GPU case), please see https://docs.python.org/3/library/profile.html or torch.autograd.profiler.profile() for more information."]}, {"name": "torch.utils.checkpoint", "path": "checkpoint", "type": "torch.utils.checkpoint", "text": ["Note", "Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply preserve_rng_state=False to checkpoint or checkpoint_sequential to omit stashing and restoring the RNG state during each checkpoint.", "The stashing logic saves and restores the RNG state for the current device and the device of all cuda Tensor arguments to the run_fn. However, the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself. Therefore, if you move Tensors to a new device (\u201cnew\u201d meaning not belonging to the set of [current device + devices of Tensor arguments]) within run_fn, deterministic output compared to non-checkpointed passes is never guaranteed.", "Checkpoint a model or part of the model", "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.", "Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.", "Warning", "Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().", "Warning", "If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected.", "Warning", "If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.", "Output of running function on *args", "A helper function for checkpointing sequential models.", "Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.", "See checkpoint() on how checkpointing works.", "Warning", "Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().", "Output of running functions sequentially on *inputs"]}, {"name": "torch.utils.checkpoint.checkpoint()", "path": "checkpoint#torch.utils.checkpoint.checkpoint", "type": "torch.utils.checkpoint", "text": ["Checkpoint a model or part of the model", "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.", "Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.", "Warning", "Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().", "Warning", "If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected.", "Warning", "If checkpointed segment contains tensors detached from the computational graph by detach() or torch.no_grad(), the backward pass will raise an error. This is because checkpoint makes all the outputs require gradients which causes issues when a tensor is defined to have no gradient in the model. To circumvent this, detach the tensors outside of the checkpoint function.", "Output of running function on *args"]}, {"name": "torch.utils.checkpoint.checkpoint_sequential()", "path": "checkpoint#torch.utils.checkpoint.checkpoint_sequential", "type": "torch.utils.checkpoint", "text": ["A helper function for checkpointing sequential models.", "Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.", "See checkpoint() on how checkpointing works.", "Warning", "Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().", "Output of running functions sequentially on *inputs"]}, {"name": "torch.utils.cpp_extension", "path": "cpp_extension", "type": "torch.utils.cpp_extension", "text": ["Creates a setuptools.Extension for C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension.", "All arguments are forwarded to the setuptools.Extension constructor.", "Creates a setuptools.Extension for CUDA/C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.", "All arguments are forwarded to the setuptools.Extension constructor.", "Compute capabilities:", "By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).", "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support:", "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py", "The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better.", "Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.", "A custom setuptools build extension .", "This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general).", "When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.", "use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.", "Loads a PyTorch C++ extension just-in-time (JIT).", "To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.", "By default, the directory to which the build file is emitted and the resulting library compiled to is <tmp>/torch_extensions/<name>, where <tmp> is the temporary folder on the current platform and <name> the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces <tmp>/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.", "To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories.", "CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.", "Returns the loaded PyTorch extension as a Python module.", "Returns nothing. (The shared library is loaded into the process as a side effect.)", "Return the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)", "If is_python_module is True", "Loads a PyTorch C++ extension just-in-time (JIT) from string sources.", "This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load().", "See the tests for good examples of using this function.", "Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include\n<torch/extension.h>.", "Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.", "The sources in cuda_sources are concatenated into a separate .cu file and prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions).", "See load() for a description of arguments omitted below.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number.", "Get the include paths required to build a C++ or CUDA extension.", "cuda \u2013 If True, includes CUDA-specific include paths.", "A list of include path strings.", "Verifies that the given compiler is ABI-compatible with PyTorch.", "compiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.", "False if the compiler is (likely) ABI-incompatible with PyTorch, else True.", "Raises RuntimeError if ninja build system is not available on the system, does nothing otherwise.", "Returns True if the ninja build system is available on the system, False otherwise."]}, {"name": "torch.utils.cpp_extension.BuildExtension()", "path": "cpp_extension#torch.utils.cpp_extension.BuildExtension", "type": "torch.utils.cpp_extension", "text": ["A custom setuptools build extension .", "This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++14) as well as mixed C++/CUDA compilation (and support for CUDA files in general).", "When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.", "use_ninja (bool): If use_ninja is True (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard setuptools.build_ext. Fallbacks to the standard distutils backend if Ninja is not available.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number."]}, {"name": "torch.utils.cpp_extension.check_compiler_abi_compatibility()", "path": "cpp_extension#torch.utils.cpp_extension.check_compiler_abi_compatibility", "type": "torch.utils.cpp_extension", "text": ["Verifies that the given compiler is ABI-compatible with PyTorch.", "compiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.", "False if the compiler is (likely) ABI-incompatible with PyTorch, else True."]}, {"name": "torch.utils.cpp_extension.CppExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CppExtension", "type": "torch.utils.cpp_extension", "text": ["Creates a setuptools.Extension for C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension.", "All arguments are forwarded to the setuptools.Extension constructor."]}, {"name": "torch.utils.cpp_extension.CUDAExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CUDAExtension", "type": "torch.utils.cpp_extension", "text": ["Creates a setuptools.Extension for CUDA/C++.", "Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.", "All arguments are forwarded to the setuptools.Extension constructor.", "Compute capabilities:", "By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).", "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which CCs you want the extension to support:", "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py TORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py", "The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but \u201c8.0 8.6\u201d would be better.", "Note that while it\u2019s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch."]}, {"name": "torch.utils.cpp_extension.include_paths()", "path": "cpp_extension#torch.utils.cpp_extension.include_paths", "type": "torch.utils.cpp_extension", "text": ["Get the include paths required to build a C++ or CUDA extension.", "cuda \u2013 If True, includes CUDA-specific include paths.", "A list of include path strings."]}, {"name": "torch.utils.cpp_extension.is_ninja_available()", "path": "cpp_extension#torch.utils.cpp_extension.is_ninja_available", "type": "torch.utils.cpp_extension", "text": ["Returns True if the ninja build system is available on the system, False otherwise."]}, {"name": "torch.utils.cpp_extension.load()", "path": "cpp_extension#torch.utils.cpp_extension.load", "type": "torch.utils.cpp_extension", "text": ["Loads a PyTorch C++ extension just-in-time (JIT).", "To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.", "By default, the directory to which the build file is emitted and the resulting library compiled to is <tmp>/torch_extensions/<name>, where <tmp> is the temporary folder on the current platform and <name> the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces <tmp>/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.", "To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories.", "CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.", "Returns the loaded PyTorch extension as a Python module.", "Returns nothing. (The shared library is loaded into the process as a side effect.)", "Return the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)", "If is_python_module is True"]}, {"name": "torch.utils.cpp_extension.load_inline()", "path": "cpp_extension#torch.utils.cpp_extension.load_inline", "type": "torch.utils.cpp_extension", "text": ["Loads a PyTorch C++ extension just-in-time (JIT) from string sources.", "This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load().", "See the tests for good examples of using this function.", "Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include\n<torch/extension.h>.", "Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.", "The sources in cuda_sources are concatenated into a separate .cu file and prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions).", "See load() for a description of arguments omitted below.", "Note", "By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the MAX_JOBS environment variable to a non-negative number."]}, {"name": "torch.utils.cpp_extension.verify_ninja_availability()", "path": "cpp_extension#torch.utils.cpp_extension.verify_ninja_availability", "type": "torch.utils.cpp_extension", "text": ["Raises RuntimeError if ninja build system is not available on the system, does nothing otherwise."]}, {"name": "torch.utils.data", "path": "data", "type": "torch.utils.data", "text": ["At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for", "These options are configured by the constructor arguments of a DataLoader, which has signature:", "The sections below describe in details the effects and usages of these options.", "The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:", "A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples.", "For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.", "See Dataset for more details.", "An iterable-style dataset is an instance of a subclass of IterableDataset that implements the __iter__() protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.", "For example, such a dataset, when called iter(dataset), could return a stream of data reading from a database, a remote server, or even logs generated in real time.", "See IterableDataset for more details.", "Note", "When using an IterableDataset with multi-process data loading. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See IterableDataset documentations for how to achieve this.", "For iterable-style datasets, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).", "The rest of this section concerns the case with map-style datasets. torch.utils.data.Sampler classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a Sampler could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.", "A sequential or shuffled sampler will be automatically constructed based on the shuffle argument to a DataLoader. Alternatively, users may use the sampler argument to specify a custom Sampler object that at each time yields the next index/key to fetch.", "A custom Sampler that yields a list of batch indices at a time can be passed as the batch_sampler argument. Automatic batching can also be enabled via batch_size and drop_last arguments. See the next section for more details on this.", "Note", "Neither sampler nor batch_sampler is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.", "DataLoader supports automatically collating individual fetched data samples into batches via arguments batch_size, drop_last, and batch_sampler.", "This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).", "When batch_size (default 1) is not None, the data loader yields batched samples instead of individual samples. batch_size and drop_last arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify batch_sampler, which yields a list of keys at a time.", "Note", "The batch_size and drop_last arguments essentially are used to construct a batch_sampler from sampler. For map-style datasets, the sampler is either provided by user or constructed based on the shuffle argument. For iterable-style datasets, the sampler is a dummy infinite one. See this section on more details on samplers.", "Note", "When fetching from iterable-style datasets with multi-processing, the drop_last argument drops the last non-full batch of each worker\u2019s dataset replica.", "After fetching a list of samples using the indices from sampler, the function passed as the collate_fn argument is used to collate lists of samples into batches.", "In this case, loading from a map-style dataset is roughly equivalent with:", "and loading from an iterable-style dataset is roughly equivalent with:", "A custom collate_fn can be used to customize collation, e.g., padding sequential data to max length of a batch. See this section on more about collate_fn.", "In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it\u2019s likely better to not use automatic batching (where collate_fn is used to collate the samples), but let the data loader directly return each member of the dataset object.", "When both batch_size and batch_sampler are None (default value for batch_sampler is already None), automatic batching is disabled. Each sample obtained from the dataset is processed with the function passed as the collate_fn argument.", "When automatic batching is disabled, the default collate_fn simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.", "In this case, loading from a map-style dataset is roughly equivalent with:", "and loading from an iterable-style dataset is roughly equivalent with:", "See this section on more about collate_fn.", "The use of collate_fn is slightly different when automatic batching is enabled or disabled.", "When automatic batching is disabled, collate_fn is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default collate_fn simply converts NumPy arrays in PyTorch tensors.", "When automatic batching is enabled, collate_fn is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes behavior of the default collate_fn in this case.", "For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple (image, class_index), the default collate_fn collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default collate_fn has the following properties:", "Users may use customized collate_fn to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.", "A DataLoader uses single-process data loading by default.", "Within a Python process, the Global Interpreter Lock (GIL) prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument num_workers to a positive integer.", "In this mode, data fetching is done in the same process a DataLoader is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.", "Setting the argument num_workers as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.", "In this mode, each time an iterator of a DataLoader is created (e.g., when you call enumerate(dataloader)), num_workers worker processes are created. At this point, the dataset, collate_fn, and worker_init_fn are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including collate_fn) runs in the worker process.", "torch.utils.data.get_worker_info() returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns None in main process. Users may use this function in dataset code and/or worker_init_fn to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset.", "For map-style datasets, the main process generates the indices using sampler and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.", "For iterable-style datasets, since each worker process gets a replica of the dataset object, naive multi-process loading will often result in duplicated data. Using torch.utils.data.get_worker_info() and/or worker_init_fn, users may configure each replica independently. (See IterableDataset documentations for how to achieve this. ) For similar reasons, in multi-process loading, the drop_last argument drops the last non-full batch of each worker\u2019s iterable-style dataset replica.", "Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.", "Warning", "It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see CUDA in multiprocessing). Instead, we recommend using automatic memory pinning (i.e., setting pin_memory=True), which enables fast data transfer to CUDA-enabled GPUs.", "Since workers rely on Python multiprocessing, worker launch behavior is different on Windows compared to Unix.", "This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:", "By default, each worker will have its PyTorch seed set to base_seed + worker_id, where base_seed is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily). However, seeds for other libraries may be duplicated upon initializing workers (e.g., NumPy), causing each worker to return identical random numbers. (See this section in FAQ.).", "In worker_init_fn, you may access the PyTorch seed set for each worker with either torch.utils.data.get_worker_info().seed or torch.initial_seed(), and use it to seed other libraries before data loading.", "Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See Use pinned memory buffers for more details on when and how to use pinned memory generally.", "For data loading, passing pin_memory=True to a DataLoader will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.", "The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a collate_fn that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a pin_memory() method on your custom type(s).", "See the example below.", "Example:", "Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.", "The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.", "See torch.utils.data documentation page for more details.", "Warning", "If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.", "Warning", "len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user dataset code in correctly handling multi-process loading to avoid duplicate data.", "However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when drop_last is set. Unfortunately, PyTorch can not detect such cases in general.", "See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.", "Warning", "See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions.", "An abstract class representing a Dataset.", "All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader.", "Note", "DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.", "An iterable Dataset.", "All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.", "All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset.", "When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers > 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior.", "Example 1: splitting workload across all workers in __iter__():", "Example 2: splitting workload across all workers using worker_init_fn:", "Dataset wrapping tensors.", "Each sample will be retrieved by indexing tensors along the first dimension.", "*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.", "Dataset as a concatenation of multiple datasets.", "This class is useful to assemble different existing datasets.", "datasets (sequence) \u2013 List of datasets to be concatenated", "Dataset for chainning multiple IterableDataset s.", "This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.", "datasets (iterable of IterableDataset) \u2013 datasets to be chained together", "Dataset shuffled from the original dataset.", "This class is useful to shuffle an existing instance of an IterableDataset. The buffer with buffer_size is filled with the items from the dataset first. Then, each item will be yielded from the buffer by reservoir sampling via iterator.", "buffer_size is required to be larger than 0. For buffer_size == 1, the dataset is not shuffled. In order to fully shuffle the whole dataset, buffer_size is required to be greater than or equal to the size of dataset.", "When it is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. And, the method to set up a random seed is different based on num_workers.", "For single-process mode (num_workers == 0), the random seed is required to be set before the DataLoader in the main process.", "For multi-process mode (num_workers > 0), the random seed is set by a callable function in each worker.", "Subset of a dataset at specified indices.", "Returns the information about the current DataLoader iterator worker process.", "When called in a worker, this returns an object guaranteed to have the following attributes:", "When called in the main process, this returns None.", "Note", "When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code (e.g., NumPy).", "Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:", "Base class for all Samplers.", "Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices of dataset elements, and a __len__() method that returns the length of the returned iterators.", "Note", "The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader.", "Samples elements sequentially, always in the same order.", "data_source (Dataset) \u2013 dataset to sample from", "Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw.", "Samples elements randomly from a given list of indices, without replacement.", "Samples elements from [0,..,len(weights)-1] with given probabilities (weights).", "Wraps another sampler to yield a mini-batch of indices.", "Sampler that restricts data loading to a subset of the dataset.", "It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.", "Note", "Dataset is assumed to be of constant size.", "Warning", "In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.", "Example:"]}, {"name": "torch.utils.data.BatchSampler", "path": "data#torch.utils.data.BatchSampler", "type": "torch.utils.data", "text": ["Wraps another sampler to yield a mini-batch of indices."]}, {"name": "torch.utils.data.BufferedShuffleDataset", "path": "data#torch.utils.data.BufferedShuffleDataset", "type": "torch.utils.data", "text": ["Dataset shuffled from the original dataset.", "This class is useful to shuffle an existing instance of an IterableDataset. The buffer with buffer_size is filled with the items from the dataset first. Then, each item will be yielded from the buffer by reservoir sampling via iterator.", "buffer_size is required to be larger than 0. For buffer_size == 1, the dataset is not shuffled. In order to fully shuffle the whole dataset, buffer_size is required to be greater than or equal to the size of dataset.", "When it is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. And, the method to set up a random seed is different based on num_workers.", "For single-process mode (num_workers == 0), the random seed is required to be set before the DataLoader in the main process.", "For multi-process mode (num_workers > 0), the random seed is set by a callable function in each worker."]}, {"name": "torch.utils.data.ChainDataset", "path": "data#torch.utils.data.ChainDataset", "type": "torch.utils.data", "text": ["Dataset for chainning multiple IterableDataset s.", "This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.", "datasets (iterable of IterableDataset) \u2013 datasets to be chained together"]}, {"name": "torch.utils.data.ConcatDataset", "path": "data#torch.utils.data.ConcatDataset", "type": "torch.utils.data", "text": ["Dataset as a concatenation of multiple datasets.", "This class is useful to assemble different existing datasets.", "datasets (sequence) \u2013 List of datasets to be concatenated"]}, {"name": "torch.utils.data.DataLoader", "path": "data#torch.utils.data.DataLoader", "type": "torch.utils.data", "text": ["Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.", "The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.", "See torch.utils.data documentation page for more details.", "Warning", "If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.", "Warning", "len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, it instead returns an estimate based on len(dataset) / batch_size, with proper rounding depending on drop_last, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user dataset code in correctly handling multi-process loading to avoid duplicate data.", "However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when drop_last is set. Unfortunately, PyTorch can not detect such cases in general.", "See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.", "Warning", "See Reproducibility, and My data loader workers return identical random numbers, and Randomness in multi-process data loading notes for random seed related questions."]}, {"name": "torch.utils.data.Dataset", "path": "data#torch.utils.data.Dataset", "type": "torch.utils.data", "text": ["An abstract class representing a Dataset.", "All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader.", "Note", "DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided."]}, {"name": "torch.utils.data.distributed.DistributedSampler", "path": "data#torch.utils.data.distributed.DistributedSampler", "type": "torch.utils.data", "text": ["Sampler that restricts data loading to a subset of the dataset.", "It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such a case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.", "Note", "Dataset is assumed to be of constant size.", "Warning", "In distributed mode, calling the set_epoch() method at the beginning of each epoch before creating the DataLoader iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.", "Example:"]}, {"name": "torch.utils.data.get_worker_info()", "path": "data#torch.utils.data.get_worker_info", "type": "torch.utils.data", "text": ["Returns the information about the current DataLoader iterator worker process.", "When called in a worker, this returns an object guaranteed to have the following attributes:", "When called in the main process, this returns None.", "Note", "When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code (e.g., NumPy)."]}, {"name": "torch.utils.data.IterableDataset", "path": "data#torch.utils.data.IterableDataset", "type": "torch.utils.data", "text": ["An iterable Dataset.", "All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.", "All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset.", "When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers > 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior.", "Example 1: splitting workload across all workers in __iter__():", "Example 2: splitting workload across all workers using worker_init_fn:"]}, {"name": "torch.utils.data.RandomSampler", "path": "data#torch.utils.data.RandomSampler", "type": "torch.utils.data", "text": ["Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw."]}, {"name": "torch.utils.data.random_split()", "path": "data#torch.utils.data.random_split", "type": "torch.utils.data", "text": ["Randomly split a dataset into non-overlapping new datasets of given lengths. Optionally fix the generator for reproducible results, e.g.:"]}, {"name": "torch.utils.data.Sampler", "path": "data#torch.utils.data.Sampler", "type": "torch.utils.data", "text": ["Base class for all Samplers.", "Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices of dataset elements, and a __len__() method that returns the length of the returned iterators.", "Note", "The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader."]}, {"name": "torch.utils.data.SequentialSampler", "path": "data#torch.utils.data.SequentialSampler", "type": "torch.utils.data", "text": ["Samples elements sequentially, always in the same order.", "data_source (Dataset) \u2013 dataset to sample from"]}, {"name": "torch.utils.data.Subset", "path": "data#torch.utils.data.Subset", "type": "torch.utils.data", "text": ["Subset of a dataset at specified indices."]}, {"name": "torch.utils.data.SubsetRandomSampler", "path": "data#torch.utils.data.SubsetRandomSampler", "type": "torch.utils.data", "text": ["Samples elements randomly from a given list of indices, without replacement."]}, {"name": "torch.utils.data.TensorDataset", "path": "data#torch.utils.data.TensorDataset", "type": "torch.utils.data", "text": ["Dataset wrapping tensors.", "Each sample will be retrieved by indexing tensors along the first dimension.", "*tensors (Tensor) \u2013 tensors that have the same size of the first dimension."]}, {"name": "torch.utils.data.WeightedRandomSampler", "path": "data#torch.utils.data.WeightedRandomSampler", "type": "torch.utils.data", "text": ["Samples elements from [0,..,len(weights)-1] with given probabilities (weights)."]}, {"name": "torch.utils.dlpack", "path": "dlpack", "type": "torch.utils.dlpack", "text": ["Decodes a DLPack to a tensor.", "dlpack \u2013 a PyCapsule object with the dltensor", "The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once.", "Returns a DLPack representing the tensor.", "tensor \u2013 a tensor to be exported", "The dlpack shares the tensors memory. Note that each dlpack can only be consumed once."]}, {"name": "torch.utils.dlpack.from_dlpack()", "path": "dlpack#torch.utils.dlpack.from_dlpack", "type": "torch.utils.dlpack", "text": ["Decodes a DLPack to a tensor.", "dlpack \u2013 a PyCapsule object with the dltensor", "The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once."]}, {"name": "torch.utils.dlpack.to_dlpack()", "path": "dlpack#torch.utils.dlpack.to_dlpack", "type": "torch.utils.dlpack", "text": ["Returns a DLPack representing the tensor.", "tensor \u2013 a tensor to be exported", "The dlpack shares the tensors memory. Note that each dlpack can only be consumed once."]}, {"name": "torch.utils.mobile_optimizer", "path": "mobile_optimizer", "type": "torch.utils.mobile_optimizer", "text": ["Warning", "This API is in beta and may change in the near future.", "Torch mobile supports torch.mobile_optimizer.optimize_for_mobile utility to run a list of optimization pass with modules in eval mode. The method takes the following parameters: a torch.jit.ScriptModule object, a blocklisting optimization set and a preserved method list", "optimize_for_mobile will also invoke freeze_module pass which only preserves forward method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method.", "A new optimized torch script module"]}, {"name": "torch.utils.mobile_optimizer.optimize_for_mobile()", "path": "mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile", "type": "torch.utils.mobile_optimizer", "text": ["A new optimized torch script module"]}, {"name": "torch.utils.model_zoo", "path": "model_zoo", "type": "torch.utils.model_zoo", "text": ["Moved to torch.hub.", "Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir()."]}, {"name": "torch.utils.model_zoo.load_url()", "path": "model_zoo#torch.utils.model_zoo.load_url", "type": "torch.utils.model_zoo", "text": ["Loads the Torch serialized object at the given URL.", "If downloaded file is a zip file, it will be automatically decompressed.", "If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is <hub_dir>/checkpoints where hub_dir is the directory returned by get_dir()."]}, {"name": "torch.utils.tensorboard", "path": "tensorboard", "type": "torch.utils.tensorboard", "text": ["Before going further, more details on TensorBoard can be found at https://www.tensorflow.org/tensorboard/", "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models and metrics into a directory for visualization within the TensorBoard UI. Scalars, images, histograms, graphs, and embedding visualizations are all supported for PyTorch models and tensors as well as Caffe2 nets and blobs.", "The SummaryWriter class is your main entry to log data for consumption and visualization by TensorBoard. For example:", "This can then be visualized with TensorBoard, which should be installable and runnable with:", "Lots of information can be logged for one experiment. To avoid cluttering the UI and have better result clustering, we can group plots by naming them hierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped together, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately in the TensorBoard interface.", "Expected result:", "Writes entries directly to event files in the log_dir to be consumed by TensorBoard.", "The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.", "Creates a SummaryWriter that will write out events and summaries to the event file.", "Examples:", "Add scalar data to summary.", "Examples:", "Expected result:", "Adds many scalar data to summary.", "Examples:", "Expected result:", "Add histogram to summary.", "Examples:", "Expected result:", "Add image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (3,H,W)(3, H, W) . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W) , (H,W)(H, W) , (H,W,3)(H, W, 3)  is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.", "Examples:", "Expected result:", "Add batched image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (N,3,H,W)(N, 3, H, W) . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.", "Examples:", "Expected result:", "Render matplotlib figure into an image and add it to summary.", "Note that this requires the matplotlib package.", "Add video data to summary.", "Note that this requires the moviepy package.", "vid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "Add audio data to summary.", "snd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].", "Add text data to summary.", "Examples:", "Add graph data to summary.", "Add embedding projector data to summary.", "mat: (N,D)(N, D) , where N is number of data and D is feature dimension", "label_img: (N,C,H,W)(N, C, H, W) ", "Examples:", "Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.", "Examples:", "Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.", "layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.", "Examples:", "Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.", "vertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels)", "colors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "faces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for type uint8.", "Examples:", "Add a set of hyperparameters to be compared in TensorBoard.", "Examples:", "Expected result:", "Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter", "type": "torch.utils.tensorboard", "text": ["Writes entries directly to event files in the log_dir to be consumed by TensorBoard.", "The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.", "Creates a SummaryWriter that will write out events and summaries to the event file.", "Examples:", "Add scalar data to summary.", "Examples:", "Expected result:", "Adds many scalar data to summary.", "Examples:", "Expected result:", "Add histogram to summary.", "Examples:", "Expected result:", "Add image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (3,H,W)(3, H, W) . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W) , (H,W)(H, W) , (H,W,3)(H, W, 3)  is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.", "Examples:", "Expected result:", "Add batched image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (N,3,H,W)(N, 3, H, W) . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.", "Examples:", "Expected result:", "Render matplotlib figure into an image and add it to summary.", "Note that this requires the matplotlib package.", "Add video data to summary.", "Note that this requires the moviepy package.", "vid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "Add audio data to summary.", "snd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].", "Add text data to summary.", "Examples:", "Add graph data to summary.", "Add embedding projector data to summary.", "mat: (N,D)(N, D) , where N is number of data and D is feature dimension", "label_img: (N,C,H,W)(N, C, H, W) ", "Examples:", "Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.", "Examples:", "Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.", "layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.", "Examples:", "Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.", "vertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels)", "colors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "faces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for type uint8.", "Examples:", "Add a set of hyperparameters to be compared in TensorBoard.", "Examples:", "Expected result:", "Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_audio()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_audio", "type": "torch.utils.tensorboard", "text": ["Add audio data to summary.", "snd_tensor: (1,L)(1, L) . The values should lie between [-1, 1]."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars", "type": "torch.utils.tensorboard", "text": ["Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.", "layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_embedding", "type": "torch.utils.tensorboard", "text": ["Add embedding projector data to summary.", "mat: (N,D)(N, D) , where N is number of data and D is feature dimension", "label_img: (N,C,H,W)(N, C, H, W) ", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_figure()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_figure", "type": "torch.utils.tensorboard", "text": ["Render matplotlib figure into an image and add it to summary.", "Note that this requires the matplotlib package."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_graph()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_graph", "type": "torch.utils.tensorboard", "text": ["Add graph data to summary."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_histogram", "type": "torch.utils.tensorboard", "text": ["Add histogram to summary.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_hparams", "type": "torch.utils.tensorboard", "text": ["Add a set of hyperparameters to be compared in TensorBoard.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_image()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_image", "type": "torch.utils.tensorboard", "text": ["Add image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (3,H,W)(3, H, W) . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W) , (H,W)(H, W) , (H,W,3)(H, W, 3)  is also suitable as long as corresponding dataformats argument is passed, e.g. CHW, HWC, HW.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_images()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_images", "type": "torch.utils.tensorboard", "text": ["Add batched image data to summary.", "Note that this requires the pillow package.", "img_tensor: Default is (N,3,H,W)(N, 3, H, W) . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_mesh", "type": "torch.utils.tensorboard", "text": ["Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.", "vertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels)", "colors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.", "faces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for type uint8.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve", "type": "torch.utils.tensorboard", "text": ["Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalar", "type": "torch.utils.tensorboard", "text": ["Add scalar data to summary.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalars", "type": "torch.utils.tensorboard", "text": ["Adds many scalar data to summary.", "Examples:", "Expected result:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_text()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_text", "type": "torch.utils.tensorboard", "text": ["Add text data to summary.", "Examples:"]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_video()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_video", "type": "torch.utils.tensorboard", "text": ["Add video data to summary.", "Note that this requires the moviepy package.", "vid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for type uint8 or [0, 1] for type float."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.close()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.close", "type": "torch.utils.tensorboard", "text": []}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.flush()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.flush", "type": "torch.utils.tensorboard", "text": ["Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk."]}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.__init__()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.__init__", "type": "torch.utils.tensorboard", "text": ["Creates a SummaryWriter that will write out events and summaries to the event file.", "Examples:"]}, {"name": "torch.vander()", "path": "generated/torch.vander#torch.vander", "type": "torch", "text": ["Generates a Vandermonde matrix.", "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0 . If increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)} . Such a matrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde.", "Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)} , the second x(N\u22122)x^{(N-2)}  and so forth. If increasing is True, the columns are x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)} .", "Tensor", "Example:"]}, {"name": "torch.var()", "path": "generated/torch.var#torch.var", "type": "torch", "text": ["Returns the variance of all elements in the input tensor.", "If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "Example:", "Returns the variance of each row of the input tensor in the given dimension dim.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.var_mean()", "path": "generated/torch.var_mean#torch.var_mean", "type": "torch", "text": ["Returns the variance and mean of all elements in the input tensor.", "If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "Example:", "Returns the variance and mean of each row of the input tensor in the given dimension dim.", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.", "Example:"]}, {"name": "torch.vdot()", "path": "generated/torch.vdot#torch.vdot", "type": "torch", "text": ["Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers differently than dot(a, b). If the first argument is complex, the complex conjugate of the first argument is used for the calculation of the dot product.", "Note", "Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.view_as_complex()", "path": "generated/torch.view_as_complex#torch.view_as_complex", "type": "torch", "text": ["Returns a view of input as a complex tensor. For an input complex tensor of size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , this function returns a new complex tensor of size m1,m2,\u2026,mim1, m2, \\dots, mi  where the last dimension of the input tensor is expected to represent the real and imaginary components of complex numbers.", "Warning", "view_as_complex() is only supported for tensors with torch.dtype torch.float64 and torch.float32. The input is expected to have the last dimension of size 2. In addition, the tensor must have a stride of 1 for its last dimension. The strides of all other dimensions must be even numbers.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.view_as_real()", "path": "generated/torch.view_as_real#torch.view_as_real", "type": "torch", "text": ["Returns a view of input as a real tensor. For an input complex tensor of size m1,m2,\u2026,mim1, m2, \\dots, mi , this function returns a new real tensor of size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , where the last dimension of size 2 represents the real and imaginary components of complex numbers.", "Warning", "view_as_real() is only supported for tensors with complex dtypes.", "input (Tensor) \u2013 the input tensor."]}, {"name": "torch.vstack()", "path": "generated/torch.vstack#torch.vstack", "type": "torch", "text": ["Stack tensors in sequence vertically (row wise).", "This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by torch.atleast_2d().", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.where()", "path": "generated/torch.where#torch.where", "type": "torch", "text": ["Return a tensor of elements selected from either x or y, depending on condition.", "The operation is defined as:", "Note", "The tensors condition, x, y must be broadcastable.", "Note", "Currently valid scalar and tensor combination are 1. Scalar of floating dtype and torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of complex dtype and torch.complex128", "A tensor of shape equal to the broadcasted shape of condition, x, y", "Tensor", "Example:", "torch.where(condition) is identical to torch.nonzero(condition, as_tuple=True).", "Note", "See also torch.nonzero()."]}, {"name": "torch.xlogy()", "path": "generated/torch.xlogy#torch.xlogy", "type": "torch", "text": ["Computes input * log(other) with the following cases.", "Similar to SciPy\u2019s scipy.special.xlogy.", "Note", "At least one of input or other must be a tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.zeros()", "path": "generated/torch.zeros#torch.zeros", "type": "torch", "text": ["Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.zeros_like()", "path": "generated/torch.zeros_like#torch.zeros_like", "type": "torch", "text": ["Returns a tensor filled with the scalar value 0, with the same size as input. torch.zeros_like(input) is equivalent to torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "Warning", "As of 0.4, this function does not support an out keyword. As an alternative, the old torch.zeros_like(input, out=output) is equivalent to torch.zeros(input.size(), out=output).", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}, {"name": "torch._assert()", "path": "generated/torch._assert#torch._assert", "type": "torch", "text": ["A wrapper around Python\u2019s assert which is symbolically traceable."]}]