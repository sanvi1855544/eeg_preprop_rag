<h1 id="torch-nn-quantized">torch.nn.quantized</h1> <p>This module implements the quantized versions of the nn modules and functionals.</p>  <h2 id="functional-interface">Functional interface</h2> <p id="module-torch.nn.quantized.functional">Functional interface (quantized).</p> <dl class="function"> <dt id="torch.nn.quantized.functional.linear">
<code>torch.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#linear"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a linear transformation to the incoming quantized data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span></span> </span>. See <a class="reference internal" href="#torch.nn.quantized.Linear" title="torch.nn.quantized.Linear"><code>Linear</code></a></p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use <a class="reference internal" href="#torch.nn.quantized.Linear" title="torch.nn.quantized.Linear"><code>Linear</code></a>.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Quantized input of type <code>torch.quint8</code>
</li> <li>
<strong>weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Quantized weight of type <code>torch.qint8</code>
</li> <li>
<strong>bias</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – None or fp32 bias of type <code>torch.float</code>
</li> <li>
<strong>scale</strong> (<em>double</em>) – output scale. If None, derived from the input scale</li> <li>
<strong>zero_point</strong> (<em>long</em>) – output zero point. If None, derived from the input zero_point</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo separator="true">,</mo><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *, in\_features)</annotation></semantics></math></span></span> </span> where <code>*</code> means any number of additional dimensions</li> <li>Weight: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo separator="true">,</mo><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(out\_features, in\_features)</annotation></semantics></math></span></span> </span>
</li> <li>Bias: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(out\_features)</annotation></semantics></math></span></span> </span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo separator="true">,</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *, out\_features)</annotation></semantics></math></span></span> </span>
</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.conv1d">
<code>torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#conv1d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 1D convolution over a quantized 1D input composed of several input planes.</p> <p>See <a class="reference internal" href="#torch.nn.quantized.Conv1d" title="torch.nn.quantized.Conv1d"><code>Conv1d</code></a> for details and output shape.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> – quantized input tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>minibatch</mtext><mo separator="true">,</mo><mtext>in_channels</mtext><mo separator="true">,</mo><mi>i</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{minibatch} , \text{in\_channels} , iW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>weight</strong> – quantized filters of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo separator="true">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo><mi>i</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , iW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>bias</strong> – <strong>non-quantized</strong> bias tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels})</annotation></semantics></math></span></span> </span>. The tensor type must be <code>torch.float</code>.</li> <li>
<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sW,)</code>. Default: 1</li> <li>
<strong>padding</strong> – implicit paddings on both sides of the input. Can be a single number or a tuple <code>(padW,)</code>. Default: 0</li> <li>
<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dW,)</code>. Default: 1</li> <li>
<strong>groups</strong> – split input into groups, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding="application/x-tex">\text{in\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> <li>
<strong>padding_mode</strong> – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”</li> <li>
<strong>scale</strong> – quantization scale for the output. Default: 1.0</li> <li>
<strong>zero_point</strong> – quantization zero_point for the output. Default: 0</li> <li>
<strong>dtype</strong> – quantization data type to use. Default: <code>torch.quint8</code>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; from torch.nn.quantized import functional as qF
&gt;&gt;&gt; filters = torch.randn(33, 16, 3, dtype=torch.float)
&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, dtype=torch.float)
&gt;&gt;&gt; bias = torch.randn(33, dtype=torch.float)
&gt;&gt;&gt;
&gt;&gt;&gt; scale, zero_point = 1.0, 0
&gt;&gt;&gt; dtype_inputs = torch.quint8
&gt;&gt;&gt; dtype_filters = torch.qint8
&gt;&gt;&gt;
&gt;&gt;&gt; q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
&gt;&gt;&gt; q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
&gt;&gt;&gt; qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.conv2d">
<code>torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#conv2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 2D convolution over a quantized 2D input composed of several input planes.</p> <p>See <a class="reference internal" href="#torch.nn.quantized.Conv2d" title="torch.nn.quantized.Conv2d"><code>Conv2d</code></a> for details and output shape.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> – quantized input tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>minibatch</mtext><mo separator="true">,</mo><mtext>in_channels</mtext><mo separator="true">,</mo><mi>i</mi><mi>H</mi><mo separator="true">,</mo><mi>i</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{minibatch} , \text{in\_channels} , iH , iW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>weight</strong> – quantized filters of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo separator="true">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo><mi>k</mi><mi>H</mi><mo separator="true">,</mo><mi>k</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>bias</strong> – <strong>non-quantized</strong> bias tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels})</annotation></semantics></math></span></span> </span>. The tensor type must be <code>torch.float</code>.</li> <li>
<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sH, sW)</code>. Default: 1</li> <li>
<strong>padding</strong> – implicit paddings on both sides of the input. Can be a single number or a tuple <code>(padH, padW)</code>. Default: 0</li> <li>
<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dH, dW)</code>. Default: 1</li> <li>
<strong>groups</strong> – split input into groups, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding="application/x-tex">\text{in\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> <li>
<strong>padding_mode</strong> – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”</li> <li>
<strong>scale</strong> – quantization scale for the output. Default: 1.0</li> <li>
<strong>zero_point</strong> – quantization zero_point for the output. Default: 0</li> <li>
<strong>dtype</strong> – quantization data type to use. Default: <code>torch.quint8</code>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; from torch.nn.quantized import functional as qF
&gt;&gt;&gt; filters = torch.randn(8, 4, 3, 3, dtype=torch.float)
&gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)
&gt;&gt;&gt; bias = torch.randn(8, dtype=torch.float)
&gt;&gt;&gt;
&gt;&gt;&gt; scale, zero_point = 1.0, 0
&gt;&gt;&gt; dtype_inputs = torch.quint8
&gt;&gt;&gt; dtype_filters = torch.qint8
&gt;&gt;&gt;
&gt;&gt;&gt; q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
&gt;&gt;&gt; q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
&gt;&gt;&gt; qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.conv3d">
<code>torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#conv3d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 3D convolution over a quantized 3D input composed of several input planes.</p> <p>See <a class="reference internal" href="#torch.nn.quantized.Conv3d" title="torch.nn.quantized.Conv3d"><code>Conv3d</code></a> for details and output shape.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> – quantized input tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>minibatch</mtext><mo separator="true">,</mo><mtext>in_channels</mtext><mo separator="true">,</mo><mi>i</mi><mi>D</mi><mo separator="true">,</mo><mi>i</mi><mi>H</mi><mo separator="true">,</mo><mi>i</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{minibatch} , \text{in\_channels} , iD , iH , iW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>weight</strong> – quantized filters of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo separator="true">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo><mi>k</mi><mi>D</mi><mo separator="true">,</mo><mi>k</mi><mi>H</mi><mo separator="true">,</mo><mi>k</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kD , kH , kW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>bias</strong> – <strong>non-quantized</strong> bias tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels})</annotation></semantics></math></span></span> </span>. The tensor type must be <code>torch.float</code>.</li> <li>
<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sD, sH, sW)</code>. Default: 1</li> <li>
<strong>padding</strong> – implicit paddings on both sides of the input. Can be a single number or a tuple <code>(padD, padH, padW)</code>. Default: 0</li> <li>
<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dD, dH, dW)</code>. Default: 1</li> <li>
<strong>groups</strong> – split input into groups, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding="application/x-tex">\text{in\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> <li>
<strong>padding_mode</strong> – the padding mode to use. Only “zeros” is supported for quantized convolution at the moment. Default: “zeros”</li> <li>
<strong>scale</strong> – quantization scale for the output. Default: 1.0</li> <li>
<strong>zero_point</strong> – quantization zero_point for the output. Default: 0</li> <li>
<strong>dtype</strong> – quantization data type to use. Default: <code>torch.quint8</code>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; from torch.nn.quantized import functional as qF
&gt;&gt;&gt; filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float)
&gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float)
&gt;&gt;&gt; bias = torch.randn(8, dtype=torch.float)
&gt;&gt;&gt;
&gt;&gt;&gt; scale, zero_point = 1.0, 0
&gt;&gt;&gt; dtype_inputs = torch.quint8
&gt;&gt;&gt; dtype_filters = torch.qint8
&gt;&gt;&gt;
&gt;&gt;&gt; q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
&gt;&gt;&gt; q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
&gt;&gt;&gt; qF.conv3d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.max_pool2d">
<code>torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#max_pool2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters are propagated to the output.</p> </div> <p>See <code>MaxPool2d</code> for details.</p> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.adaptive_avg_pool2d">
<code>torch.nn.quantized.functional.adaptive_avg_pool2d(input, output_size)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#adaptive_avg_pool2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters propagate to the output.</p> </div> <p>See <code>AdaptiveAvgPool2d</code> for details and output shape.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>output_size</strong> – the target output size (single integer or double-integer tuple)</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.avg_pool2d">
<code>torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#avg_pool2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies 2D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kH \times kW</annotation></semantics></math></span></span> </span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sH \times sW</annotation></semantics></math></span></span> </span> steps. The number of output features is equal to the number of input planes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters propagate to the output.</p> </div> <p>See <code>AvgPool2d</code> for details and output shape.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> – quantized input tensor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>minibatch</mtext><mo separator="true">,</mo><mtext>in_channels</mtext><mo separator="true">,</mo><mi>i</mi><mi>H</mi><mo separator="true">,</mo><mi>i</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{minibatch} , \text{in\_channels} , iH , iW)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>kernel_size</strong> – size of the pooling region. Can be a single number or a tuple <code>(kH, kW)</code>
</li> <li>
<strong>stride</strong> – stride of the pooling operation. Can be a single number or a tuple <code>(sH, sW)</code>. Default: <code>kernel_size</code>
</li> <li>
<strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a single number or a tuple <code>(padH, padW)</code>. Default: 0</li> <li>
<strong>ceil_mode</strong> – when True, will use <code>ceil</code> instead of <code>floor</code> in the formula to compute the output shape. Default: <code>False</code>
</li> <li>
<strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation. Default: <code>True</code>
</li> <li>
<strong>divisor_override</strong> – if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.interpolate">
<code>torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#interpolate"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Down/up samples the input to either the given <code>size</code> or the given <code>scale_factor</code></p> <p>See <a class="reference internal" href="nn.functional#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a> for implementation details.</p> <p>The input dimensions are interpreted in the form: <code>mini-batch x channels x [optional depth] x [optional height] x width</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters propagate to the output.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only 2D/3D input is supported for quantized inputs</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only the following modes are supported for the quantized inputs:</p> <ul class="simple"> <li><code>bilinear</code></li> <li><code>nearest</code></li> </ul> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor</li> <li>
<strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – output spatial size.</li> <li>
<strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</li> <li>
<strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – algorithm used for upsampling: <code>'nearest'</code> | <code>'bilinear'</code>
</li> <li>
<strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to <code>True</code>, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to <code>False</code>, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation <em>independent</em> of input size when <code>scale_factor</code> is kept the same. This only has an effect when <code>mode</code> is <code>'bilinear'</code>. Default: <code>False</code>
</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.hardswish">
<code>torch.nn.quantized.functional.hardswish(input, scale, zero_point)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#hardswish"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="nn.functional#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish"><code>hardswish()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> – quantized input</li> <li>
<strong>scale</strong> – quantization scale of the output tensor</li> <li>
<strong>zero_point</strong> – quantization zero point of the output tensor</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.upsample">
<code>torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#upsample"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Upsamples the input to either the given <code>size</code> or the given <code>scale_factor</code></p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.quantized.functional.interpolate" title="torch.nn.quantized.functional.interpolate"><code>torch.nn.quantized.functional.interpolate()</code></a>. This is equivalent with <code>nn.quantized.functional.interpolate(...)</code>.</p> </div> <p>See <a class="reference internal" href="nn.functional#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a> for implementation details.</p> <p>The input dimensions are interpreted in the form: <code>mini-batch x channels x [optional depth] x [optional height] x width</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters propagate to the output.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only 2D input is supported for quantized inputs</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only the following modes are supported for the quantized inputs:</p> <ul class="simple"> <li><code>bilinear</code></li> <li><code>nearest</code></li> </ul> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – quantized input tensor</li> <li>
<strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – output spatial size.</li> <li>
<strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>]</em>) – multiplier for spatial size. Has to be an integer.</li> <li>
<strong>mode</strong> (<em>string</em>) – algorithm used for upsampling: <code>'nearest'</code> | <code>'bilinear'</code>
</li> <li>
<strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to <code>True</code>, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to <code>False</code>, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation <em>independent</em> of input size when <code>scale_factor</code> is kept the same. This only has an effect when <code>mode</code> is <code>'bilinear'</code>. Default: <code>False</code>
</li> </ul> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>With <code>align_corners = True</code>, the linearly interpolating modes (<code>bilinear</code>) don’t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is <code>align_corners = False</code>. See <a class="reference internal" href="generated/torch.nn.upsample#torch.nn.Upsample" title="torch.nn.Upsample"><code>Upsample</code></a> for concrete examples on how this affects the outputs.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.nn.quantized.functional.upsample_bilinear">
<code>torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#upsample_bilinear"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Upsamples the input, using bilinear upsampling.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.quantized.functional.interpolate" title="torch.nn.quantized.functional.interpolate"><code>torch.nn.quantized.functional.interpolate()</code></a>. This is equivalent with <code>nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters propagate to the output.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only 2D inputs are supported</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – quantized input</li> <li>
<strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – output spatial size.</li> <li>
<strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – multiplier for spatial size</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function" id="module-torch.nn.quantized"> <dt id="torch.nn.quantized.functional.upsample_nearest">
<code>torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/functional.html#upsample_nearest"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Upsamples the input, using nearest neighbours’ pixel values.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function is deprecated in favor of <a class="reference internal" href="#torch.nn.quantized.functional.interpolate" title="torch.nn.quantized.functional.interpolate"><code>torch.nn.quantized.functional.interpolate()</code></a>. This is equivalent with <code>nn.quantized.functional.interpolate(..., mode='nearest')</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The input quantization parameters propagate to the output.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only 2D inputs are supported</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – quantized input</li> <li>
<strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>] or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – output spatial size.</li> <li>
<strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – multiplier for spatial size. Has to be an integer.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="relu6">ReLU6</h2> <dl class="class"> <dt id="torch.nn.quantized.ReLU6">
<code>class torch.nn.quantized.ReLU6(inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/activation.html#ReLU6"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies the element-wise function:</p> <p><span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU6</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU6}(x) = \min(\max(x_0, x), q(6))</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span></span> </span> is the zero_point, and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(6)</annotation></semantics></math></span></span> </span> is the quantized representation of number 6.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>inplace</strong> – can optionally do the operation in-place. Default: <code>False</code></p> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span> </span> where <code>*</code> means, any number of additional dimensions</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span> </span>, same shape as the input</li> </ul> </dd> </dl> <img alt="_images/ReLU61.png" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAMAAAACDyzWAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAAtFBMVEX///8fd7TV1dXg4OAAAAD+/v79/f78/Pzx8fHMzMzl5eVup87r6+vPz8/a2tr5+vr29vaawd3Jycmfn58hISHAwMANDQ2hxuCAgICtra09PT1fX18XFxcxMTHv7+/z+PqlpaUGBgYofLdRUVFHR0dpaWmYmJhQlMMzg7uvzuRwcHCHh4dAi7/B2ep3d3dfncmRkZHR4++6urvp8ve0tbbe6/SLuNh7r9NtbW15oLsdcKr/d9WSIoZRAAAY6klEQVR42uxda2PauBYUIGPLizF1jBeMTexAaAstSwnpdu///2FXEo88AD+wABtmPig0oLEUT8/oCDgiBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAsa9CHxedpLfHoVxnSY6UIpREAV8EA5goH54Zdd2t/c4mf5o0+74kfke56/TCCLFokCXPUy6Kbl+Q+RnTzoTERARQTYiyYD6nYSBKitBdijo8niOSm6jV358o5++OlnKn8ce3ojLrpMHXQmIqAiAmS8ndMoNQI6dJFGthZgmm6yDIgUJwIqJMAVnfG2PvLisH8sAo6Cpma87+kHsTtZRy3uzWFEfGHnrrTgGW2IJ+bxkHRHbhxMeYQdiKfpxjmtMbdzRw6gOw28UWvDKjkGxB3If/hiLNEsiP22+LfDO4WLbERAlQS4oFxJdS+cLXyhvYMRMBgtAhq8RcFwsFiMZVR8oOFsMhiQZRhEUV8KsCEVTdwxIX/G/Ek6EvKhURStdVP3gt7MjR3Z2V9MxfMSy2e+JnA+CDAMFz0vFM/Fbm/yZ5yNCKiKAJctsx/EPAsZh3ztr/nu4Qg4pIE3i8Z0su3Z2Zru0AvFY21rwTIJ8YViHPFIvk5GxI1zCt2MYh7STM8XAxjzntNd4rv+H/FegK4t/ofUie66w/V1MhEB1cmCqcsX/i3aa3H0qHkwAjaoWCc2w+Bd52Frxu93f/vq9wJcUC6Mabz27E5LMr7pRvdG67WcwQfQl1ewjgpQxFKLv4rtFqGZiICKCHCy7I89mWRsYB2MgC0a6zIVbmwzlbEnXt3g0a29L0CT60ZzhToaA/m6h/e6Mdc7KCKuPVBHrjq7RwUYbTijXX6ciQiozhpQ9wODC3C+lDDeBBgPNi9ySDOWsW+yjTDtOJysltNjAhQe7Ajt6G4w6y8fEgTIUgTYzypABgFWNQkRPtei871tGNeXP+bclnl+assIuNmzXshQOOftQQvmSm1PvY4wT/GvpWj/HHbOQwIMR9vLvwnwzYIzEQEVEiAJgw7xPamt1jsBTuWzQzfcZsod+XAnwGHAW2OXhIy8NwHyJWUwIhsBamPRztcJwjp3aOxyh0MCHAW22Bz6IMDmLgnJRARUSYB9rq66F8wnvXEo7uKoJ9AwA2866bmxuKmdMJ4vQroiOwtezNxQyFBuwzwPRK47jVbbt+L4ElFIx+YWvPBD8bs+HbzbPZltd08OCXBJ/ck8+BgB19sw03FGIqBKAuTRRSftQRAHY7kPuEaXmPxX3tiRL20NvDh8e59sFcbu7EEa8cqPxUY06fB8w90KcEI9uQNTH3vBs4yD+p/g0P7xIQGSRRD77OMaUOY9YiM6GxFw49D2H2r4qwAAAAAAAAAAAAAAAACAgG4ODeB+MDSb5RKgSYH7glkuAQ6pmfqfpqskSIKmDDRm2T6hbVAj1aWZkq8bgqYMNBnuNwQIGgjwPbSWkjf1QVMGmioKELghVFGAzbqSzB00ZaDBGhA0WANCgBAgBAgaCDBrFmyoSdFAUwIaZMEA7ndeC7bU+ANoSkCDNSBosAaEACFACBA0EGDWLLijJkUDTQloKpkFKyr+DpoS0MCCQQMLhgAhQAgQNBAgBAgBXhymKKrHkIQgCbkOhu7AaSzbuQaEjY8borm2AOc+LPh2aR6dxzS0rixAdzoKwu3hQvb6e6K63iRNXde1D62+a3Vm84Zounjh+zapk3zJh06SJm+n/Svpjp2/0/6VJE3hOUma4nMiNqcpPKfm76daKv53ZQHG8dyaxJsjTnvrwsuMNUiDMWaSNm9bpM5bg1i87ei8YXbXYcwiBn9YJy3etonJ0jrpHfapk75y8nfav5IeOfk77V/J4DTF50QanKb4nIjFaYrOqfu1VquAAIUF/1n7sJY1AlpqIqClJgIyNRGQqYmATE0EZIUj4M9vtafvjlF6CxYn/kyCEu8LASctIn/Xal9fN1XZNe1Tq721V77f2kAEv6mfLwvGh+BLTqO9cPv9167CR/KdeNaOvAhZ8E3RSPutxka0Jo5zmWAb5pZo7LX9VkOAe2e3QIBVp3ld229VBHjCgFAMo8Q02s+ntf1mpMHXMgGVsP/d2m9V7zfKs1WZRtjvbzsPDT6OBRp1NN+fat9+5qOBAEGjikba7wuBAKGcq9BI+9XJHQiw2VaT6YFGJc1n+81IgywYUJb9vtzC/c6SBZtqMj3QKKN5/bFnvxlpsAYETXGaQ/aLJAQ0F6J5/KtW++eFQIBQzlVopP02yR0JsNlQk+mBRgXN30fsNyMNsmCgCI7b7w1nwQhdpaH5xe33S7MIDdaAoDmZRhP2+6sYDQQImlNp0u0XAgTN+WhS7PeGBYj3HkpAI+z3x6/io0EWDJyc/T7e4v3Gp2GqQMPt9+mLpmI0WAOCJjeN9qWWbr9IQkBzHhrt8Z9a7a9HAgFCOVeh+fWt9vS3Ru5YgPge2hVpmlntN+NokAUDefCS2X5vOQtGLYJr0fzMbr8ZR4M1IGgy0zR/57BfJCGgUUyT334hQNCoo9kW/YMAUZP0CjTSfl/VjwZZMJAp+xVF/x7v4X5nsWBLjc2AJjPNCfabcTRYA4ImlUb/nbPoH5IQKEchzctbzV0IEMq5OM2J9nvLWTDOp7wczcn2m3E0lcyCcULvxWheT7bfjKOBBYPmOI1WwH6xBgRNUZrcJe8hQChHIU1B+62OAGd0CgGWj+Z7QtWhmxKg44b5BIjs4QI0J9fcrVwSYrhLP58AsX9yfppfXw/W3FU/musLcDAlPiy4ZDQK7LcqFhyFnZ0AbcMwTDpMO6lenm5f/Hh7SVP8eHvdsfWix9vzl0uawnOSNIXn9Ciz3+Jz0mw+mpROwysLsBFYZCfAHhXoMtYgDcaYSdq8bZE6bw1i8baj84bZXYcxixj8YZ20eNsmJkvrpHfYp076ysnfaf9KeuTk77R/JYPTFJ8TaXCaonNa/uD6cxTMqeNETlon58oC7NM19DwR0FITAS01EZCpiYBMTQRkhSOgtF9OoyICstJHQKPOEQ7q5d2YvDPYRWvuqt/3PTM0kjcJwWfpz0ezKXl/udGUIuAgCy4JTXOb/V5uNHgrDjQ7vNXchQChnMvTvKu5CwEmGgVqapyB5kPJ+8uNBl/LBD7Z753fb5RnuwrNp5L3lxsN1oCgOVDyHmtAKOeCNPsl7yFAKOdyNAdK3kOAibkVzldQSKN9OXDizOVGgyz4vpGj5D2y4N0fDWdsKaM5UvL+cqPBGvCeaY6WvMcaEMq5AM3xmrsQIJRzdhot4cQZCDDROCp5un3ZaBJPnLncaJAF3ylerpz9IgLeN01K1SFEQCzezkmTWvIea0Ao54w0wn6TS95DgFDO+WgyFP2DABP3D/AWRgEaPcuJM3gnBDhX9lu46B+yYHyM5XSajDV38WkYLN7OQWNnLXmPNSCUcwaa7PYLAUI5ymmaOUreQ4CJuRW+znYCTa6S9/hWHKAYryXLfiudBaOkQW6anDV3URkBizeVNLlL3mMNCOUopHnNXfIeAoRy1NGcUPIeAkzMrVDaNAfNSSfO3FmFVGTBZ8x+f6g4cQZZ8IfAbqnxh3ugOfHEmctNCmvAW6YRJ86cVPQPa0AIUAHNpuQ9BAgBXoOmWeDANwgwMbfCMZcZaArV3L3cpCqZBeOg33SaTzV3SzspWPBN0nwoeQ8LhgAvTFO45D0ECAEWoClov/clwFnoBaM2BKiOZq/kPQSYhPFD3Rq7HSQhqmj2S94jCUlDi3bzDAjbMAn4uV/yvtSTKoUA2zTXgdWw4ON3PKnoHyz42F9t7K8f2IZhmHSYdhS8PN2++PH2kqbg8fa81R1bV3C8vaQpOidRdeifl+JzIjYfTfE5aYImpdOwBAJ8ds31gx4V6DLWIA3GmEnavG2ROm8NYvG2o/OG2V2HMYsY/GGdtHjbJiZL66R32KdO+srJ32n/Snrk5O+0fyWD0xSd0+pbrfafo2BOxOKjKT6njhM5aZ2c6wvwOWhsImHWCGipiYCWmgjI1ERAVjQC2tJ+OY2KCMjUREBW/gioPQftUm9MVgVlqbmrft/3zPHP65qmmWsbBh/JP9B9U/L+/+yda3OqShaGewDD5Yi4ESmN4NHycmqSKk8mZc3smf//w6bB7FtItmC3Is3zfmAnO8UryGP3Wi2s1bmTah3AMuxzJmTBSjbfS9537qT4Ks4Emx/TLwAC4O1tyqpDAwGANwKQ0hy/7vlzzd3OnRSPZZqQ/f7nz84ePuXZOm7zruhf506KGLDTNpWS98SAAHhDm2rNXQAEwNvZfFBzFwBvkQXTpuHb9Fupudu5kyIL7mz2e6c1d/uQBdOqSww+KXnfuZMiBuykzacl74kBAfAGNp+XvAdAALy+zW+qDgHgLbLg23a3vzeb33ac6dxJkQV3Tc1L3pMFMwLqszlT9I8RkBjwmjZnS94TAwLgFW3Ol7wHQAC8nk2NmrsAeAMAe/pNSK2S93wTQhZ8rez34pL3ZMG6s+Ae3g0zqFnynrthiAGvYVO75i4xIABewaZ+zV0ABEDtNk1K3gNgVcmw2C4TfVlwv56Ka1TynqfiqnJKAIM1WfBFUi953+8seD53JvPjfL7TNwL2qTJC044zVEaojH8nrZMjMWBzm8Yl74kBP4sBSUIusPlX45L3AEgWrM1m8KV5xxkArGr2Jn1ZcD8qpP55Sc1dKqRWlRdK05wsuGn2W9TcNV23ut7eZqJvCg71zA/3bPNwYceZzr03NxtwQn3LMD2IAS8ueU8M+JmmKQDWtflW8h4Ateil0CFbAGBNmweFhm8AWFVSKD942gA0vFumUseZzr03nbwj2uh+wT+VvDfnpNoG0LZ1AmjyFPzwvuYuU7D6UcxSx0kPEQCet1Fu+AaAVe2yfRjusx0AnrX5pOgfACopLe+DOX62DPOSrPMYAKWNqzj9AuDHyqxia2Uf/3W1nli7dEgS4uqpuUsSUtFsId/VaPHJzQi5nJofsmeWYf6pPP2yDPOxNmm23WbpRqr6x8iZy+3i9JdTx/S//zinv/7Qoruy+bss+qfaBV64sZ6O6bGejunxPXRMX/xQ9Y+BU8R/h9OtMrPy5un//aOf+jsOhef7viWGcjsSgdzawpbbQIzkdigsufVEKLdjV258dyw3v+5kr+LmO1VfKZQ2jXeqvlK8OntOccsL0T8DODiNgP/+ck7//aJF92VzjC4YYhgBa+hxWc71jx/+8ZcpuG4W3IPbsXpjc7PHMoefPJaZf22chCCDdPXrHYbONJR6Tn63DBM0yoL7cUt+P2xu9Vimk05YiMamBQBt2/Ft2w7qnhAAAuC9xwRccgBsosmbtAHYp9Icxtvc4maEQs5a3zMhiCy4sUbbV20H1LPybGbb3GzA8XksE5s2AQx5LBObVgAcHIvygPt8C4DYtDICluvQ2SLQlwX3sE2DsTY3mYKHDSoEkgWTBevVcpfJAXC31HdAPW3VZabN1QFcJunu5WWXJkttABJ1EQPW11NeBn9B/gSA2LQAYPK2AH1kHRCbNgBcv2W/GvuEdK4tPTYtAphNT/9OM7Jg1ML1XjyWz1pHj/rqAzLmMALWV5Alz/P5c5LZ2gAk6iIGbCB7W3wTsh0JAMSmnZBrGcf1vwoBQAC896CU7x4MsulkiV5EFtzmAXH/iUE2PBWHDTEgAAIgAGIDgHWzYJ5DM8eGLBhxvZtmwdQiMMeGGBAbYkAABEAAxAYA62bB1CQ1x4YsGHG9m07B1KU3x4YYEBtiQAAEQADEBgDrZsFGdsvsqU0ns2Az+wX304YpGBumYAAEQADEpo8A2otkncwiAATAdnRcvI7m2RNJCElIi/qlkyvLMP2yuQcAD/nbD5HneYGzPNcK3vUjV0N7+9JGvb29G0euhvb2pY3yOZU26uckImmjfk6DwubMTsv2ARyl+7efZmVHh6nv28L2fT8QI7kdCktuPRHK7diVGz+axr4fCk/+aImh3I5E4J/byR3773Zyj3Hznaqv5K7i5jtVX8mTNurnJGxpo35OIpQ26uc0jlfxuZ3iNgA8fOtibclfguR75UBGQEbAm2hovSkq+XtougzDDVDm2LQdAwbJxr3vhUnU9rrvdfnbBlINs2BugjfHpmUAJ6dYkIVoFqI7NCRzyQEQALHpL4AUwzDIhscyEde74QFREM0gG2JAbIgBARAAARAbAKybBdMYwRwbsmDE9W6aBdMcyxwbYkBsiAEBEAABEBsArJsF97S7vZE2ZMGI680IyAhIDIgNMSAAAiAAYgOAZ8R3DwbZkAUjrnfTLJj7T8yxIQbEhhgQAAEQALEBwLpZMM+hmWNDFoy43k2zYGoRmGNDDIgNMSAAAiAAYgOAdbNgapKaY0MWjLjeTadg6tKbY0MMiA0xIAACIABiA4B1s2D6U5pj08ksmA695tgwBWPDFAyAAAiA2PQUwCh3QgAEwNb0ddsUQMJ+khB9OiZWUwBZ+DDIpvWG1Zlvfwcw8jwvcJbnWsG7fuRqaG9f2qi3t3fjyNXQ3r60UT6n0kb9nEQkbdTPaVDYnNlp2S6Ag+1M/ABwVjYPnvq+LWzf9wMxktuhsOTWE6Hcjl258aNp7Puh8OSPlhjK7UgE/rmd3LH/bif3GDffqfpK7ipuvlP1lTxpo35OwpY26uckQmmjfk7jeBWf2yluA8CD8ybr5dH9AeCAEZAR8CYaWm+KNicQF82yYG6AMsem5RjQlhS+OvPgjhcmUdvrvleOAn+KAetmwdwEb47NHQw4NgvRLER3akjmkgMgAGLTXwAphmGQDY9lIq53wwOiIJpBNsSA2BADAiAAAiA2AFg3C6Yxgjk2ZMGI6900C6Y5ljk2xIDYEAMCIAACIDYAWDcL7ml3eyNtyIIR15sRkBGQGBAbYkAABEAAxAYAz4jvHgyyIQtGXO+mWTD3n5hjQwyIDTEgAAIgAGIDgHWzYJ5DM8eGLBhxvZtmwdQiMMeGGBAbYkAABEAAxAYA62bB1CQ1x4YsGHG9m07B1KU3x4YYEBtiQAAEQADEBgDrZsH0pzTHppNZMB16zbFhCsaGKRgAAbA1HfN1ugFAAGxJ83Q/slYACIDtyM0mJCEkIe0pdiZ5trWaHRALHwbZtAzgyknm/iIbnn6LPM8LnOW5VvCuH7ka2tuXNurt7d04cjW0ty9tlM+ptFE/JxFJG/VzGhQ2Z3ZatgHgwXmTtXL2krtsf/r/WfmfU9+3he37fiBGcjsUltx6IpTbsSs3fjSNfT8UnvzREkO5HYnAP7eTO/bf7eQe4+Y7VV/JXcXNd6q+kidt1M9J2NJG/ZxEKG3Uz2kcr+JzO8VtADi03hRNnan8PT8wAjICtrPqt578NALWzYK5Acocm7aXYZ6y19EiW97xyji67hDU8vWOnrK0cRbMTfDm2PBVHDZ9XogGQAAEQGwAsNEBUQzDIBsey0Rc74YHREE0g2yIAbEhBgRAAARAbACwbhZMYwRzbMiCEde7aRZMcyxzbIgBsSEGBEAABEBsALBuFtzT7vZG2pAFI643IyAjIDEgNsSAAAiArWvpBN4ZLadLT4OwuQebwFneF4CBg/ql4L4AfAhqfGgCHR9PbO7BZhk8iK5JU9iATVdsABAbAORaAeC9KJpF2PTJBiGEEEIIIYTQ+w5LFydquRMqWtiLZJ0o5nsvyTqPVc/lOU+zzUjL2/vsPCl7BIt0nfuG8lfpsHSpvm6VATwuXkfzTOmCrdYTa5cOFQ9kO7HCbTLW8K7ESa4M4DJZxPbryEz+qh2WLoUnsZQBPA0Zicre+U6Ih+xZw3EMy74DivKS10dlAA+PBk/A1Q5LF84SmW/rAXCWq8QBzlxuFzpCipGj/q6IxZNQBzB52mT53lAAf+2wdLEG25nQA+AoVXmrA6eI/w65+nE8bDWMO6t8rAHA9foQ7tcTk7D7rMPSxTYvj+7FAP5wKQhKFuIuANwl6rfX2Zl8RzQAWHwUvho1D3/WYelim80JoYWaS8mf0l1t2qbgXWZrSPDePlmKd+WXn8l9ZuYUXO2wdNlnXfLz6sxVB40g2SherPyrjiRksMt0JJ1e8bHKF4qx5GBRDH5PpmYilQ5Ll75NGmLAINkGUurLMKofhF06lcehYRlmoGMKjtfPo1W6MhTASoely0dBZQAnpxlLyUPLQvTpOPSE/eoADo75OjE1C0Yd0IC3ACGEEEIIIYQQQgghhFrUb+88mKS8QQgAkdkAPn49pNlM/uzst+tkLsS0KNYYOva0+Ep3xnuErgxgOhtNnFcJYLYfzRzrO4DRSxoENAtA1wawuFeuuKnW2YnyuaRvADIFo5sAWHC3WUgAizuonh4BEN0WwKePAIwBEN0awGIolOOhVTzgtJcArgAQ3RLAbFImIVGyGR0TCWDsvA7HvEfoVgAWyzAr+cs0Xz/OJYBil7EMg26m8qFMhAAQASD6f3twSAAAAAAg6P9rgzcAAAAAAAAAFk/NiBkAZ20uAAAAAElFTkSuQmCC"> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.quantized.ReLU6()
&gt;&gt;&gt; input = torch.randn(2)
&gt;&gt;&gt; input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl>   <h2 id="elu">ELU</h2> <dl class="class"> <dt id="torch.nn.quantized.ELU">
<code>class torch.nn.quantized.ELU(scale, zero_point, alpha=1.0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/activation.html#ELU"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized equivalent of <a class="reference internal" href="generated/torch.nn.elu#torch.nn.ELU" title="torch.nn.ELU"><code>ELU</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>scale</strong> – quantization scale of the output tensor</li> <li>
<strong>zero_point</strong> – quantization zero point of the output tensor</li> <li>
<strong>alpha</strong> – the alpha constant</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="hardswish">Hardswish</h2> <dl class="class"> <dt id="torch.nn.quantized.Hardswish">
<code>class torch.nn.quantized.Hardswish(scale, zero_point)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/activation.html#Hardswish"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.hardswish#torch.nn.Hardswish" title="torch.nn.Hardswish"><code>Hardswish</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>scale</strong> – quantization scale of the output tensor</li> <li>
<strong>zero_point</strong> – quantization zero point of the output tensor</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="conv1d">Conv1d</h2> <dl class="class"> <dt id="torch.nn.quantized.Conv1d">
<code>class torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/conv.html#Conv1d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 1D convolution over a quantized input signal composed of several quantized input planes.</p> <p>For details on input arguments, parameters, and implementation see <a class="reference internal" href="generated/torch.nn.conv1d#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only <code>zeros</code> is supported for the <code>padding_mode</code> argument.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only <code>torch.quint8</code> is supported for the input data type.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Conv1d.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – packed tensor derived from the learnable weight parameter.</li> <li>
<strong>~Conv1d.scale</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scalar for the output scale</li> <li>
<strong>~Conv1d.zero_point</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scalar for the output zero point</li> </ul> </dd> </dl> <p>See <a class="reference internal" href="generated/torch.nn.conv1d#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>Conv1d</code></a> for other attributes.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.quantized.Conv1d(16, 33, 3, stride=2)
&gt;&gt;&gt; input = torch.randn(20, 16, 100)
&gt;&gt;&gt; # quantize input to quint8
&gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0,
                                        dtype=torch.quint8)
&gt;&gt;&gt; output = m(q_input)
</pre> <dl class="method"> <dt id="torch.nn.quantized.Conv1d.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/conv.html#Conv1d.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a quantized module from a float module or qparams_dict.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by the user</p> </dd> </dl> </dd>
</dl> </dd>
</dl>   <h2 id="conv2d">Conv2d</h2> <dl class="class"> <dt id="torch.nn.quantized.Conv2d">
<code>class torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/conv.html#Conv2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 2D convolution over a quantized input signal composed of several quantized input planes.</p> <p>For details on input arguments, parameters, and implementation see <a class="reference internal" href="generated/torch.nn.conv2d#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only <code>zeros</code> is supported for the <code>padding_mode</code> argument.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only <code>torch.quint8</code> is supported for the input data type.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Conv2d.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – packed tensor derived from the learnable weight parameter.</li> <li>
<strong>~Conv2d.scale</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scalar for the output scale</li> <li>
<strong>~Conv2d.zero_point</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scalar for the output zero point</li> </ul> </dd> </dl> <p>See <a class="reference internal" href="generated/torch.nn.conv2d#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>Conv2d</code></a> for other attributes.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation
&gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
&gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)
&gt;&gt;&gt; # quantize input to quint8
&gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
&gt;&gt;&gt; output = m(q_input)
</pre> <dl class="method"> <dt id="torch.nn.quantized.Conv2d.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/conv.html#Conv2d.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a quantized module from a float module or qparams_dict.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by the user</p> </dd> </dl> </dd>
</dl> </dd>
</dl>   <h2 id="conv3d">Conv3d</h2> <dl class="class"> <dt id="torch.nn.quantized.Conv3d">
<code>class torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/conv.html#Conv3d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 3D convolution over a quantized input signal composed of several quantized input planes.</p> <p>For details on input arguments, parameters, and implementation see <a class="reference internal" href="generated/torch.nn.conv3d#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only <code>zeros</code> is supported for the <code>padding_mode</code> argument.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only <code>torch.quint8</code> is supported for the input data type.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Conv3d.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – packed tensor derived from the learnable weight parameter.</li> <li>
<strong>~Conv3d.scale</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scalar for the output scale</li> <li>
<strong>~Conv3d.zero_point</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scalar for the output zero point</li> </ul> </dd> </dl> <p>See <a class="reference internal" href="generated/torch.nn.conv3d#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>Conv3d</code></a> for other attributes.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation
&gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))
&gt;&gt;&gt; input = torch.randn(20, 16, 56, 56, 56)
&gt;&gt;&gt; # quantize input to quint8
&gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
&gt;&gt;&gt; output = m(q_input)
</pre> <dl class="method"> <dt id="torch.nn.quantized.Conv3d.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/conv.html#Conv3d.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a quantized module from a float module or qparams_dict.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by the user</p> </dd> </dl> </dd>
</dl> </dd>
</dl>   <h2 id="floatfunctional">FloatFunctional</h2> <dl class="class"> <dt id="torch.nn.quantized.FloatFunctional">
<code>class torch.nn.quantized.FloatFunctional</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/functional_modules.html#FloatFunctional"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>State collector class for float operations.</p> <p>The instance of this class can be used instead of the <code>torch.</code> prefix for some operations. See example usage below.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This class does not provide a <code>forward</code> hook. Instead, you must use one of the underlying functions (e.g. <code>add</code>).</p> </div> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; f_add = FloatFunctional()
&gt;&gt;&gt; a = torch.tensor(3.0)
&gt;&gt;&gt; b = torch.tensor(4.0)
&gt;&gt;&gt; f_add.add(a, b)  # Equivalent to ``torch.add(a, b)``
</pre> <dl class="simple"> <dt>Valid operation names:</dt>
<dd>
<ul class="simple"> <li>add</li> <li>cat</li> <li>mul</li> <li>add_relu</li> <li>add_scalar</li> <li>mul_scalar</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="qfunctional">QFunctional</h2> <dl class="class"> <dt id="torch.nn.quantized.QFunctional">
<code>class torch.nn.quantized.QFunctional</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/functional_modules.html#QFunctional"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wrapper class for quantized operations.</p> <p>The instance of this class can be used instead of the <code>torch.ops.quantized</code> prefix. See example usage below.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This class does not provide a <code>forward</code> hook. Instead, you must use one of the underlying functions (e.g. <code>add</code>).</p> </div> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; q_add = QFunctional()
&gt;&gt;&gt; a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)
&gt;&gt;&gt; b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)
&gt;&gt;&gt; q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(a, b, 1.0, 0)``
</pre> <dl class="simple"> <dt>Valid operation names:</dt>
<dd>
<ul class="simple"> <li>add</li> <li>cat</li> <li>mul</li> <li>add_relu</li> <li>add_scalar</li> <li>mul_scalar</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="quantize">Quantize</h2> <dl class="class"> <dt id="torch.nn.quantized.Quantize">
<code>class torch.nn.quantized.Quantize(scale, zero_point, dtype)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules.html#Quantize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Quantizes an incoming tensor</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>scale</strong> – scale of the output Quantized Tensor</li> <li>
<strong>zero_point</strong> – zero_point of output Quantized Tensor</li> <li>
<strong>dtype</strong> – data type of output Quantized Tensor</li> </ul> </dd> <dt class="field-even">Variables</dt> <dd class="field-even">
<p><strong>zero_point, dtype</strong> (<em>`scale`</em><em>,</em>) – </p> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; t = torch.tensor([[1., -1.], [1., -1.]])
&gt;&gt;&gt; scale, zero_point, dtype = 1.0, 2, torch.qint8
&gt;&gt;&gt; qm = Quantize(scale, zero_point, dtype)
&gt;&gt;&gt; qt = qm(t)
&gt;&gt;&gt; print(qt)
tensor([[ 1., -1.],
        [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)
</pre> </dd> </dl> </dd>
</dl>   <h2 id="dequantize">DeQuantize</h2> <dl class="class"> <dt id="torch.nn.quantized.DeQuantize">
<code>class torch.nn.quantized.DeQuantize</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules.html#DeQuantize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dequantizes an incoming tensor</p> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; input = torch.tensor([[1., -1.], [1., -1.]])
&gt;&gt;&gt; scale, zero_point, dtype = 1.0, 2, torch.qint8
&gt;&gt;&gt; qm = Quantize(scale, zero_point, dtype)
&gt;&gt;&gt; quantized_input = qm(input)
&gt;&gt;&gt; dqm = DeQuantize()
&gt;&gt;&gt; dequantized = dqm(quantized_input)
&gt;&gt;&gt; print(dequantized)
tensor([[ 1., -1.],
        [ 1., -1.]], dtype=torch.float32)
</pre> </dd> </dl> </dd>
</dl>   <h2 id="linear">Linear</h2> <dl class="class"> <dt id="torch.nn.quantized.Linear">
<code>class torch.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as <code>torch.nn.Linear</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Linear">https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</a> for documentation.</p> <p>Similar to <a class="reference internal" href="generated/torch.nn.linear#torch.nn.Linear" title="torch.nn.Linear"><code>Linear</code></a>, attributes will be randomly initialized at module creation time and will be overwritten later</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Linear.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the non-learnable quantized weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_features</mtext><mo separator="true">,</mo><mtext>in_features</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_features}, \text{in\_features})</annotation></semantics></math></span></span> </span>.</li> <li>
<strong>~Linear.bias</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the non-learnable bias of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_features</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_features})</annotation></semantics></math></span></span> </span>. If <code>bias</code> is <code>True</code>, the values are initialized to zero.</li> <li>
<strong>~Linear.scale</strong> – <code>scale</code> parameter of output Quantized Tensor, type: double</li> <li>
<strong>~Linear.zero_point</strong> – <code>zero_point</code> parameter for output Quantized Tensor, type: long</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.quantized.Linear(20, 30)
&gt;&gt;&gt; input = torch.randn(128, 20)
&gt;&gt;&gt; input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 30])
</pre> <dl class="method"> <dt id="torch.nn.quantized.Linear.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/linear.html#Linear.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a quantized module from a float module or qparams_dict</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by the user</p> </dd> </dl> </dd>
</dl> </dd>
</dl>   <h2 id="batchnorm2d">BatchNorm2d</h2> <dl class="class"> <dt id="torch.nn.quantized.BatchNorm2d">
<code>class torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/batchnorm.html#BatchNorm2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d" title="torch.nn.BatchNorm2d"><code>BatchNorm2d</code></a>.</p> </dd>
</dl>   <h2 id="batchnorm3d">BatchNorm3d</h2> <dl class="class"> <dt id="torch.nn.quantized.BatchNorm3d">
<code>class torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/batchnorm.html#BatchNorm3d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d" title="torch.nn.BatchNorm3d"><code>BatchNorm3d</code></a>.</p> </dd>
</dl>   <h2 id="layernorm">LayerNorm</h2> <dl class="class"> <dt id="torch.nn.quantized.LayerNorm">
<code>class torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/normalization.html#LayerNorm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.layernorm#torch.nn.LayerNorm" title="torch.nn.LayerNorm"><code>LayerNorm</code></a>.</p> <dl class="simple"> <dt>Additional args:</dt>
<dd>
<ul class="simple"> <li>
<strong>scale</strong> - quantization scale of the output, type: double.</li> <li>
<strong>zero_point</strong> - quantization zero point of the output, type: long.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="groupnorm">GroupNorm</h2> <dl class="class"> <dt id="torch.nn.quantized.GroupNorm">
<code>class torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/normalization.html#GroupNorm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.groupnorm#torch.nn.GroupNorm" title="torch.nn.GroupNorm"><code>GroupNorm</code></a>.</p> <dl class="simple"> <dt>Additional args:</dt>
<dd>
<ul class="simple"> <li>
<strong>scale</strong> - quantization scale of the output, type: double.</li> <li>
<strong>zero_point</strong> - quantization zero point of the output, type: long.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="instancenorm1d">InstanceNorm1d</h2> <dl class="class"> <dt id="torch.nn.quantized.InstanceNorm1d">
<code>class torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm1d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d" title="torch.nn.InstanceNorm1d"><code>InstanceNorm1d</code></a>.</p> <dl class="simple"> <dt>Additional args:</dt>
<dd>
<ul class="simple"> <li>
<strong>scale</strong> - quantization scale of the output, type: double.</li> <li>
<strong>zero_point</strong> - quantization zero point of the output, type: long.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="instancenorm2d">InstanceNorm2d</h2> <dl class="class"> <dt id="torch.nn.quantized.InstanceNorm2d">
<code>class torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d" title="torch.nn.InstanceNorm2d"><code>InstanceNorm2d</code></a>.</p> <dl class="simple"> <dt>Additional args:</dt>
<dd>
<ul class="simple"> <li>
<strong>scale</strong> - quantization scale of the output, type: double.</li> <li>
<strong>zero_point</strong> - quantization zero point of the output, type: long.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="instancenorm3d">InstanceNorm3d</h2> <dl class="class"> <dt id="torch.nn.quantized.InstanceNorm3d">
<code>class torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/normalization.html#InstanceNorm3d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This is the quantized version of <a class="reference internal" href="generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d" title="torch.nn.InstanceNorm3d"><code>InstanceNorm3d</code></a>.</p> <dl class="simple"> <dt>Additional args:</dt>
<dd>
<ul class="simple"> <li>
<strong>scale</strong> - quantization scale of the output, type: double.</li> <li>
<strong>zero_point</strong> - quantization zero point of the output, type: long.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="embedding">Embedding</h2> <dl class="class"> <dt id="torch.nn.quantized.Embedding">
<code>class torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, dtype=torch.quint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/embedding_ops.html#Embedding"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A quantized Embedding module with quantized packed weights as inputs. We adopt the same interface as <code>torch.nn.Embedding</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding">https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</a> for documentation.</p> <p>Similar to <a class="reference internal" href="generated/torch.nn.embedding#torch.nn.Embedding" title="torch.nn.Embedding"><code>Embedding</code></a>, attributes will be randomly initialized at module creation time and will be overwritten later</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<p><strong>~Embedding.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the non-learnable quantized weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>num_embeddings</mtext><mo separator="true">,</mo><mtext>embedding_dim</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{num\_embeddings}, \text{embedding\_dim})</annotation></semantics></math></span></span> </span>.</p> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; m = nn.quantized.Embedding(num_embeddings=10, embedding_dim=12)
&gt;&gt;&gt; indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8])
&gt;&gt;&gt; output = m(indices)
&gt;&gt;&gt; print(output.size())
torch.Size([9, 12]
</pre> </dd> </dl> <dl class="method"> <dt id="torch.nn.quantized.Embedding.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/embedding_ops.html#Embedding.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a quantized embedding module from a float module</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by user</p> </dd> </dl> </dd>
</dl> </dd>
</dl>   <h2 id="embeddingbag">EmbeddingBag</h2> <dl class="class"> <dt id="torch.nn.quantized.EmbeddingBag">
<code>class torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='sum', sparse=False, _weight=None, include_last_offset=False, dtype=torch.quint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/embedding_ops.html#EmbeddingBag"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A quantized EmbeddingBag module with quantized packed weights as inputs. We adopt the same interface as <code>torch.nn.EmbeddingBag</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag">https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag</a> for documentation.</p> <p>Similar to <a class="reference internal" href="generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code>EmbeddingBag</code></a>, attributes will be randomly initialized at module creation time and will be overwritten later</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<p><strong>~EmbeddingBag.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the non-learnable quantized weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>num_embeddings</mtext><mo separator="true">,</mo><mtext>embedding_dim</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{num\_embeddings}, \text{embedding\_dim})</annotation></semantics></math></span></span> </span>.</p> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; m = nn.quantized.EmbeddingBag(num_embeddings=10, embedding_dim=12, include_last_offset=True, mode='sum')
&gt;&gt;&gt; indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8, 6, 6, 9, 1, 6, 8, 8, 3, 2, 3, 6, 3, 6, 5, 7, 0, 8, 4, 6, 5, 8, 2, 3])
&gt;&gt;&gt; offsets = torch.tensor([0, 19, 20, 28, 28, 32])
&gt;&gt;&gt; output = m(indices, offsets)
&gt;&gt;&gt; print(output.size())
torch.Size([5, 12]
</pre> </dd> </dl> <dl class="method"> <dt id="torch.nn.quantized.EmbeddingBag.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/modules/embedding_ops.html#EmbeddingBag.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a quantized embedding_bag module from a float module</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by user</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/torch.nn.quantized.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/torch.nn.quantized.html</a>
  </p>
</div>
