[{"name": "clear()", "path": "backends#clear", "type": "torch.backends", "text": "\nClears the cuFFT plan cache.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "max_size", "path": "backends#max_size", "type": "torch.backends", "text": "\nA `int` that controls cache capacity of cuFFT plan.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch", "path": "torch", "type": "torch", "text": "\nThe torch package contains data structures for multi-dimensional tensors and\ndefines mathematical operations over these tensors. Additionally, it provides\nmany utilities for efficient serializing of Tensors and arbitrary types, and\nother useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations on\nan NVIDIA GPU with compute capability >= 3.0\n\nReturns True if `obj` is a PyTorch tensor.\n\nReturns True if `obj` is a PyTorch storage object.\n\nReturns True if the data type of `input` is a complex data type i.e., one of\n`torch.complex64`, and `torch.complex128`.\n\nReturns True if the data type of `input` is a floating point data type i.e.,\none of `torch.float64`, `torch.float32`, `torch.float16`, and\n`torch.bfloat16`.\n\nReturns True if the `input` is a single element tensor which is not equal to\nzero after type conversions.\n\nSets the default floating point dtype to `d`.\n\nGet the current default floating point `torch.dtype`.\n\nSets the default `torch.Tensor` type to floating point tensor type `t`.\n\nReturns the total number of elements in the `input` tensor.\n\nSet options for printing.\n\nDisables denormal floating numbers on CPU.\n\nNote\n\nRandom sampling creation ops are listed under Random sampling and include:\n`torch.rand()` `torch.rand_like()` `torch.randn()` `torch.randn_like()`\n`torch.randint()` `torch.randint_like()` `torch.randperm()` You may also use\n`torch.empty()` with the In-place random sampling methods to create\n`torch.Tensor` s with values sampled from a broader range of distributions.\n\nConstructs a tensor with `data`.\n\nConstructs a sparse tensor in COO(rdinate) format with specified values at the\ngiven `indices`.\n\nConvert the data into a `torch.Tensor`.\n\nCreate a view of an existing `torch.Tensor` `input` with specified `size`,\n`stride` and `storage_offset`.\n\nCreates a `Tensor` from a `numpy.ndarray`.\n\nReturns a tensor filled with the scalar value `0`, with the shape defined by\nthe variable argument `size`.\n\nReturns a tensor filled with the scalar value `0`, with the same size as\n`input`.\n\nReturns a tensor filled with the scalar value `1`, with the shape defined by\nthe variable argument `size`.\n\nReturns a tensor filled with the scalar value `1`, with the same size as\n`input`.\n\nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rceil with values from the interval `[start,\nend)` taken with common difference `step` beginning from `start`.\n\nReturns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rfloor + 1 with values from `start` to `end`\nwith step `step`.\n\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from `start` to `end`, inclusive.\n\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}} to\nbaseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale\nwith base `base`.\n\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\nReturns a tensor filled with uninitialized data.\n\nReturns an uninitialized tensor with the same size as `input`.\n\nReturns a tensor filled with uninitialized data.\n\nCreates a tensor of size `size` filled with `fill_value`.\n\nReturns a tensor with the same size as `input` filled with `fill_value`.\n\nConverts a float tensor to a quantized tensor with given scale and zero point.\n\nConverts a float tensor to a per-channel quantized tensor with given scales\nand zero points.\n\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\nConstructs a complex tensor with its real part equal to `real` and its\nimaginary part equal to `imag`.\n\nConstructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value `abs` and angle\n`angle`.\n\nComputes the Heaviside step function for each element in `input`.\n\nConcatenates the given sequence of `seq` tensors in the given dimension.\n\nSplits a tensor into a specific number of chunks.\n\nCreates a new tensor by horizontally stacking the tensors in `tensors`.\n\nStack tensors in sequence depthwise (along third axis).\n\nGathers values along an axis specified by `dim`.\n\nStack tensors in sequence horizontally (column wise).\n\nReturns a new tensor which indexes the `input` tensor along dimension `dim`\nusing the entries in `index` which is a `LongTensor`.\n\nReturns a new 1-D tensor which indexes the `input` tensor according to the\nboolean mask `mask` which is a `BoolTensor`.\n\nMoves the dimension(s) of `input` at the position(s) in `source` to the\nposition(s) in `destination`.\n\nAlias for `torch.movedim()`.\n\nReturns a new tensor that is a narrowed version of `input` tensor.\n\nReturns a tensor with the same data and number of elements as `input`, but\nwith the specified shape.\n\nAlias of `torch.vstack()`.\n\nOut-of-place version of `torch.Tensor.scatter_()`\n\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\nSplits the tensor into chunks.\n\nReturns a tensor with all the dimensions of `input` of size `1` removed.\n\nConcatenates a sequence of tensors along a new dimension.\n\nAlias for `torch.transpose()`.\n\nAlias for `torch.transpose()`.\n\nExpects `input` to be <= 2-D tensor and transposes dimensions 0 and 1.\n\nReturns a new tensor with the elements of `input` at the given indices.\n\nSplits a tensor into multiple sub-tensors, all of which are views of `input`,\nalong dimension `dim` according to the indices or number of sections specified\nby `indices_or_sections`.\n\nConstructs a tensor by repeating the elements of `input`.\n\nReturns a tensor that is a transposed version of `input`.\n\nRemoves a tensor dimension.\n\nReturns a new tensor with a dimension of size one inserted at the specified\nposition.\n\nStack tensors in sequence vertically (row wise).\n\nReturn a tensor of elements selected from either `x` or `y`, depending on\n`condition`.\n\nCreates and returns a generator object that manages the state of the algorithm\nwhich produces pseudo random numbers.\n\nSets the seed for generating random numbers to a non-deterministic random\nnumber.\n\nSets the seed for generating random numbers.\n\nReturns the initial seed for generating random numbers as a Python `long`.\n\nReturns the random number generator state as a `torch.ByteTensor`.\n\nSets the random number generator state.\n\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\n\nReturns a tensor where each row contains `num_samples` indices sampled from\nthe multinomial probability distribution located in the corresponding row of\ntensor `input`.\n\nReturns a tensor of random numbers drawn from separate normal distributions\nwhose mean and standard deviation are given.\n\nReturns a tensor of the same size as `input` with each element sampled from a\nPoisson distribution with rate parameter given by the corresponding element in\n`input` i.e.,\n\nReturns a tensor filled with random numbers from a uniform distribution on the\ninterval [0,1)[0, 1)\n\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a uniform distribution on the interval [0,1)[0, 1) .\n\nReturns a tensor filled with random integers generated uniformly between `low`\n(inclusive) and `high` (exclusive).\n\nReturns a tensor with the same shape as Tensor `input` filled with random\nintegers generated uniformly between `low` (inclusive) and `high` (exclusive).\n\nReturns a tensor filled with random numbers from a normal distribution with\nmean `0` and variance `1` (also called the standard normal distribution).\n\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a normal distribution with mean 0 and variance 1.\n\nReturns a random permutation of integers from `0` to `n - 1`.\n\nThere are a few more in-place random sampling functions defined on Tensors as\nwell. Click through to refer to their documentation:\n\n`quasirandom.SobolEngine`\n\nThe `torch.quasirandom.SobolEngine` is an engine for generating (scrambled)\nSobol sequences.\n\nSaves an object to a disk file.\n\nLoads an object saved with `torch.save()` from a file.\n\nReturns the number of threads used for parallelizing CPU operations\n\nSets the number of threads used for intraop parallelism on CPU.\n\nReturns the number of threads used for inter-op parallelism on CPU (e.g.\n\nSets the number of threads used for interop parallelism (e.g.\n\nThe context managers `torch.no_grad()`, `torch.enable_grad()`, and\n`torch.set_grad_enabled()` are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more\ndetails on their usage. These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the `threading` module, etc.\n\nExamples:\n\nContext-manager that disabled gradient calculation.\n\nContext-manager that enables gradient calculation.\n\nContext-manager that sets gradient calculation to on or off.\n\nComputes the absolute value of each element in `input`.\n\nAlias for `torch.abs()`\n\nComputes the inverse cosine of each element in `input`.\n\nAlias for `torch.acos()`.\n\nReturns a new tensor with the inverse hyperbolic cosine of the elements of\n`input`.\n\nAlias for `torch.acosh()`.\n\nAdds the scalar `other` to each element of the input `input` and returns a new\nresulting tensor.\n\nPerforms the element-wise division of `tensor1` by `tensor2`, multiply the\nresult by the scalar `value` and add it to `input`.\n\nPerforms the element-wise multiplication of `tensor1` by `tensor2`, multiply\nthe result by the scalar `value` and add it to `input`.\n\nComputes the element-wise angle (in radians) of the given `input` tensor.\n\nReturns a new tensor with the arcsine of the elements of `input`.\n\nAlias for `torch.asin()`.\n\nReturns a new tensor with the inverse hyperbolic sine of the elements of\n`input`.\n\nAlias for `torch.asinh()`.\n\nReturns a new tensor with the arctangent of the elements of `input`.\n\nAlias for `torch.atan()`.\n\nReturns a new tensor with the inverse hyperbolic tangent of the elements of\n`input`.\n\nAlias for `torch.atanh()`.\n\nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}\nwith consideration of the quadrant.\n\nComputes the bitwise NOT of the given input tensor.\n\nComputes the bitwise AND of `input` and `other`.\n\nComputes the bitwise OR of `input` and `other`.\n\nComputes the bitwise XOR of `input` and `other`.\n\nReturns a new tensor with the ceil of the elements of `input`, the smallest\ninteger greater than or equal to each element.\n\nClamp all elements in `input` into the range `[` `min`, `max` `]`.\n\nAlias for `torch.clamp()`.\n\nComputes the element-wise conjugate of the given `input` tensor.\n\nCreate a new floating-point tensor with the magnitude of `input` and the sign\nof `other`, elementwise.\n\nReturns a new tensor with the cosine of the elements of `input`.\n\nReturns a new tensor with the hyperbolic cosine of the elements of `input`.\n\nReturns a new tensor with each of the elements of `input` converted from\nangles in degrees to radians.\n\nDivides each element of the input `input` by the corresponding element of\n`other`.\n\nAlias for `torch.div()`.\n\nComputes the logarithmic derivative of the gamma function on `input`.\n\nComputes the error function of each element.\n\nComputes the complementary error function of each element of `input`.\n\nComputes the inverse error function of each element of `input`.\n\nReturns a new tensor with the exponential of the elements of the input tensor\n`input`.\n\nComputes the base two exponential function of `input`.\n\nReturns a new tensor with the exponential of the elements minus 1 of `input`.\n\nReturns a new tensor with the data in `input` fake quantized per channel using\n`scale`, `zero_point`, `quant_min` and `quant_max`, across the channel\nspecified by `axis`.\n\nReturns a new tensor with the data in `input` fake quantized using `scale`,\n`zero_point`, `quant_min` and `quant_max`.\n\nAlias for `torch.trunc()`\n\nRaises `input` to the power of `exponent`, elementwise, in double precision.\n\nReturns a new tensor with the floor of the elements of `input`, the largest\ninteger less than or equal to each element.\n\nComputes the element-wise remainder of division.\n\nComputes the fractional portion of each element in `input`.\n\nReturns a new tensor containing imaginary values of the `self` tensor.\n\nMultiplies `input` by 2**:attr:`other`.\n\nDoes a linear interpolation of two tensors `start` (given by `input`) and\n`end` based on a scalar or tensor `weight` and returns the resulting `out`\ntensor.\n\nComputes the logarithm of the gamma function on `input`.\n\nReturns a new tensor with the natural logarithm of the elements of `input`.\n\nReturns a new tensor with the logarithm to the base 10 of the elements of\n`input`.\n\nReturns a new tensor with the natural logarithm of (1 + `input`).\n\nReturns a new tensor with the logarithm to the base 2 of the elements of\n`input`.\n\nLogarithm of the sum of exponentiations of the inputs.\n\nLogarithm of the sum of exponentiations of the inputs in base-2.\n\nComputes the element-wise logical AND of the given input tensors.\n\nComputes the element-wise logical NOT of the given input tensor.\n\nComputes the element-wise logical OR of the given input tensors.\n\nComputes the element-wise logical XOR of the given input tensors.\n\nReturns a new tensor with the logit of the elements of `input`.\n\nGiven the legs of a right triangle, return its hypotenuse.\n\nComputes the zeroth order modified Bessel function of the first kind for each\nelement of `input`.\n\nComputes the regularized lower incomplete gamma function:\n\nComputes the regularized upper incomplete gamma function:\n\nMultiplies each element of the input `input` with the scalar `other` and\nreturns a new resulting tensor.\n\nAlias for `torch.mul()`.\n\nComputes the multivariate log-gamma function) with dimension pp element-wise,\ngiven by\n\nReplaces `NaN`, positive infinity, and negative infinity values in `input`\nwith the values specified by `nan`, `posinf`, and `neginf`, respectively.\n\nReturns a new tensor with the negative of the elements of `input`.\n\nAlias for `torch.neg()`\n\nReturn the next floating-point value after `input` towards `other`,\nelementwise.\n\nComputes the nthn^{th} derivative of the digamma function on `input`.\n\nTakes the power of each element in `input` with `exponent` and returns a\ntensor with the result.\n\nReturns a new tensor with each of the elements of `input` converted from\nangles in radians to degrees.\n\nReturns a new tensor containing real values of the `self` tensor.\n\nReturns a new tensor with the reciprocal of the elements of `input`\n\nComputes the element-wise remainder of division.\n\nReturns a new tensor with each of the elements of `input` rounded to the\nclosest integer.\n\nReturns a new tensor with the reciprocal of the square-root of each of the\nelements of `input`.\n\nReturns a new tensor with the sigmoid of the elements of `input`.\n\nReturns a new tensor with the signs of the elements of `input`.\n\nFor complex tensors, this function returns a new tensor whose elemants have\nthe same angle as that of the elements of `input` and absolute value 1.\n\nTests if each element of `input` has its sign bit set (is less than zero) or\nnot.\n\nReturns a new tensor with the sine of the elements of `input`.\n\nComputes the normalized sinc of `input.`\n\nReturns a new tensor with the hyperbolic sine of the elements of `input`.\n\nReturns a new tensor with the square-root of the elements of `input`.\n\nReturns a new tensor with the square of the elements of `input`.\n\nSubtracts `other`, scaled by `alpha`, from `input`.\n\nAlias for `torch.sub()`.\n\nReturns a new tensor with the tangent of the elements of `input`.\n\nReturns a new tensor with the hyperbolic tangent of the elements of `input`.\n\nAlias for `torch.div()` with `rounding_mode=None`.\n\nReturns a new tensor with the truncated integer values of the elements of\n`input`.\n\nComputes `input * log(other)` with the following cases.\n\nReturns the indices of the maximum value of all elements in the `input`\ntensor.\n\nReturns the indices of the minimum value(s) of the flattened tensor or along a\ndimension\n\nReturns the maximum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nReturns the minimum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nTests if all elements in `input` evaluate to `True`.\n\nthe input tensor.\n\nReturns the maximum value of all elements in the `input` tensor.\n\nReturns the minimum value of all elements in the `input` tensor.\n\nReturns the p-norm of (`input` \\- `other`)\n\nReturns the log of summed exponentials of each row of the `input` tensor in\nthe given dimension `dim`.\n\nReturns the mean value of all elements in the `input` tensor.\n\nReturns the median of the values in `input`.\n\nReturns the median of the values in `input`, ignoring `NaN` values.\n\nReturns a namedtuple `(values, indices)` where `values` is the mode value of\neach row of the `input` tensor in the given dimension `dim`, i.e.\n\nReturns the matrix norm or vector norm of a given tensor.\n\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\n\nReturns the product of all elements in the `input` tensor.\n\nReturns the q-th quantiles of all elements in the `input` tensor, doing a\nlinear interpolation when the q-th quantile lies between two data points.\n\nThis is a variant of `torch.quantile()` that \u201cignores\u201d `NaN` values, computing\nthe quantiles `q` as if `NaN` values in `input` did not exist.\n\nReturns the standard-deviation of all elements in the `input` tensor.\n\nReturns the standard-deviation and mean of all elements in the `input` tensor.\n\nReturns the sum of all elements in the `input` tensor.\n\nReturns the unique elements of the input tensor.\n\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\nReturns the variance of all elements in the `input` tensor.\n\nReturns the variance and mean of all elements in the `input` tensor.\n\nCounts the number of non-zero values in the tensor `input` along the given\n`dim`.\n\nThis function checks if all `input` and `other` satisfy the condition:\n\nReturns the indices that sort a tensor along a given dimension in ascending\norder by value.\n\nComputes element-wise equality\n\n`True` if two tensors have the same size and elements, `False` otherwise.\n\nComputes input\u2265other\\text{input} \\geq \\text{other} element-wise.\n\nAlias for `torch.ge()`.\n\nComputes input>other\\text{input} > \\text{other} element-wise.\n\nAlias for `torch.gt()`.\n\nReturns a new tensor with boolean elements representing if each element of\n`input` is \u201cclose\u201d to the corresponding element of `other`.\n\nReturns a new tensor with boolean elements representing if each element is\n`finite` or not.\n\nTests if each element of `input` is infinite (positive or negative infinity)\nor not.\n\nTests if each element of `input` is positive infinity or not.\n\nTests if each element of `input` is negative infinity or not.\n\nReturns a new tensor with boolean elements representing if each element of\n`input` is NaN or not.\n\nReturns a new tensor with boolean elements representing if each element of\n`input` is real-valued or not.\n\nReturns a namedtuple `(values, indices)` where `values` is the `k` th smallest\nelement of each row of the `input` tensor in the given dimension `dim`.\n\nComputes input\u2264other\\text{input} \\leq \\text{other} element-wise.\n\nAlias for `torch.le()`.\n\nComputes input<other\\text{input} < \\text{other} element-wise.\n\nAlias for `torch.lt()`.\n\nComputes the element-wise maximum of `input` and `other`.\n\nComputes the element-wise minimum of `input` and `other`.\n\nComputes the element-wise maximum of `input` and `other`.\n\nComputes the element-wise minimum of `input` and `other`.\n\nComputes input\u2260other\\text{input} \\neq \\text{other} element-wise.\n\nAlias for `torch.ne()`.\n\nSorts the elements of the `input` tensor along a given dimension in ascending\norder by value.\n\nReturns the `k` largest elements of the given `input` tensor along a given\ndimension.\n\nSorts the elements of the `input` tensor along its first dimension in\nascending order by value.\n\nShort-time Fourier transform (STFT).\n\nInverse short time Fourier Transform.\n\nBartlett window function.\n\nBlackman window function.\n\nHamming window function.\n\nHann window function.\n\nComputes the Kaiser window with window length `window_length` and shape\nparameter `beta`.\n\nReturns a 1-dimensional view of each input tensor with zero dimensions.\n\nReturns a 2-dimensional view of each input tensor with zero dimensions.\n\nReturns a 3-dimensional view of each input tensor with zero dimensions.\n\nCount the frequency of each value in an array of non-negative ints.\n\nCreate a block diagonal matrix from provided tensors.\n\nBroadcasts the given tensors according to Broadcasting semantics.\n\nBroadcasts `input` to the shape `shape`.\n\nSimilar to `broadcast_tensors()` but for shapes.\n\nReturns the indices of the buckets to which each value in the `input` belongs,\nwhere the boundaries of the buckets are set by `boundaries`.\n\nDo cartesian product of the given sequence of tensors.\n\nComputes batched the p-norm distance between each pair of the two collections\nof row vectors.\n\nReturns a copy of `input`.\n\nCompute combinations of length rr of the given tensor.\n\nReturns the cross product of vectors in dimension `dim` of `input` and\n`other`.\n\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nmaximum of elements of `input` in the dimension `dim`.\n\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nminimum of elements of `input` in the dimension `dim`.\n\nReturns the cumulative product of elements of `input` in the dimension `dim`.\n\nReturns the cumulative sum of elements of `input` in the dimension `dim`.\n\nCreates a tensor whose diagonals of certain 2D planes (specified by `dim1` and\n`dim2`) are filled by `input`.\n\nReturns a partial view of `input` with the its diagonal elements with respect\nto `dim1` and `dim2` appended as a dimension at the end of the shape.\n\nComputes the n-th forward difference along the given dimension.\n\nSums the product of the elements of the input `operands` along dimensions\nspecified using a notation based on the Einstein summation convention.\n\nFlattens `input` by reshaping it into a one-dimensional tensor.\n\nReverse the order of a n-D tensor along given axis in dims.\n\nFlip tensor in the left/right direction, returning a new tensor.\n\nFlip tensor in the up/down direction, returning a new tensor.\n\nComputes the Kronecker product, denoted by \u2297\\otimes , of `input` and `other`.\n\nRotate a n-D tensor by 90 degrees in the plane specified by dims axis.\n\nComputes the element-wise greatest common divisor (GCD) of `input` and\n`other`.\n\nComputes the histogram of a tensor.\n\nTake NN tensors, each of which can be either scalar or 1-dimensional vector,\nand create NN N-dimensional grids, where the ii th grid is defined by\nexpanding the ii th input over dimensions defined by other inputs.\n\nComputes the element-wise least common multiple (LCM) of `input` and `other`.\n\nReturns the logarithm of the cumulative summation of the exponentiation of\nelements of `input` in the dimension `dim`.\n\nReturn a contiguous flattened tensor.\n\nReturns a tensor where each sub-tensor of `input` along dimension `dim` is\nnormalized such that the `p`-norm of the sub-tensor is lower than the value\n`maxnorm`\n\nRepeat elements of a tensor.\n\nRoll the tensor along the given dimension(s).\n\nFind the indices from the innermost dimension of `sorted_sequence` such that,\nif the corresponding values in `values` were inserted before the indices, the\norder of the corresponding innermost dimension within `sorted_sequence` would\nbe preserved.\n\nReturns a contraction of a and b over multiple dimensions.\n\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\n\nReturns the lower triangular part of the matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\nReturns the indices of the lower triangular part of a `row`-by- `col` matrix\nin a 2-by-N Tensor, where the first row contains row coordinates of all\nindices and the second row contains column coordinates.\n\nReturns the upper triangular part of a matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\nReturns the indices of the upper triangular part of a `row` by `col` matrix in\na 2-by-N Tensor, where the first row contains row coordinates of all indices\nand the second row contains column coordinates.\n\nGenerates a Vandermonde matrix.\n\nReturns a view of `input` as a real tensor.\n\nReturns a view of `input` as a complex tensor.\n\nPerforms a batch matrix-matrix product of matrices stored in `batch1` and\n`batch2`, with a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).\n\nPerforms a matrix multiplication of the matrices `mat1` and `mat2`.\n\nPerforms a matrix-vector product of the matrix `mat` and the vector `vec`.\n\nPerforms the outer-product of vectors `vec1` and `vec2` and adds it to the\nmatrix `input`.\n\nPerforms a batch matrix-matrix product of matrices in `batch1` and `batch2`.\n\nPerforms a batch matrix-matrix product of matrices stored in `input` and\n`mat2`.\n\nReturns the matrix product of the NN 2-D tensors.\n\nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA\nor for batches of symmetric positive-definite matrices.\n\nComputes the inverse of a symmetric positive-definite matrix AA using its\nCholesky factor uu : returns matrix `inv`.\n\nSolves a linear system of equations with a positive semidefinite matrix to be\ninverted given its Cholesky factor matrix uu .\n\nComputes the dot product of two 1D tensors.\n\nComputes the eigenvalues and eigenvectors of a real square matrix.\n\nThis is a low-level function for calling LAPACK directly.\n\nAlias of `torch.outer()`.\n\nComputes the dot product for 1D tensors.\n\nTakes the inverse of the square matrix `input`.\n\nCalculates determinant of a square matrix or batches of square matrices.\n\nCalculates log determinant of a square matrix or batches of square matrices.\n\nCalculates the sign and log absolute value of the determinant(s) of a square\nmatrix or batches of square matrices.\n\nComputes the solution to the least squares and least norm problems for a full\nrank matrix AA of size (m\u00d7n)(m \\times n) and a matrix BB of size (m\u00d7k)(m\n\\times k) .\n\nComputes the LU factorization of a matrix or batches of matrices `A`.\n\nReturns the LU solve of the linear system Ax=bAx = b using the partially\npivoted LU factorization of A from `torch.lu()`.\n\nUnpacks the data and pivots from a LU factorization of a tensor.\n\nMatrix product of two tensors.\n\nReturns the matrix raised to the power `n` for square matrices.\n\nReturns the numerical rank of a 2-D tensor.\n\nReturns the matrix exponential.\n\nPerforms a matrix multiplication of the matrices `input` and `mat2`.\n\nPerforms a matrix-vector product of the matrix `input` and the vector `vec`.\n\nComputes the orthogonal matrix `Q` of a QR factorization, from the `(input,\ninput2)` tuple returned by `torch.geqrf()`.\n\nMultiplies `mat` (given by `input3`) by the orthogonal `Q` matrix of the QR\nfactorization formed by `torch.geqrf()` that is represented by `(a, tau)`\n(given by (`input`, `input2`)).\n\nOuter product of `input` and `vec2`.\n\nCalculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a\n2D tensor.\n\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nThis function returns the solution to the system of linear equations\nrepresented by AX=BAX = B and the LU factorization of A, in order as a\nnamedtuple `solution, LU`.\n\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\n\nReturn the singular value decomposition `(U, S, V)` of a matrix, batches of\nmatrices, or a sparse matrix AA such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T\n.\n\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix,\nbatches of such matrices, or sparse matrix.\n\nThis function returns eigenvalues and eigenvectors of a real symmetric matrix\n`input` or a batch of real symmetric matrices, represented by a namedtuple\n(eigenvalues, eigenvectors).\n\nFind the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized eigenvalue problem\nusing matrix-free LOBPCG methods.\n\nEstimate \u222bydx\\int y\\,dx along `dim`, using the trapezoid rule.\n\nSolves a system of equations with a triangular coefficient matrix AA and\nmultiple right-hand sides bb .\n\nComputes the dot product of two 1D tensors.\n\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\nReturns the `torch.dtype` that would result from performing an arithmetic\noperation on the provided input tensors.\n\nDetermines if a type conversion is allowed under PyTorch casting rules\ndescribed in the type promotion documentation.\n\nReturns the `torch.dtype` with the smallest size and scalar kind that is not\nsmaller nor of lower kind than either `type1` or `type2`.\n\nSets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.\n\nReturns True if the global deterministic flag is turned on.\n\nA wrapper around Python\u2019s assert which is symbolically traceable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.abs()", "path": "generated/torch.abs#torch.abs", "type": "torch", "text": "\nComputes the absolute value of each element in `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.absolute()", "path": "generated/torch.absolute#torch.absolute", "type": "torch", "text": "\nAlias for `torch.abs()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.acos()", "path": "generated/torch.acos#torch.acos", "type": "torch", "text": "\nComputes the inverse cosine of each element in `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.acosh()", "path": "generated/torch.acosh#torch.acosh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic cosine of the elements of\n`input`.\n\nNote\n\nThe domain of the inverse hyperbolic cosine is `[1, inf)` and values outside\nthis range will be mapped to `NaN`, except for `+ INF` for which the output is\nmapped to `+ INF`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.add()", "path": "generated/torch.add#torch.add", "type": "torch", "text": "\nAdds the scalar `other` to each element of the input `input` and returns a new\nresulting tensor.\n\nIf `input` is of type FloatTensor or DoubleTensor, `other` must be a real\nnumber, otherwise it should be an integer.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nEach element of the tensor `other` is multiplied by the scalar `alpha` and\nadded to each element of the tensor `input`. The resulting tensor is returned.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nIf `other` is of type FloatTensor or DoubleTensor, `alpha` must be a real\nnumber, otherwise it should be an integer.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addbmm()", "path": "generated/torch.addbmm#torch.addbmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices stored in `batch1` and\n`batch2`, with a reduced add step (all matrix multiplications get accumulated\nalong the first dimension). `input` is added to the final result.\n\n`batch1` and `batch2` must be 3-D tensors each containing the same number of\nmatrices.\n\nIf `batch1` is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, `batch2` is a (b\u00d7m\u00d7p)(b\n\\times m \\times p) tensor, `input` must be broadcastable with a (n\u00d7p)(n \\times\np) tensor and `out` will be a (n\u00d7p)(n \\times p) tensor.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers.\n\nThis operator supports TensorFloat32.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addcdiv()", "path": "generated/torch.addcdiv#torch.addcdiv", "type": "torch", "text": "\nPerforms the element-wise division of `tensor1` by `tensor2`, multiply the\nresult by the scalar `value` and add it to `input`.\n\nWarning\n\nInteger division with addcdiv is no longer supported, and in a future release\naddcdiv will perform a true division of tensor1 and tensor2. The historic\naddcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 /\ntensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 /\ntensor2) for float inputs. The future addcdiv behavior is just the latter\nimplementation: (input + value * tensor1 / tensor2), for all dtypes.\n\nThe shapes of `input`, `tensor1`, and `tensor2` must be broadcastable.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, `value` must be a real\nnumber, otherwise an integer.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addcmul()", "path": "generated/torch.addcmul#torch.addcmul", "type": "torch", "text": "\nPerforms the element-wise multiplication of `tensor1` by `tensor2`, multiply\nthe result by the scalar `value` and add it to `input`.\n\nThe shapes of `tensor`, `tensor1`, and `tensor2` must be broadcastable.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, `value` must be a real\nnumber, otherwise an integer.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addmm()", "path": "generated/torch.addmm#torch.addmm", "type": "torch", "text": "\nPerforms a matrix multiplication of the matrices `mat1` and `mat2`. The matrix\n`input` is added to the final result.\n\nIf `mat1` is a (n\u00d7m)(n \\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p) tensor,\nthen `input` must be broadcastable with a (n\u00d7p)(n \\times p) tensor and `out`\nwill be a (n\u00d7p)(n \\times p) tensor.\n\n`alpha` and `beta` are scaling factors on matrix-vector product between `mat1`\nand `mat2` and the added matrix `input` respectively.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers.\n\nThis operator supports TensorFloat32.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addmv()", "path": "generated/torch.addmv#torch.addmv", "type": "torch", "text": "\nPerforms a matrix-vector product of the matrix `mat` and the vector `vec`. The\nvector `input` is added to the final result.\n\nIf `mat` is a (n\u00d7m)(n \\times m) tensor, `vec` is a 1-D tensor of size `m`,\nthen `input` must be broadcastable with a 1-D tensor of size `n` and `out`\nwill be 1-D tensor of size `n`.\n\n`alpha` and `beta` are scaling factors on matrix-vector product between `mat`\nand `vec` and the added tensor `input` respectively.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addr()", "path": "generated/torch.addr#torch.addr", "type": "torch", "text": "\nPerforms the outer-product of vectors `vec1` and `vec2` and adds it to the\nmatrix `input`.\n\nOptional values `beta` and `alpha` are scaling factors on the outer product\nbetween `vec1` and `vec2` and the added matrix `input` respectively.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nIf `vec1` is a vector of size `n` and `vec2` is a vector of size `m`, then\n`input` must be broadcastable with a matrix of size (n\u00d7m)(n \\times m) and\n`out` will be a matrix of size (n\u00d7m)(n \\times m) .\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.all()", "path": "generated/torch.all#torch.all", "type": "torch", "text": "\nTests if all elements in `input` evaluate to `True`.\n\nNote\n\nThis function matches the behaviour of NumPy in returning output of dtype\n`bool` for all supported dtypes except `uint8`. For `uint8` the dtype of\noutput is `uint8` itself.\n\nExample:\n\nFor each row of `input` in the given dimension `dim`, returns `True` if all\nelements in the row evaluate to `True` and `False` otherwise.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1\nfewer dimension than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.allclose()", "path": "generated/torch.allclose#torch.allclose", "type": "torch", "text": "\nThis function checks if all `input` and `other` satisfy the condition:\n\nelementwise, for all elements of `input` and `other`. The behaviour of this\nfunction is analogous to numpy.allclose\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.amax()", "path": "generated/torch.amax#torch.amax", "type": "torch", "text": "\nReturns the maximum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nNote\n\nIf `keepdim is ``True``, the output tensors are of the same size as `input`\nexcept in the dimension(s) `dim` where they are of size 1. Otherwise, `dim`s\nare squeezed (see :func:`torch.squeeze`), resulting in the output tensors\nhaving fewer dimension than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.amin()", "path": "generated/torch.amin#torch.amin", "type": "torch", "text": "\nReturns the minimum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nNote\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension(s) `dim` where they are of size 1. Otherwise, `dim`s\nare squeezed (see :func:`torch.squeeze`), resulting in the output tensors\nhaving fewer dimensions than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.angle()", "path": "generated/torch.angle#torch.angle", "type": "torch", "text": "\nComputes the element-wise angle (in radians) of the given `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nNote\n\nStarting in PyTorch 1.8, angle returns pi for negative real numbers, zero for\nnon-negative real numbers, and propagates NaNs. Previously the function would\nreturn zero for all real numbers and not propagate floating-point NaNs.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.any()", "path": "generated/torch.any#torch.any", "type": "torch", "text": "\ninput (Tensor) \u2013 the input tensor.\n\nTests if any element in `input` evaluates to `True`.\n\nNote\n\nThis function matches the behaviour of NumPy in returning output of dtype\n`bool` for all supported dtypes except `uint8`. For `uint8` the dtype of\noutput is `uint8` itself.\n\nExample:\n\nFor each row of `input` in the given dimension `dim`, returns `True` if any\nelement in the row evaluate to `True` and `False` otherwise.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1\nfewer dimension than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arange()", "path": "generated/torch.arange#torch.arange", "type": "torch", "text": "\nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rceil with values from the interval `[start,\nend)` taken with common difference `step` beginning from `start`.\n\nNote that non-integer `step` is subject to floating point rounding errors when\ncomparing against `end`; to avoid inconsistency, we advise adding a small\nepsilon to `end` in such cases.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arccos()", "path": "generated/torch.arccos#torch.arccos", "type": "torch", "text": "\nAlias for `torch.acos()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arccosh()", "path": "generated/torch.arccosh#torch.arccosh", "type": "torch", "text": "\nAlias for `torch.acosh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arcsin()", "path": "generated/torch.arcsin#torch.arcsin", "type": "torch", "text": "\nAlias for `torch.asin()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arcsinh()", "path": "generated/torch.arcsinh#torch.arcsinh", "type": "torch", "text": "\nAlias for `torch.asinh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arctan()", "path": "generated/torch.arctan#torch.arctan", "type": "torch", "text": "\nAlias for `torch.atan()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arctanh()", "path": "generated/torch.arctanh#torch.arctanh", "type": "torch", "text": "\nAlias for `torch.atanh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.are_deterministic_algorithms_enabled()", "path": "generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled", "type": "torch", "text": "\nReturns True if the global deterministic flag is turned on. Refer to\n`torch.use_deterministic_algorithms()` documentation for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.argmax()", "path": "generated/torch.argmax#torch.argmax", "type": "torch", "text": "\nReturns the indices of the maximum value of all elements in the `input`\ntensor.\n\nThis is the second value returned by `torch.max()`. See its documentation for\nthe exact semantics of this method.\n\nNote\n\nIf there are multiple minimal values then the indices of the first minimal\nvalue are returned.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns the indices of the maximum values of a tensor across a dimension.\n\nThis is the second value returned by `torch.max()`. See its documentation for\nthe exact semantics of this method.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.argmin()", "path": "generated/torch.argmin#torch.argmin", "type": "torch", "text": "\nReturns the indices of the minimum value(s) of the flattened tensor or along a\ndimension\n\nThis is the second value returned by `torch.min()`. See its documentation for\nthe exact semantics of this method.\n\nNote\n\nIf there are multiple minimal values then the indices of the first minimal\nvalue are returned.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.argsort()", "path": "generated/torch.argsort#torch.argsort", "type": "torch", "text": "\nReturns the indices that sort a tensor along a given dimension in ascending\norder by value.\n\nThis is the second value returned by `torch.sort()`. See its documentation for\nthe exact semantics of this method.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.asin()", "path": "generated/torch.asin#torch.asin", "type": "torch", "text": "\nReturns a new tensor with the arcsine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.asinh()", "path": "generated/torch.asinh#torch.asinh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic sine of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.as_strided()", "path": "generated/torch.as_strided#torch.as_strided", "type": "torch", "text": "\nCreate a view of an existing `torch.Tensor` `input` with specified `size`,\n`stride` and `storage_offset`.\n\nWarning\n\nMore than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nMany PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, like `torch.Tensor.expand()`,\nare easier to read and are therefore more advisable to use.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.as_tensor()", "path": "generated/torch.as_tensor#torch.as_tensor", "type": "torch", "text": "\nConvert the data into a `torch.Tensor`. If the data is already a `Tensor` with\nthe same `dtype` and `device`, no copy will be performed, otherwise a new\n`Tensor` will be returned with computational graph retained if data `Tensor`\nhas `requires_grad=True`. Similarly, if the data is an `ndarray` of the\ncorresponding `dtype` and the `device` is the cpu, no copy will be performed.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atan()", "path": "generated/torch.atan#torch.atan", "type": "torch", "text": "\nReturns a new tensor with the arctangent of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atan2()", "path": "generated/torch.atan2#torch.atan2", "type": "torch", "text": "\nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}\nwith consideration of the quadrant. Returns a new tensor with the signed\nangles in radians between vector (otheri,inputi)(\\text{other}_{i},\n\\text{input}_{i}) and vector (1,0)(1, 0) . (Note that otheri\\text{other}_{i} ,\nthe second parameter, is the x-coordinate, while inputi\\text{input}_{i} , the\nfirst parameter, is the y-coordinate.)\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atanh()", "path": "generated/torch.atanh#torch.atanh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic tangent of the elements of\n`input`.\n\nNote\n\nThe domain of the inverse hyperbolic tangent is `(-1, 1)` and values outside\nthis range will be mapped to `NaN`, except for the values `1` and `-1` for\nwhich the output is mapped to `+/-INF` respectively.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atleast_1d()", "path": "generated/torch.atleast_1d#torch.atleast_1d", "type": "torch", "text": "\nReturns a 1-dimensional view of each input tensor with zero dimensions. Input\ntensors with one or more dimensions are returned as-is.\n\ninput (Tensor or list of Tensors) \u2013\n\noutput (Tensor or tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atleast_2d()", "path": "generated/torch.atleast_2d#torch.atleast_2d", "type": "torch", "text": "\nReturns a 2-dimensional view of each input tensor with zero dimensions. Input\ntensors with two or more dimensions are returned as-is. :param input: :type\ninput: Tensor or list of Tensors\n\noutput (Tensor or tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atleast_3d()", "path": "generated/torch.atleast_3d#torch.atleast_3d", "type": "torch", "text": "\nReturns a 3-dimensional view of each input tensor with zero dimensions. Input\ntensors with three or more dimensions are returned as-is. :param input: :type\ninput: Tensor or list of Tensors\n\noutput (Tensor or tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd", "path": "autograd", "type": "torch.autograd", "text": "\n`torch.autograd` provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare `Tensor` s for which\ngradients should be computed with the `requires_grad=True` keyword. As of now,\nwe only support autograd for floating point `Tensor` types ( half, float,\ndouble and bfloat16) and complex `Tensor` types (cfloat, cdouble).\n\nComputes the sum of gradients of given tensors w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If any of `tensors` are non-\nscalar (i.e. their data has more than one element) and require gradient, then\nthe Jacobian-vector product would be computed, in this case the function\nadditionally requires specifying `grad_tensors`. It should be a sequence of\nmatching length, that contains the \u201cvector\u201d in the Jacobian-vector product,\nusually the gradient of the differentiated function w.r.t. corresponding\ntensors (`None` is an acceptable value for all tensors that don\u2019t need\ngradient tensors).\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nUsing this method with `create_graph=True` will create a reference cycle\nbetween the parameter and its gradient which can cause a memory leak. We\nrecommend using `autograd.grad` when creating the graph to avoid this. If you\nhave to use this function, make sure to reset the `.grad` fields of your\nparameters to `None` after use to break the cycle and avoid the leak.\n\nNote\n\nIf you run any forward ops, create `grad_tensors`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nComputes and returns the sum of gradients of outputs w.r.t. the inputs.\n\n`grad_outputs` should be a sequence of length matching `output` containing the\n\u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t.\neach of the outputs. If an output doesn\u2019t require_grad, then the gradient can\nbe `None`).\n\nIf `only_inputs` is `True`, the function will only return a list of gradients\nw.r.t the specified inputs. If it\u2019s `False`, then gradient w.r.t. all\nremaining leaves will still be computed, and will be accumulated into their\n`.grad` attribute.\n\nNote\n\nIf you run any forward ops, create `grad_outputs`, and/or call `grad` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nWarning\n\nThis API is in beta. Even though the function signatures are very unlikely to\nchange, major improvements to performances are planned before we consider this\nstable.\n\nThis section contains the higher level API for the autograd that builds on the\nbasic API above and allows you to compute jacobians, hessians, etc.\n\nThis API works with user-provided functions that take only Tensors as input\nand return only Tensors. If your function takes other arguments that are not\nTensors or Tensors that don\u2019t have requires_grad set, you can use a lambda to\ncapture them. For example, for a function `f` that takes three inputs, a\nTensor for which we want the jacobian, another tensor that should be\nconsidered constant and a boolean flag as `f(input, constant, flag=flag)` you\ncan use it as `functional.jacobian(lambda x: f(x, constant, flag=flag),\ninput)`.\n\nFunction that computes the Jacobian of a given function.\n\nif there is a single input and output, this will be a single Tensor containing\nthe Jacobian for the linearized inputs and output. If one of the two is a\ntuple, then the Jacobian will be a tuple of Tensors. If both of them are\ntuples, then the Jacobian will be a tuple of tuple of Tensors where\n`Jacobian[i][j]` will contain the Jacobian of the `i`th output and `j`th input\nand will have as size the concatenation of the sizes of the corresponding\noutput and the corresponding input and will have same dtype and device as the\ncorresponding input.\n\nJacobian (Tensor or nested tuple of Tensors)\n\nFunction that computes the Hessian of a given scalar function.\n\nif there is a single input, this will be a single Tensor containing the\nHessian for the input. If it is a tuple, then the Hessian will be a tuple of\ntuples where `Hessian[i][j]` will contain the Hessian of the `i`th input and\n`j`th input with size the sum of the size of the `i`th input plus the size of\nthe `j`th input. `Hessian[i][j]` will have the same dtype and device as the\ncorresponding `i`th input.\n\nHessian (Tensor or a tuple of tuple of Tensors)\n\nFunction that computes the dot product between a vector `v` and the Jacobian\nof the given function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvjp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nFunction that computes the dot product between the Jacobian of the given\nfunction at the point given by the inputs and a vector `v`.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\njvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the output.\n\noutput (tuple)\n\nNote\n\nThe jvp is currently computed by using the backward of the backward (sometimes\ncalled the double backwards trick) as we don\u2019t have support for forward mode\nAD in PyTorch at the moment.\n\nFunction that computes the dot product between a vector `v` and the Hessian of\na given scalar function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvhp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nFunction that computes the dot product between the Hessian of a given scalar\nfunction and a vector `v` at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nhvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nNote\n\nThis function is significantly slower than `vhp` due to backward mode AD\nconstraints. If your functions is twice continuously differentiable, then hvp\n= vhp.t(). So if you know that your function satisfies this condition, you\nshould use vhp instead that is much faster with the current implementation.\n\nContext-manager that disabled gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call `Tensor.backward()`. It will reduce memory consumption for\ncomputations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have `requires_grad=False`,\neven when the inputs have `requires_grad=True`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\nContext-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via `no_grad` or\n`set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\nContext-manager that sets gradient calculation to on or off.\n\n`set_grad_enabled` will enable or disable grads based on its argument `mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nmode (bool) \u2013 Flag whether to enable grad (`True`), or disable (`False`). This\ncan be used to conditionally enable gradients.\n\nExample:\n\nWhen a non-sparse `param` receives a non-sparse gradient during\n`torch.autograd.backward()` or `torch.Tensor.backward()` `param.grad` is\naccumulated as follows.\n\nIf `param.grad` is initially `None`:\n\nIf `param` already has a non-sparse `.grad` attribute:\n\nThe default behavior (letting `.grad`s be `None` before the first\n`backward()`, such that their layout is created according to 1 or 2, and\nretained over time according to 3 or 4) is recommended for best performance.\nCalls to `model.zero_grad()` or `optimizer.zero_grad()` will not affect\n`.grad` layouts.\n\nIn fact, resetting all `.grad`s to `None` before each accumulation phase,\ne.g.:\n\nsuch that they\u2019re recreated according to 1 or 2 every time, is a valid\nalternative to `model.zero_grad()` or `optimizer.zero_grad()` that may improve\nperformance for some networks.\n\nIf you need manual control over `.grad`\u2019s strides, assign `param.grad =` a\nzeroed tensor with desired strides before the first `backward()`, and never\nreset it to `None`. 3 guarantees your layout is preserved as long as\n`create_graph=False`. 4 indicates your layout is likely preserved even if\n`create_graph=True`.\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.\n\nAll `Tensor` s keep track of in-place operations applied to them, and if the\nimplementation detects that a tensor was saved for backward in one of the\nfunctions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\n\nWarning\n\nThe Variable API has been deprecated: Variables are no longer necessary to use\nautograd with tensors. Autograd automatically supports Tensors with\n`requires_grad` set to `True`. Below please find a quick guide on what has\nchanged:\n\nIn addition, one can now create tensors with `requires_grad=True` using\nfactory methods such as `torch.randn()`, `torch.zeros()`, `torch.ones()`, and\nothers like the following:\n\n`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`\n\nThis attribute is `None` by default and becomes a Tensor the first time a call\nto `backward()` computes gradients for `self`. The attribute will then contain\nthe gradients computed and future calls to `backward()` will accumulate (add)\ngradients into it.\n\nIs `True` if gradients need to be computed for this Tensor, `False` otherwise.\n\nNote\n\nThe fact that gradients need to be computed for a Tensor do not mean that the\n`grad` attribute will be populated, see `is_leaf` for more details.\n\nAll Tensors that have `requires_grad` which is `False` will be leaf Tensors by\nconvention.\n\nFor Tensors that have `requires_grad` which is `True`, they will be leaf\nTensors if they were created by the user. This means that they are not the\nresult of an operation and so `grad_fn` is None.\n\nOnly leaf Tensors will have their `grad` populated during a call to\n`backward()`. To get `grad` populated for non-leaf Tensors, you can use\n`retain_grad()`.\n\nExample:\n\nComputes the gradient of current tensor w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If the tensor is non-scalar\n(i.e. its data has more than one element) and requires gradient, the function\nadditionally requires specifying `gradient`. It should be a tensor of matching\ntype and location, that contains the gradient of the differentiated function\nw.r.t. `self`.\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nIf you run any forward ops, create `gradient`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nReturns a new Tensor, detached from the current graph.\n\nThe result will never require gradient.\n\nNote\n\nReturned Tensor shares the same storage with the original one. In-place\nmodifications on either of them will be seen, and may trigger errors in\ncorrectness checks. IMPORTANT NOTE: Previously, in-place size / stride /\nstorage changes (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to\nthe returned tensor also update the original tensor. Now, these in-place\nchanges will not update the original tensor anymore, and will instead trigger\nan error. For sparse tensors: In-place indices / values changes (such as\n`zero_` / `copy_` / `add_`) to the returned tensor will not update the\noriginal tensor anymore, and will instead trigger an error.\n\nDetaches the Tensor from the graph that created it, making it a leaf. Views\ncannot be detached in-place.\n\nRegisters a backward hook.\n\nThe hook will be called every time a gradient with respect to the Tensor is\ncomputed. The hook should have the following signature:\n\nThe hook should not modify its argument, but it can optionally return a new\ngradient which will be used in place of `grad`.\n\nThis function returns a handle with a method `handle.remove()` that removes\nthe hook from the module.\n\nExample:\n\nEnables .grad attribute for non-leaf Tensors.\n\nRecords operation history and defines formulas for differentiating ops.\n\nSee the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-\ntorch-autograd\n\nEvery operation performed on `Tensor` s creates a new function object, that\nperforms the computation, and records that it happened. The history is\nretained in the form of a DAG of functions, with edges denoting data\ndependencies (`input <- output`). Then, when backward is called, the graph is\nprocessed in the topological ordering, by calling `backward()` methods of each\n`Function` object, and passing returned gradients on to next `Function` s.\n\nNormally, the only way users interact with functions is by creating subclasses\nand defining new operations. This is a recommended way of extending\ntorch.autograd.\n\nExamples:\n\nDefines a formula for differentiating the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context `ctx` as the first argument, followed by as many\noutputs did `forward()` return, and it should return as many tensors, as there\nwere inputs to `forward()`. Each argument is the gradient w.r.t the given\noutput, and each returned value should be the gradient w.r.t. the\ncorresponding input.\n\nThe context can be used to retrieve tensors saved during the forward pass. It\nalso has an attribute `ctx.needs_input_grad` as a tuple of booleans\nrepresenting whether each input needs gradient. E.g., `backward()` will have\n`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs\ngradient computated w.r.t. the output.\n\nPerforms the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context ctx as the first argument, followed by any number of\narguments (tensors or other types).\n\nThe context can be used to store tensors that can be then retrieved during the\nbackward pass.\n\nWhen creating a new `Function`, the following methods are available to `ctx`.\n\nMarks given tensors as modified in an in-place operation.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be inputs.\n\nEvery tensor that\u2019s been modified in-place in a call to `forward()` should be\ngiven to this function, to ensure correctness of our checks. It doesn\u2019t matter\nwhether the function is called before or after modification.\n\nMarks outputs as non-differentiable.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be outputs.\n\nThis will mark outputs as not requiring gradients, increasing the efficiency\nof backward computation. You still need to accept a gradient for each output\nin `backward()`, but it\u2019s always going to be a zero tensor with the same shape\nas the shape of a corresponding output.\n\nThis is used e.g. for indices returned from a max `Function`.\n\nSaves given tensors for a future call to `backward()`.\n\nThis should be called at most once, and only from inside the `forward()`\nmethod.\n\nLater, saved tensors can be accessed through the `saved_tensors` attribute.\nBefore returning them to the user, a check is made to ensure they weren\u2019t used\nin any in-place operation that modified their content.\n\nArguments can also be `None`.\n\nSets whether to materialize output grad tensors. Default is true.\n\nThis should be called only from inside the `forward()` method\n\nIf true, undefined output grad tensors will be expanded to tensors full of\nzeros prior to calling the `backward()` method.\n\nCheck gradients computed via small finite differences against analytical\ngradients w.r.t. tensors in `inputs` that are of floating point or complex\ntype and with `requires_grad=True`.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nFor complex functions, no notion of Jacobian exists. Gradcheck verifies if the\nnumerical and analytical values of Wirtinger and Conjugate Wirtinger\nderivative are consistent. The gradient computation is done under the\nassumption that the overall function has a real valued output. For functions\nwith complex output, gradcheck compares the numerical and analytical gradients\nfor two values of `grad_output`: 1 and 1j. For more details, check out\nAutograd for Complex Numbers.\n\nNote\n\nThe default values are designed for `input` of double precision. This check\nwill likely fail if `input` is of less precision, e.g., `FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` has overlapping memory, i.e., different\nindices pointing to the same memory address (e.g., from `torch.expand()`),\nthis check will likely fail because the numerical gradients computed by point\nperturbation at such indices will change values at all other indices that\nshare the same memory address.\n\nTrue if all differences satisfy allclose condition\n\nCheck gradients of gradients computed via small finite differences against\nanalytical gradients w.r.t. tensors in `inputs` and `grad_outputs` that are of\nfloating point or complex type and with `requires_grad=True`.\n\nThis function checks that backpropagating through the gradients computed to\nthe given `grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nNote\n\nThe default values are designed for `input` and `grad_outputs` of double\nprecision. This check will likely fail if they are of less precision, e.g.,\n`FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` and `grad_outputs` has overlapping memory,\ni.e., different indices pointing to the same memory address (e.g., from\n`torch.expand()`), this check will likely fail because the numerical gradients\ncomputed by point perturbation at such indices will change values at all other\nindices that share the same memory address.\n\nTrue if all differences satisfy allclose condition\n\nAutograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using `profile`. and nvprof based\n(registers both CPU and GPU activity) using `emit_nvtx`.\n\nContext manager that manages autograd profiler state and holds a summary of\nresults. Under the hood it just records events of functions being executed in\nC++ and exposes those events to Python. You can wrap any code into it and it\nwill only report runtime of PyTorch functions. Note: profiler is thread local\nand is automatically propagated into the async tasks\n\nExports an EventList as a Chrome tracing tools file.\n\nThe checkpoint can be later loaded and inspected under `chrome://tracing` URL.\n\npath (str) \u2013 Path where the trace will be written.\n\nAverages all function events over their keys.\n\nAn EventList containing FunctionEventAvg objects.\n\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\nPrints an EventList as a nicely formatted table.\n\nA string containing the table.\n\nAverages all events.\n\nA FunctionEventAvg object.\n\nContext manager that makes every autograd operation emit an NVTX range.\n\nIt is useful when running the program under nvprof:\n\nUnfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them. Then,\neither NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\n`torch.autograd.profiler.load_nvprof()` can load the results for inspection\ne.g. in Python REPL.\n\nForward-backward correlation\n\nWhen viewing a profile created using `emit_nvtx` in the Nvidia Visual\nProfiler, correlating each backward-pass op with the corresponding forward-\npass op can be difficult. To ease this task, `emit_nvtx` appends sequence\nnumber information to the ranges it generates.\n\nDuring the forward pass, each function range is decorated with `seq=<N>`.\n`seq` is a running counter, incremented each time a new backward Function\nobject is created and stashed for backward. Thus, the `seq=<N>` annotation\nassociated with each forward function range tells you that if a backward\nFunction object is created by this forward function, the backward object will\nreceive sequence number N. During the backward pass, the top-level range\nwrapping each C++ backward Function\u2019s `apply()` call is decorated with\n`stashed seq=<M>`. `M` is the sequence number that the backward object was\ncreated with. By comparing `stashed seq` numbers in backward with `seq`\nnumbers in forward, you can track down which forward op created each backward\nFunction.\n\nAny functions executed during the backward pass are also decorated with\n`seq=<N>`. During default backward (with `create_graph=False`) this\ninformation is irrelevant, and in fact, `N` may simply be 0 for all such\nfunctions. Only the top-level ranges associated with backward Function\nobjects\u2019 `apply()` methods are useful, as a way to correlate these Function\nobjects with the earlier forward pass.\n\nDouble-backward\n\nIf, on the other hand, a backward pass with `create_graph=True` is underway\n(in other words, if you are setting up for a double-backward), each function\u2019s\nexecution during backward is given a nonzero, useful `seq=<N>`. Those\nfunctions may themselves create Function objects to be executed later during\ndouble-backward, just as the original functions in the forward pass did. The\nrelationship between backward and double-backward is conceptually the same as\nthe relationship between forward and backward: The functions still emit\ncurrent-sequence-number-tagged ranges, the Function objects they create still\nstash those sequence numbers, and during the eventual double-backward, the\nFunction objects\u2019 `apply()` ranges are still tagged with `stashed seq`\nnumbers, which can be compared to `seq` numbers from the backward pass.\n\nOpens an nvprof trace file and parses autograd annotations.\n\npath (str) \u2013 path to nvprof trace\n\nContext-manager that enable anomaly detection for the autograd engine.\n\nThis does two things:\n\nWarning\n\nThis mode should be enabled only for debugging as the different tests will\nslow down your program execution.\n\nContext-manager that sets the anomaly detection for the autograd engine on or\noff.\n\n`set_detect_anomaly` will enable or disable the autograd anomaly detection\nbased on its argument `mode`. It can be used as a context-manager or as a\nfunction.\n\nSee `detect_anomaly` above for details of the anomaly detection behaviour.\n\nmode (bool) \u2013 Flag whether to enable anomaly detection (`True`), or disable\n(`False`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.backward()", "path": "autograd#torch.autograd.backward", "type": "torch.autograd", "text": "\nComputes the sum of gradients of given tensors w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If any of `tensors` are non-\nscalar (i.e. their data has more than one element) and require gradient, then\nthe Jacobian-vector product would be computed, in this case the function\nadditionally requires specifying `grad_tensors`. It should be a sequence of\nmatching length, that contains the \u201cvector\u201d in the Jacobian-vector product,\nusually the gradient of the differentiated function w.r.t. corresponding\ntensors (`None` is an acceptable value for all tensors that don\u2019t need\ngradient tensors).\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nUsing this method with `create_graph=True` will create a reference cycle\nbetween the parameter and its gradient which can cause a memory leak. We\nrecommend using `autograd.grad` when creating the graph to avoid this. If you\nhave to use this function, make sure to reset the `.grad` fields of your\nparameters to `None` after use to break the cycle and avoid the leak.\n\nNote\n\nIf you run any forward ops, create `grad_tensors`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "torch.autograd", "text": "\nContext-manager that enable anomaly detection for the autograd engine.\n\nThis does two things:\n\nWarning\n\nThis mode should be enabled only for debugging as the different tests will\nslow down your program execution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.enable_grad", "path": "autograd#torch.autograd.enable_grad", "type": "torch.autograd", "text": "\nContext-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via `no_grad` or\n`set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "torch.autograd", "text": "\nRecords operation history and defines formulas for differentiating ops.\n\nSee the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-\ntorch-autograd\n\nEvery operation performed on `Tensor` s creates a new function object, that\nperforms the computation, and records that it happened. The history is\nretained in the form of a DAG of functions, with edges denoting data\ndependencies (`input <- output`). Then, when backward is called, the graph is\nprocessed in the topological ordering, by calling `backward()` methods of each\n`Function` object, and passing returned gradients on to next `Function` s.\n\nNormally, the only way users interact with functions is by creating subclasses\nand defining new operations. This is a recommended way of extending\ntorch.autograd.\n\nExamples:\n\nDefines a formula for differentiating the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context `ctx` as the first argument, followed by as many\noutputs did `forward()` return, and it should return as many tensors, as there\nwere inputs to `forward()`. Each argument is the gradient w.r.t the given\noutput, and each returned value should be the gradient w.r.t. the\ncorresponding input.\n\nThe context can be used to retrieve tensors saved during the forward pass. It\nalso has an attribute `ctx.needs_input_grad` as a tuple of booleans\nrepresenting whether each input needs gradient. E.g., `backward()` will have\n`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs\ngradient computated w.r.t. the output.\n\nPerforms the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context ctx as the first argument, followed by any number of\narguments (tensors or other types).\n\nThe context can be used to store tensors that can be then retrieved during the\nbackward pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.Function.backward()", "path": "autograd#torch.autograd.Function.backward", "type": "torch.autograd", "text": "\nDefines a formula for differentiating the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context `ctx` as the first argument, followed by as many\noutputs did `forward()` return, and it should return as many tensors, as there\nwere inputs to `forward()`. Each argument is the gradient w.r.t the given\noutput, and each returned value should be the gradient w.r.t. the\ncorresponding input.\n\nThe context can be used to retrieve tensors saved during the forward pass. It\nalso has an attribute `ctx.needs_input_grad` as a tuple of booleans\nrepresenting whether each input needs gradient. E.g., `backward()` will have\n`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs\ngradient computated w.r.t. the output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.Function.forward()", "path": "autograd#torch.autograd.Function.forward", "type": "torch.autograd", "text": "\nPerforms the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context ctx as the first argument, followed by any number of\narguments (tensors or other types).\n\nThe context can be used to store tensors that can be then retrieved during the\nbackward pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin", "path": "autograd#torch.autograd.function._ContextMethodMixin", "type": "torch.autograd", "text": "\nMarks given tensors as modified in an in-place operation.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be inputs.\n\nEvery tensor that\u2019s been modified in-place in a call to `forward()` should be\ngiven to this function, to ensure correctness of our checks. It doesn\u2019t matter\nwhether the function is called before or after modification.\n\nMarks outputs as non-differentiable.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be outputs.\n\nThis will mark outputs as not requiring gradients, increasing the efficiency\nof backward computation. You still need to accept a gradient for each output\nin `backward()`, but it\u2019s always going to be a zero tensor with the same shape\nas the shape of a corresponding output.\n\nThis is used e.g. for indices returned from a max `Function`.\n\nSaves given tensors for a future call to `backward()`.\n\nThis should be called at most once, and only from inside the `forward()`\nmethod.\n\nLater, saved tensors can be accessed through the `saved_tensors` attribute.\nBefore returning them to the user, a check is made to ensure they weren\u2019t used\nin any in-place operation that modified their content.\n\nArguments can also be `None`.\n\nSets whether to materialize output grad tensors. Default is true.\n\nThis should be called only from inside the `forward()` method\n\nIf true, undefined output grad tensors will be expanded to tensors full of\nzeros prior to calling the `backward()` method.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_dirty()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_dirty", "type": "torch.autograd", "text": "\nMarks given tensors as modified in an in-place operation.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be inputs.\n\nEvery tensor that\u2019s been modified in-place in a call to `forward()` should be\ngiven to this function, to ensure correctness of our checks. It doesn\u2019t matter\nwhether the function is called before or after modification.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "type": "torch.autograd", "text": "\nMarks outputs as non-differentiable.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be outputs.\n\nThis will mark outputs as not requiring gradients, increasing the efficiency\nof backward computation. You still need to accept a gradient for each output\nin `backward()`, but it\u2019s always going to be a zero tensor with the same shape\nas the shape of a corresponding output.\n\nThis is used e.g. for indices returned from a max `Function`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.save_for_backward()", "path": "autograd#torch.autograd.function._ContextMethodMixin.save_for_backward", "type": "torch.autograd", "text": "\nSaves given tensors for a future call to `backward()`.\n\nThis should be called at most once, and only from inside the `forward()`\nmethod.\n\nLater, saved tensors can be accessed through the `saved_tensors` attribute.\nBefore returning them to the user, a check is made to ensure they weren\u2019t used\nin any in-place operation that modified their content.\n\nArguments can also be `None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.set_materialize_grads()", "path": "autograd#torch.autograd.function._ContextMethodMixin.set_materialize_grads", "type": "torch.autograd", "text": "\nSets whether to materialize output grad tensors. Default is true.\n\nThis should be called only from inside the `forward()` method\n\nIf true, undefined output grad tensors will be expanded to tensors full of\nzeros prior to calling the `backward()` method.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.hessian()", "path": "autograd#torch.autograd.functional.hessian", "type": "torch.autograd", "text": "\nFunction that computes the Hessian of a given scalar function.\n\nif there is a single input, this will be a single Tensor containing the\nHessian for the input. If it is a tuple, then the Hessian will be a tuple of\ntuples where `Hessian[i][j]` will contain the Hessian of the `i`th input and\n`j`th input with size the sum of the size of the `i`th input plus the size of\nthe `j`th input. `Hessian[i][j]` will have the same dtype and device as the\ncorresponding `i`th input.\n\nHessian (Tensor or a tuple of tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.hvp()", "path": "autograd#torch.autograd.functional.hvp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between the Hessian of a given scalar\nfunction and a vector `v` at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nhvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nNote\n\nThis function is significantly slower than `vhp` due to backward mode AD\nconstraints. If your functions is twice continuously differentiable, then hvp\n= vhp.t(). So if you know that your function satisfies this condition, you\nshould use vhp instead that is much faster with the current implementation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.jacobian()", "path": "autograd#torch.autograd.functional.jacobian", "type": "torch.autograd", "text": "\nFunction that computes the Jacobian of a given function.\n\nif there is a single input and output, this will be a single Tensor containing\nthe Jacobian for the linearized inputs and output. If one of the two is a\ntuple, then the Jacobian will be a tuple of Tensors. If both of them are\ntuples, then the Jacobian will be a tuple of tuple of Tensors where\n`Jacobian[i][j]` will contain the Jacobian of the `i`th output and `j`th input\nand will have as size the concatenation of the sizes of the corresponding\noutput and the corresponding input and will have same dtype and device as the\ncorresponding input.\n\nJacobian (Tensor or nested tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.jvp()", "path": "autograd#torch.autograd.functional.jvp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between the Jacobian of the given\nfunction at the point given by the inputs and a vector `v`.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\njvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the output.\n\noutput (tuple)\n\nNote\n\nThe jvp is currently computed by using the backward of the backward (sometimes\ncalled the double backwards trick) as we don\u2019t have support for forward mode\nAD in PyTorch at the moment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.vhp()", "path": "autograd#torch.autograd.functional.vhp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between a vector `v` and the Hessian of\na given scalar function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvhp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.vjp()", "path": "autograd#torch.autograd.functional.vjp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between a vector `v` and the Jacobian\nof the given function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvjp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.grad()", "path": "autograd#torch.autograd.grad", "type": "torch.autograd", "text": "\nComputes and returns the sum of gradients of outputs w.r.t. the inputs.\n\n`grad_outputs` should be a sequence of length matching `output` containing the\n\u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t.\neach of the outputs. If an output doesn\u2019t require_grad, then the gradient can\nbe `None`).\n\nIf `only_inputs` is `True`, the function will only return a list of gradients\nw.r.t the specified inputs. If it\u2019s `False`, then gradient w.r.t. all\nremaining leaves will still be computed, and will be accumulated into their\n`.grad` attribute.\n\nNote\n\nIf you run any forward ops, create `grad_outputs`, and/or call `grad` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.gradcheck()", "path": "autograd#torch.autograd.gradcheck", "type": "torch.autograd", "text": "\nCheck gradients computed via small finite differences against analytical\ngradients w.r.t. tensors in `inputs` that are of floating point or complex\ntype and with `requires_grad=True`.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nFor complex functions, no notion of Jacobian exists. Gradcheck verifies if the\nnumerical and analytical values of Wirtinger and Conjugate Wirtinger\nderivative are consistent. The gradient computation is done under the\nassumption that the overall function has a real valued output. For functions\nwith complex output, gradcheck compares the numerical and analytical gradients\nfor two values of `grad_output`: 1 and 1j. For more details, check out\nAutograd for Complex Numbers.\n\nNote\n\nThe default values are designed for `input` of double precision. This check\nwill likely fail if `input` is of less precision, e.g., `FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` has overlapping memory, i.e., different\nindices pointing to the same memory address (e.g., from `torch.expand()`),\nthis check will likely fail because the numerical gradients computed by point\nperturbation at such indices will change values at all other indices that\nshare the same memory address.\n\nTrue if all differences satisfy allclose condition\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.gradgradcheck()", "path": "autograd#torch.autograd.gradgradcheck", "type": "torch.autograd", "text": "\nCheck gradients of gradients computed via small finite differences against\nanalytical gradients w.r.t. tensors in `inputs` and `grad_outputs` that are of\nfloating point or complex type and with `requires_grad=True`.\n\nThis function checks that backpropagating through the gradients computed to\nthe given `grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nNote\n\nThe default values are designed for `input` and `grad_outputs` of double\nprecision. This check will likely fail if they are of less precision, e.g.,\n`FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` and `grad_outputs` has overlapping memory,\ni.e., different indices pointing to the same memory address (e.g., from\n`torch.expand()`), this check will likely fail because the numerical gradients\ncomputed by point perturbation at such indices will change values at all other\nindices that share the same memory address.\n\nTrue if all differences satisfy allclose condition\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.no_grad", "path": "autograd#torch.autograd.no_grad", "type": "torch.autograd", "text": "\nContext-manager that disabled gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call `Tensor.backward()`. It will reduce memory consumption for\ncomputations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have `requires_grad=False`,\neven when the inputs have `requires_grad=True`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "torch.autograd", "text": "\nContext manager that makes every autograd operation emit an NVTX range.\n\nIt is useful when running the program under nvprof:\n\nUnfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them. Then,\neither NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\n`torch.autograd.profiler.load_nvprof()` can load the results for inspection\ne.g. in Python REPL.\n\nForward-backward correlation\n\nWhen viewing a profile created using `emit_nvtx` in the Nvidia Visual\nProfiler, correlating each backward-pass op with the corresponding forward-\npass op can be difficult. To ease this task, `emit_nvtx` appends sequence\nnumber information to the ranges it generates.\n\nDuring the forward pass, each function range is decorated with `seq=<N>`.\n`seq` is a running counter, incremented each time a new backward Function\nobject is created and stashed for backward. Thus, the `seq=<N>` annotation\nassociated with each forward function range tells you that if a backward\nFunction object is created by this forward function, the backward object will\nreceive sequence number N. During the backward pass, the top-level range\nwrapping each C++ backward Function\u2019s `apply()` call is decorated with\n`stashed seq=<M>`. `M` is the sequence number that the backward object was\ncreated with. By comparing `stashed seq` numbers in backward with `seq`\nnumbers in forward, you can track down which forward op created each backward\nFunction.\n\nAny functions executed during the backward pass are also decorated with\n`seq=<N>`. During default backward (with `create_graph=False`) this\ninformation is irrelevant, and in fact, `N` may simply be 0 for all such\nfunctions. Only the top-level ranges associated with backward Function\nobjects\u2019 `apply()` methods are useful, as a way to correlate these Function\nobjects with the earlier forward pass.\n\nDouble-backward\n\nIf, on the other hand, a backward pass with `create_graph=True` is underway\n(in other words, if you are setting up for a double-backward), each function\u2019s\nexecution during backward is given a nonzero, useful `seq=<N>`. Those\nfunctions may themselves create Function objects to be executed later during\ndouble-backward, just as the original functions in the forward pass did. The\nrelationship between backward and double-backward is conceptually the same as\nthe relationship between forward and backward: The functions still emit\ncurrent-sequence-number-tagged ranges, the Function objects they create still\nstash those sequence numbers, and during the eventual double-backward, the\nFunction objects\u2019 `apply()` ranges are still tagged with `stashed seq`\nnumbers, which can be compared to `seq` numbers from the backward pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "autograd#torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "text": "\nOpens an nvprof trace file and parses autograd annotations.\n\npath (str) \u2013 path to nvprof trace\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "torch.autograd", "text": "\nContext manager that manages autograd profiler state and holds a summary of\nresults. Under the hood it just records events of functions being executed in\nC++ and exposes those events to Python. You can wrap any code into it and it\nwill only report runtime of PyTorch functions. Note: profiler is thread local\nand is automatically propagated into the async tasks\n\nExports an EventList as a Chrome tracing tools file.\n\nThe checkpoint can be later loaded and inspected under `chrome://tracing` URL.\n\npath (str) \u2013 Path where the trace will be written.\n\nAverages all function events over their keys.\n\nAn EventList containing FunctionEventAvg objects.\n\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\nPrints an EventList as a nicely formatted table.\n\nA string containing the table.\n\nAverages all events.\n\nA FunctionEventAvg object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "autograd#torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "text": "\nExports an EventList as a Chrome tracing tools file.\n\nThe checkpoint can be later loaded and inspected under `chrome://tracing` URL.\n\npath (str) \u2013 Path where the trace will be written.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "autograd#torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "text": "\nAverages all function events over their keys.\n\nAn EventList containing FunctionEventAvg objects.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total()", "path": "autograd#torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "text": "\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.table()", "path": "autograd#torch.autograd.profiler.profile.table", "type": "torch.autograd", "text": "\nPrints an EventList as a nicely formatted table.\n\nA string containing the table.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "autograd#torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "text": "\nAverages all events.\n\nA FunctionEventAvg object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "torch.autograd", "text": "\nContext-manager that sets the anomaly detection for the autograd engine on or\noff.\n\n`set_detect_anomaly` will enable or disable the autograd anomaly detection\nbased on its argument `mode`. It can be used as a context-manager or as a\nfunction.\n\nSee `detect_anomaly` above for details of the anomaly detection behaviour.\n\nmode (bool) \u2013 Flag whether to enable anomaly detection (`True`), or disable\n(`False`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.set_grad_enabled", "path": "autograd#torch.autograd.set_grad_enabled", "type": "torch.autograd", "text": "\nContext-manager that sets gradient calculation to on or off.\n\n`set_grad_enabled` will enable or disable grads based on its argument `mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nmode (bool) \u2013 Flag whether to enable grad (`True`), or disable (`False`). This\ncan be used to conditionally enable gradients.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends", "path": "backends", "type": "torch.backends", "text": "\n`torch.backends` controls the behavior of various backends that PyTorch\nsupports.\n\nThese backends include:\n\nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t\nnecessarily mean CUDA is available; just that if this PyTorch binary were run\na machine with working CUDA drivers and devices, we would be able to use it.\n\nA `bool` that controls whether TensorFloat-32 tensor cores may be used in\nmatrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on\nAmpere devices.\n\n`cufft_plan_cache` caches the cuFFT plans\n\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\nA `int` that controls cache capacity of cuFFT plan.\n\nClears the cuFFT plan cache.\n\nReturns the version of cuDNN\n\nReturns a bool indicating if CUDNN is currently available.\n\nA `bool` that controls whether cuDNN is enabled.\n\nA `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere\ndevices.\n\nA `bool` that, if True, causes cuDNN to only use deterministic convolution\nalgorithms. See also `torch.are_deterministic_algorithms_enabled()` and\n`torch.use_deterministic_algorithms()`.\n\nA `bool` that, if True, causes cuDNN to benchmark multiple convolution\nalgorithms and select the fastest.\n\nReturns whether PyTorch is built with MKL support.\n\nReturns whether PyTorch is built with MKL-DNN support.\n\nReturns whether PyTorch is built with OpenMP support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "torch.backends", "text": "\n`cufft_plan_cache` caches the cuFFT plans\n\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\nA `int` that controls cache capacity of cuFFT plan.\n\nClears the cuFFT plan cache.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t\nnecessarily mean CUDA is available; just that if this PyTorch binary were run\na machine with working CUDA drivers and devices, we would be able to use it.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "torch.backends", "text": "\nA `bool` that controls whether TensorFloat-32 tensor cores may be used in\nmatrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on\nAmpere devices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.size", "path": "backends#torch.backends.cuda.size", "type": "torch.backends", "text": "\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "torch.backends", "text": "\nA `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere\ndevices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "torch.backends", "text": "\nA `bool` that, if True, causes cuDNN to benchmark multiple convolution\nalgorithms and select the fastest.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "torch.backends", "text": "\nA `bool` that, if True, causes cuDNN to only use deterministic convolution\nalgorithms. See also `torch.are_deterministic_algorithms_enabled()` and\n`torch.use_deterministic_algorithms()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "torch.backends", "text": "\nA `bool` that controls whether cuDNN is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "torch.backends", "text": "\nReturns a bool indicating if CUDNN is currently available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "torch.backends", "text": "\nReturns the version of cuDNN\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with MKL support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with MKL-DNN support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with OpenMP support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.baddbmm()", "path": "generated/torch.baddbmm#torch.baddbmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices in `batch1` and `batch2`.\n`input` is added to the final result.\n\n`batch1` and `batch2` must be 3-D tensors each containing the same number of\nmatrices.\n\nIf `batch1` is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, `batch2` is a (b\u00d7m\u00d7p)(b\n\\times m \\times p) tensor, then `input` must be broadcastable with a (b\u00d7n\u00d7p)(b\n\\times n \\times p) tensor and `out` will be a (b\u00d7n\u00d7p)(b \\times n \\times p)\ntensor. Both `alpha` and `beta` mean the same as the scaling factors used in\n`torch.addbmm()`.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers.\n\nThis operator supports TensorFloat32.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bartlett_window()", "path": "generated/torch.bartlett_window#torch.bartlett_window", "type": "torch", "text": "\nBartlett window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.bartlett_window(L, periodic=True)` equal to `torch.bartlett_window(L +\n1, periodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bernoulli()", "path": "generated/torch.bernoulli#torch.bernoulli", "type": "torch", "text": "\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\n\nThe `input` tensor should be a tensor containing probabilities to be used for\ndrawing the binary random number. Hence, all values in `input` have to be in\nthe range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1 .\n\nThe ith\\text{i}^{th} element of the output tensor will draw a value 11\naccording to the ith\\text{i}^{th} probability value given in `input`.\n\nThe returned `out` tensor only has values 0 or 1 and is of the same shape as\n`input`.\n\n`out` can have integral `dtype`, but `input` must have floating point `dtype`.\n\ninput (Tensor) \u2013 the input tensor of probability values for the Bernoulli\ndistribution\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bincount()", "path": "generated/torch.bincount#torch.bincount", "type": "torch", "text": "\nCount the frequency of each value in an array of non-negative ints.\n\nThe number of bins (size 1) is one larger than the largest value in `input`\nunless `input` is empty, in which case the result is a tensor of size 0. If\n`minlength` is specified, the number of bins is at least `minlength` and if\n`input` is empty, then the result is tensor of size `minlength` filled with\nzeros. If `n` is the value at position `i`, `out[n] += weights[i]` if\n`weights` is specified else `out[n] += 1`.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\na tensor of shape `Size([max(input) + 1])` if `input` is non-empty, else\n`Size(0)`\n\noutput (Tensor)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_and()", "path": "generated/torch.bitwise_and#torch.bitwise_and", "type": "torch", "text": "\nComputes the bitwise AND of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical AND.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_not()", "path": "generated/torch.bitwise_not#torch.bitwise_not", "type": "torch", "text": "\nComputes the bitwise NOT of the given input tensor. The input tensor must be\nof integral or Boolean types. For bool tensors, it computes the logical NOT.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_or()", "path": "generated/torch.bitwise_or#torch.bitwise_or", "type": "torch", "text": "\nComputes the bitwise OR of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical OR.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_xor()", "path": "generated/torch.bitwise_xor#torch.bitwise_xor", "type": "torch", "text": "\nComputes the bitwise XOR of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.blackman_window()", "path": "generated/torch.blackman_window#torch.blackman_window", "type": "torch", "text": "\nBlackman window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.blackman_window(L, periodic=True)` equal to `torch.blackman_window(L +\n1, periodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.block_diag()", "path": "generated/torch.block_diag#torch.block_diag", "type": "torch", "text": "\nCreate a block diagonal matrix from provided tensors.\n\n*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.\norder such that their upper left and lower right corners are diagonally\nadjacent. All other elements are set to 0.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bmm()", "path": "generated/torch.bmm#torch.bmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices stored in `input` and\n`mat2`.\n\n`input` and `mat2` must be 3-D tensors each containing the same number of\nmatrices.\n\nIf `input` is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, `mat2` is a (b\u00d7m\u00d7p)(b\n\\times m \\times p) tensor, `out` will be a (b\u00d7n\u00d7p)(b \\times n \\times p)\ntensor.\n\nThis operator supports TensorFloat32.\n\nNote\n\nThis function does not broadcast. For broadcasting matrix products, see\n`torch.matmul()`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.broadcast_shapes()", "path": "generated/torch.broadcast_shapes#torch.broadcast_shapes", "type": "torch", "text": "\nSimilar to `broadcast_tensors()` but for shapes.\n\nThis is equivalent to `torch.broadcast_tensors(*map(torch.empty,\nshapes))[0].shape` but avoids the need create to intermediate tensors. This is\nuseful for broadcasting tensors of common batch shape but different rightmost\nshape, e.g. to broadcast mean vectors with covariance matrices.\n\nExample:\n\n*shapes (torch.Size) \u2013 Shapes of tensors.\nA shape compatible with all input shapes.\n\nshape (torch.Size)\n\nRuntimeError \u2013 If shapes are incompatible.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.broadcast_tensors()", "path": "generated/torch.broadcast_tensors#torch.broadcast_tensors", "type": "torch", "text": "\nBroadcasts the given tensors according to Broadcasting semantics.\n\n*tensors \u2013 any number of tensors of the same type\nWarning\n\nMore than one element of a broadcasted tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.broadcast_to()", "path": "generated/torch.broadcast_to#torch.broadcast_to", "type": "torch", "text": "\nBroadcasts `input` to the shape `shape`. Equivalent to calling\n`input.expand(shape)`. See `expand()` for details.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bucketize()", "path": "generated/torch.bucketize#torch.bucketize", "type": "torch", "text": "\nReturns the indices of the buckets to which each value in the `input` belongs,\nwhere the boundaries of the buckets are set by `boundaries`. Return a new\ntensor with the same size as `input`. If `right` is False (default), then the\nleft boundary is closed. More formally, the returned index satisfies the\nfollowing rules:\n\n`right`\n\nreturned index satisfies\n\nFalse\n\n`boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]`\n\nTrue\n\n`boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.can_cast()", "path": "generated/torch.can_cast#torch.can_cast", "type": "torch", "text": "\nDetermines if a type conversion is allowed under PyTorch casting rules\ndescribed in the type promotion documentation.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cartesian_prod()", "path": "generated/torch.cartesian_prod#torch.cartesian_prod", "type": "torch", "text": "\nDo cartesian product of the given sequence of tensors. The behavior is similar\nto python\u2019s `itertools.product`.\n\n*tensors \u2013 any number of 1 dimensional tensors.\ndo `itertools.product` on these lists, and finally convert the resulting list\ninto tensor.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cat()", "path": "generated/torch.cat#torch.cat", "type": "torch", "text": "\nConcatenates the given sequence of `seq` tensors in the given dimension. All\ntensors must either have the same shape (except in the concatenating\ndimension) or be empty.\n\n`torch.cat()` can be seen as an inverse operation for `torch.split()` and\n`torch.chunk()`.\n\n`torch.cat()` can be best understood via examples.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cdist()", "path": "generated/torch.cdist#torch.cdist", "type": "torch", "text": "\nComputes batched the p-norm distance between each pair of the two collections\nof row vectors.\n\nIf x1 has shape B\u00d7P\u00d7MB \\times P \\times M and x2 has shape B\u00d7R\u00d7MB \\times R\n\\times M then the output will have shape B\u00d7P\u00d7RB \\times P \\times R .\n\nThis function is equivalent to\n`scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p)` if p\u2208(0,\u221e)p \\in (0,\n\\infty) . When p=0p = 0 it is equivalent to\n`scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M`. When p=\u221ep = \\infty , the\nclosest scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y:\nnp.abs(x - y).max())`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ceil()", "path": "generated/torch.ceil#torch.ceil", "type": "torch", "text": "\nReturns a new tensor with the ceil of the elements of `input`, the smallest\ninteger greater than or equal to each element.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.chain_matmul()", "path": "generated/torch.chain_matmul#torch.chain_matmul", "type": "torch", "text": "\nReturns the matrix product of the NN 2-D tensors. This product is efficiently\ncomputed using the matrix chain order algorithm which selects the order in\nwhich incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note\nthat since this is a function to compute the product, NN needs to be greater\nthan or equal to 2; if equal to 2 then a trivial matrix-matrix product is\nreturned. If NN is 1, then this is a no-op - the original matrix is returned\nas is.\n\nmatrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is\nto be determined.\n\nif the ithi^{th} tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1} , then\nthe product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1} .\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cholesky()", "path": "generated/torch.cholesky#torch.cholesky", "type": "torch", "text": "\nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA\nor for batches of symmetric positive-definite matrices.\n\nIf `upper` is `True`, the returned matrix `U` is upper-triangular, and the\ndecomposition has the form:\n\nIf `upper` is `False`, the returned matrix `L` is lower-triangular, and the\ndecomposition has the form:\n\nIf `upper` is `True`, and AA is a batch of symmetric positive-definite\nmatrices, then the returned tensor will be composed of upper-triangular\nCholesky factors of each of the individual matrices. Similarly, when `upper`\nis `False`, the returned tensor will be composed of lower-triangular Cholesky\nfactors of each of the individual matrices.\n\nNote\n\n`torch.linalg.cholesky()` should be used over `torch.cholesky` when possible.\nNote however that `torch.linalg.cholesky()` does not yet support the `upper`\nparameter and instead always returns the lower triangular matrix.\n\nout (Tensor, optional) \u2013 the output matrix\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cholesky_inverse()", "path": "generated/torch.cholesky_inverse#torch.cholesky_inverse", "type": "torch", "text": "\nComputes the inverse of a symmetric positive-definite matrix AA using its\nCholesky factor uu : returns matrix `inv`. The inverse is computed using\nLAPACK routines `dpotri` and `spotri` (and the corresponding MAGMA routines).\n\nIf `upper` is `False`, uu is lower triangular such that the returned tensor is\n\nIf `upper` is `True` or not provided, uu is upper triangular such that the\nreturned tensor is\n\nout (Tensor, optional) \u2013 the output tensor for `inv`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cholesky_solve()", "path": "generated/torch.cholesky_solve#torch.cholesky_solve", "type": "torch", "text": "\nSolves a linear system of equations with a positive semidefinite matrix to be\ninverted given its Cholesky factor matrix uu .\n\nIf `upper` is `False`, uu is and lower triangular and `c` is returned such\nthat:\n\nIf `upper` is `True` or not provided, uu is upper triangular and `c` is\nreturned such that:\n\n`torch.cholesky_solve(b, u)` can take in 2D inputs `b, u` or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns batched\noutputs `c`\n\nSupports real-valued and complex-valued inputs. For the complex-valued inputs\nthe transpose operator above is the conjugate transpose.\n\nout (Tensor, optional) \u2013 the output tensor for `c`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.chunk()", "path": "generated/torch.chunk#torch.chunk", "type": "torch", "text": "\nSplits a tensor into a specific number of chunks. Each chunk is a view of the\ninput tensor.\n\nLast chunk will be smaller if the tensor size along the given dimension `dim`\nis not divisible by `chunks`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.clamp()", "path": "generated/torch.clamp#torch.clamp", "type": "torch", "text": "\nClamp all elements in `input` into the range `[` `min`, `max` `]`. Let\nmin_value and max_value be `min` and `max`, respectively, this returns:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nClamps all elements in `input` to be larger or equal `min`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nClamps all elements in `input` to be smaller or equal `max`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.clip()", "path": "generated/torch.clip#torch.clip", "type": "torch", "text": "\nAlias for `torch.clamp()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.clone()", "path": "generated/torch.clone#torch.clone", "type": "torch", "text": "\nReturns a copy of `input`.\n\nNote\n\nThis function is differentiable, so gradients will flow back from the result\nof this operation to `input`. To create a tensor without an autograd\nrelationship to `input` see `detach()`.\n\ninput (Tensor) \u2013 the input tensor.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.column_stack()", "path": "generated/torch.column_stack#torch.column_stack", "type": "torch", "text": "\nCreates a new tensor by horizontally stacking the tensors in `tensors`.\n\nEquivalent to `torch.hstack(tensors)`, except each zero or one dimensional\ntensor `t` in `tensors` is first reshaped into a `(t.numel(), 1)` column\nbefore being stacked horizontally.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.combinations()", "path": "generated/torch.combinations#torch.combinations", "type": "torch", "text": "\nCompute combinations of length rr of the given tensor. The behavior is similar\nto python\u2019s `itertools.combinations` when `with_replacement` is set to\n`False`, and `itertools.combinations_with_replacement` when `with_replacement`\nis set to `True`.\n\nA tensor equivalent to converting all the input tensors into lists, do\n`itertools.combinations` or `itertools.combinations_with_replacement` on these\nlists, and finally convert the resulting list into tensor.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.compiled_with_cxx11_abi()", "path": "generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi", "type": "torch", "text": "\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.complex()", "path": "generated/torch.complex#torch.complex", "type": "torch", "text": "\nConstructs a complex tensor with its real part equal to `real` and its\nimaginary part equal to `imag`.\n\nout (Tensor) \u2013 If the inputs are `torch.float32`, must be `torch.complex64`.\nIf the inputs are `torch.float64`, must be `torch.complex128`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.conj()", "path": "generated/torch.conj#torch.conj", "type": "torch", "text": "\nComputes the element-wise conjugate of the given `input` tensor. If\n:attr`input` has a non-complex dtype, this function just returns `input`.\n\nWarning\n\nIn the future, `torch.conj()` may return a non-writeable view for an `input`\nof non-complex dtype. It\u2019s recommended that programs not modify the tensor\nreturned by `torch.conj()` when `input` is of non-complex dtype to be\ncompatible with this change.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.copysign()", "path": "generated/torch.copysign#torch.copysign", "type": "torch", "text": "\nCreate a new floating-point tensor with the magnitude of `input` and the sign\nof `other`, elementwise.\n\nSupports broadcasting to a common shape, and integer and float inputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cos()", "path": "generated/torch.cos#torch.cos", "type": "torch", "text": "\nReturns a new tensor with the cosine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cosh()", "path": "generated/torch.cosh#torch.cosh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic cosine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nNote\n\nWhen `input` is on the CPU, the implementation of torch.cosh may use the Sleef\nlibrary, which rounds very large results to infinity or negative infinity. See\nhere for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.count_nonzero()", "path": "generated/torch.count_nonzero#torch.count_nonzero", "type": "torch", "text": "\nCounts the number of non-zero values in the tensor `input` along the given\n`dim`. If no dim is specified then all non-zeros in the tensor are counted.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cross()", "path": "generated/torch.cross#torch.cross", "type": "torch", "text": "\nReturns the cross product of vectors in dimension `dim` of `input` and\n`other`.\n\n`input` and `other` must have the same size, and the size of their `dim`\ndimension should be 3.\n\nIf `dim` is not given, it defaults to the first dimension found with the size\n3. Note that this might be unexpected.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda", "path": "cuda", "type": "torch.cuda", "text": "\nThis package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation.\n\nIt is lazily initialized, so you can always import it, and use\n`is_available()` to determine if your system supports CUDA.\n\nCUDA semantics has more details about working with CUDA.\n\nChecks if peer access between two devices is possible.\n\nReturns cublasHandle_t pointer to current cuBLAS handle\n\nReturns the index of a currently selected device.\n\nReturns the currently selected `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the\ncurrently selected `Stream` for the current device, given by\n`current_device()`, if `device` is `None` (default).\n\nReturns the default `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the default\n`Stream` for the current device, given by `current_device()`, if `device` is\n`None` (default).\n\nContext-manager that changes the selected device.\n\ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this\nargument is a negative integer or `None`.\n\nReturns the number of GPUs available.\n\nContext-manager that changes the current device to that of given object.\n\nYou can use both tensors and storages as arguments. If a given object is not\nallocated on a GPU, this is a no-op.\n\nobj (Tensor or Storage) \u2013 object allocated on the selected device.\n\nReturns list CUDA architectures this library was compiled for.\n\nGets the cuda capability of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the device\ncapability. This function is a no-op if this argument is a negative integer.\nIt uses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nthe major and minor cuda capability of the device\n\ntuple(int, int)\n\nGets the name of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the name.\nThis function is a no-op if this argument is a negative integer. It uses the\ncurrent device, given by `current_device()`, if `device` is `None` (default).\n\nthe name of the device\n\nstr\n\nGets the properties of a device.\n\ndevice (torch.device or int or str) \u2013 device for which to return the\nproperties of the device.\n\nthe properties of the device\n\n_CudaDeviceProperties\n\nReturns NVCC gencode flags this library were compiled with.\n\nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you\nare interacting with PyTorch via its C API, as Python bindings for CUDA\nfunctionality will not be available until this initialization takes place.\nOrdinary users should not need this, as all of PyTorch\u2019s CUDA methods\nautomatically initialize CUDA state on-demand.\n\nDoes nothing if the CUDA state is already initialized.\n\nForce collects GPU memory after it has been released by CUDA IPC.\n\nNote\n\nChecks if any sent CUDA tensors could be cleaned from the memory. Force closes\nshared memory file used for reference counting if there is no active counters.\nUseful when the producer process stopped actively sending tensors and want to\nrelease unused memory.\n\nReturns a bool indicating if CUDA is currently available.\n\nReturns whether PyTorch\u2019s CUDA state has been initialized.\n\nSets the current device.\n\nUsage of this function is discouraged in favor of `device`. In most cases it\u2019s\nbetter to use `CUDA_VISIBLE_DEVICES` environmental variable.\n\ndevice (torch.device or int) \u2013 selected device. This function is a no-op if\nthis argument is negative.\n\nContext-manager that selects a given stream.\n\nAll CUDA kernels queued within its context will be enqueued on a selected\nstream.\n\nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s `None`.\n\nNote\n\nStreams are per-device. If the selected stream is not on the current device,\nthis function will also change the current device to match the stream.\n\nWaits for all kernels in all streams on a CUDA device to complete.\n\ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It\nuses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nReturns the random number generator state of the specified GPU as a\nByteTensor.\n\ndevice (torch.device or int, optional) \u2013 The device to return the RNG state\nof. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).\n\nWarning\n\nThis function eagerly initializes CUDA.\n\nReturns a list of ByteTensor representing the random number states of all\ndevices.\n\nSets the random number generator state of the specified GPU.\n\nSets the random number generator state of all devices.\n\nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device\n\nSets the seed for generating random numbers for the current GPU. It\u2019s safe to\ncall this function if CUDA is not available; in that case, it is silently\nignored.\n\nseed (int) \u2013 The desired seed.\n\nWarning\n\nIf you are working with a multi-GPU model, this function is insufficient to\nget determinism. To seed all GPUs, use `manual_seed_all()`.\n\nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call\nthis function if CUDA is not available; in that case, it is silently ignored.\n\nseed (int) \u2013 The desired seed.\n\nSets the seed for generating random numbers to a random number for the current\nGPU. It\u2019s safe to call this function if CUDA is not available; in that case,\nit is silently ignored.\n\nWarning\n\nIf you are working with a multi-GPU model, this function will only initialize\nthe seed on one GPU. To initialize all GPUs, use `seed_all()`.\n\nSets the seed for generating random numbers to a random number on all GPUs.\nIt\u2019s safe to call this function if CUDA is not available; in that case, it is\nsilently ignored.\n\nReturns the current random seed of the current GPU.\n\nWarning\n\nThis function eagerly initializes CUDA.\n\nBroadcasts a tensor to specified GPU devices.\n\nNote\n\nExactly one of `devices` and `out` must be specified.\n\na tuple containing copies of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a copy of `tensor`.\n\nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first\ncoalesced into a buffer to reduce the number of synchronizations.\n\nA tuple containing copies of `tensor`, placed on `devices`.\n\nSums tensors from multiple GPUs.\n\nAll inputs should have matching shapes, dtype, and layout. The output tensor\nwill be of the same shape, dtype, and layout.\n\nA tensor containing an elementwise sum of all inputs, placed on the\n`destination` device.\n\nScatters tensor across multiple GPUs.\n\nNote\n\nExactly one of `devices` and `out` must be specified. When `out` is specified,\n`chunk_sizes` must not be specified and will be inferred from sizes of `out`.\n\na tuple containing chunks of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a chunk of `tensor`.\n\nGathers tensors from multiple GPU devices.\n\nNote\n\n`destination` must not be specified when `out` is specified.\n\na tensor located on `destination` device, that is a result of concatenating\n`tensors` along `dim`.\n\nthe `out` tensor, now containing results of concatenating `tensors` along\n`dim`.\n\nWrapper around a CUDA stream.\n\nA CUDA stream is a linear sequence of execution that belongs to a specific\ndevice, independent from other streams. See CUDA semantics for details.\n\nNote\n\nAlthough CUDA versions >= 11 support more than two levels of priorities, in\nPyTorch, we only support two levels of priorities.\n\nChecks if all the work submitted has been completed.\n\nA boolean indicating if all kernels in this stream are completed.\n\nRecords an event.\n\nevent (Event, optional) \u2013 event to record. If not given, a new one will be\nallocated.\n\nRecorded event.\n\nWait for all the kernels in this stream to complete.\n\nNote\n\nThis is a wrapper around `cudaStreamSynchronize()`: see CUDA Stream\ndocumentation for more info.\n\nMakes all future work submitted to the stream wait for an event.\n\nevent (Event) \u2013 an event to wait for.\n\nNote\n\nThis is a wrapper around `cudaStreamWaitEvent()`: see CUDA Stream\ndocumentation for more info.\n\nThis function returns without waiting for `event`: only future operations are\naffected.\n\nSynchronizes with another stream.\n\nAll future work submitted to this stream will wait until all kernels submitted\nto a given stream at the time of call complete.\n\nstream (Stream) \u2013 a stream to synchronize.\n\nNote\n\nThis function returns without waiting for currently enqueued kernels in\n`stream`: only future operations are affected.\n\nWrapper around a CUDA event.\n\nCUDA events are synchronization markers that can be used to monitor the\ndevice\u2019s progress, to accurately measure timing, and to synchronize CUDA\nstreams.\n\nThe underlying CUDA events are lazily initialized when the event is first\nrecorded or exported to another process. After creation, only streams on the\nsame device may record the event. However, streams on any device can wait on\nthe event.\n\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\nReconstruct an event from an IPC handle on the given device.\n\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\nChecks if all work currently captured by event has completed.\n\nA boolean indicating if all work currently captured by event has completed.\n\nRecords the event in a given stream.\n\nUses `torch.cuda.current_stream()` if no stream is specified. The stream\u2019s\ndevice must match the event\u2019s device.\n\nWaits for the event to complete.\n\nWaits until the completion of all work currently captured in this event. This\nprevents the CPU thread from proceeding until the event completes.\n\nNote\n\nThis is a wrapper around `cudaEventSynchronize()`: see CUDA Event\ndocumentation for more info.\n\nMakes all future work submitted to the given stream wait for this event.\n\nUse `torch.cuda.current_stream()` if no stream is specified.\n\nReleases all unoccupied cached memory currently held by the caching allocator\nso that those can be used in other GPU application and visible in `nvidia-\nsmi`.\n\nNote\n\n`empty_cache()` doesn\u2019t increase the amount of GPU memory available for\nPyTorch. However, it may help reduce fragmentation of GPU memory in certain\ncases. See Memory management for more details about GPU memory management.\n\nReturns a human-readable printout of the running processes and their GPU\nmemory use for a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for\nthe current device, given by `current_device()`, if `device` is `None`\n(default).\n\nReturns a dictionary of CUDA memory allocator statistics for a given device.\n\nThe return value of this function is a dictionary of statistics, each of which\nis a non-negative integer.\n\nCore statistics:\n\nFor these core statistics, values are broken down as follows.\n\nPool type:\n\nMetric type:\n\nIn addition to the core statistics, we also provide some simple event\ncounters:\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns a human-readable printout of the current memory allocator statistics\nfor a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns a snapshot of the CUDA memory allocator state across all devices.\n\nInterpreting the output of this function requires familiarity with the memory\nallocator internals.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns the current GPU memory occupied by tensors in bytes for a given\ndevice.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nThis is likely less than the amount shown in `nvidia-smi` since some unused\nmemory can be held by the caching allocator and some context needs to be\ncreated on GPU. See Memory management for more details about GPU memory\nmanagement.\n\nReturns the maximum GPU memory occupied by tensors in bytes for a given\ndevice.\n\nBy default, this returns the peak allocated memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\nallocated memory usage of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nResets the starting point in tracking maximum GPU memory occupied by tensors\nfor a given device.\n\nSee `max_memory_allocated()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns the current GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns the maximum GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\nBy default, this returns the peak cached memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\ncached memory amount of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nSet memory fraction for a process. The fraction is used to limit an caching\nallocator to allocated memory on a CUDA device. The allowed value equals the\ntotal visible memory multiplied fraction. If trying to allocate more than the\nallowed value in a process, will raise an out of memory error in allocator.\n\nNote\n\nIn general, the total available free memory is less than the total capacity.\n\nDeprecated; see `memory_reserved()`.\n\nDeprecated; see `max_memory_reserved()`.\n\nResets the starting point in tracking maximum GPU memory managed by the\ncaching allocator for a given device.\n\nSee `max_memory_cached()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nDescribe an instantaneous event that occurred at some point.\n\nmsg (string) \u2013 ASCII message to associate with the event.\n\nPushes a range onto a stack of nested range span. Returns zero-based depth of\nthe range that is started.\n\nmsg (string) \u2013 ASCII message to associate with range\n\nPops a range off of a stack of nested range spans. Returns the zero-based\ndepth of the range that is ended.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp", "path": "amp", "type": "torch.cuda.amp", "text": "\n`torch.cuda.amp` provides convenience methods for mixed precision, where some\noperations use the `torch.float32` (`float`) datatype and other operations use\n`torch.float16` (`half`). Some ops, like linear layers and convolutions, are\nmuch faster in `float16`. Other ops, like reductions, often require the\ndynamic range of `float32`. Mixed precision tries to match each op to its\nappropriate datatype.\n\nOrdinarily, \u201cautomatic mixed precision training\u201d uses\n`torch.cuda.amp.autocast` and `torch.cuda.amp.GradScaler` together, as shown\nin the Automatic Mixed Precision examples and Automatic Mixed Precision\nrecipe. However, `autocast` and `GradScaler` are modular, and may be used\nseparately if desired.\n\nAutocast Op Reference\n\nOp-Specific Behavior\n\nInstances of `autocast` serve as context managers or decorators that allow\nregions of your script to run in mixed precision.\n\nIn these regions, CUDA ops run in an op-specific dtype chosen by autocast to\nimprove performance while maintaining accuracy. See the Autocast Op Reference\nfor details.\n\nWhen entering an autocast-enabled region, Tensors may be any type. You should\nnot call `.half()` on your model(s) or inputs when using autocasting.\n\n`autocast` should wrap only the forward pass(es) of your network, including\nthe loss computation(s). Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward\nops.\n\nExample:\n\nSee the Automatic Mixed Precision examples for usage (along with gradient\nscaling) in more complex scenarios (e.g., gradient penalty, multiple\nmodels/losses, custom autograd functions).\n\n`autocast` can also be used as a decorator, e.g., on the `forward` method of\nyour model:\n\nFloating-point Tensors produced in an autocast-enabled region may be\n`float16`. After returning to an autocast-disabled region, using them with\nfloating-point Tensors of different dtypes may cause type mismatch errors. If\nso, cast the Tensor(s) produced in the autocast region back to `float32` (or\nother dtype if desired). If a Tensor from the autocast region is already\n`float32`, the cast is a no-op, and incurs no additional overhead. Example:\n\nType mismatch errors in an autocast-enabled region are a bug; if this is what\nyou observe, please file an issue.\n\n`autocast(enabled=False)` subregions can be nested in autocast-enabled\nregions. Locally disabling autocast can be useful, for example, if you want to\nforce a subregion to run in a particular `dtype`. Disabling autocast gives you\nexplicit control over the execution type. In the subregion, inputs from the\nsurrounding region should be cast to `dtype` before use:\n\nThe autocast state is thread-local. If you want it enabled in a new thread,\nthe context manager or decorator must be invoked in that thread. This affects\n`torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel` when\nused with more than one GPU per process (see Working with Multiple GPUs).\n\nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled\nin the region.\n\nHelper decorator for `forward` methods of custom autograd functions\n(subclasses of `torch.autograd.Function`). See the example page for more\ndetail.\n\ncast_inputs (`torch.dtype` or None, optional, default=None) \u2013 If not `None`,\nwhen `forward` runs in an autocast-enabled region, casts incoming floating-\npoint CUDA Tensors to the target dtype (non-floating-point Tensors are not\naffected), then executes `forward` with autocast disabled. If `None`,\n`forward`\u2019s internal ops execute with the current autocast state.\n\nNote\n\nIf the decorated `forward` is called outside an autocast-enabled region,\n`custom_fwd` is a no-op and `cast_inputs` has no effect.\n\nHelper decorator for backward methods of custom autograd functions (subclasses\nof `torch.autograd.Function`). Ensures that `backward` executes with the same\nautocast state as `forward`. See the example page for more detail.\n\nIf the forward pass for a particular op has `float16` inputs, the backward\npass for that op will produce `float16` gradients. Gradient values with small\nmagnitudes may not be representable in `float16`. These values will flush to\nzero (\u201cunderflow\u201d), so the update for the corresponding parameters will be\nlost.\n\nTo prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by\na scale factor and invokes a backward pass on the scaled loss(es). Gradients\nflowing backward through the network are then scaled by the same factor. In\nother words, gradient values have a larger magnitude, so they don\u2019t flush to\nzero.\n\nEach parameter\u2019s gradient (`.grad` attribute) should be unscaled before the\noptimizer updates the parameters, so the scale factor does not interfere with\nthe learning rate.\n\nReturns a Python float containing the scale backoff factor.\n\nReturns a Python float containing the scale growth factor.\n\nReturns a Python int containing the growth interval.\n\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\nWarning\n\n`get_scale()` incurs a CPU-GPU sync.\n\nReturns a bool indicating whether this instance is enabled.\n\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to\n`state_dict()`.\n\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of `GradScaler` is not enabled,\noutputs are returned unmodified.\n\noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.\n\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\nReturns the state of the scaler as a `dict`. It contains five entries:\n\nIf this instance is not enabled, returns an empty dict.\n\nNote\n\nIf you wish to checkpoint the scaler\u2019s state after a particular iteration,\n`state_dict()` should be called after `update()`.\n\n`step()` carries out the following two operations:\n\n`*args` and `**kwargs` are forwarded to `optimizer.step()`.\n\nReturns the return value of `optimizer.step(*args, **kwargs)`.\n\nWarning\n\nClosure use is not currently supported.\n\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n`unscale_()` is optional, serving cases where you need to modify or inspect\ngradients between the backward pass(es) and `step()`. If `unscale_()` is not\ncalled explicitly, gradients will be unscaled automatically during `step()`.\n\nSimple example, using `unscale_()` to enable clipping of unscaled gradients:\n\noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be\nunscaled.\n\nNote\n\n`unscale_()` does not incur a CPU-GPU sync.\n\nWarning\n\n`unscale_()` should only be called once per optimizer per `step()` call, and\nonly after all gradients for that optimizer\u2019s assigned parameters have been\naccumulated. Calling `unscale_()` twice for a given optimizer between each\n`step()` triggers a RuntimeError.\n\nWarning\n\n`unscale_()` may unscale sparse gradients out of place, replacing the `.grad`\nattribute.\n\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by\n`backoff_factor` to reduce it. If `growth_interval` unskipped iterations\noccurred consecutively, the scale is multiplied by `growth_factor` to increase\nit.\n\nPassing `new_scale` sets the scale directly.\n\nnew_scale (float or `torch.cuda.FloatTensor`, optional, default=None) \u2013 New\nscale factor.\n\nWarning\n\n`update()` should only be called at the end of the iteration, after\n`scaler.step(optimizer)` has been invoked for all optimizers used this\niteration.\n\nOnly CUDA ops are eligible for autocasting.\n\nOps that run in `float64` or non-floating-point dtypes are not eligible, and\nwill run in these types whether or not autocast is enabled.\n\nOnly out-of-place ops and Tensor methods are eligible. In-place variants and\ncalls that explicitly supply an `out=...` Tensor are allowed in autocast-\nenabled regions, but won\u2019t go through autocasting. For example, in an\nautocast-enabled region `a.addmm(b, c)` can autocast, but `a.addmm_(b, c)` and\n`a.addmm(b, c, out=d)` cannot. For best performance and stability, prefer out-\nof-place ops in autocast-enabled regions.\n\nOps called with an explicit `dtype=...` argument are not eligible, and will\nproduce output that respects the `dtype` argument.\n\nThe following lists describe the behavior of eligible ops in autocast-enabled\nregions. These ops always go through autocasting whether they are invoked as\npart of a `torch.nn.Module`, as a function, or as a `torch.Tensor` method. If\nfunctions are exposed in multiple namespaces, they go through autocasting\nregardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type in\nwhich unlisted ops run if they\u2019re downstream from autocasted ops.\n\nIf an op is unlisted, we assume it\u2019s numerically stable in `float16`. If you\nbelieve an unlisted op is numerically unstable in `float16`, please file an\nissue.\n\n`__matmul__`, `addbmm`, `addmm`, `addmv`, `addr`, `baddbmm`, `bmm`,\n`chain_matmul`, `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`,\n`conv_transpose2d`, `conv_transpose3d`, `GRUCell`, `linear`, `LSTMCell`,\n`matmul`, `mm`, `mv`, `prelu`, `RNNCell`\n\n`__pow__`, `__rdiv__`, `__rpow__`, `__rtruediv__`, `acos`, `asin`,\n`binary_cross_entropy_with_logits`, `cosh`, `cosine_embedding_loss`, `cdist`,\n`cosine_similarity`, `cross_entropy`, `cumprod`, `cumsum`, `dist`, `erfinv`,\n`exp`, `expm1`, `gelu`, `group_norm`, `hinge_embedding_loss`, `kl_div`,\n`l1_loss`, `layer_norm`, `log`, `log_softmax`, `log10`, `log1p`, `log2`,\n`margin_ranking_loss`, `mse_loss`, `multilabel_margin_loss`,\n`multi_margin_loss`, `nll_loss`, `norm`, `normalize`, `pdist`,\n`poisson_nll_loss`, `pow`, `prod`, `reciprocal`, `rsqrt`, `sinh`,\n`smooth_l1_loss`, `soft_margin_loss`, `softmax`, `softmin`, `softplus`, `sum`,\n`renorm`, `tan`, `triplet_margin_loss`\n\nThese ops don\u2019t require a particular dtype for stability, but take multiple\ninputs and require that the inputs\u2019 dtypes match. If all of the inputs are\n`float16`, the op runs in `float16`. If any of the inputs is `float32`,\nautocast casts all inputs to `float32` and runs the op in `float32`.\n\n`addcdiv`, `addcmul`, `atan2`, `bilinear`, `cat`, `cross`, `dot`, `equal`,\n`index_put`, `stack`, `tensordot`\n\nSome ops not listed here (e.g., binary ops like `add`) natively promote inputs\nwithout autocasting\u2019s intervention. If inputs are a mixture of `float16` and\n`float32`, these ops run in `float32` and produce `float32` output, regardless\nof whether autocast is enabled.\n\nThe backward passes of `torch.nn.functional.binary_cross_entropy()` (and\n`torch.nn.BCELoss`, which wraps it) can produce gradients that aren\u2019t\nrepresentable in `float16`. In autocast-enabled regions, the forward input may\nbe `float16`, which means the backward gradient must be representable in\n`float16` (autocasting `float16` forward inputs to `float32` doesn\u2019t help,\nbecause that cast must be reversed in backward). Therefore,\n`binary_cross_entropy` and `BCELoss` raise an error in autocast-enabled\nregions.\n\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using\n`torch.nn.functional.binary_cross_entropy_with_logits()` or\n`torch.nn.BCEWithLogitsLoss`. `binary_cross_entropy_with_logits` and\n`BCEWithLogits` are safe to autocast.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "torch.cuda.amp", "text": "\nInstances of `autocast` serve as context managers or decorators that allow\nregions of your script to run in mixed precision.\n\nIn these regions, CUDA ops run in an op-specific dtype chosen by autocast to\nimprove performance while maintaining accuracy. See the Autocast Op Reference\nfor details.\n\nWhen entering an autocast-enabled region, Tensors may be any type. You should\nnot call `.half()` on your model(s) or inputs when using autocasting.\n\n`autocast` should wrap only the forward pass(es) of your network, including\nthe loss computation(s). Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward\nops.\n\nExample:\n\nSee the Automatic Mixed Precision examples for usage (along with gradient\nscaling) in more complex scenarios (e.g., gradient penalty, multiple\nmodels/losses, custom autograd functions).\n\n`autocast` can also be used as a decorator, e.g., on the `forward` method of\nyour model:\n\nFloating-point Tensors produced in an autocast-enabled region may be\n`float16`. After returning to an autocast-disabled region, using them with\nfloating-point Tensors of different dtypes may cause type mismatch errors. If\nso, cast the Tensor(s) produced in the autocast region back to `float32` (or\nother dtype if desired). If a Tensor from the autocast region is already\n`float32`, the cast is a no-op, and incurs no additional overhead. Example:\n\nType mismatch errors in an autocast-enabled region are a bug; if this is what\nyou observe, please file an issue.\n\n`autocast(enabled=False)` subregions can be nested in autocast-enabled\nregions. Locally disabling autocast can be useful, for example, if you want to\nforce a subregion to run in a particular `dtype`. Disabling autocast gives you\nexplicit control over the execution type. In the subregion, inputs from the\nsurrounding region should be cast to `dtype` before use:\n\nThe autocast state is thread-local. If you want it enabled in a new thread,\nthe context manager or decorator must be invoked in that thread. This affects\n`torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel` when\nused with more than one GPU per process (see Working with Multiple GPUs).\n\nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled\nin the region.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "torch.cuda.amp", "text": "\nHelper decorator for backward methods of custom autograd functions (subclasses\nof `torch.autograd.Function`). Ensures that `backward` executes with the same\nautocast state as `forward`. See the example page for more detail.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "torch.cuda.amp", "text": "\nHelper decorator for `forward` methods of custom autograd functions\n(subclasses of `torch.autograd.Function`). See the example page for more\ndetail.\n\ncast_inputs (`torch.dtype` or None, optional, default=None) \u2013 If not `None`,\nwhen `forward` runs in an autocast-enabled region, casts incoming floating-\npoint CUDA Tensors to the target dtype (non-floating-point Tensors are not\naffected), then executes `forward` with autocast disabled. If `None`,\n`forward`\u2019s internal ops execute with the current autocast state.\n\nNote\n\nIf the decorated `forward` is called outside an autocast-enabled region,\n`custom_fwd` is a no-op and `cast_inputs` has no effect.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale backoff factor.\n\nReturns a Python float containing the scale growth factor.\n\nReturns a Python int containing the growth interval.\n\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\nWarning\n\n`get_scale()` incurs a CPU-GPU sync.\n\nReturns a bool indicating whether this instance is enabled.\n\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to\n`state_dict()`.\n\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of `GradScaler` is not enabled,\noutputs are returned unmodified.\n\noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.\n\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\nReturns the state of the scaler as a `dict`. It contains five entries:\n\nIf this instance is not enabled, returns an empty dict.\n\nNote\n\nIf you wish to checkpoint the scaler\u2019s state after a particular iteration,\n`state_dict()` should be called after `update()`.\n\n`step()` carries out the following two operations:\n\n`*args` and `**kwargs` are forwarded to `optimizer.step()`.\n\nReturns the return value of `optimizer.step(*args, **kwargs)`.\n\nWarning\n\nClosure use is not currently supported.\n\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n`unscale_()` is optional, serving cases where you need to modify or inspect\ngradients between the backward pass(es) and `step()`. If `unscale_()` is not\ncalled explicitly, gradients will be unscaled automatically during `step()`.\n\nSimple example, using `unscale_()` to enable clipping of unscaled gradients:\n\noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be\nunscaled.\n\nNote\n\n`unscale_()` does not incur a CPU-GPU sync.\n\nWarning\n\n`unscale_()` should only be called once per optimizer per `step()` call, and\nonly after all gradients for that optimizer\u2019s assigned parameters have been\naccumulated. Calling `unscale_()` twice for a given optimizer between each\n`step()` triggers a RuntimeError.\n\nWarning\n\n`unscale_()` may unscale sparse gradients out of place, replacing the `.grad`\nattribute.\n\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by\n`backoff_factor` to reduce it. If `growth_interval` unskipped iterations\noccurred consecutively, the scale is multiplied by `growth_factor` to increase\nit.\n\nPassing `new_scale` sets the scale directly.\n\nnew_scale (float or `torch.cuda.FloatTensor`, optional, default=None) \u2013 New\nscale factor.\n\nWarning\n\n`update()` should only be called at the end of the iteration, after\n`scaler.step(optimizer)` has been invoked for all optimizers used this\niteration.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale backoff factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale growth factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "torch.cuda.amp", "text": "\nReturns a Python int containing the growth interval.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\nWarning\n\n`get_scale()` incurs a CPU-GPU sync.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "torch.cuda.amp", "text": "\nReturns a bool indicating whether this instance is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "torch.cuda.amp", "text": "\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to\n`state_dict()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "torch.cuda.amp", "text": "\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of `GradScaler` is not enabled,\noutputs are returned unmodified.\n\noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "torch.cuda.amp", "text": "\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "torch.cuda.amp", "text": "\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "torch.cuda.amp", "text": "\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "torch.cuda.amp", "text": "\nReturns the state of the scaler as a `dict`. It contains five entries:\n\nIf this instance is not enabled, returns an empty dict.\n\nNote\n\nIf you wish to checkpoint the scaler\u2019s state after a particular iteration,\n`state_dict()` should be called after `update()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "torch.cuda.amp", "text": "\n`step()` carries out the following two operations:\n\n`*args` and `**kwargs` are forwarded to `optimizer.step()`.\n\nReturns the return value of `optimizer.step(*args, **kwargs)`.\n\nWarning\n\nClosure use is not currently supported.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "torch.cuda.amp", "text": "\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n`unscale_()` is optional, serving cases where you need to modify or inspect\ngradients between the backward pass(es) and `step()`. If `unscale_()` is not\ncalled explicitly, gradients will be unscaled automatically during `step()`.\n\nSimple example, using `unscale_()` to enable clipping of unscaled gradients:\n\noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be\nunscaled.\n\nNote\n\n`unscale_()` does not incur a CPU-GPU sync.\n\nWarning\n\n`unscale_()` should only be called once per optimizer per `step()` call, and\nonly after all gradients for that optimizer\u2019s assigned parameters have been\naccumulated. Calling `unscale_()` twice for a given optimizer between each\n`step()` triggers a RuntimeError.\n\nWarning\n\n`unscale_()` may unscale sparse gradients out of place, replacing the `.grad`\nattribute.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "torch.cuda.amp", "text": "\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by\n`backoff_factor` to reduce it. If `growth_interval` unskipped iterations\noccurred consecutively, the scale is multiplied by `growth_factor` to increase\nit.\n\nPassing `new_scale` sets the scale directly.\n\nnew_scale (float or `torch.cuda.FloatTensor`, optional, default=None) \u2013 New\nscale factor.\n\nWarning\n\n`update()` should only be called at the end of the iteration, after\n`scaler.step(optimizer)` has been invoked for all optimizers used this\niteration.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.can_device_access_peer()", "path": "cuda#torch.cuda.can_device_access_peer", "type": "torch.cuda", "text": "\nChecks if peer access between two devices is possible.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.broadcast()", "path": "cuda#torch.cuda.comm.broadcast", "type": "torch.cuda", "text": "\nBroadcasts a tensor to specified GPU devices.\n\nNote\n\nExactly one of `devices` and `out` must be specified.\n\na tuple containing copies of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a copy of `tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "cuda#torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "text": "\nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first\ncoalesced into a buffer to reduce the number of synchronizations.\n\nA tuple containing copies of `tensor`, placed on `devices`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.gather()", "path": "cuda#torch.cuda.comm.gather", "type": "torch.cuda", "text": "\nGathers tensors from multiple GPU devices.\n\nNote\n\n`destination` must not be specified when `out` is specified.\n\na tensor located on `destination` device, that is a result of concatenating\n`tensors` along `dim`.\n\nthe `out` tensor, now containing results of concatenating `tensors` along\n`dim`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.reduce_add()", "path": "cuda#torch.cuda.comm.reduce_add", "type": "torch.cuda", "text": "\nSums tensors from multiple GPUs.\n\nAll inputs should have matching shapes, dtype, and layout. The output tensor\nwill be of the same shape, dtype, and layout.\n\nA tensor containing an elementwise sum of all inputs, placed on the\n`destination` device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.scatter()", "path": "cuda#torch.cuda.comm.scatter", "type": "torch.cuda", "text": "\nScatters tensor across multiple GPUs.\n\nNote\n\nExactly one of `devices` and `out` must be specified. When `out` is specified,\n`chunk_sizes` must not be specified and will be inferred from sizes of `out`.\n\na tuple containing chunks of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a chunk of `tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.current_blas_handle()", "path": "cuda#torch.cuda.current_blas_handle", "type": "torch.cuda", "text": "\nReturns cublasHandle_t pointer to current cuBLAS handle\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.current_device()", "path": "cuda#torch.cuda.current_device", "type": "torch.cuda", "text": "\nReturns the index of a currently selected device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.current_stream()", "path": "cuda#torch.cuda.current_stream", "type": "torch.cuda", "text": "\nReturns the currently selected `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the\ncurrently selected `Stream` for the current device, given by\n`current_device()`, if `device` is `None` (default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.default_stream()", "path": "cuda#torch.cuda.default_stream", "type": "torch.cuda", "text": "\nReturns the default `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the default\n`Stream` for the current device, given by `current_device()`, if `device` is\n`None` (default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.device", "path": "cuda#torch.cuda.device", "type": "torch.cuda", "text": "\nContext-manager that changes the selected device.\n\ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this\nargument is a negative integer or `None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.device_count()", "path": "cuda#torch.cuda.device_count", "type": "torch.cuda", "text": "\nReturns the number of GPUs available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.device_of", "path": "cuda#torch.cuda.device_of", "type": "torch.cuda", "text": "\nContext-manager that changes the current device to that of given object.\n\nYou can use both tensors and storages as arguments. If a given object is not\nallocated on a GPU, this is a no-op.\n\nobj (Tensor or Storage) \u2013 object allocated on the selected device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.empty_cache()", "path": "cuda#torch.cuda.empty_cache", "type": "torch.cuda", "text": "\nReleases all unoccupied cached memory currently held by the caching allocator\nso that those can be used in other GPU application and visible in `nvidia-\nsmi`.\n\nNote\n\n`empty_cache()` doesn\u2019t increase the amount of GPU memory available for\nPyTorch. However, it may help reduce fragmentation of GPU memory in certain\ncases. See Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event", "path": "cuda#torch.cuda.Event", "type": "torch.cuda", "text": "\nWrapper around a CUDA event.\n\nCUDA events are synchronization markers that can be used to monitor the\ndevice\u2019s progress, to accurately measure timing, and to synchronize CUDA\nstreams.\n\nThe underlying CUDA events are lazily initialized when the event is first\nrecorded or exported to another process. After creation, only streams on the\nsame device may record the event. However, streams on any device can wait on\nthe event.\n\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\nReconstruct an event from an IPC handle on the given device.\n\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\nChecks if all work currently captured by event has completed.\n\nA boolean indicating if all work currently captured by event has completed.\n\nRecords the event in a given stream.\n\nUses `torch.cuda.current_stream()` if no stream is specified. The stream\u2019s\ndevice must match the event\u2019s device.\n\nWaits for the event to complete.\n\nWaits until the completion of all work currently captured in this event. This\nprevents the CPU thread from proceeding until the event completes.\n\nNote\n\nThis is a wrapper around `cudaEventSynchronize()`: see CUDA Event\ndocumentation for more info.\n\nMakes all future work submitted to the given stream wait for this event.\n\nUse `torch.cuda.current_stream()` if no stream is specified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.elapsed_time()", "path": "cuda#torch.cuda.Event.elapsed_time", "type": "torch.cuda", "text": "\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "cuda#torch.cuda.Event.from_ipc_handle", "type": "torch.cuda", "text": "\nReconstruct an event from an IPC handle on the given device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.ipc_handle()", "path": "cuda#torch.cuda.Event.ipc_handle", "type": "torch.cuda", "text": "\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.query()", "path": "cuda#torch.cuda.Event.query", "type": "torch.cuda", "text": "\nChecks if all work currently captured by event has completed.\n\nA boolean indicating if all work currently captured by event has completed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.record()", "path": "cuda#torch.cuda.Event.record", "type": "torch.cuda", "text": "\nRecords the event in a given stream.\n\nUses `torch.cuda.current_stream()` if no stream is specified. The stream\u2019s\ndevice must match the event\u2019s device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.synchronize()", "path": "cuda#torch.cuda.Event.synchronize", "type": "torch.cuda", "text": "\nWaits for the event to complete.\n\nWaits until the completion of all work currently captured in this event. This\nprevents the CPU thread from proceeding until the event completes.\n\nNote\n\nThis is a wrapper around `cudaEventSynchronize()`: see CUDA Event\ndocumentation for more info.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.wait()", "path": "cuda#torch.cuda.Event.wait", "type": "torch.cuda", "text": "\nMakes all future work submitted to the given stream wait for this event.\n\nUse `torch.cuda.current_stream()` if no stream is specified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_arch_list()", "path": "cuda#torch.cuda.get_arch_list", "type": "torch.cuda", "text": "\nReturns list CUDA architectures this library was compiled for.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_device_capability()", "path": "cuda#torch.cuda.get_device_capability", "type": "torch.cuda", "text": "\nGets the cuda capability of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the device\ncapability. This function is a no-op if this argument is a negative integer.\nIt uses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nthe major and minor cuda capability of the device\n\ntuple(int, int)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_device_name()", "path": "cuda#torch.cuda.get_device_name", "type": "torch.cuda", "text": "\nGets the name of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the name.\nThis function is a no-op if this argument is a negative integer. It uses the\ncurrent device, given by `current_device()`, if `device` is `None` (default).\n\nthe name of the device\n\nstr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_device_properties()", "path": "cuda#torch.cuda.get_device_properties", "type": "torch.cuda", "text": "\nGets the properties of a device.\n\ndevice (torch.device or int or str) \u2013 device for which to return the\nproperties of the device.\n\nthe properties of the device\n\n_CudaDeviceProperties\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_gencode_flags()", "path": "cuda#torch.cuda.get_gencode_flags", "type": "torch.cuda", "text": "\nReturns NVCC gencode flags this library were compiled with.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_rng_state()", "path": "cuda#torch.cuda.get_rng_state", "type": "torch.cuda", "text": "\nReturns the random number generator state of the specified GPU as a\nByteTensor.\n\ndevice (torch.device or int, optional) \u2013 The device to return the RNG state\nof. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).\n\nWarning\n\nThis function eagerly initializes CUDA.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_rng_state_all()", "path": "cuda#torch.cuda.get_rng_state_all", "type": "torch.cuda", "text": "\nReturns a list of ByteTensor representing the random number states of all\ndevices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.init()", "path": "cuda#torch.cuda.init", "type": "torch.cuda", "text": "\nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you\nare interacting with PyTorch via its C API, as Python bindings for CUDA\nfunctionality will not be available until this initialization takes place.\nOrdinary users should not need this, as all of PyTorch\u2019s CUDA methods\nautomatically initialize CUDA state on-demand.\n\nDoes nothing if the CUDA state is already initialized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.initial_seed()", "path": "cuda#torch.cuda.initial_seed", "type": "torch.cuda", "text": "\nReturns the current random seed of the current GPU.\n\nWarning\n\nThis function eagerly initializes CUDA.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.ipc_collect()", "path": "cuda#torch.cuda.ipc_collect", "type": "torch.cuda", "text": "\nForce collects GPU memory after it has been released by CUDA IPC.\n\nNote\n\nChecks if any sent CUDA tensors could be cleaned from the memory. Force closes\nshared memory file used for reference counting if there is no active counters.\nUseful when the producer process stopped actively sending tensors and want to\nrelease unused memory.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.is_available()", "path": "cuda#torch.cuda.is_available", "type": "torch.cuda", "text": "\nReturns a bool indicating if CUDA is currently available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.is_initialized()", "path": "cuda#torch.cuda.is_initialized", "type": "torch.cuda", "text": "\nReturns whether PyTorch\u2019s CUDA state has been initialized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.list_gpu_processes()", "path": "cuda#torch.cuda.list_gpu_processes", "type": "torch.cuda", "text": "\nReturns a human-readable printout of the running processes and their GPU\nmemory use for a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for\nthe current device, given by `current_device()`, if `device` is `None`\n(default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.manual_seed()", "path": "cuda#torch.cuda.manual_seed", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers for the current GPU. It\u2019s safe to\ncall this function if CUDA is not available; in that case, it is silently\nignored.\n\nseed (int) \u2013 The desired seed.\n\nWarning\n\nIf you are working with a multi-GPU model, this function is insufficient to\nget determinism. To seed all GPUs, use `manual_seed_all()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.manual_seed_all()", "path": "cuda#torch.cuda.manual_seed_all", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call\nthis function if CUDA is not available; in that case, it is silently ignored.\n\nseed (int) \u2013 The desired seed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_allocated()", "path": "cuda#torch.cuda.max_memory_allocated", "type": "torch.cuda", "text": "\nReturns the maximum GPU memory occupied by tensors in bytes for a given\ndevice.\n\nBy default, this returns the peak allocated memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\nallocated memory usage of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_cached()", "path": "cuda#torch.cuda.max_memory_cached", "type": "torch.cuda", "text": "\nDeprecated; see `max_memory_reserved()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_reserved()", "path": "cuda#torch.cuda.max_memory_reserved", "type": "torch.cuda", "text": "\nReturns the maximum GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\nBy default, this returns the peak cached memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\ncached memory amount of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_allocated()", "path": "cuda#torch.cuda.memory_allocated", "type": "torch.cuda", "text": "\nReturns the current GPU memory occupied by tensors in bytes for a given\ndevice.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nThis is likely less than the amount shown in `nvidia-smi` since some unused\nmemory can be held by the caching allocator and some context needs to be\ncreated on GPU. See Memory management for more details about GPU memory\nmanagement.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_cached()", "path": "cuda#torch.cuda.memory_cached", "type": "torch.cuda", "text": "\nDeprecated; see `memory_reserved()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_reserved()", "path": "cuda#torch.cuda.memory_reserved", "type": "torch.cuda", "text": "\nReturns the current GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_snapshot()", "path": "cuda#torch.cuda.memory_snapshot", "type": "torch.cuda", "text": "\nReturns a snapshot of the CUDA memory allocator state across all devices.\n\nInterpreting the output of this function requires familiarity with the memory\nallocator internals.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_stats()", "path": "cuda#torch.cuda.memory_stats", "type": "torch.cuda", "text": "\nReturns a dictionary of CUDA memory allocator statistics for a given device.\n\nThe return value of this function is a dictionary of statistics, each of which\nis a non-negative integer.\n\nCore statistics:\n\nFor these core statistics, values are broken down as follows.\n\nPool type:\n\nMetric type:\n\nIn addition to the core statistics, we also provide some simple event\ncounters:\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_summary()", "path": "cuda#torch.cuda.memory_summary", "type": "torch.cuda", "text": "\nReturns a human-readable printout of the current memory allocator statistics\nfor a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.mark()", "path": "cuda#torch.cuda.nvtx.mark", "type": "torch.cuda", "text": "\nDescribe an instantaneous event that occurred at some point.\n\nmsg (string) \u2013 ASCII message to associate with the event.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.range_pop()", "path": "cuda#torch.cuda.nvtx.range_pop", "type": "torch.cuda", "text": "\nPops a range off of a stack of nested range spans. Returns the zero-based\ndepth of the range that is ended.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.range_push()", "path": "cuda#torch.cuda.nvtx.range_push", "type": "torch.cuda", "text": "\nPushes a range onto a stack of nested range span. Returns zero-based depth of\nthe range that is started.\n\nmsg (string) \u2013 ASCII message to associate with range\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "cuda#torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "text": "\nResets the starting point in tracking maximum GPU memory occupied by tensors\nfor a given device.\n\nSee `max_memory_allocated()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "cuda#torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "text": "\nResets the starting point in tracking maximum GPU memory managed by the\ncaching allocator for a given device.\n\nSee `max_memory_cached()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.seed()", "path": "cuda#torch.cuda.seed", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers to a random number for the current\nGPU. It\u2019s safe to call this function if CUDA is not available; in that case,\nit is silently ignored.\n\nWarning\n\nIf you are working with a multi-GPU model, this function will only initialize\nthe seed on one GPU. To initialize all GPUs, use `seed_all()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.seed_all()", "path": "cuda#torch.cuda.seed_all", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers to a random number on all GPUs.\nIt\u2019s safe to call this function if CUDA is not available; in that case, it is\nsilently ignored.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_device()", "path": "cuda#torch.cuda.set_device", "type": "torch.cuda", "text": "\nSets the current device.\n\nUsage of this function is discouraged in favor of `device`. In most cases it\u2019s\nbetter to use `CUDA_VISIBLE_DEVICES` environmental variable.\n\ndevice (torch.device or int) \u2013 selected device. This function is a no-op if\nthis argument is negative.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "cuda#torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "text": "\nSet memory fraction for a process. The fraction is used to limit an caching\nallocator to allocated memory on a CUDA device. The allowed value equals the\ntotal visible memory multiplied fraction. If trying to allocate more than the\nallowed value in a process, will raise an out of memory error in allocator.\n\nNote\n\nIn general, the total available free memory is less than the total capacity.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_rng_state()", "path": "cuda#torch.cuda.set_rng_state", "type": "torch.cuda", "text": "\nSets the random number generator state of the specified GPU.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_rng_state_all()", "path": "cuda#torch.cuda.set_rng_state_all", "type": "torch.cuda", "text": "\nSets the random number generator state of all devices.\n\nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream", "path": "cuda#torch.cuda.Stream", "type": "torch.cuda", "text": "\nWrapper around a CUDA stream.\n\nA CUDA stream is a linear sequence of execution that belongs to a specific\ndevice, independent from other streams. See CUDA semantics for details.\n\nNote\n\nAlthough CUDA versions >= 11 support more than two levels of priorities, in\nPyTorch, we only support two levels of priorities.\n\nChecks if all the work submitted has been completed.\n\nA boolean indicating if all kernels in this stream are completed.\n\nRecords an event.\n\nevent (Event, optional) \u2013 event to record. If not given, a new one will be\nallocated.\n\nRecorded event.\n\nWait for all the kernels in this stream to complete.\n\nNote\n\nThis is a wrapper around `cudaStreamSynchronize()`: see CUDA Stream\ndocumentation for more info.\n\nMakes all future work submitted to the stream wait for an event.\n\nevent (Event) \u2013 an event to wait for.\n\nNote\n\nThis is a wrapper around `cudaStreamWaitEvent()`: see CUDA Stream\ndocumentation for more info.\n\nThis function returns without waiting for `event`: only future operations are\naffected.\n\nSynchronizes with another stream.\n\nAll future work submitted to this stream will wait until all kernels submitted\nto a given stream at the time of call complete.\n\nstream (Stream) \u2013 a stream to synchronize.\n\nNote\n\nThis function returns without waiting for currently enqueued kernels in\n`stream`: only future operations are affected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.stream()", "path": "cuda#torch.cuda.stream", "type": "torch.cuda", "text": "\nContext-manager that selects a given stream.\n\nAll CUDA kernels queued within its context will be enqueued on a selected\nstream.\n\nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s `None`.\n\nNote\n\nStreams are per-device. If the selected stream is not on the current device,\nthis function will also change the current device to match the stream.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.query()", "path": "cuda#torch.cuda.Stream.query", "type": "torch.cuda", "text": "\nChecks if all the work submitted has been completed.\n\nA boolean indicating if all kernels in this stream are completed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.record_event()", "path": "cuda#torch.cuda.Stream.record_event", "type": "torch.cuda", "text": "\nRecords an event.\n\nevent (Event, optional) \u2013 event to record. If not given, a new one will be\nallocated.\n\nRecorded event.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.synchronize()", "path": "cuda#torch.cuda.Stream.synchronize", "type": "torch.cuda", "text": "\nWait for all the kernels in this stream to complete.\n\nNote\n\nThis is a wrapper around `cudaStreamSynchronize()`: see CUDA Stream\ndocumentation for more info.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.wait_event()", "path": "cuda#torch.cuda.Stream.wait_event", "type": "torch.cuda", "text": "\nMakes all future work submitted to the stream wait for an event.\n\nevent (Event) \u2013 an event to wait for.\n\nNote\n\nThis is a wrapper around `cudaStreamWaitEvent()`: see CUDA Stream\ndocumentation for more info.\n\nThis function returns without waiting for `event`: only future operations are\naffected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.wait_stream()", "path": "cuda#torch.cuda.Stream.wait_stream", "type": "torch.cuda", "text": "\nSynchronizes with another stream.\n\nAll future work submitted to this stream will wait until all kernels submitted\nto a given stream at the time of call complete.\n\nstream (Stream) \u2013 a stream to synchronize.\n\nNote\n\nThis function returns without waiting for currently enqueued kernels in\n`stream`: only future operations are affected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.synchronize()", "path": "cuda#torch.cuda.synchronize", "type": "torch.cuda", "text": "\nWaits for all kernels in all streams on a CUDA device to complete.\n\ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It\nuses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cummax()", "path": "generated/torch.cummax#torch.cummax", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nmaximum of elements of `input` in the dimension `dim`. And `indices` is the\nindex location of each maximum value found in the dimension `dim`.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (values,\nindices)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cummin()", "path": "generated/torch.cummin#torch.cummin", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nminimum of elements of `input` in the dimension `dim`. And `indices` is the\nindex location of each maximum value found in the dimension `dim`.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (values,\nindices)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cumprod()", "path": "generated/torch.cumprod#torch.cumprod", "type": "torch", "text": "\nReturns the cumulative product of elements of `input` in the dimension `dim`.\n\nFor example, if `input` is a vector of size N, the result will also be a\nvector of size N, with elements.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cumsum()", "path": "generated/torch.cumsum#torch.cumsum", "type": "torch", "text": "\nReturns the cumulative sum of elements of `input` in the dimension `dim`.\n\nFor example, if `input` is a vector of size N, the result will also be a\nvector of size N, with elements.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.deg2rad()", "path": "generated/torch.deg2rad#torch.deg2rad", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` converted from\nangles in degrees to radians.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dequantize()", "path": "generated/torch.dequantize#torch.dequantize", "type": "torch", "text": "\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\ntensor (Tensor) \u2013 A quantized Tensor\n\nGiven a list of quantized Tensors, dequantize them and return a list of fp32\nTensors\n\ntensors (sequence of Tensors) \u2013 A list of quantized Tensors\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.det()", "path": "generated/torch.det#torch.det", "type": "torch", "text": "\nCalculates determinant of a square matrix or batches of square matrices.\n\nNote\n\n`torch.det()` is deprecated. Please use `torch.linalg.det()` instead.\n\nNote\n\nBackward through detdet internally uses SVD results when `input` is not\ninvertible. In this case, double backward through detdet will be unstable when\n`input` doesn\u2019t have distinct singular values. See torch.svd~torch.svd for\ndetails.\n\ninput (Tensor) \u2013 the input tensor of size `(*, n, n)` where `*` is zero or\nmore batch dimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diag()", "path": "generated/torch.diag#torch.diag", "type": "torch", "text": "\nThe argument `diagonal` controls which diagonal to consider:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nSee also\n\n`torch.diagonal()` always returns the diagonal of its input.\n\n`torch.diagflat()` always constructs a tensor with diagonal elements specified\nby the input.\n\nExamples:\n\nGet the square matrix where the input vector is the diagonal:\n\nGet the k-th diagonal of a given matrix:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diagflat()", "path": "generated/torch.diagflat#torch.diagflat", "type": "torch", "text": "\nThe argument `offset` controls which diagonal to consider:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diagonal()", "path": "generated/torch.diagonal#torch.diagonal", "type": "torch", "text": "\nReturns a partial view of `input` with the its diagonal elements with respect\nto `dim1` and `dim2` appended as a dimension at the end of the shape.\n\nThe argument `offset` controls which diagonal to consider:\n\nApplying `torch.diag_embed()` to the output of this function with the same\narguments yields a diagonal matrix with the diagonal entries of the input.\nHowever, `torch.diag_embed()` has different default dimensions, so those need\nto be explicitly specified.\n\nNote\n\nTo take a batch diagonal, pass in dim1=-2, dim2=-1.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diag_embed()", "path": "generated/torch.diag_embed#torch.diag_embed", "type": "torch", "text": "\nCreates a tensor whose diagonals of certain 2D planes (specified by `dim1` and\n`dim2`) are filled by `input`. To facilitate creating batched diagonal\nmatrices, the 2D planes formed by the last two dimensions of the returned\ntensor are chosen by default.\n\nThe argument `offset` controls which diagonal to consider:\n\nThe size of the new matrix will be calculated to make the specified diagonal\nof the size of the last input dimension. Note that for `offset` other than 00\n, the order of `dim1` and `dim2` matters. Exchanging them is equivalent to\nchanging the sign of `offset`.\n\nApplying `torch.diagonal()` to the output of this function with the same\narguments yields a matrix identical to input. However, `torch.diagonal()` has\ndifferent default dimensions, so those need to be explicitly specified.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diff()", "path": "generated/torch.diff#torch.diff", "type": "torch", "text": "\nComputes the n-th forward difference along the given dimension.\n\nThe first-order differences are given by `out[i] = input[i + 1] - input[i]`.\nHigher-order differences are calculated by using `torch.diff()` recursively.\n\nNote\n\nOnly `n = 1` is currently supported\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.digamma()", "path": "generated/torch.digamma#torch.digamma", "type": "torch", "text": "\nComputes the logarithmic derivative of the gamma function on `input`.\n\ninput (Tensor) \u2013 the tensor to compute the digamma function on\n\nout (Tensor, optional) \u2013 the output tensor.\n\nNote\n\nThis function is similar to SciPy\u2019s `scipy.special.digamma`.\n\nNote\n\nFrom PyTorch 1.8 onwards, the digamma function returns `-Inf` for `0`.\nPreviously it returned `NaN` for `0`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dist()", "path": "generated/torch.dist#torch.dist", "type": "torch", "text": "\nReturns the p-norm of (`input` \\- `other`)\n\nThe shapes of `input` and `other` must be broadcastable.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed", "path": "distributed", "type": "torch.distributed", "text": "\nNote\n\nPlease refer to PyTorch Distributed Overview for a brief introduction to all\nfeatures related to distributed training.\n\n`torch.distributed` supports three built-in backends, each with different\ncapabilities. The table below shows which functions are available for use with\nCPU / CUDA tensors. MPI supports CUDA only if the implementation used to build\nPyTorch supports it.\n\nBackend\n\n`gloo`\n\n`mpi`\n\n`nccl`\n\nDevice\n\nCPU\n\nGPU\n\nCPU\n\nGPU\n\nCPU\n\nGPU\n\nsend\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nrecv\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nbroadcast\n\n\u2713\n\n\u2713\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nall_reduce\n\n\u2713\n\n\u2713\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nreduce\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nall_gather\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\ngather\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nscatter\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nreduce_scatter\n\n\u2718\n\n\u2718\n\n\u2718\n\n\u2718\n\n\u2718\n\n\u2713\n\nall_to_all\n\n\u2718\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nbarrier\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nPyTorch distributed package supports Linux (stable), MacOS (stable), and\nWindows (prototype). By default for Linux, the Gloo and NCCL backends are\nbuilt and included in PyTorch distributed (NCCL only when building with CUDA).\nMPI is an optional backend that can only be included if you build PyTorch from\nsource. (e.g.building PyTorch on a host that has MPI installed.)\n\nNote\n\nAs of PyTorch v1.8, Windows supports all collective communications backend but\nNCCL, If the `init_method` argument of `init_process_group()` points to a file\nit must adhere to the following schema:\n\nSame as on Linux platform, you can enable TcpStore by setting environment\nvariables, MASTER_ADDR and MASTER_PORT.\n\nIn the past, we were often asked: \u201cwhich backend should I use?\u201d.\n\nRule of thumb\n\nGPU hosts with InfiniBand interconnect\n\nGPU hosts with Ethernet interconnect\n\nCPU hosts with InfiniBand interconnect\n\nCPU hosts with Ethernet interconnect\n\nBy default, both the NCCL and Gloo backends will try to find the right network\ninterface to use. If the automatically detected interface is not correct, you\ncan override it using the following environment variables (applicable to the\nrespective backend):\n\nIf you\u2019re using the Gloo backend, you can specify multiple interfaces by\nseparating them by a comma, like this: `export\nGLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3`. The backend will dispatch operations\nin a round-robin fashion across these interfaces. It is imperative that all\nprocesses specify the same number of interfaces in this variable.\n\nNCCL has also provided a number of environment variables for fine-tuning\npurposes.\n\nCommonly used ones include the following for debugging purposes:\n\nFor the full list of NCCL environment variables, please refer to NVIDIA NCCL\u2019s\nofficial documentation\n\nThe `torch.distributed` package provides PyTorch support and communication\nprimitives for multiprocess parallelism across several computation nodes\nrunning on one or more machines. The class\n`torch.nn.parallel.DistributedDataParallel()` builds on this functionality to\nprovide synchronous distributed training as a wrapper around any PyTorch\nmodel. This differs from the kinds of parallelism provided by Multiprocessing\npackage - torch.multiprocessing and `torch.nn.DataParallel()` in that it\nsupports multiple network-connected machines and in that the user must\nexplicitly launch a separate copy of the main training script for each\nprocess.\n\nIn the single-machine synchronous case, `torch.distributed` or the\n`torch.nn.parallel.DistributedDataParallel()` wrapper may still have\nadvantages over other approaches to data-parallelism, including\n`torch.nn.DataParallel()`:\n\nThe package needs to be initialized using the\n`torch.distributed.init_process_group()` function before calling any other\nmethods. This blocks until all processes have joined.\n\nReturns `True` if the distributed package is available. Otherwise,\n`torch.distributed` does not expose any other APIs. Currently,\n`torch.distributed` is available on Linux, MacOS and Windows. Set\n`USE_DISTRIBUTED=1` to enable it when building PyTorch from source. Currently,\nthe default value is `USE_DISTRIBUTED=1` for Linux and Windows,\n`USE_DISTRIBUTED=0` for MacOS.\n\nInitializes the default distributed process group, and this will also\ninitialize the distributed package.\n\nIf neither is specified, `init_method` is assumed to be \u201cenv://\u201d.\n\nTo enable `backend == Backend.MPI`, PyTorch needs to be built from source on a\nsystem that supports MPI.\n\nAn enum-like class of available backends: GLOO, NCCL, MPI, and other\nregistered backends.\n\nThe values of this class are lowercase strings, e.g., `\"gloo\"`. They can be\naccessed as attributes, e.g., `Backend.NCCL`.\n\nThis class can be directly called to parse the string, e.g.,\n`Backend(backend_str)` will check if `backend_str` is valid, and return the\nparsed lowercase string if so. It also accepts uppercase strings, e.g.,\n`Backend(\"GLOO\")` returns `\"gloo\"`.\n\nNote\n\nThe entry `Backend.UNDEFINED` is present but only used as initial value of\nsome fields. Users should neither use it directly nor assume its existence.\n\nReturns the backend of the given process group.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is\nthe general main process group. If another specific group is specified, the\ncalling process must be part of `group`.\n\nThe backend of the given process group as a lower case string.\n\nReturns the rank of current process group\n\nRank is a unique identifier assigned to each process within a distributed\nprocess group. They are always consecutive integers ranging from 0 to\n`world_size`.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe rank of the process group -1, if not part of the group\n\nReturns the number of processes in the current process group\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe world size of the process group -1, if not part of the group\n\nChecking if the default process group has been initialized\n\nChecks if the MPI backend is available.\n\nChecks if the NCCL backend is available.\n\nCurrently three initialization methods are supported:\n\nThere are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desired `world_size`. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified\nranks.\n\nNote that multicast address is not supported anymore in the latest distributed\npackage. `group_name` is deprecated as well.\n\nAnother initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desired `world_size`. The\nURL should start with `file://` and contain a path to a non-existent file (in\nan existing directory) on a shared file system. File-system initialization\nwill automatically create that file if it doesn\u2019t exist, but will not delete\nthe file. Therefore, it is your responsibility to make sure that the file is\ncleaned up before the next `init_process_group()` call on the same file\npath/name.\n\nNote that automatic rank assignment is not supported anymore in the latest\ndistributed package and `group_name` is deprecated as well.\n\nWarning\n\nThis method assumes that the file system supports locking using `fcntl` \\-\nmost local systems and NFS support it.\n\nWarning\n\nThis method will always create the file and try its best to clean up and\nremove the file at the end of the program. In other words, each initialization\nwith the file init method will need a brand new empty file in order for the\ninitialization to succeed. If the same file used by the previous\ninitialization (which happens not to get cleaned up) is used again, this is\nunexpected behavior and can often cause deadlocks and failures. Therefore,\neven though this method will try its best to clean up the file, if the auto-\ndelete happens to be unsuccessful, it is your responsibility to ensure that\nthe file is removed at the end of the training to prevent the same file to be\nreused again during the next time. This is especially important if you plan to\ncall `init_process_group()` multiple times on the same file name. In other\nwords, if the file is not removed/cleaned up and you call\n`init_process_group()` again on that file, failures are expected. The rule of\nthumb here is that, make sure that the file is non-existent or empty every\ntime `init_process_group()` is called.\n\nThis method will read the configuration from environment variables, allowing\none to fully customize how the information is obtained. The variables to be\nset are:\n\nThe machine with rank 0 will be used to set up all connections.\n\nThis is the default method, meaning that `init_method` does not have to be\nspecified (or can be `env://`).\n\nThe distributed package comes with a distributed key-value store, which can be\nused to share information between processes in the group as well as to\ninitialize the distributed pacakge in `torch.distributed.init_process_group()`\n(by explicitly creating the store as an alternative to specifying\n`init_method`.) There are 3 choices for Key-Value Stores: `TCPStore`,\n`FileStore`, and `HashStore`.\n\nBase class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (`TCPStore`, `FileStore`, and `HashStore`).\n\nA TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as `set()` to insert a key-value pair, `get()` to\nretrieve a key-value pair, etc.\n\nA thread-safe store implementation based on an underlying hashmap. This store\ncan be used within the same process (for example, by other threads), but\ncannot be used across processes.\n\nA store implementation that uses a file to store the underlying key-value\npairs.\n\nA wrapper around any of the 3 key-value stores (`TCPStore`, `FileStore`, and\n`HashStore`) that adds a prefix to each key inserted to the store.\n\nInserts the key-value pair into the store based on the supplied `key` and\n`value`. If `key` already exists in the store, it will overwrite the old value\nwith the new supplied `value`.\n\nRetrieves the value associated with the given `key` in the store. If `key` is\nnot present in the store, the function will wait for `timeout`, which is\ndefined when initializing the store, before throwing an exception.\n\nkey (str) \u2013 The function will return the value associated with this key.\n\nValue associated with `key` if `key` is in the store.\n\nThe first call to add for a given `key` creates a counter associated with\n`key` in the store, initialized to `amount`. Subsequent calls to add with the\nsame `key` increment the counter by the specified `amount`. Calling `add()`\nwith a key that has already been set in the store by `set()` will result in an\nexception.\n\nOverloaded function.\n\nWaits for each key in `keys` to be added to the store. If not all keys are set\nbefore the `timeout` (set during store initialization), then `wait` will throw\nan exception.\n\nkeys (list) \u2013 List of keys on which to wait until they are set in the store.\n\nWaits for each key in `keys` to be added to the store, and throws an exception\nif the keys have not been set by the supplied `timeout`.\n\nReturns the number of keys set in the store. Note that this number will\ntypically be one greater than the number of keys added by `set()` and `add()`\nsince one key is used to coordinate all the workers using the store.\n\nWarning\n\nWhen used with the `TCPStore`, `num_keys` returns the number of keys written\nto the underlying file. If the store is destructed and another store is\ncreated with the same file, the original keys will be retained.\n\nThe number of keys present in the store.\n\nDeletes the key-value pair associated with `key` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\nWarning\n\nThe `delete_key` API is only supported by the `TCPStore` and `HashStore`.\nUsing this API with the `FileStore` will result in an exception.\n\nkey (str) \u2013 The key to be deleted from the store\n\n`True` if `key` was deleted, otherwise `False`.\n\nSets the store\u2019s default timeout. This timeout is used during initialization\nand in `wait()` and `get()`.\n\ntimeout (timedelta) \u2013 timeout to be set in the store.\n\nBy default collectives operate on the default group (also called the world)\nand require all processes to enter the distributed function call. However,\nsome workloads can benefit from more fine-grained communication. This is where\ndistributed groups come into play. `new_group()` function can be used to\ncreate new groups, with arbitrary subsets of all processes. It returns an\nopaque group handle that can be given as a `group` argument to all collectives\n(collectives are distributed functions to exchange information in certain\nwell-known programming patterns).\n\nCreates a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even if\nthey are not going to be members of the group. Additionally, groups should be\ncreated in the same order in all processes.\n\nWarning\n\nUsing multiple process groups with the `NCCL` backend concurrently is not safe\nand the user should perform explicit synchronization in their application to\nensure only one process group is used at a time. This means collectives from\none process group should have completed execution on the device (not just\nenqueued since CUDA execution is async) before collectives from another\nprocess group are enqueued. See Using multiple NCCL communicators concurrently\nfor more details.\n\nA handle of distributed group that can be given to collective calls.\n\nSends a tensor synchronously.\n\nReceives a tensor synchronously.\n\nSender rank -1, if not part of the group\n\n`isend()` and `irecv()` return distributed request objects when used. In\ngeneral, the type of this object is unspecified as they should never be\ncreated manually, but they are guaranteed to support two methods:\n\nSends a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\nReceives a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\nEvery collective operation function supports the following two kinds of\noperations, depending on the setting of the `async_op` flag passed into the\ncollective:\n\nSynchronous operation \\- the default mode, when `async_op` is set to `False`.\nWhen the function returns, it is guaranteed that the collective operation is\nperformed. In the case of CUDA operations, it is not guaranteed that the CUDA\noperation is completed, since CUDA operations are asynchronous. For CPU\ncollectives, any further function calls utilizing the output of the collective\ncall will behave as expected. For CUDA collectives, function calls utilizing\nthe output on the same CUDA stream will behave as expected. Users must take\ncare of synchronization under the scenario of running under different streams.\nFor details on CUDA semantics such as stream synchronization, see CUDA\nSemantics. See the below script to see examples of differences in these\nsemantics for CPU and CUDA operations.\n\nAsynchronous operation \\- when `async_op` is set to True. The collective\noperation function returns a distributed request object. In general, you don\u2019t\nneed to create it manually and it is guaranteed to support two methods:\n\nExample\n\nThe following code can serve as a reference regarding semantics for CUDA\noperations when using distributed collectives. It shows the explicit need to\nsynchronize when using collective outputs on different CUDA streams:\n\nBroadcasts the tensor to the whole group.\n\n`tensor` must have the same number of elements in all processes participating\nin the collective.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nBroadcasts picklable objects in `object_list` to the whole group. Similar to\n`broadcast()`, but Python objects can be passed in. Note that all objects in\n`object_list` must be picklable in order to be broadcasted.\n\n`None`. If rank is part of the group, `object_list` will contain the\nbroadcasted objects from `src` rank.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`broadcast_object_list()` uses `pickle` module implicitly, which is known to\nbe insecure. It is possible to construct malicious pickle data which will\nexecute arbitrary code during unpickling. Only call this function with data\nyou trust.\n\nReduces the tensor data across all machines in such a way that all get the\nfinal result.\n\nAfter the call `tensor` is going to be bitwise identical in all processes.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduces the tensor data across all machines.\n\nOnly the process with rank `dst` is going to receive the final result.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nGathers tensors from the whole group in a list.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nGathers picklable objects from the whole group into a list. Similar to\n`all_gather()`, but Python objects can be passed in. Note that the object must\nbe picklable in order to be gathered.\n\nNone. If the calling rank is part of this group, the output of the collective\nwill be populated into the input `object_list`. If the calling rank is not\npart of the group, the passed in `object_list` will be unmodified.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nWarning\n\n`all_gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\nGathers a list of tensors in a single process.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nGathers picklable objects from the whole group in a single process. Similar to\n`gather()`, but Python objects can be passed in. Note that the object must be\npicklable in order to be gathered.\n\nNone. On the `dst` rank, `object_gather_list` will contain the output of the\ncollective.\n\nNote\n\nNote that this API differs slightly from the gather collective since it does\nnot provide an async_op handle and thus will be a blocking call.\n\nNote\n\nNote that this API is not supported when using the NCCL backend.\n\nWarning\n\n`gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\nScatters a list of tensors to all processes in a group.\n\nEach process will receive exactly one tensor and store its data in the\n`tensor` argument.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nScatters picklable objects in `scatter_object_input_list` to the whole group.\nSimilar to `scatter()`, but Python objects can be passed in. On each rank, the\nscattered object will be stored as the first element of\n`scatter_object_output_list`. Note that all objects in\n`scatter_object_input_list` must be picklable in order to be scattered.\n\n`None`. If rank is part of the group, `scatter_object_output_list` will have\nits first element set to the scattered object for this rank.\n\nNote\n\nNote that this API differs slightly from the scatter collective since it does\nnot provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`scatter_object_list()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\nReduces, then scatters a list of tensors to all processes in a group.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nEach process scatters list of input tensors to all processes in a group and\nreturn gathered list of tensors in output list.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nWarning\n\n`all_to_all` is experimental and subject to change.\n\nSynchronizes all processes.\n\nThis collective blocks processes until the whole group enters this function,\nif async_op is False, or if async work handle is called on wait().\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nAn enum-like class for available reduction operations: `SUM`, `PRODUCT`,\n`MIN`, `MAX`, `BAND`, `BOR`, and `BXOR`.\n\nNote that `BAND`, `BOR`, and `BXOR` reductions are not available when using\nthe `NCCL` backend.\n\nAdditionally, `MAX`, `MIN` and `PRODUCT` are not supported for complex\ntensors.\n\nThe values of this class can be accessed as attributes, e.g., `ReduceOp.SUM`.\nThey are used in specifying strategies for reduction collectives, e.g.,\n`reduce()`, `all_reduce_multigpu()`, etc.\n\nMembers:\n\nSUM\n\nPRODUCT\n\nMIN\n\nMAX\n\nBAND\n\nBOR\n\nBXOR\n\nDeprecated enum-like class for reduction operations: `SUM`, `PRODUCT`, `MIN`,\nand `MAX`.\n\n`ReduceOp` is recommended to use instead.\n\nIf you want to use collective communication functions supporting autograd you\ncan find an implementation of those in the `torch.distributed.nn.*` module.\n\nFunctions here are synchronous and will be inserted in the autograd graph, so\nyou need to ensure that all the processes that participated in the collective\noperation will do the backward pass for the backward communication to\neffectively happen and don\u2019t cause a deadlock.\n\nPlease notice that currently the only backend where all the functions are\nguaranteed to work is `gloo`. .. autofunction:: torch.distributed.nn.broadcast\n.. autofunction:: torch.distributed.nn.gather .. autofunction::\ntorch.distributed.nn.scatter .. autofunction:: torch.distributed.nn.reduce ..\nautofunction:: torch.distributed.nn.all_gather .. autofunction::\ntorch.distributed.nn.all_to_all .. autofunction::\ntorch.distributed.nn.all_reduce\n\nIf you have more than one GPU on each node, when using the NCCL and Gloo\nbackend, `broadcast_multigpu()` `all_reduce_multigpu()` `reduce_multigpu()`\n`all_gather_multigpu()` and `reduce_scatter_multigpu()` support distributed\ncollective operations among multiple GPUs within each node. These functions\ncan potentially improve the overall distributed training performance and be\neasily used by passing a list of tensors. Each Tensor in the passed tensor\nlist needs to be on a separate GPU device of the host where the function is\ncalled. Note that the length of the tensor list needs to be identical among\nall the distributed processes. Also note that currently the multi-GPU\ncollective functions are only supported by the NCCL backend.\n\nFor example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:\n\nCode running on Node 0\n\nCode running on Node 1\n\nAfter the call, all 16 tensors on the two nodes will have the all-reduced\nvalue of 16\n\nBroadcasts the tensor to the whole group with multiple GPU tensors per node.\n\n`tensor` must have the same number of elements in all the GPUs from all\nprocesses participating in the collective. each tensor in the list must be on\na different GPU\n\nOnly nccl and gloo backend are currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduces the tensor data across all machines in such a way that all get the\nfinal result. This function reduces a number of tensors on every node, while\neach tensor resides on different GPUs. Therefore, the input tensor in the\ntensor list needs to be GPU tensors. Also, each tensor in the tensor list\nneeds to reside on a different GPU.\n\nAfter the call, all `tensor` in `tensor_list` is going to be bitwise identical\nin all processes.\n\nComplex tensors are supported.\n\nOnly nccl and gloo backend is currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduces the tensor data on multiple GPUs across all machines. Each tensor in\n`tensor_list` should reside on a separate GPU\n\nOnly the GPU of `tensor_list[dst_tensor]` on the process with rank `dst` is\ngoing to receive the final result.\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nAsync work handle, if async_op is set to True. None, otherwise\n\nGathers tensors from the whole group in a list. Each tensor in `tensor_list`\nshould reside on a separate GPU\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nComplex tensors are supported.\n\noutput_tensor_lists (List[List[Tensor]]) \u2013\n\nOutput lists. It should contain correctly-sized tensors on each GPU to be used\nfor output of the collective, e.g. `output_tensor_lists[i]` contains the\nall_gather result that resides on the GPU of `input_tensor_list[i]`.\n\nNote that each element of `output_tensor_lists` has the size of `world_size *\nlen(input_tensor_list)`, since the function all gathers the result from every\nsingle GPU in the group. To interpret each element of\n`output_tensor_lists[i]`, note that `input_tensor_list[j]` of rank k will be\nappear in `output_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(output_tensor_lists)`, and the size of each element in\n`output_tensor_lists` (each element is a list, therefore\n`len(output_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduce and scatter a list of tensors to the whole group. Only nccl backend is\ncurrently supported.\n\nEach tensor in `output_tensor_list` should reside on a separate GPU, as should\neach list of tensors in `input_tensor_lists`.\n\noutput_tensor_list (List[Tensor]) \u2013\n\nOutput tensors (on different GPUs) to receive the result of the operation.\n\nNote that `len(output_tensor_list)` needs to be the same for all the\ndistributed processes calling this function.\n\ninput_tensor_lists (List[List[Tensor]]) \u2013\n\nInput lists. It should contain correctly-sized tensors on each GPU to be used\nfor input of the collective, e.g. `input_tensor_lists[i]` contains the\nreduce_scatter input that resides on the GPU of `output_tensor_list[i]`.\n\nNote that each element of `input_tensor_lists` has the size of `world_size *\nlen(output_tensor_list)`, since the function scatters the result from every\nsingle GPU in the group. To interpret each element of `input_tensor_lists[i]`,\nnote that `output_tensor_list[j]` of rank k receives the reduce-scattered\nresult from `input_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(input_tensor_lists)`, and the size of each element in\n`input_tensor_lists` (each element is a list, therefore\n`len(input_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nBesides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party\nbackends through a run-time register mechanism. For references on how to\ndevelop a third-party backend through C++ Extension, please refer to Tutorials\n- Custom C++ and CUDA Extensions and\n`test/cpp_extensions/cpp_c10d_extension.cpp`. The capability of third-party\nbackends are decided by their own implementations.\n\nThe new backend derives from `c10d.ProcessGroup` and registers the backend\nname and the instantiating interface through\n`torch.distributed.Backend.register_backend()` when imported.\n\nWhen manually importing this backend and invoking\n`torch.distributed.init_process_group()` with the corresponding backend name,\nthe `torch.distributed` package runs on the new backend.\n\nWarning\n\nThe support of third-party backend is experimental and subject to change.\n\nThe `torch.distributed` package also provides a launch utility in\n`torch.distributed.launch`. This helper utility can be used to launch multiple\nprocesses per node for distributed training.\n\n`torch.distributed.launch` is a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\n\nThe utility can be used for single-node distributed training, in which one or\nmore processes per node will be spawned. The utility can be used for either\nCPU training or GPU training. If the utility is used for GPU training, each\ndistributed process will be operating on a single GPU. This can achieve well-\nimproved single-node training performance. It can also be used in multi-node\ndistributed training, by spawning up multiple processes on each node for well-\nimproved multi-node distributed training performance as well. This will\nespecially be benefitial for systems with multiple Infiniband interfaces that\nhave direct-GPU support, since all of them can be utilized for aggregated\ncommunication bandwidth.\n\nIn both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(`--nproc_per_node`). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (`nproc_per_node`), and\neach process will be operating on a single GPU from GPU 0 to GPU\n(nproc_per_node - 1).\n\nHow to use this module:\n\nNode 1: (IP: 192.168.1.1, and has a free port: 1234)\n\nNode 2:\n\nImportant Notices:\n\n1\\. This utility and multi-process distributed (single-node or multi-node) GPU\ntraining currently only achieves the best performance using the NCCL\ndistributed backend. Thus NCCL backend is the recommended backend to use for\nGPU training.\n\n2\\. In your training program, you must parse the command-line argument:\n`--local_rank=LOCAL_PROCESS_RANK`, which will be provided by this module. If\nyour training program uses GPUs, you should ensure that your code only runs on\nthe GPU device of LOCAL_PROCESS_RANK. This can be done by:\n\nParsing the local_rank argument\n\nSet your device to local rank using either\n\nor\n\n3\\. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses `env://`, which is the only supported `init_method` by\nthis module.\n\n4\\. In your training program, you can either use regular distributed functions\nor use `torch.nn.parallel.DistributedDataParallel()` module. If your training\nprogram uses GPUs for training and you would like to use\n`torch.nn.parallel.DistributedDataParallel()` module, here is how to configure\nit.\n\nPlease ensure that `device_ids` argument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, the `device_ids` needs to be `[args.local_rank]`, and\n`output_device` needs to be `args.local_rank` in order to use this utility\n\n5\\. Another way to pass `local_rank` to the subprocesses via environment\nvariable `LOCAL_RANK`. This behavior is enabled when you launch the script\nwith `--use_env=True`. You must adjust the subprocess example above to replace\n`args.local_rank` with `os.environ['LOCAL_RANK']`; the launcher will not pass\n`--local_rank` when you specify this flag.\n\nWarning\n\n`local_rank` is NOT globally unique: it is only unique per process on a\nmachine. Thus, don\u2019t use it to decide if you should, e.g., write to a\nnetworked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for\nan example of how things can go wrong if you don\u2019t do this correctly.\n\nThe Multiprocessing package - torch.multiprocessing package also provides a\n`spawn` function in `torch.multiprocessing.spawn()`. This helper function can\nbe used to spawn multiple processes. It works by passing in the function that\nyou want to run and spawns N processes to run it. This can be used for\nmultiprocess distributed training as well.\n\nFor references on how to use it, please refer to PyTorch example - ImageNet\nimplementation\n\nNote that this function requires Python 3.4 or higher.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook just calls `allreduce` using `GradBucket` tensors.\nOnce gradient tensors are aggregated across all workers, its `then` callback\ntakes the mean and returns the result. If user registers this hook, DDP\nresults is expected to be same as the case where no hook was registered.\nHence, this won\u2019t change behavior of DDP and user can use this as a reference\nor modify this hook to log useful information or any other purposes while\nunaffecting DDP behavior.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements a simple gradient compression approach\nthat converts `GradBucket` tensors whose type is assumed to be `torch.float32`\nto half-precision floating point format (`torch.float16`). It allreduces those\n`float16` gradient tensors. Once compressed gradient tensors are allreduced,\nits then callback called `decompress` converts the aggregated result back to\n`float32` and takes the mean.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements a simplified PowerSGD gradient\ncompression algorithm described in the paper. This variant does not compress\nthe gradients layer by layer, but instead compresses the flattened input\ntensor that batches all the gradients. Therefore, it is faster than\n`powerSGD_hook()`, but usually results in a much lower accuracy, unless\n`matrix_approximation_rank` is 1.\n\nWarning\n\nIncreasing `matrix_approximation_rank` here may not necessarily increase the\naccuracy, because batching per-parameter tensors without column/row alignment\ncan destroy low-rank structure. Therefore, the user should always consider\n`powerSGD_hook()` first, and only consider this variant when a satisfactory\naccuracy can be achieved when `matrix_approximation_rank` is 1.\n\nOnce gradient tensors are aggregated across all workers, this hook applies\ncompression as follows:\n\nNote that this communication hook enforces vanilla allreduce for the first\n`state.start_powerSGD_iter` iterations. This not only gives the user more\ncontrol over the tradeoff between speedup and accuracy, but also helps\nabstract away some complexity of the internal optimization of DDP for future\ncommunication hook developers.\n\nFuture handler of the communication, which updates the gradients in place.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": "\nStores both the algorithm\u2019s hyperparameters and the internal state for all the\ngradients during the training. Particularly, `matrix_approximation_rank` and\n`start_powerSGD_iter` are the main hyperparameters that should be tuned by the\nuser. For performance, we suggest to keep binary hyperparameters\n`use_error_feedback` and `warm_start` on.\n\n`matrix_approximation_rank` controls the size of compressed low-rank tensors,\nwhich determines the compression rate. The lower the rank, the stronger the\ncompression.\n\n1.1. If `matrix_approximation_rank` is too low, the full model quality will\nneed more training steps to reach or will never reach and yield loss in\naccuracy.\n\n1.2. The increase of `matrix_approximation_rank` can substantially increase\nthe computation costs of the compression, and the accuracy may not be futher\nimproved beyond a certain `matrix_approximation_rank` threshold.\n\nTo tune `matrix_approximation_rank`, we suggest to start from 1 and increase\nby factors of 2 (like an expoential grid search, 1, 2, 4, \u2026), until a\nsatisfactory accuracy is reached. Typically only a small value 1-4 is used.\nFor some NLP tasks (as shown in Appendix D of the original paper), this value\nhas been increased to 32.\n\nTo tune `start_powerSGD_iter`, we suggest to start with 10% of total training\nsteps, and increase it until a satisfactory accuracy is reached.\n\nWarning\n\nIf error feedback or warm-up is enabled, the minimum value of\n`start_powerSGD_iter` allowed in DDP is 2. This is because there is another\ninternal optimization that rebuilds buckets at iteration 1 in DDP, and this\ncan conflict with any tensor memorized before the rebuild process.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements PowerSGD gradient compression algorithm\ndescribed in the paper. Once gradient tensors are aggregated across all\nworkers, this hook applies compression as follows:\n\nHandles rank-1 tensors by allreducing them without compression:\n\n2.1. Allocate contiguous memory for those rank-1 tensors, and allreduces all\nthe rank-1 tensors as a batch, without compression;\n\n2.2. Copies the individual rank-1 tensors from the contiguous memory back to\nthe input tensor.\n\nHandles high-rank tensors by PowerSGD compression:\n\n3.1. For each high-rank tensor M, creates two low-rank tensors P and Q for\ndecomposing M, such that M = PQ^T, where Q is initialized from a standard\nnormal distribution and orthogonalized;\n\n3.2. Computes each P in Ps, which is equal to MQ;\n\n3.3. Allreduces Ps as a batch;\n\n3.4. Orthogonalizes each P in Ps;\n\n3.5. Computes each Q in Qs, which is approximately equal to M^TP;\n\n3.6. Allreduces Qs as a batch;\n\n3.7. Computes each M among all the high-rank tensors, which is approximately\nequal to PQ^T.\n\nNote that this communication hook enforces vanilla allreduce for the first\n`state.start_powerSGD_iter` iterations. This not only gives the user more\ncontrol over the tradeoff between speedup and accuracy, but also helps\nabstract away some complexity of the internal optimization of DDP for future\ncommunication hook developers.\n\nFuture handler of the communication, which updates the gradients in place.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "torch.distributed", "text": "\nGathers tensors from the whole group in a list.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "torch.distributed", "text": "\nGathers tensors from the whole group in a list. Each tensor in `tensor_list`\nshould reside on a separate GPU\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nComplex tensors are supported.\n\noutput_tensor_lists (List[List[Tensor]]) \u2013\n\nOutput lists. It should contain correctly-sized tensors on each GPU to be used\nfor output of the collective, e.g. `output_tensor_lists[i]` contains the\nall_gather result that resides on the GPU of `input_tensor_list[i]`.\n\nNote that each element of `output_tensor_lists` has the size of `world_size *\nlen(input_tensor_list)`, since the function all gathers the result from every\nsingle GPU in the group. To interpret each element of\n`output_tensor_lists[i]`, note that `input_tensor_list[j]` of rank k will be\nappear in `output_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(output_tensor_lists)`, and the size of each element in\n`output_tensor_lists` (each element is a list, therefore\n`len(output_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "torch.distributed", "text": "\nGathers picklable objects from the whole group into a list. Similar to\n`all_gather()`, but Python objects can be passed in. Note that the object must\nbe picklable in order to be gathered.\n\nNone. If the calling rank is part of this group, the output of the collective\nwill be populated into the input `object_list`. If the calling rank is not\npart of the group, the passed in `object_list` will be unmodified.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nWarning\n\n`all_gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines in such a way that all get the\nfinal result.\n\nAfter the call `tensor` is going to be bitwise identical in all processes.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines in such a way that all get the\nfinal result. This function reduces a number of tensors on every node, while\neach tensor resides on different GPUs. Therefore, the input tensor in the\ntensor list needs to be GPU tensors. Also, each tensor in the tensor list\nneeds to reside on a different GPU.\n\nAfter the call, all `tensor` in `tensor_list` is going to be bitwise identical\nin all processes.\n\nComplex tensors are supported.\n\nOnly nccl and gloo backend is currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "torch.distributed", "text": "\nEach process scatters list of input tensors to all processes in a group and\nreturn gathered list of tensors in output list.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nWarning\n\n`all_to_all` is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC Framework", "text": "\nKicks off the distributed backward pass using the provided roots. This\ncurrently implements the FAST mode algorithm which assumes all RPC messages\nsent in the same distributed autograd context across workers would be part of\nthe autograd graph during the backward pass.\n\nWe use the provided roots to discover the autograd graph and compute\nappropriate dependencies. This method blocks until the entire autograd\ncomputation is done.\n\nWe accumulate the gradients in the appropriate\n`torch.distributed.autograd.context` on each of the nodes. The autograd\ncontext to be used is looked up given the `context_id` that is passed in when\n`torch.distributed.autograd.backward()` is called. If there is no valid\nautograd context corresponding to the given ID, we throw an error. You can\nretrieve the accumulated gradients using the `get_gradients()` API.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC Framework", "text": "\nContext object to wrap forward and backward passes when using distributed\nautograd. The `context_id` generated in the `with` statement is required to\nuniquely identify a distributed backward pass on all workers. Each worker\nstores metadata associated with this `context_id`, which is required to\ncorrectly execute a distributed autograd pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC Framework", "text": "\nRetrieves a map from Tensor to the appropriate gradient for that Tensor\naccumulated in the provided context corresponding to the given `context_id` as\npart of the distributed autograd backward pass.\n\ncontext_id (int) \u2013 The autograd context id for which we should retrieve the\ngradients.\n\nA map where the key is the Tensor and the value is the associated gradient for\nthat Tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "torch.distributed", "text": "\nAn enum-like class of available backends: GLOO, NCCL, MPI, and other\nregistered backends.\n\nThe values of this class are lowercase strings, e.g., `\"gloo\"`. They can be\naccessed as attributes, e.g., `Backend.NCCL`.\n\nThis class can be directly called to parse the string, e.g.,\n`Backend(backend_str)` will check if `backend_str` is valid, and return the\nparsed lowercase string if so. It also accepts uppercase strings, e.g.,\n`Backend(\"GLOO\")` returns `\"gloo\"`.\n\nNote\n\nThe entry `Backend.UNDEFINED` is present but only used as initial value of\nsome fields. Users should neither use it directly nor assume its existence.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "torch.distributed", "text": "\nSynchronizes all processes.\n\nThis collective blocks processes until the whole group enters this function,\nif async_op is False, or if async work handle is called on wait().\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "torch.distributed", "text": "\nBroadcasts the tensor to the whole group.\n\n`tensor` must have the same number of elements in all processes participating\nin the collective.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "torch.distributed", "text": "\nBroadcasts the tensor to the whole group with multiple GPU tensors per node.\n\n`tensor` must have the same number of elements in all the GPUs from all\nprocesses participating in the collective. each tensor in the list must be on\na different GPU\n\nOnly nccl and gloo backend are currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "torch.distributed", "text": "\nBroadcasts picklable objects in `object_list` to the whole group. Similar to\n`broadcast()`, but Python objects can be passed in. Note that all objects in\n`object_list` must be picklable in order to be broadcasted.\n\n`None`. If rank is part of the group, `object_list` will contain the\nbroadcasted objects from `src` rank.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`broadcast_object_list()` uses `pickle` module implicitly, which is known to\nbe insecure. It is possible to construct malicious pickle data which will\nexecute arbitrary code during unpickling. Only call this function with data\nyou trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "torch.distributed", "text": "\nA store implementation that uses a file to store the underlying key-value\npairs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "torch.distributed", "text": "\nGathers a list of tensors in a single process.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "torch.distributed", "text": "\nGathers picklable objects from the whole group in a single process. Similar to\n`gather()`, but Python objects can be passed in. Note that the object must be\npicklable in order to be gathered.\n\nNone. On the `dst` rank, `object_gather_list` will contain the output of the\ncollective.\n\nNote\n\nNote that this API differs slightly from the gather collective since it does\nnot provide an async_op handle and thus will be a blocking call.\n\nNote\n\nNote that this API is not supported when using the NCCL backend.\n\nWarning\n\n`gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "torch.distributed", "text": "\nReturns the backend of the given process group.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is\nthe general main process group. If another specific group is specified, the\ncalling process must be part of `group`.\n\nThe backend of the given process group as a lower case string.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "torch.distributed", "text": "\nReturns the rank of current process group\n\nRank is a unique identifier assigned to each process within a distributed\nprocess group. They are always consecutive integers ranging from 0 to\n`world_size`.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe rank of the process group -1, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "torch.distributed", "text": "\nReturns the number of processes in the current process group\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe world size of the process group -1, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "torch.distributed", "text": "\nA thread-safe store implementation based on an underlying hashmap. This store\ncan be used within the same process (for example, by other threads), but\ncannot be used across processes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "torch.distributed", "text": "\nInitializes the default distributed process group, and this will also\ninitialize the distributed package.\n\nIf neither is specified, `init_method` is assumed to be \u201cenv://\u201d.\n\nTo enable `backend == Backend.MPI`, PyTorch needs to be built from source on a\nsystem that supports MPI.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "torch.distributed", "text": "\nReceives a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "torch.distributed", "text": "\nSends a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "torch.distributed", "text": "\nReturns `True` if the distributed package is available. Otherwise,\n`torch.distributed` does not expose any other APIs. Currently,\n`torch.distributed` is available on Linux, MacOS and Windows. Set\n`USE_DISTRIBUTED=1` to enable it when building PyTorch from source. Currently,\nthe default value is `USE_DISTRIBUTED=1` for Linux and Windows,\n`USE_DISTRIBUTED=0` for MacOS.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "torch.distributed", "text": "\nChecking if the default process group has been initialized\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "torch.distributed", "text": "\nChecks if the MPI backend is available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "torch.distributed", "text": "\nChecks if the NCCL backend is available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "torch.distributed", "text": "\nCreates a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even if\nthey are not going to be members of the group. Additionally, groups should be\ncreated in the same order in all processes.\n\nWarning\n\nUsing multiple process groups with the `NCCL` backend concurrently is not safe\nand the user should perform explicit synchronization in their application to\nensure only one process group is used at a time. This means collectives from\none process group should have completed execution on the device (not just\nenqueued since CUDA execution is async) before collectives from another\nprocess group are enqueued. See Using multiple NCCL communicators concurrently\nfor more details.\n\nA handle of distributed group that can be given to collective calls.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "rpc#torch.distributed.optim.DistributedOptimizer", "type": "Distributed RPC Framework", "text": "\nDistributedOptimizer takes remote references to parameters scattered across\nworkers and applies the given optimizer locally for each parameter.\n\nThis class uses `get_gradients()` in order to retrieve the gradients for\nspecific parameters.\n\nConcurrent calls to `step()`, either from the same or different clients, will\nbe serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one\nset of gradients at a time. However, there is no guarantee that the full\nforward-backward-optimizer sequence will execute for one client at a time.\nThis means that the gradients being applied may not correspond to the latest\nforward pass executed on a given worker. Also, there is no guaranteed ordering\nacross workers.\n\n`DistributedOptimizer` creates the local optimizer with TorchScript enabled by\ndefault, so that optimizer updates are not blocked by the Python Global\nInterpreter Lock (GIL) during multithreaded training (e.g. Distributed Model\nParallel). This feature is currently in beta stage, enabled for optimizers\nincluding `Adagrad`, `Adam`, `SGD`, `RMSprop`, `AdamW` and `Adadelta`. We are\nincreasing the coverage to all optimizers in future releases.\n\nPerforms a single optimization step.\n\nThis will call `torch.optim.Optimizer.step()` on each worker containing\nparameters to be optimized, and will block until all workers return. The\nprovided `context_id` will be used to retrieve the corresponding `context`\nthat contains the gradients that should be applied to the parameters.\n\ncontext_id \u2013 the autograd context id for which we should run the optimizer\nstep.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "rpc#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed RPC Framework", "text": "\nPerforms a single optimization step.\n\nThis will call `torch.optim.Optimizer.step()` on each worker containing\nparameters to be optimized, and will block until all workers return. The\nprovided `context_id` will be used to retrieve the corresponding `context`\nthat contains the gradients that should be applied to the parameters.\n\ncontext_id \u2013 the autograd context id for which we should run the optimizer\nstep.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Pipeline Parallelism", "text": "\nWraps an arbitrary `nn.Sequential` module to train on using synchronous\npipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on\na single GPU, pipeline parallelism is a useful technique to employ for\ntraining.\n\nThe implementation is based on the torchgpipe paper.\n\nPipe combines pipeline parallelism with checkpointing to reduce peak memory\nrequired to train while minimizing device under-utilization.\n\nYou should place all the modules on the appropriate devices and wrap them into\nan `nn.Sequential` module defining the desired order of execution.\n\nPipeline of two FC layers across GPUs 0 and 1.\n\nNote\n\nYou can wrap a `Pipe` model with `torch.nn.parallel.DistributedDataParallel`\nonly when the checkpoint parameter of `Pipe` is `'never'`.\n\nNote\n\n`Pipe` only supports intra-node pipelining currently, but will be expanded to\nsupport inter-node pipelining in the future. The forward function returns an\n`RRef` to allow for inter-node pipelining in the future, where the output\nmight be on a remote host. For intra-node pipelinining you can use\n`local_value()` to retrieve the output locally.\n\nWarning\n\n`Pipe` is experimental and subject to change.\n\nProcesses a single input mini-batch through the pipe and returns an `RRef`\npointing to the output. `Pipe` is a fairly transparent module wrapper. It\ndoesn\u2019t modify the input and output signature of the underlying module. But\nthere\u2019s type restriction. Input and output have to be a `Tensor` or a sequence\nof tensors. This restriction is applied at partition boundaries too.\n\nThe input tensor is split into multiple micro-batches based on the `chunks`\nparameter used to initialize `Pipe`. The batch size is assumed to be the first\ndimension of the tensor and if the batch size is less than `chunks`, the\nnumber of micro-batches is equal to the batch size.\n\ninput (torch.Tensor or sequence of `Tensor`) \u2013 input mini-batch\n\n`RRef` to the output of the mini-batch\n\nTypeError \u2013 input is not a tensor or sequence of tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Pipeline Parallelism", "text": "\nProcesses a single input mini-batch through the pipe and returns an `RRef`\npointing to the output. `Pipe` is a fairly transparent module wrapper. It\ndoesn\u2019t modify the input and output signature of the underlying module. But\nthere\u2019s type restriction. Input and output have to be a `Tensor` or a sequence\nof tensors. This restriction is applied at partition boundaries too.\n\nThe input tensor is split into multiple micro-batches based on the `chunks`\nparameter used to initialize `Pipe`. The batch size is assumed to be the first\ndimension of the tensor and if the batch size is less than `chunks`, the\nnumber of micro-batches is equal to the batch size.\n\ninput (torch.Tensor or sequence of `Tensor`) \u2013 input mini-batch\n\n`RRef` to the output of the mini-batch\n\nTypeError \u2013 input is not a tensor or sequence of tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Pipeline Parallelism", "text": "\nThe command to pop a skip tensor.\n\nname (str) \u2013 name of skip tensor\n\nthe skip tensor previously stashed by another layer under the same name\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Pipeline Parallelism", "text": "\nThe decorator to define a `nn.Module` with skip connections. Decorated modules\nare called \u201cskippable\u201d. This functionality works perfectly fine even when the\nmodule is not wrapped by `Pipe`.\n\nEach skip tensor is managed by its name. Before manipulating skip tensors, a\nskippable module must statically declare the names for skip tensors by `stash`\nand/or `pop` parameters. Skip tensors with pre-declared name can be stashed by\n`yield stash(name, tensor)` or popped by `tensor = yield pop(name)`.\n\nHere is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed\nand popped at the first and last layer, respectively:\n\nOne skippable module can stash or pop multiple skip tensors:\n\nEvery skip tensor must be associated with exactly one pair of `stash` and\n`pop`. `Pipe` checks this restriction automatically when wrapping a module.\nYou can also check the restriction by `verify_skippables()` without `Pipe`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Pipeline Parallelism", "text": "\nThe command to stash a skip tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Pipeline Parallelism", "text": "\nVerifies if the underlying skippable modules satisfy integrity.\n\nEvery skip tensor must have only one pair of `stash` and `pop`. If there are\none or more unmatched pairs, it will raise `TypeError` with the detailed\nmessages.\n\nHere are a few failure cases. `verify_skippables()` will report failure for\nthese cases:\n\nTo use the same name for multiple skip tensors, they must be isolated by\ndifferent namespaces. See `isolate()`.\n\nTypeError \u2013 one or more pairs of `stash` and `pop` are not matched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "torch.distributed", "text": "\nA wrapper around any of the 3 key-value stores (`TCPStore`, `FileStore`, and\n`HashStore`) that adds a prefix to each key inserted to the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "torch.distributed", "text": "\nReceives a tensor synchronously.\n\nSender rank -1, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines.\n\nOnly the process with rank `dst` is going to receive the final result.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "torch.distributed", "text": "\nAn enum-like class for available reduction operations: `SUM`, `PRODUCT`,\n`MIN`, `MAX`, `BAND`, `BOR`, and `BXOR`.\n\nNote that `BAND`, `BOR`, and `BXOR` reductions are not available when using\nthe `NCCL` backend.\n\nAdditionally, `MAX`, `MIN` and `PRODUCT` are not supported for complex\ntensors.\n\nThe values of this class can be accessed as attributes, e.g., `ReduceOp.SUM`.\nThey are used in specifying strategies for reduction collectives, e.g.,\n`reduce()`, `all_reduce_multigpu()`, etc.\n\nMembers:\n\nSUM\n\nPRODUCT\n\nMIN\n\nMAX\n\nBAND\n\nBOR\n\nBXOR\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "torch.distributed", "text": "\nReduces the tensor data on multiple GPUs across all machines. Each tensor in\n`tensor_list` should reside on a separate GPU\n\nOnly the GPU of `tensor_list[dst_tensor]` on the process with rank `dst` is\ngoing to receive the final result.\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nAsync work handle, if async_op is set to True. None, otherwise\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "torch.distributed", "text": "\nDeprecated enum-like class for reduction operations: `SUM`, `PRODUCT`, `MIN`,\nand `MAX`.\n\n`ReduceOp` is recommended to use instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "torch.distributed", "text": "\nReduces, then scatters a list of tensors to all processes in a group.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "torch.distributed", "text": "\nReduce and scatter a list of tensors to the whole group. Only nccl backend is\ncurrently supported.\n\nEach tensor in `output_tensor_list` should reside on a separate GPU, as should\neach list of tensors in `input_tensor_lists`.\n\noutput_tensor_list (List[Tensor]) \u2013\n\nOutput tensors (on different GPUs) to receive the result of the operation.\n\nNote that `len(output_tensor_list)` needs to be the same for all the\ndistributed processes calling this function.\n\ninput_tensor_lists (List[List[Tensor]]) \u2013\n\nInput lists. It should contain correctly-sized tensors on each GPU to be used\nfor input of the collective, e.g. `input_tensor_lists[i]` contains the\nreduce_scatter input that resides on the GPU of `output_tensor_list[i]`.\n\nNote that each element of `input_tensor_lists` has the size of `world_size *\nlen(output_tensor_list)`, since the function scatters the result from every\nsingle GPU in the group. To interpret each element of `input_tensor_lists[i]`,\nnote that `output_tensor_list[j]` of rank k receives the reduce-scattered\nresult from `input_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(input_tensor_lists)`, and the size of each element in\n`input_tensor_lists` (each element is a list, therefore\n`len(input_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC Framework", "text": "\nAn enum class of available backends.\n\nPyTorch ships with two builtin backends: `BackendType.TENSORPIPE` and\n`BackendType.PROCESS_GROUP`. Additional ones can be registered using the\n`register_backend()` function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC Framework", "text": "\nA decorator for a function indicating that the return value of the function is\nguaranteed to be a `Future` object and this function can run asynchronously on\nthe RPC callee. More specifically, the callee extracts the `Future` returned\nby the wrapped function and installs subsequent processing steps as a callback\nto that `Future`. The installed callback will read the value from the `Future`\nwhen completed and send the value back as the RPC response. That also means\nthe returned `Future` only exists on the callee side and is never sent through\nRPC. This decorator is useful when the wrapped function\u2019s (`fn`) execution\nneeds to pause and resume due to, e.g., containing `rpc_async()` or waiting\nfor other signals.\n\nNote\n\nTo enable asynchronous execution, applications must pass the function object\nreturned by this decorator to RPC APIs. If RPC detected attributes installed\nby this decorator, it knows that this function returns a `Future` object and\nwill handle that accordingly. However, this does not mean this decorator has\nto be outmost one when defining a function. For example, when combined with\n`@staticmethod` or `@classmethod`, `@rpc.functions.async_execution` needs to\nbe the inner decorator to allow the target function be recognized as a static\nor class function. This target function can still execute asynchronously\nbecause, when accessed, the static or class method preserves attributes\ninstalled by `@rpc.functions.async_execution`.\n\nThe returned `Future` object can come from `rpc_async()`, `then()`, or\n`Future` constructor. The example below shows directly using the `Future`\nreturned by `then()`.\n\nWhen combined with TorchScript decorators, this decorator must be the outmost\none.\n\nWhen combined with static or class method, this decorator must be the inner\none.\n\nThis decorator also works with RRef helpers, i.e., .\n`torch.distributed.rpc.RRef.rpc_sync()`,\n`torch.distributed.rpc.RRef.rpc_async()`, and\n`torch.distributed.rpc.RRef.remote()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC Framework", "text": "\nGet `WorkerInfo` of a given worker name. Use this `WorkerInfo` to avoid\npassing an expensive string on every invocation.\n\nworker_name (str) \u2013 the string name of a worker. If `None`, return the the id\nof the current worker. (default `None`)\n\n`WorkerInfo` instance for the given `worker_name` or `WorkerInfo` of the\ncurrent worker if `worker_name` is `None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC Framework", "text": "\nInitializes RPC primitives such as the local RPC agent and distributed\nautograd, which immediately makes the current process ready to send and\nreceive RPCs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nThe backend options class for `ProcessGroupAgent`, which is derived from\n`RpcBackendOptions`.\n\nURL specifying how to initialize the process group. Default is `env://`\n\nThe number of threads in the thread-pool used by ProcessGroupAgent.\n\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads", "type": "Distributed RPC Framework", "text": "\nThe number of threads in the thread-pool used by ProcessGroupAgent.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC Framework", "text": "\nMake a remote call to run `func` on worker `to` and return an `RRef` to the\nresult value immediately. Worker `to` will be the owner of the returned\n`RRef`, and the worker calling `remote` is a user. The owner manages the\nglobal reference count of its `RRef`, and the owner `RRef` is only destructed\nwhen globally there are no living references to it.\n\nA user `RRef` instance to the result value. Use the blocking API\n`torch.distributed.rpc.RRef.to_here()` to retrieve the result value locally.\n\nWarning\n\nUsing GPU tensors as arguments or return values of `func` is not supported\nsince we don\u2019t support sending GPU tensors over the wire. You need to\nexplicitly copy GPU tensors to CPU before using them as arguments or return\nvalues of `func`.\n\nWarning\n\nThe `remote` API does not copy storages of argument tensors until sending them\nover the wire, which could be done by a different thread depending on the RPC\nbackend type. The caller should make sure that the contents of those tensors\nstay intact until the returned RRef is confirmed by the owner, which can be\nchecked using the `torch.distributed.rpc.RRef.confirmed_by_owner()` API.\n\nWarning\n\nErrors such as timeouts for the `remote` API are handled on a best-effort\nbasis. This means that when remote calls initiated by `remote` fail, such as\nwith a timeout error, we take a best-effort approach to error handling. This\nmeans that errors are handled and set on the resulting RRef on an asynchronous\nbasis. If the RRef has not been used by the application before this handling\n(such as `to_here` or fork call), then future uses of the `RRef` will\nappropriately raise errors. However, it is possible that the user application\nwill use the `RRef` before the errors are handled. In this case, errors may\nnot be raised as they have not yet been handled.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\nBelow is an example of running a TorchScript function using RPC.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nAn abstract structure encapsulating the options passed into the RPC backend.\nAn instance of this class can be passed in to `init_rpc()` in order to\ninitialize RPC with specific configurations, such as the RPC timeout and\n`init_method` to be used.\n\nURL specifying how to initialize the process group. Default is `env://`\n\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC Framework", "text": "\nMake a non-blocking RPC call to run function `func` on worker `to`. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe. This method will immediately return a `Future` that can\nbe awaited on.\n\nReturns a `Future` object that can be waited on. When completed, the return\nvalue of `func` on `args` and `kwargs` can be retrieved from the `Future`\nobject.\n\nWarning\n\nUsing GPU tensors as arguments or return values of `func` is not supported\nsince we don\u2019t support sending GPU tensors over the wire. You need to\nexplicitly copy GPU tensors to CPU before using them as arguments or return\nvalues of `func`.\n\nWarning\n\nThe `rpc_async` API does not copy storages of argument tensors until sending\nthem over the wire, which could be done by a different thread depending on the\nRPC backend type. The caller should make sure that the contents of those\ntensors stay intact until the returned `Future` completes.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\nBelow is an example of running a TorchScript function using RPC.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC Framework", "text": "\nMake a blocking RPC call to run function `func` on worker `to`. RPC messages\nare sent and received in parallel to execution of Python code. This method is\nthread-safe.\n\nReturns the result of running `func` with `args` and `kwargs`.\n\nWarning\n\nUsing GPU tensors as arguments or return values of `func` is not supported\nsince we don\u2019t support sending GPU tensors over the wire. You need to\nexplicitly copy GPU tensors to CPU before using them as arguments or return\nvalues of `func`.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\nBelow is an example of running a TorchScript function using RPC.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef", "path": "rpc#torch.distributed.rpc.RRef", "type": "Distributed RPC Framework", "text": "\nRuns the backward pass using the RRef as the root of the backward pass. If\n`dist_autograd_ctx_id` is provided, we perform a distributed backward pass\nusing the provided ctx_id starting from the owner of the RRef. In this case,\n`get_gradients()` should be used to retrieve the gradients. If\n`dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd\ngraph and we only perform a local backward pass. In the local case, the node\ncalling this API has to be the owner of the RRef. The value of the RRef is\nexpected to be a scalar Tensor.\n\nReturns whether this `RRef` has been confirmed by the owner. `OwnerRRef`\nalways returns true, while `UserRRef` only returns true when the owner knowns\nabout this `UserRRef`.\n\nReturns whether or not the current node is the owner of this `RRef`.\n\nIf the current node is the owner, returns a reference to the local value.\nOtherwise, throws an exception.\n\nReturns worker information of the node that owns this `RRef`.\n\nReturns worker name of the node that owns this `RRef`.\n\nCreate a helper proxy to easily launch a `remote` using the owner of the RRef\nas the destination to run functions on the object referenced by this RRef.\nMore specifically, `rref.remote().func_name(*args, **kwargs)` is the same as\nthe following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.remote()`. If the creation of\nthis `RRef` is not successfully completed within the timeout, then the next\ntime there is an attempt to use the RRef (such as `to_here`), a timeout will\nbe raised. If not provided, the default RPC timeout will be used. Please see\n`rpc.remote()` for specific timeout semantics for `RRef`.\n\nCreate a helper proxy to easily launch an `rpc_async` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_async()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\nCreate a helper proxy to easily launch an `rpc_sync` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_sync()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\nBlocking call that copies the value of the RRef from the owner to the local\nnode and returns it. If the current node is the owner, returns a reference to\nthe local value.\n\ntimeout (float, optional) \u2013 Timeout for `to_here`. If the call does not\ncomplete within this timeframe, an exception indicating so will be raised. If\nthis argument is not provided, the default RPC timeout (60s) will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.backward()", "path": "rpc#torch.distributed.rpc.RRef.backward", "type": "Distributed RPC Framework", "text": "\nRuns the backward pass using the RRef as the root of the backward pass. If\n`dist_autograd_ctx_id` is provided, we perform a distributed backward pass\nusing the provided ctx_id starting from the owner of the RRef. In this case,\n`get_gradients()` should be used to retrieve the gradients. If\n`dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd\ngraph and we only perform a local backward pass. In the local case, the node\ncalling this API has to be the owner of the RRef. The value of the RRef is\nexpected to be a scalar Tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.RRef.confirmed_by_owner", "type": "Distributed RPC Framework", "text": "\nReturns whether this `RRef` has been confirmed by the owner. `OwnerRRef`\nalways returns true, while `UserRRef` only returns true when the owner knowns\nabout this `UserRRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.is_owner()", "path": "rpc#torch.distributed.rpc.RRef.is_owner", "type": "Distributed RPC Framework", "text": "\nReturns whether or not the current node is the owner of this `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.local_value()", "path": "rpc#torch.distributed.rpc.RRef.local_value", "type": "Distributed RPC Framework", "text": "\nIf the current node is the owner, returns a reference to the local value.\nOtherwise, throws an exception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.owner()", "path": "rpc#torch.distributed.rpc.RRef.owner", "type": "Distributed RPC Framework", "text": "\nReturns worker information of the node that owns this `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.owner_name()", "path": "rpc#torch.distributed.rpc.RRef.owner_name", "type": "Distributed RPC Framework", "text": "\nReturns worker name of the node that owns this `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.remote()", "path": "rpc#torch.distributed.rpc.RRef.remote", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch a `remote` using the owner of the RRef\nas the destination to run functions on the object referenced by this RRef.\nMore specifically, `rref.remote().func_name(*args, **kwargs)` is the same as\nthe following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.remote()`. If the creation of\nthis `RRef` is not successfully completed within the timeout, then the next\ntime there is an attempt to use the RRef (such as `to_here`), a timeout will\nbe raised. If not provided, the default RPC timeout will be used. Please see\n`rpc.remote()` for specific timeout semantics for `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.rpc_async()", "path": "rpc#torch.distributed.rpc.RRef.rpc_async", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch an `rpc_async` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_async()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.RRef.rpc_sync", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch an `rpc_sync` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_sync()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.to_here()", "path": "rpc#torch.distributed.rpc.RRef.to_here", "type": "Distributed RPC Framework", "text": "\nBlocking call that copies the value of the RRef from the owner to the local\nnode and returns it. If the current node is the owner, returns a reference to\nthe local value.\n\ntimeout (float, optional) \u2013 Timeout for `to_here`. If the call does not\ncomplete within this timeframe, an exception indicating so will be raised. If\nthis argument is not provided, the default RPC timeout (60s) will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC Framework", "text": "\nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This\nstops the local agent from accepting outstanding requests, and shuts down the\nRPC framework by terminating all RPC threads. If `graceful=True`, this will\nblock until all local and remote RPC processes reach this method and wait for\nall outstanding work to complete. Otherwise, if `graceful=False`, this is a\nlocal shutdown, and it does not wait for other RPC processes to reach this\nmethod.\n\nWarning\n\nFor `Future` objects returned by `rpc_async()`, `future.wait()` should not be\ncalled after `shutdown()`.\n\ngraceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will\n1) wait until there is no pending system messages for `UserRRefs` and delete\nthem; 2) block until all local and remote RPC processes have reached this\nmethod and wait for all outstanding work to complete.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nThe backend options for `TensorPipeAgent`, derived from `RpcBackendOptions`.\n\nThe device map locations.\n\nURL specifying how to initialize the process group. Default is `env://`\n\nThe number of threads in the thread-pool used by `TensorPipeAgent` to execute\nrequests.\n\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\nSet device mapping between each RPC caller and callee pair. This function can\nbe called multiple times to incrementally add device placement configurations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC Framework", "text": "\nThe device map locations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC Framework", "text": "\nThe number of threads in the thread-pool used by `TensorPipeAgent` to execute\nrequests.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC Framework", "text": "\nSet device mapping between each RPC caller and callee pair. This function can\nbe called multiple times to incrementally add device placement configurations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC Framework", "text": "\nA structure that encapsulates information of a worker in the system. Contains\nthe name and ID of the worker. This class is not meant to be constructed\ndirectly, rather, an instance can be retrieved through `get_worker_info()` and\nthe result can be passed in to functions such as `rpc_sync()`, `rpc_async()`,\n`remote()` to avoid copying a string on every invocation.\n\nGlobally unique id to identify the worker.\n\nThe name of the worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo.id()", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC Framework", "text": "\nGlobally unique id to identify the worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo.name()", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC Framework", "text": "\nThe name of the worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "torch.distributed", "text": "\nScatters a list of tensors to all processes in a group.\n\nEach process will receive exactly one tensor and store its data in the\n`tensor` argument.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "torch.distributed", "text": "\nScatters picklable objects in `scatter_object_input_list` to the whole group.\nSimilar to `scatter()`, but Python objects can be passed in. On each rank, the\nscattered object will be stored as the first element of\n`scatter_object_output_list`. Note that all objects in\n`scatter_object_input_list` must be picklable in order to be scattered.\n\n`None`. If rank is part of the group, `scatter_object_output_list` will have\nits first element set to the scattered object for this rank.\n\nNote\n\nNote that this API differs slightly from the scatter collective since it does\nnot provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`scatter_object_list()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "torch.distributed", "text": "\nSends a tensor synchronously.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "torch.distributed", "text": "\nBase class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (`TCPStore`, `FileStore`, and `HashStore`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "torch.distributed", "text": "\nThe first call to add for a given `key` creates a counter associated with\n`key` in the store, initialized to `amount`. Subsequent calls to add with the\nsame `key` increment the counter by the specified `amount`. Calling `add()`\nwith a key that has already been set in the store by `set()` will result in an\nexception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "torch.distributed", "text": "\nDeletes the key-value pair associated with `key` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\nWarning\n\nThe `delete_key` API is only supported by the `TCPStore` and `HashStore`.\nUsing this API with the `FileStore` will result in an exception.\n\nkey (str) \u2013 The key to be deleted from the store\n\n`True` if `key` was deleted, otherwise `False`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "torch.distributed", "text": "\nRetrieves the value associated with the given `key` in the store. If `key` is\nnot present in the store, the function will wait for `timeout`, which is\ndefined when initializing the store, before throwing an exception.\n\nkey (str) \u2013 The function will return the value associated with this key.\n\nValue associated with `key` if `key` is in the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "torch.distributed", "text": "\nReturns the number of keys set in the store. Note that this number will\ntypically be one greater than the number of keys added by `set()` and `add()`\nsince one key is used to coordinate all the workers using the store.\n\nWarning\n\nWhen used with the `TCPStore`, `num_keys` returns the number of keys written\nto the underlying file. If the store is destructed and another store is\ncreated with the same file, the original keys will be retained.\n\nThe number of keys present in the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "torch.distributed", "text": "\nInserts the key-value pair into the store based on the supplied `key` and\n`value`. If `key` already exists in the store, it will overwrite the old value\nwith the new supplied `value`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "torch.distributed", "text": "\nSets the store\u2019s default timeout. This timeout is used during initialization\nand in `wait()` and `get()`.\n\ntimeout (timedelta) \u2013 timeout to be set in the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "torch.distributed", "text": "\nOverloaded function.\n\nWaits for each key in `keys` to be added to the store. If not all keys are set\nbefore the `timeout` (set during store initialization), then `wait` will throw\nan exception.\n\nkeys (list) \u2013 List of keys on which to wait until they are set in the store.\n\nWaits for each key in `keys` to be added to the store, and throws an exception\nif the keys have not been set by the supplied `timeout`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "torch.distributed", "text": "\nA TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as `set()` to insert a key-value pair, `get()` to\nretrieve a key-value pair, etc.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions", "path": "distributions", "type": "torch.distributions", "text": "\nThe `distributions` package contains parameterizable probability distributions\nand sampling functions. This allows the construction of stochastic computation\ngraphs and stochastic gradient estimators for optimization. This package\ngenerally follows the design of the TensorFlow Distributions package.\n\nIt is not possible to directly backpropagate through random samples. However,\nthere are two main methods for creating surrogate functions that can be\nbackpropagated through. These are the score function estimator/likelihood\nratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is\ncommonly seen as the basis for policy gradient methods in reinforcement\nlearning, and the pathwise derivative estimator is commonly seen in the\nreparameterization trick in variational autoencoders. Whilst the score\nfunction only requires the value of samples f(x)f(x) , the pathwise derivative\nrequires the derivative f\u2032(x)f'(x) . The next sections discuss these two in a\nreinforcement learning example. For more details see Gradient Estimation Using\nStochastic Computation Graphs .\n\nWhen the probability density function is differentiable with respect to its\nparameters, we only need `sample()` and `log_prob()` to implement REINFORCE:\n\nwhere \u03b8\\theta are the parameters, \u03b1\\alpha is the learning rate, rr is the\nreward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s)) is the probability of taking action aa\nin state ss given policy \u03c0\u03b8\\pi^\\theta .\n\nIn practice we would sample an action from the output of a network, apply this\naction in an environment, and then use `log_prob` to construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows:\n\nThe other way to implement these stochastic/policy gradients would be to use\nthe reparameterization trick from the `rsample()` method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The\nreparameterized sample therefore becomes differentiable. The code for\nimplementing the pathwise derivative would be as follows:\n\nBases: `object`\n\nDistribution is the abstract base class for probability distributions.\n\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\nReturns the shape over which parameters are batched.\n\nReturns the cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns entropy of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step `[[0, 0], [1,\n1], \u2026]`. With `expand=False`, enumeration happens along dim 0, but with the\nremaining batch dimensions being singleton dimensions, `[[0], [1], ..`.\n\nTo iterate over the full Cartesian product use\n`itertools.product(m.enumerate_support())`.\n\nexpand (bool) \u2013 whether to expand the support over the batch dims to match the\ndistribution\u2019s `batch_shape`.\n\nTensor iterating over dimension 0.\n\nReturns the shape of a single sample (without batching).\n\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\nNew distribution instance with batch dimensions expanded to `batch_size`.\n\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the log of the probability density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the mean of the distribution.\n\nReturns perplexity of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python\u2019s `assert` statement: validation is on by\ndefault, but is disabled if Python is run in optimized mode (via `python -O`).\nValidation may be expensive, so you may want to disable it once a model is\nworking.\n\nvalue (bool) \u2013 Whether to enable validation.\n\nReturns the standard deviation of the distribution.\n\nReturns a `Constraint` object representing this distribution\u2019s support.\n\nReturns the variance of the distribution.\n\nBases: `torch.distributions.distribution.Distribution`\n\nExponentialFamily is the abstract base class for probability distributions\nbelonging to an exponential family, whose probability mass/density function\nhas the form is defined below\n\nwhere \u03b8\\theta denotes the natural parameters, t(x)t(x) denotes the sufficient\nstatistic, F(\u03b8)F(\\theta) is the log normalizer function for a given family and\nk(x)k(x) is the carrier measure.\n\nNote\n\nThis class is an intermediary between the `Distribution` class and\ndistributions which belong to an exponential family mainly to check the\ncorrectness of the `.entropy()` and analytic KL divergence methods. We use\nthis class to compute the entropy and KL divergence using the AD framework and\nBregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies\nand Cross-entropies of Exponential Families).\n\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Bernoulli distribution parameterized by `probs` or `logits` (but not\nboth).\n\nSamples are binary (0 or 1). They take the value `1` with probability `p` and\n`0` with probability `1 - p`.\n\nExample:\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nBeta distribution parameterized by `concentration1` and `concentration0`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Binomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). `total_count` must be broadcastable with\n`probs`/`logits`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a categorical distribution parameterized by either `probs` or `logits`\n(but not both).\n\nNote\n\nIt is equivalent to the distribution that `torch.multinomial()` samples from.\n\nSamples are integers from {0,\u2026,K\u22121}\\\\{0, \\ldots, K-1\\\\} where `K` is\n`probs.size(-1)`.\n\nIf `probs` is 1-dimensional with length-`K`, each element is the relative\nprobability of sampling the class at that index.\n\nIf `probs` is N-dimensional, the first N-1 dimensions are treated as a batch\nof relative probability vectors.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.multinomial()`\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means `0` follows a\nCauchy distribution.\n\nExample:\n\nBases: `torch.distributions.gamma.Gamma`\n\nCreates a Chi2 distribution parameterized by shape parameter `df`. This is\nexactly equivalent to `Gamma(alpha=0.5*df, beta=0.5)`\n\nExample:\n\ndf (float or Tensor) \u2013 shape parameter of the distribution\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a continuous Bernoulli distribution parameterized by `probs` or\n`logits` (but not both).\n\nThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to log-\nodds, but the same names are used due to the similarity with the Bernoulli.\nSee [1] for more details.\n\nExample:\n\n[1] The continuous Bernoulli: fixing a pervasive error in variational\nautoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.\nhttps://arxiv.org/abs/1907.06845\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Dirichlet distribution parameterized by concentration\n`concentration`.\n\nExample:\n\nconcentration (Tensor) \u2013 concentration parameter of the distribution (often\nreferred to as alpha)\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Exponential distribution parameterized by `rate`.\n\nExample:\n\nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Fisher-Snedecor distribution parameterized by `df1` and `df2`.\n\nExample:\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Gamma distribution parameterized by shape `concentration` and\n`rate`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Geometric distribution parameterized by `probs`, where `probs` is\nthe probability of success of Bernoulli trials. It represents the probability\nthat in k+1k + 1 Bernoulli trials, the first kk trials failed, before seeing a\nsuccess.\n\nSamples are non-negative integers [0, inf\u2061\\inf ).\n\nExample:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Gumbel Distribution.\n\nExamples:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-Cauchy distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Cauchy distribution\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-normal distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Normal distribution\n\nBases: `torch.distributions.distribution.Distribution`\n\nReinterprets some of the batch dims of a distribution as event dims.\n\nThis is mainly useful for changing the shape of the result of `log_prob()`.\nFor example to create a diagonal Normal distribution with the same shape as a\nMultivariate Normal distribution (so they are interchangeable), you can:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Kumaraswamy distribution.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nLKJ distribution for lower Cholesky factor of correlation matrices. The\ndistribution is controlled by `concentration` parameter \u03b7\\eta to make the\nprobability of the correlation matrix MM generated from a Cholesky factor\npropotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when\n`concentration == 1`, we have a uniform distribution over Cholesky factors of\ncorrelation matrices. Note that this distribution samples the Cholesky factor\nof correlation matrices and not the correlation matrices themselves and\nthereby differs slightly from the derivations in [1] for the `LKJCorr`\ndistribution. For sampling, this uses the Onion method from [1] Section 3.\n\nL ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration)\n\nExample:\n\nReferences\n\n[1] `Generating random correlation matrices based on vines and extended onion\nmethod`, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Laplace distribution parameterized by `loc` and `scale`.\n\nExample:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a log-normal distribution parameterized by `loc` and `scale` where:\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal distribution with covariance matrix having a\nlow-rank form parameterized by `cov_factor` and `cov_diag`:\n\nNote\n\nThe computation for determinant and inverse of covariance matrix is avoided\nwhen `cov_factor.shape[1] << cov_factor.shape[0]` thanks to Woodbury matrix\nidentity and matrix determinant lemma. Thanks to these formulas, we just need\nto compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:\n\nBases: `torch.distributions.distribution.Distribution`\n\nThe `MixtureSameFamily` distribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of the\nsame distribution type. It is parameterized by a `Categorical` \u201cselecting\ndistribution\u201d (over `k` component) and a component distribution, i.e., a\n`Distribution` with a rightmost batch shape (equal to `[k]`) which indexes\neach (batch of) component.\n\nExamples:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Multinomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). The innermost dimension of `probs` indexes\nover categories. All other dimensions index over batches.\n\nNote that `total_count` need not be specified if only `log_prob()` is called\n(see example below)\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal (also called Gaussian) distribution\nparameterized by a mean vector and a covariance matrix.\n\nThe multivariate normal distribution can be parameterized either in terms of a\npositive definite covariance matrix \u03a3\\mathbf{\\Sigma} or a positive definite\nprecision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1} or a lower-triangular matrix\nL\\mathbf{L} with positive-valued diagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can\nbe obtained via e.g. Cholesky decomposition of the covariance.\n\nNote\n\nOnly one of `covariance_matrix` or `precision_matrix` or `scale_tril` can be\nspecified.\n\nUsing `scale_tril` will be more efficient: all computations internally are\nbased on `scale_tril`. If `covariance_matrix` or `precision_matrix` is passed\ninstead, it is only used to compute the corresponding lower triangular\nmatrices using a Cholesky decomposition.\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Negative Binomial distribution, i.e. distribution of the number of\nsuccessful independent and identical Bernoulli trials before `total_count`\nfailures are achieved. The probability of failure of each Bernoulli trial is\n`probs`.\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a normal (also called Gaussian) distribution parameterized by `loc`\nand `scale`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a one-hot categorical distribution parameterized by `probs` or\n`logits`.\n\nSamples are one-hot coded vectors of size `probs.size(-1)`.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.distributions.Categorical()` for specifications of `probs`\nand `logits`.\n\nExample:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Pareto Type 1 distribution.\n\nExample:\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Poisson distribution parameterized by `rate`, the rate parameter.\n\nSamples are nonnegative integers, with a pmf given by\n\nExample:\n\nrate (Number, Tensor) \u2013 the rate parameter\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedBernoulli distribution, parametrized by `temperature`, and\neither `probs` or `logits` (but not both). This is a relaxed version of the\n`Bernoulli` distribution, so the values are in (0, 1), and has\nreparametrizable samples.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a LogitRelaxedBernoulli distribution parameterized by `probs` or\n`logits` (but not both), which is the logit of a RelaxedBernoulli\ndistribution.\n\nSamples are logits of values in (0, 1). See [1] for more details.\n\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random\nVariables (Maddison et al, 2017)\n\n[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedOneHotCategorical distribution parametrized by `temperature`,\nand either `probs` or `logits`. This is a relaxed version of the\n`OneHotCategorical` distribution, so its samples are on simplex, and are\nreparametrizable.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Student\u2019s t-distribution parameterized by degree of freedom `df`,\nmean `loc` and scale `scale`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nExtension of the Distribution class, which applies a sequence of Transforms to\na base distribution. Let f be the composition of transforms applied:\n\nNote that the `.event_shape` of a `TransformedDistribution` is the maximum\nshape of its base distribution and its transforms, since transforms can\nintroduce correlations among events.\n\nAn example for the usage of `TransformedDistribution` would be:\n\nFor more examples, please look at the implementations of `Gumbel`,\n`HalfCauchy`, `HalfNormal`, `LogNormal`, `Pareto`, `Weibull`,\n`RelaxedBernoulli` and `RelaxedOneHotCategorical`\n\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\nBases: `torch.distributions.distribution.Distribution`\n\nGenerates uniformly distributed random samples from the half-open interval\n`[low, high)`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nA circular von Mises distribution.\n\nThis implementation uses polar coordinates. The `loc` and `value` args can be\nany real number (to facilitate unconstrained optimization), but are\ninterpreted as angles modulo 2 pi.\n\nThe provided mean is the circular one.\n\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\nThe provided variance is the circular one.\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a two-parameter Weibull distribution.\n\nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two\ndistributions.\n\nA batch of KL divergences of shape `batch_shape`.\n\nTensor\n\nNotImplementedError \u2013 If the distribution types have not been registered via\n`register_kl()`.\n\nDecorator to register a pairwise function with `kl_divergence()`. Usage:\n\nLookup returns the most specific (type,type) match ordered by subclass. If the\nmatch is ambiguous, a `RuntimeWarning` is raised. For example to resolve the\nambiguous situation:\n\nyou should register a third most-specific implementation, e.g.:\n\nAbstract class for invertable transformations with computable log det\njacobians. They are primarily used in\n`torch.distributions.TransformedDistribution`.\n\nCaching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values since\nthe autograd graph may be reversed. For example while the following works with\nor without caching:\n\nHowever the following will error when caching due to dependency reversal:\n\nDerived classes should implement one or both of `_call()` or `_inverse()`.\nDerived classes that set `bijective=True` should also implement\n`log_abs_det_jacobian()`.\n\ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the\nlatest single value is cached. Only 0 and 1 are supported.\n\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\nComposes multiple transforms in a chain. The transforms being composed are\nresponsible for caching.\n\nWrapper around another transform to treat `reinterpreted_batch_ndims`-many\nextra of the right most dimensions as dependent. This has no effect on the\nforward or backward transforms, but does sum out\n`reinterpreted_batch_ndims`-many of the rightmost dimensions in\n`log_abs_det_jacobian()`.\n\nUnit Jacobian transform to reshape the rightmost part of a tensor.\n\nNote that `in_shape` and `out_shape` must have the same number of elements,\njust as for `torch.Tensor.reshape()`.\n\nTransform via the mapping y=exp\u2061(x)y = \\exp(x) .\n\nTransform via the mapping y=xexponenty = x^{\\text{exponent}} .\n\nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and\nx=logit(y)x = \\text{logit}(y) .\n\nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) .\n\nIt is equivalent to `` ComposeTransform([AffineTransform(0., 2.),\nSigmoidTransform(), AffineTransform(-1., 2.)]) `` However this might not be\nnumerically stable, thus it is recommended to use `TanhTransform` instead.\n\nNote that one should use `cache_size=1` when it comes to `NaN/Inf` values.\n\nTransform via the mapping y=\u2223x\u2223y = |x| .\n\nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} +\n\\text{scale} \\times x .\n\nTransforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into\nthe Cholesky factor of a D-dimension correlation matrix. This Cholesky factor\nis a lower triangular matrix with positive diagonals and unit Euclidean norm\nfor each row. The transform is processed as follows:\n\nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)\nthen normalizing.\n\nThis is not bijective and cannot be used for HMC. However this acts mostly\ncoordinate-wise (except for the final normalization), and thus is appropriate\nfor coordinate-wise optimization algorithms.\n\nTransform from unconstrained space to the simplex of one additional dimension\nvia a stick-breaking process.\n\nThis transform arises as an iterated sigmoid transform in a stick-breaking\nconstruction of the `Dirichlet` distribution: the first logit is transformed\nvia sigmoid to the first probability and the probability of everything else,\nand then the process recurses.\n\nThis is bijective and appropriate for use in HMC; however it mixes coordinates\ntogether and is less appropriate for optimization.\n\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\n\nThis is useful for parameterizing positive definite matrices in terms of their\nCholesky factorization.\n\nTransform functor that applies a sequence of transforms `tseq` component-wise\nto each submatrix at `dim` in a way compatible with `torch.stack()`.\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t =\nStackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)\n\nThe following constraints are implemented:\n\nAbstract base class for constraints.\n\nA constraint object represents a region over which a variable is valid, e.g.\nwithin which a variable can be optimized.\n\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\nalias of `torch.distributions.constraints._DependentProperty`\n\nalias of `torch.distributions.constraints._IndependentConstraint`\n\nalias of `torch.distributions.constraints._IntegerInterval`\n\nalias of `torch.distributions.constraints._GreaterThan`\n\nalias of `torch.distributions.constraints._GreaterThanEq`\n\nalias of `torch.distributions.constraints._LessThan`\n\nalias of `torch.distributions.constraints._Multinomial`\n\nalias of `torch.distributions.constraints._Interval`\n\nalias of `torch.distributions.constraints._HalfOpenInterval`\n\nalias of `torch.distributions.constraints._Cat`\n\nalias of `torch.distributions.constraints._Stack`\n\nPyTorch provides two global `ConstraintRegistry` objects that link\n`Constraint` objects to `Transform` objects. These objects both input\nconstraints and return transforms, but they have different guarantees on\nbijectivity.\n\nThe `transform_to()` registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s `.arg_constraints` dict. These transforms\noften overparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:\n\nThe `biject_to()` registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained `.support` are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:\n\nNote\n\nAn example where `transform_to` and `biject_to` differ is\n`constraints.simplex`: `transform_to(constraints.simplex)` returns a\n`SoftmaxTransform` that simply exponentiates and normalizes its inputs; this\nis a cheap and mostly coordinate-wise operation appropriate for algorithms\nlike SVI. In contrast, `biject_to(constraints.simplex)` returns a\n`StickBreakingTransform` that bijects its input down to a one-fewer-\ndimensional space; this a more expensive less numerically stable transform but\nis needed for algorithms like HMC.\n\nThe `biject_to` and `transform_to` objects can be extended by user-defined\nconstraints and transforms using their `.register()` method either as a\nfunction on singleton constraints:\n\nor as a decorator on parameterized constraints:\n\nYou can create your own registry by creating a new `ConstraintRegistry`\nobject.\n\nRegistry to link constraints to transforms.\n\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Bernoulli distribution parameterized by `probs` or `logits` (but not\nboth).\n\nSamples are binary (0 or 1). They take the value `1` with probability `p` and\n`0` with probability `1 - p`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.mean()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.variance()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nBeta distribution parameterized by `concentration1` and `concentration0`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.concentration0()", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.concentration1()", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.mean()", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.variance()", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Binomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). `total_count` must be broadcastable with\n`probs`/`logits`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.mean()", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.param_shape()", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.support()", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.variance()", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a categorical distribution parameterized by either `probs` or `logits`\n(but not both).\n\nNote\n\nIt is equivalent to the distribution that `torch.multinomial()` samples from.\n\nSamples are integers from {0,\u2026,K\u22121}\\\\{0, \\ldots, K-1\\\\} where `K` is\n`probs.size(-1)`.\n\nIf `probs` is 1-dimensional with length-`K`, each element is the relative\nprobability of sampling the class at that index.\n\nIf `probs` is N-dimensional, the first N-1 dimensions are treated as a batch\nof relative probability vectors.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.multinomial()`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.mean()", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.param_shape()", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.support()", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.variance()", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means `0` follows a\nCauchy distribution.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.mean()", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.variance()", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "torch.distributions", "text": "\nBases: `torch.distributions.gamma.Gamma`\n\nCreates a Chi2 distribution parameterized by shape parameter `df`. This is\nexactly equivalent to `Gamma(alpha=0.5*df, beta=0.5)`\n\nExample:\n\ndf (float or Tensor) \u2013 shape parameter of the distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.df()", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Cat`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "torch.distributions", "text": "\nAbstract base class for constraints.\n\nA constraint object represents a region over which a variable is valid, e.g.\nwithin which a variable can be optimized.\n\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "torch.distributions", "text": "\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._DependentProperty`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._GreaterThan`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._GreaterThanEq`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._HalfOpenInterval`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._IndependentConstraint`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._IntegerInterval`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Interval`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._LessThan`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Multinomial`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Stack`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "torch.distributions", "text": "\nRegistry to link constraints to transforms.\n\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "torch.distributions", "text": "\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a continuous Bernoulli distribution parameterized by `probs` or\n`logits` (but not both).\n\nThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to log-\nodds, but the same names are used due to the similarity with the Bernoulli.\nSee [1] for more details.\n\nExample:\n\n[1] The continuous Bernoulli: fixing a pervasive error in variational\nautoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.\nhttps://arxiv.org/abs/1907.06845\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Dirichlet distribution parameterized by concentration\n`concentration`.\n\nExample:\n\nconcentration (Tensor) \u2013 concentration parameter of the distribution (often\nreferred to as alpha)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.mean()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.variance()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "torch.distributions", "text": "\nBases: `object`\n\nDistribution is the abstract base class for probability distributions.\n\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\nReturns the shape over which parameters are batched.\n\nReturns the cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns entropy of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step `[[0, 0], [1,\n1], \u2026]`. With `expand=False`, enumeration happens along dim 0, but with the\nremaining batch dimensions being singleton dimensions, `[[0], [1], ..`.\n\nTo iterate over the full Cartesian product use\n`itertools.product(m.enumerate_support())`.\n\nexpand (bool) \u2013 whether to expand the support over the batch dims to match the\ndistribution\u2019s `batch_shape`.\n\nTensor iterating over dimension 0.\n\nReturns the shape of a single sample (without batching).\n\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\nNew distribution instance with batch dimensions expanded to `batch_size`.\n\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the log of the probability density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the mean of the distribution.\n\nReturns perplexity of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python\u2019s `assert` statement: validation is on by\ndefault, but is disabled if Python is run in optimized mode (via `python -O`).\nValidation may be expensive, so you may want to disable it once a model is\nworking.\n\nvalue (bool) \u2013 Whether to enable validation.\n\nReturns the standard deviation of the distribution.\n\nReturns a `Constraint` object representing this distribution\u2019s support.\n\nReturns the variance of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.arg_constraints()", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "torch.distributions", "text": "\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.batch_shape()", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "torch.distributions", "text": "\nReturns the shape over which parameters are batched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "torch.distributions", "text": "\nReturns the cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "torch.distributions", "text": "\nReturns entropy of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "torch.distributions", "text": "\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step `[[0, 0], [1,\n1], \u2026]`. With `expand=False`, enumeration happens along dim 0, but with the\nremaining batch dimensions being singleton dimensions, `[[0], [1], ..`.\n\nTo iterate over the full Cartesian product use\n`itertools.product(m.enumerate_support())`.\n\nexpand (bool) \u2013 whether to expand the support over the batch dims to match the\ndistribution\u2019s `batch_shape`.\n\nTensor iterating over dimension 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.event_shape()", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "torch.distributions", "text": "\nReturns the shape of a single sample (without batching).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "torch.distributions", "text": "\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\nNew distribution instance with batch dimensions expanded to `batch_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "torch.distributions", "text": "\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "torch.distributions", "text": "\nReturns the log of the probability density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.mean()", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "torch.distributions", "text": "\nReturns the mean of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "torch.distributions", "text": "\nReturns perplexity of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "torch.distributions", "text": "\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "torch.distributions", "text": "\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python\u2019s `assert` statement: validation is on by\ndefault, but is disabled if Python is run in optimized mode (via `python -O`).\nValidation may be expensive, so you may want to disable it once a model is\nworking.\n\nvalue (bool) \u2013 Whether to enable validation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.stddev()", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "torch.distributions", "text": "\nReturns the standard deviation of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.support()", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "torch.distributions", "text": "\nReturns a `Constraint` object representing this distribution\u2019s support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.variance()", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "torch.distributions", "text": "\nReturns the variance of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Exponential distribution parameterized by `rate`.\n\nExample:\n\nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.mean()", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.stddev()", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.variance()", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nExponentialFamily is the abstract base class for probability distributions\nbelonging to an exponential family, whose probability mass/density function\nhas the form is defined below\n\nwhere \u03b8\\theta denotes the natural parameters, t(x)t(x) denotes the sufficient\nstatistic, F(\u03b8)F(\\theta) is the log normalizer function for a given family and\nk(x)k(x) is the carrier measure.\n\nNote\n\nThis class is an intermediary between the `Distribution` class and\ndistributions which belong to an exponential family mainly to check the\ncorrectness of the `.entropy()` and analytic KL divergence methods. We use\nthis class to compute the entropy and KL divergence using the AD framework and\nBregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies\nand Cross-entropies of Exponential Families).\n\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "torch.distributions", "text": "\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Fisher-Snedecor distribution parameterized by `df1` and `df2`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Gamma distribution parameterized by shape `concentration` and\n`rate`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.mean()", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.variance()", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Geometric distribution parameterized by `probs`, where `probs` is\nthe probability of success of Bernoulli trials. It represents the probability\nthat in k+1k + 1 Bernoulli trials, the first kk trials failed, before seeing a\nsuccess.\n\nSamples are non-negative integers [0, inf\u2061\\inf ).\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.mean()", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.variance()", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Gumbel Distribution.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.mean()", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.stddev()", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.variance()", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-Cauchy distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Cauchy distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-normal distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Normal distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.mean()", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.scale()", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.variance()", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nReinterprets some of the batch dims of a distribution as event dims.\n\nThis is mainly useful for changing the shape of the result of `log_prob()`.\nFor example to create a diagonal Normal distribution with the same shape as a\nMultivariate Normal distribution (so they are interchangeable), you can:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.has_enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.has_rsample()", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.mean()", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.support()", "path": "distributions#torch.distributions.independent.Independent.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.variance()", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "torch.distributions", "text": "\nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two\ndistributions.\n\nA batch of KL divergences of shape `batch_shape`.\n\nTensor\n\nNotImplementedError \u2013 If the distribution types have not been registered via\n`register_kl()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "torch.distributions", "text": "\nDecorator to register a pairwise function with `kl_divergence()`. Usage:\n\nLookup returns the most specific (type,type) match ordered by subclass. If the\nmatch is ambiguous, a `RuntimeWarning` is raised. For example to resolve the\nambiguous situation:\n\nyou should register a third most-specific implementation, e.g.:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Kumaraswamy distribution.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Laplace distribution parameterized by `loc` and `scale`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.mean()", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.stddev()", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.variance()", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nLKJ distribution for lower Cholesky factor of correlation matrices. The\ndistribution is controlled by `concentration` parameter \u03b7\\eta to make the\nprobability of the correlation matrix MM generated from a Cholesky factor\npropotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when\n`concentration == 1`, we have a uniform distribution over Cholesky factors of\ncorrelation matrices. Note that this distribution samples the Cholesky factor\nof correlation matrices and not the correlation matrices themselves and\nthereby differs slightly from the derivations in [1] for the `LKJCorr`\ndistribution. For sampling, this uses the Onion method from [1] Section 3.\n\nL ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration)\n\nExample:\n\nReferences\n\n[1] `Generating random correlation matrices based on vines and extended onion\nmethod`, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a log-normal distribution parameterized by `loc` and `scale` where:\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.loc()", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.mean()", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.scale()", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.variance()", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal distribution with covariance matrix having a\nlow-rank form parameterized by `cov_factor` and `cov_diag`:\n\nNote\n\nThe computation for determinant and inverse of covariance matrix is avoided\nwhen `cov_factor.shape[1] << cov_factor.shape[0]` thanks to Woodbury matrix\nidentity and matrix determinant lemma. Thanks to these formulas, we just need\nto compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nThe `MixtureSameFamily` distribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of the\nsame distribution type. It is parameterized by a `Categorical` \u201cselecting\ndistribution\u201d (over `k` component) and a component distribution, i.e., a\n`Distribution` with a rightmost batch shape (equal to `[k]`) which indexes\neach (batch of) component.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Multinomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). The innermost dimension of `probs` indexes\nover categories. All other dimensions index over batches.\n\nNote that `total_count` need not be specified if only `log_prob()` is called\n(see example below)\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.logits()", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.mean()", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.param_shape()", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.probs()", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.support()", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.variance()", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal (also called Gaussian) distribution\nparameterized by a mean vector and a covariance matrix.\n\nThe multivariate normal distribution can be parameterized either in terms of a\npositive definite covariance matrix \u03a3\\mathbf{\\Sigma} or a positive definite\nprecision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1} or a lower-triangular matrix\nL\\mathbf{L} with positive-valued diagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can\nbe obtained via e.g. Cholesky decomposition of the covariance.\n\nNote\n\nOnly one of `covariance_matrix` or `precision_matrix` or `scale_tril` can be\nspecified.\n\nUsing `scale_tril` will be more efficient: all computations internally are\nbased on `scale_tril`. If `covariance_matrix` or `precision_matrix` is passed\ninstead, it is only used to compute the corresponding lower triangular\nmatrices using a Cholesky decomposition.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Negative Binomial distribution, i.e. distribution of the number of\nsuccessful independent and identical Bernoulli trials before `total_count`\nfailures are achieved. The probability of failure of each Bernoulli trial is\n`probs`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a normal (also called Gaussian) distribution parameterized by `loc`\nand `scale`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.mean()", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.stddev()", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.variance()", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a one-hot categorical distribution parameterized by `probs` or\n`logits`.\n\nSamples are one-hot coded vectors of size `probs.size(-1)`.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.distributions.Categorical()` for specifications of `probs`\nand `logits`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Pareto Type 1 distribution.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.mean()", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.support()", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.variance()", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Poisson distribution parameterized by `rate`, the rate parameter.\n\nSamples are nonnegative integers, with a pmf given by\n\nExample:\n\nrate (Number, Tensor) \u2013 the rate parameter\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.mean()", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.variance()", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a LogitRelaxedBernoulli distribution parameterized by `probs` or\n`logits` (but not both), which is the logit of a RelaxedBernoulli\ndistribution.\n\nSamples are logits of values in (0, 1). See [1] for more details.\n\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random\nVariables (Maddison et al, 2017)\n\n[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedBernoulli distribution, parametrized by `temperature`, and\neither `probs` or `logits` (but not both). This is a relaxed version of the\n`Bernoulli` distribution, so the values are in (0, 1), and has\nreparametrizable samples.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedOneHotCategorical distribution parametrized by `temperature`,\nand either `probs` or `logits`. This is a relaxed version of the\n`OneHotCategorical` distribution, so its samples are on simplex, and are\nreparametrizable.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Student\u2019s t-distribution parameterized by degree of freedom `df`,\nmean `loc` and scale `scale`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.mean()", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.variance()", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nExtension of the Distribution class, which applies a sequence of Transforms to\na base distribution. Let f be the composition of transforms applied:\n\nNote that the `.event_shape` of a `TransformedDistribution` is the maximum\nshape of its base distribution and its transforms, since transforms can\nintroduce correlations among events.\n\nAn example for the usage of `TransformedDistribution` would be:\n\nFor more examples, please look at the implementations of `Gumbel`,\n`HalfCauchy`, `HalfNormal`, `LogNormal`, `Pareto`, `Weibull`,\n`RelaxedBernoulli` and `RelaxedOneHotCategorical`\n\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "torch.distributions", "text": "\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "torch.distributions", "text": "\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "torch.distributions", "text": "\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=\u2223x\u2223y = |x| .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "torch.distributions", "text": "\nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} +\n\\text{scale} \\times x .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "torch.distributions", "text": "\nComposes multiple transforms in a chain. The transforms being composed are\nresponsible for caching.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "torch.distributions", "text": "\nTransforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into\nthe Cholesky factor of a D-dimension correlation matrix. This Cholesky factor\nis a lower triangular matrix with positive diagonals and unit Euclidean norm\nfor each row. The transform is processed as follows:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=exp\u2061(x)y = \\exp(x) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "torch.distributions", "text": "\nWrapper around another transform to treat `reinterpreted_batch_ndims`-many\nextra of the right most dimensions as dependent. This has no effect on the\nforward or backward transforms, but does sum out\n`reinterpreted_batch_ndims`-many of the rightmost dimensions in\n`log_abs_det_jacobian()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\n\nThis is useful for parameterizing positive definite matrices in terms of their\nCholesky factorization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=xexponenty = x^{\\text{exponent}} .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "torch.distributions", "text": "\nUnit Jacobian transform to reshape the rightmost part of a tensor.\n\nNote that `in_shape` and `out_shape` must have the same number of elements,\njust as for `torch.Tensor.reshape()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and\nx=logit(y)x = \\text{logit}(y) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)\nthen normalizing.\n\nThis is not bijective and cannot be used for HMC. However this acts mostly\ncoordinate-wise (except for the final normalization), and thus is appropriate\nfor coordinate-wise optimization algorithms.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "torch.distributions", "text": "\nTransform functor that applies a sequence of transforms `tseq` component-wise\nto each submatrix at `dim` in a way compatible with `torch.stack()`.\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t =\nStackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained space to the simplex of one additional dimension\nvia a stick-breaking process.\n\nThis transform arises as an iterated sigmoid transform in a stick-breaking\nconstruction of the `Dirichlet` distribution: the first logit is transformed\nvia sigmoid to the first probability and the probability of everything else,\nand then the process recurses.\n\nThis is bijective and appropriate for use in HMC; however it mixes coordinates\ntogether and is less appropriate for optimization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) .\n\nIt is equivalent to `` ComposeTransform([AffineTransform(0., 2.),\nSigmoidTransform(), AffineTransform(-1., 2.)]) `` However this might not be\nnumerically stable, thus it is recommended to use `TanhTransform` instead.\n\nNote that one should use `cache_size=1` when it comes to `NaN/Inf` values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "torch.distributions", "text": "\nAbstract class for invertable transformations with computable log det\njacobians. They are primarily used in\n`torch.distributions.TransformedDistribution`.\n\nCaching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values since\nthe autograd graph may be reversed. For example while the following works with\nor without caching:\n\nHowever the following will error when caching due to dependency reversal:\n\nDerived classes should implement one or both of `_call()` or `_inverse()`.\nDerived classes that set `bijective=True` should also implement\n`log_abs_det_jacobian()`.\n\ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the\nlatest single value is cached. Only 0 and 1 are supported.\n\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "torch.distributions", "text": "\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.inv()", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "torch.distributions", "text": "\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "torch.distributions", "text": "\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "torch.distributions", "text": "\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.sign()", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "torch.distributions", "text": "\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nGenerates uniformly distributed random samples from the half-open interval\n`[low, high)`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.mean()", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.stddev()", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.support()", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.variance()", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nA circular von Mises distribution.\n\nThis implementation uses polar coordinates. The `loc` and `value` args can be\nany real number (to facilitate unconstrained optimization), but are\ninterpreted as angles modulo 2 pi.\n\nThe provided mean is the circular one.\n\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\nThe provided variance is the circular one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.mean()", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "torch.distributions", "text": "\nThe provided mean is the circular one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "torch.distributions", "text": "\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "torch.distributions", "text": "\nThe provided variance is the circular one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a two-parameter Weibull distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.mean()", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.variance()", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.div()", "path": "generated/torch.div#torch.div", "type": "torch", "text": "\nDivides each element of the input `input` by the corresponding element of\n`other`.\n\nNote\n\nBy default, this performs a \u201ctrue\u201d division like Python 3. See the\n`rounding_mode` argument for floor division.\n\nSupports broadcasting to a common shape, type promotion, and integer, float,\nand complex inputs. Always promotes integer types to the default scalar type.\n\nrounding_mode (str, optional) \u2013\n\nType of rounding applied to the result:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.divide()", "path": "generated/torch.divide#torch.divide", "type": "torch", "text": "\nAlias for `torch.div()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dot()", "path": "generated/torch.dot#torch.dot", "type": "torch", "text": "\nComputes the dot product of two 1D tensors.\n\nNote\n\nUnlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot\nproduct of two 1D tensors with the same number of elements.\n\n{out} \u2013\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dstack()", "path": "generated/torch.dstack#torch.dstack", "type": "torch", "text": "\nStack tensors in sequence depthwise (along third axis).\n\nThis is equivalent to concatenation along the third axis after 1-D and 2-D\ntensors have been reshaped by `torch.atleast_3d()`.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.eig()", "path": "generated/torch.eig#torch.eig", "type": "torch", "text": "\nComputes the eigenvalues and eigenvectors of a real square matrix.\n\nNote\n\nSince eigenvalues and eigenvectors might be complex, backward pass is\nsupported only if eigenvalues and eigenvectors are all real valued.\n\nWhen `input` is on CUDA, `torch.eig()` causes host-device synchronization.\n\nout (tuple, optional) \u2013 the output tensors\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\n(Tensor, Tensor)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.einsum()", "path": "generated/torch.einsum#torch.einsum", "type": "torch", "text": "\nSums the product of the elements of the input `operands` along dimensions\nspecified using a notation based on the Einstein summation convention.\n\nEinsum allows computing many common multi-dimensional linear algebraic array\noperations by representing them in a short-hand format based on the Einstein\nsummation convention, given by `equation`. The details of this format are\ndescribed below, but the general idea is to label every dimension of the input\n`operands` with some subscript and define which subscripts are part of the\noutput. The output is then computed by summing the product of the elements of\nthe `operands` along the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum as\n`torch.einsum(\u201cij,jk->ik\u201d, A, B)`. Here, j is the summation subscript and i\nand k the output subscripts (see section below for more details on why).\n\nEquation:\n\nThe `equation` string specifies the subscripts (lower case letters `[\u2018a\u2019,\n\u2018z\u2019]`) for each dimension of the input `operands` in the same order as the\ndimensions, separating subcripts for each operand by a comma (\u2018,\u2019), e.g.\n`\u2018ij,jk\u2019` specify subscripts for two 2D operands. The dimensions labeled with\nthe same subscript must be broadcastable, that is, their size must either\nmatch or be `1`. The exception is if a subscript is repeated for the same\ninput operand, in which case the dimensions labeled with this subscript for\nthis operand must match in size and the operand will be replaced by its\ndiagonal along these dimensions. The subscripts that appear exactly once in\nthe `equation` will be part of the output, sorted in increasing alphabetical\norder. The output is computed by multiplying the input `operands` element-\nwise, with their dimensions aligned based on the subscripts, and then summing\nout the dimensions whose subscripts are not part of the output.\n\nOptionally, the output subscripts can be explicitly defined by adding an arrow\n(\u2018->\u2019) at the end of the equation followed by the subscripts for the output.\nFor instance, the following equation computes the transpose of a matrix\nmultiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once\nfor some input operand and at most once for the output.\n\nEllipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions\ncovered by the ellipsis. Each input operand may contain at most one ellipsis\nwhich will cover the dimensions not covered by subscripts, e.g. for an input\noperand with 5 dimensions, the ellipsis in the equation `\u2018ab\u2026c\u2019` cover the\nthird and fourth dimensions. The ellipsis does not need to cover the same\nnumber of dimensions across the `operands` but the \u2018shape\u2019 of the ellipsis\n(the size of the dimensions covered by them) must broadcast together. If the\noutput is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis\nwill come first in the output (left-most dimensions), before the subscript\nlabels that appear exactly once for the input operands. e.g. the following\nequation implements batch matrix multiplication `\u2018\u2026ij,\u2026jk\u2019`.\n\nA few final notes: the equation may contain whitespaces between the different\nelements (subscripts, ellipsis, arrow and comma) but something like `\u2018\u2026\u2019` is\nnot valid. An empty string `\u2018\u2019` is valid for scalar operands.\n\nNote\n\n`torch.einsum` handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows\ndimensions covered by the ellipsis to be summed over, that is, ellipsis are\nnot required to be part of the output.\n\nNote\n\nThis function does not optimize the given expression, so a different formula\nfor the same computation may run faster or consume less memory. Projects like\nopt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) can optimize\nthe formula for you.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.empty()", "path": "generated/torch.empty#torch.empty", "type": "torch", "text": "\nReturns a tensor filled with uninitialized data. The shape of the tensor is\ndefined by the variable argument `size`.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor. Can be a variable number of arguments or a collection like a list or\ntuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.empty_like()", "path": "generated/torch.empty_like#torch.empty_like", "type": "torch", "text": "\nReturns an uninitialized tensor with the same size as `input`.\n`torch.empty_like(input)` is equivalent to `torch.empty(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\ninput (Tensor) \u2013 the size of `input` will determine size of the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.empty_strided()", "path": "generated/torch.empty_strided#torch.empty_strided", "type": "torch", "text": "\nReturns a tensor filled with uninitialized data. The shape and strides of the\ntensor is defined by the variable argument `size` and `stride` respectively.\n`torch.empty_strided(size, stride)` is equivalent to\n`torch.empty(size).as_strided(size, stride)`.\n\nWarning\n\nMore than one element of the created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "torch", "text": "\nContext-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via `no_grad` or\n`set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.eq()", "path": "generated/torch.eq#torch.eq", "type": "torch", "text": "\nComputes element-wise equality\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is equal to `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.equal()", "path": "generated/torch.equal#torch.equal", "type": "torch", "text": "\n`True` if two tensors have the same size and elements, `False` otherwise.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.erf()", "path": "generated/torch.erf#torch.erf", "type": "torch", "text": "\nComputes the error function of each element. The error function is defined as\nfollows:\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.erfc()", "path": "generated/torch.erfc#torch.erfc", "type": "torch", "text": "\nComputes the complementary error function of each element of `input`. The\ncomplementary error function is defined as follows:\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.erfinv()", "path": "generated/torch.erfinv#torch.erfinv", "type": "torch", "text": "\nComputes the inverse error function of each element of `input`. The inverse\nerror function is defined in the range (\u22121,1)(-1, 1) as:\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.exp()", "path": "generated/torch.exp#torch.exp", "type": "torch", "text": "\nReturns a new tensor with the exponential of the elements of the input tensor\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.exp2()", "path": "generated/torch.exp2#torch.exp2", "type": "torch", "text": "\nComputes the base two exponential function of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.expm1()", "path": "generated/torch.expm1#torch.expm1", "type": "torch", "text": "\nReturns a new tensor with the exponential of the elements minus 1 of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.eye()", "path": "generated/torch.eye#torch.eye", "type": "torch", "text": "\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\nA 2-D tensor with ones on the diagonal and zeros elsewhere\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fake_quantize_per_channel_affine()", "path": "generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine", "type": "torch", "text": "\nReturns a new tensor with the data in `input` fake quantized per channel using\n`scale`, `zero_point`, `quant_min` and `quant_max`, across the channel\nspecified by `axis`.\n\nA newly fake_quantized per channel tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fake_quantize_per_tensor_affine()", "path": "generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine", "type": "torch", "text": "\nReturns a new tensor with the data in `input` fake quantized using `scale`,\n`zero_point`, `quant_min` and `quant_max`.\n\nA newly fake_quantized tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft", "path": "fft", "type": "torch.fft", "text": "\nDiscrete Fourier transforms and related functions.\n\nComputes the one dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i] = conj(X[-i])`. This function always returns both the positive\nand negative frequency terms even though, for real inputs, the negative\nfrequencies are redundant. `rfft()` returns the more compact one-sided\nrepresentation where only the positive frequencies are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft()`), these correspond to:\n\nCalling the backward transform (`ifft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nComputes the one dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft()`), these correspond\nto:\n\nCalling the forward transform (`fft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nComputes the 2 dimensional discrete Fourier transform of `input`. Equivalent\nto `fftn()` but FFTs only the last two dimensions by default.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i, j] = conj(X[-i, -j])`. This function always returns all\npositive and negative frequency terms even though, for real inputs, half of\nthese values are redundant. `rfft2()` returns the more compact one-sided\nrepresentation where only the positive frequencies of the last dimension are\nreturned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft2()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fft2()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\nComputes the 2 dimensional inverse discrete Fourier transform of `input`.\nEquivalent to `ifftn()` but IFFTs only the last two dimensions by default.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifft2()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\nComputes the N dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])`. This function always\nreturns all positive and negative frequency terms even though, for real\ninputs, half of these values are redundant. `rfftn()` returns the more compact\none-sided representation where only the positive frequencies of the last\ndimension are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fftn()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fftn()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\nComputes the N dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifftn()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\nComputes the one dimensional Fourier transform of real-valued `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])` so the\noutput contains only the positive frequencies below the Nyquist frequency. To\ncompute the full output, use `fft()`\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft()`), these correspond to:\n\nCalling the backward transform (`irfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompare against the full output from `fft()`:\n\nNotice that the symmetric element `T[-1] == T[1].conj()` is omitted. At the\nNyquist frequency `T[-2] == T[2]` is it\u2019s own symmetric pair, and therefore\nmust always be real-valued.\n\nComputes the inverse of `rfft()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft()`), these correspond\nto:\n\nCalling the forward transform (`rfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length:\n\nSo, it is recommended to always pass the signal length `n`:\n\nComputes the 2-dimensional discrete Fourier transform of real `input`.\nEquivalent to `rfftn()` but FFTs only the last two dimensions by default.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i, j] = conj(X[-i, -j])`,\nso the full `fft2()` output contains redundant information. `rfft2()` instead\nomits the negative frequencies in the last dimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fft2()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfft2()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\nComputes the inverse of `rfft2()`. Equivalent to `irfftn()` but IFFTs only the\nlast two dimensions by default.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft2()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft2()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\nComputes the N-dimensional discrete Fourier transform of real `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i_1, ..., i_n] =\nconj(X[-i_1, ..., -i_n])` so the full `fftn()` output contains redundant\ninformation. `rfftn()` instead omits the negative frequencies in the last\ndimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fftn()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfftn()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\nComputes the inverse of `rfftn()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfftn()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\nComputes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric `input` signal.\n\nNote\n\n`hfft()`/`ihfft()` are analogous to `rfft()`/`irfft()`. The real FFT expects a\nreal signal in the time-domain and gives a Hermitian symmetry in the\nfrequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in\nthe time-domain and real-valued in the frequency-domain. For this reason,\nspecial care needs to be taken with the length argument `n`, in the same way\nas with `irfft()`.\n\nNote\n\nBecause the signal is Hermitian in the time-domain, the result will be real in\nthe frequency domain. Note that some input frequencies must be real-valued to\nsatisfy the Hermitian property. In these cases the imaginary component will be\nignored. For example, any imaginary component in `input[0]` would result in\none or more complex frequency terms which cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`hfft()`), these correspond to:\n\nCalling the backward transform (`ihfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nTaking a real-valued frequency signal and bringing it into the time domain\ngives Hermitian symmetric output:\n\nNote that `T[1] == T[-1].conj()` and `T[2] == T[-2].conj()` is redundant. We\ncan thus compute the forward transform without considering negative\nfrequencies:\n\nLike with `irfft()`, the output length must be given in order to recover an\neven length output:\n\nComputes the inverse of `hfft()`.\n\n`input` must be a real-valued signal, interpreted in the Fourier domain. The\nIFFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])`. `ihfft()`\nrepresents this in the one-sided form where only the positive frequencies\nbelow the Nyquist frequency are included. To compute the full output, use\n`ifft()`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ihfft()`), these correspond\nto:\n\nCalling the forward transform (`hfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nCompare against the full output from `ifft()`:\n\nComputes the discrete Fourier Transform sample frequencies for a signal of\nsize `n`.\n\nNote\n\nBy convention, `fft()` returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. For an FFT of length\n`n` and with inputs spaced in length unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftfreq()` follows NumPy\u2019s convention of taking\nit to be negative.\n\nFor even input, we can see the Nyquist frequency at `f[2]` is given as\nnegative:\n\nComputes the sample frequencies for `rfft()` with a signal of size `n`.\n\nNote\n\n`rfft()` returns Hermitian one-sided output, so only the positive frequency\nterms are returned. For a real FFT of length `n` and with inputs spaced in\nlength unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. Unlike `fftfreq()`, `rfftfreq()` always returns\nit as positive.\n\nCompared to the output from `fftfreq()`, we see that the Nyquist frequency at\n`f[2]` has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500,\n-0.5000, -0.2500])\n\nReorders n-dimensional FFT data, as provided by `fftn()`, to have negative\nfrequency terms first.\n\nThis performs a periodic shift of n-dimensional data such that the origin `(0,\n..., 0)` is moved to the center of the tensor. Specifically, to\n`input.shape[dim] // 2` in each selected dimension.\n\nNote\n\nBy convention, the FFT returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. `fftshift()` rearranges\nall frequencies into ascending order from negative to positive with the zero-\nfrequency term in the center.\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftshift()` always puts the Nyquist term at the\n0-index. This is the same convention used by `fftfreq()`.\n\nAlso notice that the Nyquist frequency term at `f[2]` was moved to the\nbeginning of the tensor.\n\nThis also works for multi-dimensional transforms:\n\n`fftshift()` can also be useful for spatial data. If our data is defined on a\ncentered grid (`[-(N//2), (N-1)//2]`) then we can use the standard FFT defined\non an uncentered grid (`[0, N)`) by first applying an `ifftshift()`.\n\nSimilarly, we can convert the frequency domain components to centered\nconvention by applying `fftshift()`.\n\nThe inverse transform, from centered Fourier space back to centered spatial\ndata, can be performed by applying the inverse shifts in reverse order:\n\nInverse of `fftshift()`.\n\nA round-trip through `fftshift()` and `ifftshift()` gives the same result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fft()", "path": "fft#torch.fft.fft", "type": "torch.fft", "text": "\nComputes the one dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i] = conj(X[-i])`. This function always returns both the positive\nand negative frequency terms even though, for real inputs, the negative\nfrequencies are redundant. `rfft()` returns the more compact one-sided\nrepresentation where only the positive frequencies are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft()`), these correspond to:\n\nCalling the backward transform (`ifft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fft2()", "path": "fft#torch.fft.fft2", "type": "torch.fft", "text": "\nComputes the 2 dimensional discrete Fourier transform of `input`. Equivalent\nto `fftn()` but FFTs only the last two dimensions by default.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i, j] = conj(X[-i, -j])`. This function always returns all\npositive and negative frequency terms even though, for real inputs, half of\nthese values are redundant. `rfft2()` returns the more compact one-sided\nrepresentation where only the positive frequencies of the last dimension are\nreturned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft2()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fft2()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fftfreq()", "path": "fft#torch.fft.fftfreq", "type": "torch.fft", "text": "\nComputes the discrete Fourier Transform sample frequencies for a signal of\nsize `n`.\n\nNote\n\nBy convention, `fft()` returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. For an FFT of length\n`n` and with inputs spaced in length unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftfreq()` follows NumPy\u2019s convention of taking\nit to be negative.\n\nFor even input, we can see the Nyquist frequency at `f[2]` is given as\nnegative:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fftn()", "path": "fft#torch.fft.fftn", "type": "torch.fft", "text": "\nComputes the N dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])`. This function always\nreturns all positive and negative frequency terms even though, for real\ninputs, half of these values are redundant. `rfftn()` returns the more compact\none-sided representation where only the positive frequencies of the last\ndimension are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fftn()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fftn()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fftshift()", "path": "fft#torch.fft.fftshift", "type": "torch.fft", "text": "\nReorders n-dimensional FFT data, as provided by `fftn()`, to have negative\nfrequency terms first.\n\nThis performs a periodic shift of n-dimensional data such that the origin `(0,\n..., 0)` is moved to the center of the tensor. Specifically, to\n`input.shape[dim] // 2` in each selected dimension.\n\nNote\n\nBy convention, the FFT returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. `fftshift()` rearranges\nall frequencies into ascending order from negative to positive with the zero-\nfrequency term in the center.\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftshift()` always puts the Nyquist term at the\n0-index. This is the same convention used by `fftfreq()`.\n\nAlso notice that the Nyquist frequency term at `f[2]` was moved to the\nbeginning of the tensor.\n\nThis also works for multi-dimensional transforms:\n\n`fftshift()` can also be useful for spatial data. If our data is defined on a\ncentered grid (`[-(N//2), (N-1)//2]`) then we can use the standard FFT defined\non an uncentered grid (`[0, N)`) by first applying an `ifftshift()`.\n\nSimilarly, we can convert the frequency domain components to centered\nconvention by applying `fftshift()`.\n\nThe inverse transform, from centered Fourier space back to centered spatial\ndata, can be performed by applying the inverse shifts in reverse order:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.hfft()", "path": "fft#torch.fft.hfft", "type": "torch.fft", "text": "\nComputes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric `input` signal.\n\nNote\n\n`hfft()`/`ihfft()` are analogous to `rfft()`/`irfft()`. The real FFT expects a\nreal signal in the time-domain and gives a Hermitian symmetry in the\nfrequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in\nthe time-domain and real-valued in the frequency-domain. For this reason,\nspecial care needs to be taken with the length argument `n`, in the same way\nas with `irfft()`.\n\nNote\n\nBecause the signal is Hermitian in the time-domain, the result will be real in\nthe frequency domain. Note that some input frequencies must be real-valued to\nsatisfy the Hermitian property. In these cases the imaginary component will be\nignored. For example, any imaginary component in `input[0]` would result in\none or more complex frequency terms which cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`hfft()`), these correspond to:\n\nCalling the backward transform (`ihfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nTaking a real-valued frequency signal and bringing it into the time domain\ngives Hermitian symmetric output:\n\nNote that `T[1] == T[-1].conj()` and `T[2] == T[-2].conj()` is redundant. We\ncan thus compute the forward transform without considering negative\nfrequencies:\n\nLike with `irfft()`, the output length must be given in order to recover an\neven length output:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifft()", "path": "fft#torch.fft.ifft", "type": "torch.fft", "text": "\nComputes the one dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft()`), these correspond\nto:\n\nCalling the forward transform (`fft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifft2()", "path": "fft#torch.fft.ifft2", "type": "torch.fft", "text": "\nComputes the 2 dimensional inverse discrete Fourier transform of `input`.\nEquivalent to `ifftn()` but IFFTs only the last two dimensions by default.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifft2()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifftn()", "path": "fft#torch.fft.ifftn", "type": "torch.fft", "text": "\nComputes the N dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifftn()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifftshift()", "path": "fft#torch.fft.ifftshift", "type": "torch.fft", "text": "\nInverse of `fftshift()`.\n\nA round-trip through `fftshift()` and `ifftshift()` gives the same result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ihfft()", "path": "fft#torch.fft.ihfft", "type": "torch.fft", "text": "\nComputes the inverse of `hfft()`.\n\n`input` must be a real-valued signal, interpreted in the Fourier domain. The\nIFFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])`. `ihfft()`\nrepresents this in the one-sided form where only the positive frequencies\nbelow the Nyquist frequency are included. To compute the full output, use\n`ifft()`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ihfft()`), these correspond\nto:\n\nCalling the forward transform (`hfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nCompare against the full output from `ifft()`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.irfft()", "path": "fft#torch.fft.irfft", "type": "torch.fft", "text": "\nComputes the inverse of `rfft()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft()`), these correspond\nto:\n\nCalling the forward transform (`rfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length:\n\nSo, it is recommended to always pass the signal length `n`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.irfft2()", "path": "fft#torch.fft.irfft2", "type": "torch.fft", "text": "\nComputes the inverse of `rfft2()`. Equivalent to `irfftn()` but IFFTs only the\nlast two dimensions by default.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft2()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft2()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.irfftn()", "path": "fft#torch.fft.irfftn", "type": "torch.fft", "text": "\nComputes the inverse of `rfftn()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfftn()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfft()", "path": "fft#torch.fft.rfft", "type": "torch.fft", "text": "\nComputes the one dimensional Fourier transform of real-valued `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])` so the\noutput contains only the positive frequencies below the Nyquist frequency. To\ncompute the full output, use `fft()`\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft()`), these correspond to:\n\nCalling the backward transform (`irfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompare against the full output from `fft()`:\n\nNotice that the symmetric element `T[-1] == T[1].conj()` is omitted. At the\nNyquist frequency `T[-2] == T[2]` is it\u2019s own symmetric pair, and therefore\nmust always be real-valued.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfft2()", "path": "fft#torch.fft.rfft2", "type": "torch.fft", "text": "\nComputes the 2-dimensional discrete Fourier transform of real `input`.\nEquivalent to `rfftn()` but FFTs only the last two dimensions by default.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i, j] = conj(X[-i, -j])`,\nso the full `fft2()` output contains redundant information. `rfft2()` instead\nomits the negative frequencies in the last dimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fft2()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfft2()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfftfreq()", "path": "fft#torch.fft.rfftfreq", "type": "torch.fft", "text": "\nComputes the sample frequencies for `rfft()` with a signal of size `n`.\n\nNote\n\n`rfft()` returns Hermitian one-sided output, so only the positive frequency\nterms are returned. For a real FFT of length `n` and with inputs spaced in\nlength unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. Unlike `fftfreq()`, `rfftfreq()` always returns\nit as positive.\n\nCompared to the output from `fftfreq()`, we see that the Nyquist frequency at\n`f[2]` has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500,\n-0.5000, -0.2500])\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfftn()", "path": "fft#torch.fft.rfftn", "type": "torch.fft", "text": "\nComputes the N-dimensional discrete Fourier transform of real `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i_1, ..., i_n] =\nconj(X[-i_1, ..., -i_n])` so the full `fftn()` output contains redundant\ninformation. `rfftn()` instead omits the negative frequencies in the last\ndimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fftn()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfftn()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fix()", "path": "generated/torch.fix#torch.fix", "type": "torch", "text": "\nAlias for `torch.trunc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.flatten()", "path": "generated/torch.flatten#torch.flatten", "type": "torch", "text": "\nFlattens `input` by reshaping it into a one-dimensional tensor. If `start_dim`\nor `end_dim` are passed, only dimensions starting with `start_dim` and ending\nwith `end_dim` are flattened. The order of elements in `input` is unchanged.\n\nUnlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may\nreturn the original object, a view, or copy. If no dimensions are flattened,\nthen the original object `input` is returned. Otherwise, if input can be\nviewed as the flattened shape, then that view is returned. Finally, only if\nthe input cannot be viewed as the flattened shape is input\u2019s data copied. See\n`torch.Tensor.view()` for details on when a view will be returned.\n\nNote\n\nFlattening a zero-dimensional tensor will return a one-dimensional view.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.flip()", "path": "generated/torch.flip#torch.flip", "type": "torch", "text": "\nReverse the order of a n-D tensor along given axis in dims.\n\nNote\n\n`torch.flip` makes a copy of `input`\u2019s data. This is different from NumPy\u2019s\n`np.flip`, which returns a view in constant time. Since copying a tensor\u2019s\ndata is more work than viewing that data, `torch.flip` is expected to be\nslower than `np.flip`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fliplr()", "path": "generated/torch.fliplr#torch.fliplr", "type": "torch", "text": "\nFlip tensor in the left/right direction, returning a new tensor.\n\nFlip the entries in each row in the left/right direction. Columns are\npreserved, but appear in a different order than before.\n\nNote\n\nRequires the tensor to be at least 2-D.\n\nNote\n\n`torch.fliplr` makes a copy of `input`\u2019s data. This is different from NumPy\u2019s\n`np.fliplr`, which returns a view in constant time. Since copying a tensor\u2019s\ndata is more work than viewing that data, `torch.fliplr` is expected to be\nslower than `np.fliplr`.\n\ninput (Tensor) \u2013 Must be at least 2-dimensional.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.flipud()", "path": "generated/torch.flipud#torch.flipud", "type": "torch", "text": "\nFlip tensor in the up/down direction, returning a new tensor.\n\nFlip the entries in each column in the up/down direction. Rows are preserved,\nbut appear in a different order than before.\n\nNote\n\nRequires the tensor to be at least 1-D.\n\nNote\n\n`torch.flipud` makes a copy of `input`\u2019s data. This is different from NumPy\u2019s\n`np.flipud`, which returns a view in constant time. Since copying a tensor\u2019s\ndata is more work than viewing that data, `torch.flipud` is expected to be\nslower than `np.flipud`.\n\ninput (Tensor) \u2013 Must be at least 1-dimensional.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "torch.Storage", "text": "\nCasts this storage to bfloat16 type\n\nCasts this storage to bool type\n\nCasts this storage to byte type\n\nCasts this storage to char type\n\nReturns a copy of this storage\n\nCasts this storage to complex double type\n\nCasts this storage to complex float type\n\nReturns a CPU copy of this storage if it\u2019s not already on the CPU\n\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\nCasts this storage to double type\n\nCasts this storage to float type\n\nIf `shared` is `True`, then memory is shared between all processes. All\nchanges are written to the file. If `shared` is `False`, then the changes on\nthe storage do not affect the file.\n\n`size` is the number of elements in the storage. If `shared` is `False`, then\nthe file must contain at least `size * sizeof(Type)` bytes (`Type` is the type\nof storage). If `shared` is `True` the file will be created if needed.\n\nCasts this storage to half type\n\nCasts this storage to int type\n\nCasts this storage to long type\n\nCopies the storage to pinned memory, if it\u2019s not already pinned.\n\nMoves the storage to shared memory.\n\nThis is a no-op for storages already in shared memory and for CUDA storages,\nwhich do not need to be moved for sharing across processes. Storages in shared\nmemory cannot be resized.\n\nReturns: self\n\nCasts this storage to short type\n\nReturns a list containing the elements of this storage\n\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.bfloat16()", "path": "storage#torch.FloatStorage.bfloat16", "type": "torch.Storage", "text": "\nCasts this storage to bfloat16 type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.bool()", "path": "storage#torch.FloatStorage.bool", "type": "torch.Storage", "text": "\nCasts this storage to bool type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.byte()", "path": "storage#torch.FloatStorage.byte", "type": "torch.Storage", "text": "\nCasts this storage to byte type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.char()", "path": "storage#torch.FloatStorage.char", "type": "torch.Storage", "text": "\nCasts this storage to char type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.clone()", "path": "storage#torch.FloatStorage.clone", "type": "torch.Storage", "text": "\nReturns a copy of this storage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.complex_double()", "path": "storage#torch.FloatStorage.complex_double", "type": "torch.Storage", "text": "\nCasts this storage to complex double type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.complex_float()", "path": "storage#torch.FloatStorage.complex_float", "type": "torch.Storage", "text": "\nCasts this storage to complex float type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.copy_()", "path": "storage#torch.FloatStorage.copy_", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.cpu()", "path": "storage#torch.FloatStorage.cpu", "type": "torch.Storage", "text": "\nReturns a CPU copy of this storage if it\u2019s not already on the CPU\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.cuda()", "path": "storage#torch.FloatStorage.cuda", "type": "torch.Storage", "text": "\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.data_ptr()", "path": "storage#torch.FloatStorage.data_ptr", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.device", "path": "storage#torch.FloatStorage.device", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.double()", "path": "storage#torch.FloatStorage.double", "type": "torch.Storage", "text": "\nCasts this storage to double type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.element_size()", "path": "storage#torch.FloatStorage.element_size", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.fill_()", "path": "storage#torch.FloatStorage.fill_", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.float()", "path": "storage#torch.FloatStorage.float", "type": "torch.Storage", "text": "\nCasts this storage to float type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.from_buffer()", "path": "storage#torch.FloatStorage.from_buffer", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.from_file()", "path": "storage#torch.FloatStorage.from_file", "type": "torch.Storage", "text": "\nIf `shared` is `True`, then memory is shared between all processes. All\nchanges are written to the file. If `shared` is `False`, then the changes on\nthe storage do not affect the file.\n\n`size` is the number of elements in the storage. If `shared` is `False`, then\nthe file must contain at least `size * sizeof(Type)` bytes (`Type` is the type\nof storage). If `shared` is `True` the file will be created if needed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.get_device()", "path": "storage#torch.FloatStorage.get_device", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.half()", "path": "storage#torch.FloatStorage.half", "type": "torch.Storage", "text": "\nCasts this storage to half type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.int()", "path": "storage#torch.FloatStorage.int", "type": "torch.Storage", "text": "\nCasts this storage to int type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_cuda", "path": "storage#torch.FloatStorage.is_cuda", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_pinned()", "path": "storage#torch.FloatStorage.is_pinned", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_shared()", "path": "storage#torch.FloatStorage.is_shared", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_sparse", "path": "storage#torch.FloatStorage.is_sparse", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.long()", "path": "storage#torch.FloatStorage.long", "type": "torch.Storage", "text": "\nCasts this storage to long type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.new()", "path": "storage#torch.FloatStorage.new", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.pin_memory()", "path": "storage#torch.FloatStorage.pin_memory", "type": "torch.Storage", "text": "\nCopies the storage to pinned memory, if it\u2019s not already pinned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.resize_()", "path": "storage#torch.FloatStorage.resize_", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.share_memory_()", "path": "storage#torch.FloatStorage.share_memory_", "type": "torch.Storage", "text": "\nMoves the storage to shared memory.\n\nThis is a no-op for storages already in shared memory and for CUDA storages,\nwhich do not need to be moved for sharing across processes. Storages in shared\nmemory cannot be resized.\n\nReturns: self\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.short()", "path": "storage#torch.FloatStorage.short", "type": "torch.Storage", "text": "\nCasts this storage to short type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.size()", "path": "storage#torch.FloatStorage.size", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.tolist()", "path": "storage#torch.FloatStorage.tolist", "type": "torch.Storage", "text": "\nReturns a list containing the elements of this storage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.type()", "path": "storage#torch.FloatStorage.type", "type": "torch.Storage", "text": "\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.float_power()", "path": "generated/torch.float_power#torch.float_power", "type": "torch", "text": "\nRaises `input` to the power of `exponent`, elementwise, in double precision.\nIf neither input is complex returns a `torch.float64` tensor, and if one or\nmore inputs is complex returns a `torch.complex128` tensor.\n\nNote\n\nThis function always computes in double precision, unlike `torch.pow()`, which\nimplements more typical type promotion. This is useful when the computation\nneeds to be performed in a wider or more precise dtype, or the results of the\ncomputation may contain fractional values not representable in the input\ndtypes, like when an integer base is raised to a negative integer exponent.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.floor()", "path": "generated/torch.floor#torch.floor", "type": "torch", "text": "\nReturns a new tensor with the floor of the elements of `input`, the largest\ninteger less than or equal to each element.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.floor_divide()", "path": "generated/torch.floor_divide#torch.floor_divide", "type": "torch", "text": "\nWarning\n\nThis function\u2019s name is a misnomer. It actually rounds the quotient towards\nzero instead of taking its floor. This behavior will be deprecated in a future\nPyTorch release.\n\nComputes `input` divided by `other`, elementwise, and rounds each quotient\ntowards zero. Equivalently, it truncates the quotient(s):\n\nSupports broadcasting to a common shape, type promotion, and integer and float\ninputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fmax()", "path": "generated/torch.fmax#torch.fmax", "type": "torch", "text": "\nComputes the element-wise maximum of `input` and `other`.\n\nThis is like `torch.maximum()` except it handles NaNs differently: if exactly\none of the two elements being compared is a NaN then the non-NaN element is\ntaken as the maximum. Only if both elements are NaN is NaN propagated.\n\nThis function is a wrapper around C++\u2019s `std::fmax` and is similar to NumPy\u2019s\n`fmax` function.\n\nSupports broadcasting to a common shape, type promotion, and integer and\nfloating-point inputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fmin()", "path": "generated/torch.fmin#torch.fmin", "type": "torch", "text": "\nComputes the element-wise minimum of `input` and `other`.\n\nThis is like `torch.minimum()` except it handles NaNs differently: if exactly\none of the two elements being compared is a NaN then the non-NaN element is\ntaken as the minimum. Only if both elements are NaN is NaN propagated.\n\nThis function is a wrapper around C++\u2019s `std::fmin` and is similar to NumPy\u2019s\n`fmin` function.\n\nSupports broadcasting to a common shape, type promotion, and integer and\nfloating-point inputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fmod()", "path": "generated/torch.fmod#torch.fmod", "type": "torch", "text": "\nComputes the element-wise remainder of division.\n\nThe dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend `input`.\n\nSupports broadcasting to a common shape, type promotion, and integer and float\ninputs.\n\nNote\n\nWhen the divisor is zero, returns `NaN` for floating point dtypes on both CPU\nand GPU; raises `RuntimeError` for integer division by zero on CPU; Integer\ndivision by zero on GPU may return any value.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.frac()", "path": "generated/torch.frac#torch.frac", "type": "torch", "text": "\nComputes the fractional portion of each element in `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.from_numpy()", "path": "generated/torch.from_numpy#torch.from_numpy", "type": "torch", "text": "\nCreates a `Tensor` from a `numpy.ndarray`.\n\nThe returned tensor and `ndarray` share the same memory. Modifications to the\ntensor will be reflected in the `ndarray` and vice versa. The returned tensor\nis not resizable.\n\nIt currently accepts `ndarray` with dtypes of `numpy.float64`,\n`numpy.float32`, `numpy.float16`, `numpy.complex64`, `numpy.complex128`,\n`numpy.int64`, `numpy.int32`, `numpy.int16`, `numpy.int8`, `numpy.uint8`, and\n`numpy.bool`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.full()", "path": "generated/torch.full#torch.full", "type": "torch", "text": "\nCreates a tensor of size `size` filled with `fill_value`. The tensor\u2019s dtype\nis inferred from `fill_value`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.full_like()", "path": "generated/torch.full_like#torch.full_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` filled with `fill_value`.\n`torch.full_like(input, fill_value)` is equivalent to\n`torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout,\ndevice=input.device)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures", "path": "futures", "type": "torch.futures", "text": "\nWarning\n\nThe `torch.futures` package is experimental and subject to change.\n\nThis package provides a `Future` type that encapsulates an asynchronous\nexecution and a set of utility functions to simplify operations on `Future`\nobjects. Currently, the `Future` type is primarily used by the Distributed RPC\nFramework.\n\nWrapper around a `torch._C.Future` which encapsulates an asynchronous\nexecution of a callable, e.g. `rpc_async()`. It also exposes a set of APIs to\nadd callback functions and set results.\n\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\nresult (BaseException) \u2013 the exception for this `Future`.\n\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\nresult (object) \u2013 the result object of this `Future`.\n\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\ncallback (`Callable`) \u2013 a `Callable` that takes this `Future` as the only\nargument.\n\nA new `Future` object that holds the return value of the `callback` and will\nbe marked as completed when the given `callback` finishes.\n\nBlock until the value of this `Future` is ready.\n\nThe value held by this `Future`. If the function (callback or RPC) creating\nthe value has thrown an error, this `wait` method will also throw an error.\n\nCollects the provided `Future` objects into a single combined `Future` that is\ncompleted when all of the sub-futures are completed.\n\nfutures (list) \u2013 a list of `Future` objects.\n\nReturns a `Future` object to a list of the passed in Futures.\n\nWaits for all provided futures to be complete, and returns the list of\ncompleted values.\n\nfutures (list) \u2013 a list of `Future` object.\n\nA list of the completed `Future` results. This method will throw an error if\n`wait` on any `Future` throws.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "torch.futures", "text": "\nCollects the provided `Future` objects into a single combined `Future` that is\ncompleted when all of the sub-futures are completed.\n\nfutures (list) \u2013 a list of `Future` objects.\n\nReturns a `Future` object to a list of the passed in Futures.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "torch.futures", "text": "\nWrapper around a `torch._C.Future` which encapsulates an asynchronous\nexecution of a callable, e.g. `rpc_async()`. It also exposes a set of APIs to\nadd callback functions and set results.\n\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\nresult (BaseException) \u2013 the exception for this `Future`.\n\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\nresult (object) \u2013 the result object of this `Future`.\n\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\ncallback (`Callable`) \u2013 a `Callable` that takes this `Future` as the only\nargument.\n\nA new `Future` object that holds the return value of the `callback` and will\nbe marked as completed when the given `callback` finishes.\n\nBlock until the value of this `Future` is ready.\n\nThe value held by this `Future`. If the function (callback or RPC) creating\nthe value has thrown an error, this `wait` method will also throw an error.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "torch.futures", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "torch.futures", "text": "\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "torch.futures", "text": "\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\nresult (BaseException) \u2013 the exception for this `Future`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "torch.futures", "text": "\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\nresult (object) \u2013 the result object of this `Future`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "torch.futures", "text": "\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\ncallback (`Callable`) \u2013 a `Callable` that takes this `Future` as the only\nargument.\n\nA new `Future` object that holds the return value of the `callback` and will\nbe marked as completed when the given `callback` finishes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "torch.futures", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "torch.futures", "text": "\nBlock until the value of this `Future` is ready.\n\nThe value held by this `Future`. If the function (callback or RPC) creating\nthe value has thrown an error, this `wait` method will also throw an error.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "torch.futures", "text": "\nWaits for all provided futures to be complete, and returns the list of\ncompleted values.\n\nfutures (list) \u2013 a list of `Future` object.\n\nA list of the completed `Future` results. This method will throw an error if\n`wait` on any `Future` throws.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx", "path": "fx", "type": "torch.fx", "text": "\nThis feature is under a Beta release and its API may change.\n\nFX is a toolkit for developers to use to transform `nn.Module` instances. FX\nconsists of three main components: a symbolic tracer, an intermediate\nrepresentation, and Python code generation. A demonstration of these\ncomponents in action:\n\nThe symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds\nfake values, called Proxies, through the code. Operations on theses Proxies\nare recorded. More information about symbolic tracing can be found in the\n`symbolic_trace()` and `Tracer` documentation.\n\nThe intermediate representation is the container for the operations that were\nrecorded during symbolic tracing. It consists of a list of Nodes that\nrepresent function inputs, callsites (to functions, methods, or\n`torch.nn.Module` instances), and return values. More information about the IR\ncan be found in the documentation for `Graph`. The IR is the format on which\ntransformations are applied.\n\nPython code generation is what makes FX a Python-to-Python (or Module-to-\nModule) transformation toolkit. For each Graph IR, we can create valid Python\ncode matching the Graph\u2019s semantics. This functionality is wrapped up in\n`GraphModule`, which is a `torch.nn.Module` instance that holds a `Graph` as\nwell as a `forward` method generated from the Graph.\n\nTaken together, this pipeline of components (symbolic tracing \u2192 intermediate\nrepresentation \u2192 transforms \u2192 Python code generation) constitutes the Python-\nto-Python transformation pipeline of FX. In addition, these components can be\nused separately. For example, symbolic tracing can be used in isolation to\ncapture a form of the code for analysis (and not transformation) purposes.\nCode generation can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX!\n\nSeveral example transformations can be found at the examples repository.\n\nWhat is an FX transform? Essentially, it\u2019s a function that looks like this.\n\nYour transform will take in an `torch.nn.Module`, acquire a `Graph` from it,\ndo some modifications, and return a new `torch.nn.Module`. You should think of\nthe `torch.nn.Module` that your FX transform returns as identical to a regular\n`torch.nn.Module` \u2013 you can pass it to another FX transform, you can pass it\nto TorchScript, or you can run it. Ensuring that the inputs and outputs of\nyour FX transform are a `torch.nn.Module` will allow for composability.\n\nNote\n\nIt is also possible to modify an existing `GraphModule` instead of creating a\nnew one, like so:\n\nNote that you MUST call `GraphModule.recompile()` to bring the generated\n`forward()` method on the `GraphModule` in sync with the modified `Graph`.\n\nGiven that you\u2019ve passed in a `torch.nn.Module` that has been traced into a\n`Graph`, there are now two primary approaches you can take to building a new\n`Graph`.\n\nFull treatment of the semantics of graphs can be found in the `Graph`\ndocumentation, but we are going to cover the basics here. A `Graph` is a data\nstructure that represents a method on a `GraphModule`. The information that\nthis requires is:\n\nAll three of these concepts are represented with `Node` instances. Let\u2019s see\nwhat we mean by that with a short example:\n\nHere we define a module `MyModule` for demonstration purposes, instantiate it,\nsymbolically trace it, then call the `Graph.print_tabular()` method to print\nout a table showing the nodes of this `Graph`:\n\nopcode\n\nname\n\ntarget\n\nargs\n\nkwargs\n\nplaceholder\n\nx\n\nx\n\n()\n\n{}\n\nget_attr\n\nlinear_weight\n\nlinear.weight\n\n()\n\n{}\n\ncall_function\n\nadd_1\n\n<built-in function add>\n\n(x, linear_weight)\n\n{}\n\ncall_module\n\nlinear_1\n\nlinear\n\n(add_1,)\n\n{}\n\ncall_method\n\nrelu_1\n\nrelu\n\n(linear_1,)\n\n{}\n\ncall_function\n\nsum_1\n\n<built-in method sum \u2026>\n\n(relu_1,)\n\n{\u2018dim\u2019: -1}\n\ncall_function\n\ntopk_1\n\n<built-in method topk \u2026>\n\n(sum_1, 3)\n\n{}\n\noutput\n\noutput\n\noutput\n\n(topk_1,)\n\n{}\n\nWe can use this information to answer the questions we posed above.\n\nGiven that we now know the basics of how code is represented in FX, we can now\nexplore how we would edit a `Graph`.\n\nOne approach to building this new `Graph` is to directly manipulate your old\none. To aid in this, we can simply take the `Graph` we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\n`torch.add()` calls with `torch.mul()` calls.\n\nWe can also do more involved `Graph` rewrites, such as deleting or appending\nnodes. To aid in these transformations, FX has utility functions for\ntransforming the graph that can be found in the `Graph` documentation. An\nexample of using these APIs to append a `torch.relu()` call can be found\nbelow.\n\nFor simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.\n\nFX also provides another level of automation on top of direct graph\nmanipulation. The `replace_pattern()` API is essentially a \u201cfind/replace\u201d tool\nfor editing `Graph`s. It allows you to specify a `pattern` and `replacement`\nfunction and it will trace through those functions, find instances of the\ngroup of operations in the `pattern` graph, and replace those instances with\ncopies of the `replacement` graph. This can help to greatly automate tedious\ngraph manipulation code, which can get unwieldy as the transformations get\nmore complex.\n\nAnother way of manipulating `Graph`s is by reusing the `Proxy` machinery used\nin symbolic tracing. For example, let\u2019s imagine that we wanted to write a\ntransformation that decomposed PyTorch functions into smaller operations. It\nwould transform every `F.relu(x)` call into `(x > 0) * x`. One possibility\nwould be to perform the requisite graph rewriting to insert the comparison and\nmultiplication after the `F.relu`, and then clean up the original `F.relu`.\nHowever, we can automate this process by using `Proxy` objects to\nautomatically record operations into the `Graph`.\n\nTo use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with `Proxy` objects as arugments. These\n`Proxy` objects will capture the operations that are performed on them and\nappend them to the `Graph`.\n\nIn addition to avoiding explicit graph manipulation, using `Proxy`s also\nallows you to specify your rewrite rules as native Python code. For\ntransformations that require a large amount of rewrite rules (such as vmap or\ngrad), this can often improve readability and maintainability of the rules.\n\nA worked example of using `Proxy`s for `Graph` manipulation can be found here.\n\nA useful code organizational pattern in FX is to loop over all the `Node`s in\na `Graph` and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the\ncode via retracing with `Proxy`s. For example, suppose we want to run a\n`GraphModule` and record the `torch.Tensor` shape and dtype properties on the\nnodes as we see them at runtime. That might look like:\n\nAs you can see, a full interpreter for FX is not that complicated but it can\nbe very useful. To ease using this pattern, we provide the `Interpreter`\nclass, which encompasses the above logic in a way that certain aspects of the\ninterpreter\u2019s execution can be overridden via method overrides.\n\nIn addition to executing operations, we can also generate a new `Graph` by\nfeeding `Proxy` values through an interpreter. Similarly, we provide the\n`Transformer` class to encompass this pattern. `Transformer` behaves similarly\nto `Interpreter`, but instead of calling the `run` method to get a concrete\noutput value from the Module, you would call the `Transformer.transform()`\nmethod to return a new `GraphModule` which was subject to any transformation\nrules you installed as overridden methods.\n\nOften in the course of authoring transformations, our code will not be quite\nright. In this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove\nor disprove correctness. Then, inspect and debug the generated code. Then,\ndebug the process of transformations that led to the generated code.\n\nIf you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.\n\nBecause the output of most deep learning modules consists of floating point\n`torch.Tensor` instances, checking for equivalence between the results of two\n`torch.nn.Module` is not as straightforward as doing a simple equality check.\nTo motivate this, let\u2019s use an example:\n\nHere, we\u2019ve tried to check equality of the values of two deep learning models\nwith the `==` equality operator. However, this is not well- defined both due\nto the issue of that operator returning a tensor and not a bool, but also\nbecause comparison of floating point values should use a margin of error (or\nepsilon) to account for the non-commutativity of floating point operations\n(see here for more details). We can use `torch.allclose()` instead, which will\ngive us an approximate comparison taking into account a relative and absolute\ntolerance threshold:\n\nThis is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.\n\nBecause FX generates the `forward()` function on `GraphModule`s, using\ntraditional debugging techniques like `print` statements or `pdb` is not as\nstraightfoward. Luckily, we have several techniques we can use for debugging\nthe generated code.\n\nInvoke `pdb` to step into the running program. Although the code that\nrepresents the `Graph` is not in any source file, we can still step into it\nmanually using `pdb` when the forward pass is invoked.\n\nIf you\u2019d like to run the same code multiple times, then it can be a bit\ntedious to step to the right code with `pdb`. In that case, one approach is to\nsimply copy-paste the generated `forward` pass into your code and examine it\nfrom there.\n\n`GraphModule.to_folder()` is a method in `GraphModule` that allows you to dump\nout the generated FX code to a folder. Although copying the forward pass into\nthe code often suffices as in Print the Generated Code, it may be easier to\nexamine modules and parameters using `to_folder`.\n\nAfter running the above example, we can then look at the code within\n`foo/module.py` and modify it as desired (e.g. adding `print` statements or\nusing `pdb`) to debug the generated code.\n\nNow that we\u2019ve identified that a transformation is creating incorrect code,\nit\u2019s time to debug the transformation itself. First, we\u2019ll check the\nLimitations of Symbolic Tracing section in the documentation. Once we verify\nthat tracing is working as expected, the goal becomes figuring out what went\nwrong during our `GraphModule` transformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to examine our\ntraced module:\n\nUsing the utility functions above, we can compare our traced Module before and\nafter we\u2019ve applied our transformations. Sometimes, a simple visual comparison\nis enough to trace down a bug. If it\u2019s still not clear what\u2019s going wrong, a\ndebugger like `pdb` can be a good next step.\n\nGoing off of the example above, consider the following code:\n\nUsing the above example, let\u2019s say that the call to `print(traced)` showed us\nthat there was an error in our transforms. We want to find what goes wrong\nusing a debugger. We start a `pdb` session. We can see what\u2019s happening during\nthe transform by breaking on `transform_graph(traced)`, then pressing `s` to\n\u201cstep into\u201d the call to `transform_graph(traced)`.\n\nWe may also have good luck by editing the `print_tabular` method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might want to\nsee the Node\u2019s `input_nodes` and `users`.)\n\nThe most common Python debugger is pdb. You can start your program in \u201cdebug\nmode\u201d with `pdb` by typing `python -m pdb FILENAME.py` into the command line,\nwhere `FILENAME` is the name of the file you want to debug. After that, you\ncan use the `pdb` debugger commands to move through your running program\nstepwise. It\u2019s common to set a breakpoint (`b LINE-NUMBER`) when you start\n`pdb`, then call `c` to run the program until that point. This prevents you\nfrom having to step through each line of execution (using `s` or `n`) to get\nto the part of the code you want to examine. Alternatively, you can write\n`import pdb; pdb.set_trace()` before the line you want to break at. If you add\n`pdb.set_trace()`, your program will automatically start in debug mode when\nyou run it. (In other words, you can just type `python FILENAME.py` into the\ncommand line instead of `python -m pdb FILENAME.py`.) Once you\u2019re running your\nfile in debug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent tutorials on\n`pdb` online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d.\n\nIDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you\ncan choose to either a) use `pdb` by pulling up a terminal window in your IDE\n(e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a\ngraphical wrapper around `pdb`).\n\nFX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the\nsemantics of programs in a transformable/analyzable form. The system is\ntracing in that it executes the program (really a `torch.nn.Module` or\nfunction) to record operations. It is symbolic in that the data flowing\nthrough the program during this execution is not real data, but rather symbols\n(`Proxy` in FX parlance).\n\nAlthough symbolic tracing works for most neural net code, it has some\nlimitations.\n\nThe main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or `if` statements where the condition\nmay depend on the input values of the program.\n\nFor example, let\u2019s examine the following program:\n\nThe condition to the `if` statement relies on the value of `dim0`, which\neventually relies on the value of `x`, a function input. Since `x` can change\n(i.e. if you pass a new input tensor to the traced function), this is dynamic\ncontrol flow. The traceback walks back up through your code to show you where\nthis situation happens.\n\nOn the other hand, so-called static control flow is supported. Static control\nflow is loops or `if` statements whose value cannot change across invocations.\nTypically, in PyTorch programs, this control flow arises for code making\ndecisions about a model\u2019s architecture based on hyper-parameters. As a\nconcrete example:\n\nThe if-statement `if self.do_activation` does not depend on any function\ninputs, thus it is static. `do_activation` can be considered to be a hyper-\nparameter, and the traces of different instances of `MyModule` with different\nvalues for that parameter have different code. This is a valid pattern that is\nsupported by symbolic tracing.\n\nMany instances of dynamic control flow are semantically static control flow.\nThese instances can be made to support symbolic tracing by removing the data\ndependencies on input values, for example by moving values to `Module`\nattributes or by passing constant values during symbolic tracing:\n\nIn the case of truly dynamic control flow, the sections of the program that\ncontain this code can be traced as calls to the Method (see Customizing\nTracing with the Tracer class) or function (see `wrap()`) rather than tracing\nthrough them.\n\nFX uses `__torch_function__` as the mechanism by which it intercepts calls\n(see the technical overview for more information about this). Some functions,\nsuch as builtin Python functions or those in the `math` module, are things\nthat are not covered by `__torch_function__`, but we would still like to\ncapture them in symbolic tracing. For example:\n\nThe error tells us that the built-in function `len` is not supported. We can\nmake it so that functions like this are recorded in the trace as direct calls\nusing the `wrap()` API:\n\nThe `Tracer` class is the class that underlies the implementation of\n`symbolic_trace`. The behavior of tracing can be customized by subclassing\nTracer, like so:\n\nLeaf Modules are the modules that appear as calls in the symbolic trace rather\nthan being traced through. The default set of leaf modules is the set of\nstandard `torch.nn` module instances. For example:\n\nThe set of leaf modules can be customized by overriding\n`Tracer.is_leaf_module()`.\n\nTensor constructors (e.g. `torch.zeros`, `torch.ones`, `torch.rand`,\n`torch.randn`, `torch.sparse_coo_tensor`) are currently not traceable.\n\nType annotations\n\nSymbolic tracing API\n\nGiven an `nn.Module` or function instance `root`, this function will return a\n`GraphModule` constructed by recording operations seen while tracing through\n`root`.\n\na Module created from the recorded operations from `root`.\n\nGraphModule\n\nThis function can be called at module-level scope to register fn_or_name as a\n\u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in\nthe FX trace instead of being traced through:\n\nThis function can also equivalently be used as a decorator:\n\nA wrapped function can be thought of a \u201cleaf function\u201d, analogous to the\nconcept of \u201cleaf modules\u201d, that is, they are functions that are left as calls\nin the FX trace rather than traced through.\n\nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global\nfunction to insert into the graph when it\u2019s called\n\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\n`graph` attribute, as well as `code` and `forward` attributes generated from\nthat `graph`.\n\nWarning\n\nWhen `graph` is reassigned, `code` and `forward` will be automatically\nregenerated. However, if you edit the contents of the `graph` without\nreassigning the `graph` attribute itself, you must call `recompile()` to\nupdate the generated code.\n\nConstruct a GraphModule.\n\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\nReturn the `Graph` underlying this `GraphModule`\n\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n`Graph` is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of `Node` s, each representing callsites (or other\nsyntactic constructs). The list of `Node` s, taken together, constitute a\nvalid Python function.\n\nFor example, the following code\n\nWill produce the following Graph:\n\nFor the semantics of operations represented in the `Graph`, please see `Node`.\n\nConstruct an empty Graph.\n\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\nReturns\n\nThe newly created and inserted `call_function` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\nThe newly created and inserted `call_method` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\nThe newly-created and inserted `call_module` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\nThe newly-created and inserted node.\n\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\nto_erase (Node) \u2013 The `Node` to erase from the `Graph`.\n\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\nThe newly-created and inserted `get_attr` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nCopy all nodes from a given graph into `self`.\n\nThe value in `self` that is now equivalent to the output value in `g`, if `g`\nhad an `output` node. `None` otherwise.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nafter the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nbefore the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for\ntargets. This is equivalent to the `root` argument that is passed when\nconstructing a `GraphModule`.\n\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\nGet the list of Nodes that constitute this Graph.\n\nNote that this `Node` list representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\n\nA doubly-linked list of Nodes. Note that `reversed` can be called on this list\nto switch iteration order.\n\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nPrints the intermediate representation of the graph in tabular format.\n\nTurn this `Graph` into valid Python code.\n\nroot_module (str) \u2013 The name of the root module on which to look-up qualified\nname targets. This is usually \u2018self\u2019.\n\nThe string source code generated from this `Graph`.\n\n`Node` is the data structure that represents individual operations within a\n`Graph`. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each `Node` has a function specified by\nits `op` property. The `Node` semantics for each value of `op` are as follows:\n\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\nList of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that\norder.\n\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\nx (Node) \u2013 The node to put after this node. Must be a member of the same\ngraph.\n\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nReturns the next `Node` in the linked list of Nodes.\n\nThe next `Node` in the linked list of Nodes.\n\nInsert x before this node in the list of nodes in the graph. Example:\n\nx (Node) \u2013 The node to put before this node. Must be a member of the same\ngraph.\n\nReturns the previous `Node` in the linked list of Nodes.\n\nThe previous `Node` in the linked list of Nodes.\n\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\nreplace_with (Node) \u2013 The node to replace all uses of `self` with.\n\nThe list of Nodes on which this change was made.\n\n`Tracer` is the class that implements the symbolic tracing functionality of\n`torch.fx.symbolic_trace`. A call to `symbolic_trace(m)` is equivalent to\n`Tracer().trace(m)`.\n\nTracer can be subclassed to override various behaviors of the tracing process.\nThe different behaviors that can be overridden are described in the docstrings\nof the methods on this class.\n\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\nBy default, the behavior is to check if the called module is a leaf module via\n`is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the\n`Graph`. Otherwise, call the `Module` normally, tracing through the operations\nin its `forward` function.\n\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing across\n`Module` boundaries. `Module` boundaries.\n\nThe return value from the Module call. In the case that a `call_module` node\nwas emitted, this is a `Proxy` value. Otherwise, it is whatever value was\nreturned from the `Module` invocation.\n\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\nBy default, the behavior includes:\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nThis method can be overridden to support more types.\n\na (Any) \u2013 The value to be emitted as an `Argument` in the `Graph`.\n\nThe value `a` converted into the appropriate `Argument`\n\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by\n`call_module` calls. By default, Modules in the PyTorch standard library\nnamespace (torch.nn) are leaf modules. All other modules are traced through\nand their constituent ops are recorded, unless specified otherwise via this\nparameter.\n\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\nmod (str) \u2013 The `Module` to retrieve the qualified name for.\n\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\nNote that after this call, `self.root` may be different from the `root` passed\nin here. For example, when a free function is passed to `trace()`, we will\ncreate an `nn.Module` instance to use as the root and add embedded constants\nto.\n\nroot (Union[Module, Callable]) \u2013 Either a `Module` or a function to be traced\nthrough.\n\nA `Graph` representing the semantics of the passed-in `root`.\n\n`Proxy` objects are `Node` wrappers that flow through the program during\nsymbolic tracing and record all the operations (`torch` function calls, method\ncalls, operators) that they touch into the growing FX Graph.\n\nIf you\u2019re doing graph transforms, you can wrap your own `Proxy` method around\na raw `Node` so that you can use the overloaded operators to add additional\nthings to a `Graph`.\n\nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful\nfor many things, including writing code transformations as well as analysis\npasses.\n\nMethods in the Interpreter class can be overridden to customize the behavior\nof execution. The map of overrideable methods in terms of call hierarchy:\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\nInterpreter like so:\n\nmodule (GraphModule) \u2013 The module to be executed\n\nExecute a `call_function` node and return the result.\n\nAny: The value returned by the function invocation\n\nExecute a `call_method` node and return the result.\n\nAny: The value returned by the method invocation\n\nExecute a `call_module` node and return the result.\n\nAny: The value returned by the module invocation\n\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\nn (Node) \u2013 The node for which `args` and `kwargs` should be fetched.\n\n`args` and `kwargs` with concrete values for `n`.\n\nTuple[Tuple, Dict]\n\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch\n\nThe value of the attribute.\n\nAny\n\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\nThe value of the attribute that was retrieved\n\nAny\n\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\nThe return value referenced by the output node\n\nAny\n\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\nThe argument value that was retrieved.\n\nAny\n\nRun `module` via interpretation and return the result.\n\nThe value returned from executing the Module\n\nAny\n\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\nn (Node) \u2013 The Node to execute\n\nThe result of executing `n`\n\nAny\n\n`Transformer` is a special type of interpreter that produces a new `Module`.\nIt exposes a `transform()` method that returns the transformed `Module`.\n`Transformer` does not require arguments to run, as `Interpreter` does.\n`Transformer` works entirely symbolically.\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\n`Transformer` like so:\n\nmodule (GraphModule) \u2013 The `Module` to be transformed.\n\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\nTransform `self.module` and return the transformed `GraphModule`.\n\nMatches all possible non-overlapping sets of operators and their data\ndependencies (`pattern`) in the Graph of a GraphModule (`gm`), then replaces\neach of these matched subgraphs with another subgraph (`replacement`).\n\nA list of `Match` objects representing the places in the original graph that\n`pattern` was matched to. The list is empty if there are no matches. `Match`\nis defined as:\n\nList[Match]\n\nExamples:\n\nThe above code will first match `pattern` in the `forward` method of\n`traced_module`. Pattern-matching is done based on use-def relationships, not\nnode names. For example, if you had `p = torch.cat([a, b])` in `pattern`, you\ncould match `m = torch.cat([a, b])` in the original `forward` function,\ndespite the variable names being different (`p` vs `m`).\n\nThe `return` statement in `pattern` is matched based on its value only; it may\nor may not match to the `return` statement in the larger graph. In other\nwords, the pattern doesn\u2019t have to extend to the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger function and\nreplaced by `replacement`. If there are multiple matches for `pattern` in the\nlarger function, each non-overlapping match will be replaced. In the case of a\nmatch overlap, the first found match in the set of overlapping matches will be\nreplaced. (\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node is the\nparameter that appears directly after `self`, while the last Node is whatever\nthe function returns.)\n\nOne important thing to note is that the parameters of the `pattern` Callable\nmust be used in the Callable itself, and the parameters of the `replacement`\nCallable must match the pattern. The first rule is why, in the above code\nblock, the `forward` function has parameters `x, w1, w2`, but the `pattern`\nfunction only has parameters `w1, w2`. `pattern` doesn\u2019t use `x`, so it\nshouldn\u2019t specify `x` as a parameter. As an example of the second rule,\nconsider replacing\n\nwith\n\nIn this case, `replacement` needs the same number of parameters as `pattern`\n(both `x` and `y`), even though the parameter `y` isn\u2019t used in `replacement`.\n\nAfter calling `subgraph_rewriter.replace_pattern`, the generated Python code\nlooks like this:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "torch.fx", "text": "\n`Graph` is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of `Node` s, each representing callsites (or other\nsyntactic constructs). The list of `Node` s, taken together, constitute a\nvalid Python function.\n\nFor example, the following code\n\nWill produce the following Graph:\n\nFor the semantics of operations represented in the `Graph`, please see `Node`.\n\nConstruct an empty Graph.\n\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\nReturns\n\nThe newly created and inserted `call_function` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\nThe newly created and inserted `call_method` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\nThe newly-created and inserted `call_module` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\nThe newly-created and inserted node.\n\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\nto_erase (Node) \u2013 The `Node` to erase from the `Graph`.\n\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\nThe newly-created and inserted `get_attr` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nCopy all nodes from a given graph into `self`.\n\nThe value in `self` that is now equivalent to the output value in `g`, if `g`\nhad an `output` node. `None` otherwise.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nafter the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nbefore the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for\ntargets. This is equivalent to the `root` argument that is passed when\nconstructing a `GraphModule`.\n\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\nGet the list of Nodes that constitute this Graph.\n\nNote that this `Node` list representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\n\nA doubly-linked list of Nodes. Note that `reversed` can be called on this list\nto switch iteration order.\n\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nPrints the intermediate representation of the graph in tabular format.\n\nTurn this `Graph` into valid Python code.\n\nroot_module (str) \u2013 The name of the root module on which to look-up qualified\nname targets. This is usually \u2018self\u2019.\n\nThe string source code generated from this `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "torch.fx", "text": "\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\nReturns\n\nThe newly created and inserted `call_function` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "torch.fx", "text": "\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\nThe newly created and inserted `call_method` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "torch.fx", "text": "\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\nThe newly-created and inserted `call_module` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "torch.fx", "text": "\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\nThe newly-created and inserted node.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "torch.fx", "text": "\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\nto_erase (Node) \u2013 The `Node` to erase from the `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "torch.fx", "text": "\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\nThe newly-created and inserted `get_attr` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "torch.fx", "text": "\nCopy all nodes from a given graph into `self`.\n\nThe value in `self` that is now equivalent to the output value in `g`, if `g`\nhad an `output` node. `None` otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "torch.fx", "text": "\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nafter the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "torch.fx", "text": "\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nbefore the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "torch.fx", "text": "\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for\ntargets. This is equivalent to the `root` argument that is passed when\nconstructing a `GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.nodes()", "path": "fx#torch.fx.Graph.nodes", "type": "torch.fx", "text": "\nGet the list of Nodes that constitute this Graph.\n\nNote that this `Node` list representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\n\nA doubly-linked list of Nodes. Note that `reversed` can be called on this list\nto switch iteration order.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "torch.fx", "text": "\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "torch.fx", "text": "\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "torch.fx", "text": "\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "torch.fx", "text": "\nPrints the intermediate representation of the graph in tabular format.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "torch.fx", "text": "\nTurn this `Graph` into valid Python code.\n\nroot_module (str) \u2013 The name of the root module on which to look-up qualified\nname targets. This is usually \u2018self\u2019.\n\nThe string source code generated from this `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "torch.fx", "text": "\nConstruct an empty Graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "torch.fx", "text": "\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\n`graph` attribute, as well as `code` and `forward` attributes generated from\nthat `graph`.\n\nWarning\n\nWhen `graph` is reassigned, `code` and `forward` will be automatically\nregenerated. However, if you edit the contents of the `graph` without\nreassigning the `graph` attribute itself, you must call `recompile()` to\nupdate the generated code.\n\nConstruct a GraphModule.\n\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\nReturn the `Graph` underlying this `GraphModule`\n\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.code()", "path": "fx#torch.fx.GraphModule.code", "type": "torch.fx", "text": "\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.graph()", "path": "fx#torch.fx.GraphModule.graph", "type": "torch.fx", "text": "\nReturn the `Graph` underlying this `GraphModule`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "torch.fx", "text": "\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "torch.fx", "text": "\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "torch.fx", "text": "\nConstruct a GraphModule.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "torch.fx", "text": "\nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful\nfor many things, including writing code transformations as well as analysis\npasses.\n\nMethods in the Interpreter class can be overridden to customize the behavior\nof execution. The map of overrideable methods in terms of call hierarchy:\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\nInterpreter like so:\n\nmodule (GraphModule) \u2013 The module to be executed\n\nExecute a `call_function` node and return the result.\n\nAny: The value returned by the function invocation\n\nExecute a `call_method` node and return the result.\n\nAny: The value returned by the method invocation\n\nExecute a `call_module` node and return the result.\n\nAny: The value returned by the module invocation\n\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\nn (Node) \u2013 The node for which `args` and `kwargs` should be fetched.\n\n`args` and `kwargs` with concrete values for `n`.\n\nTuple[Tuple, Dict]\n\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch\n\nThe value of the attribute.\n\nAny\n\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\nThe value of the attribute that was retrieved\n\nAny\n\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\nThe return value referenced by the output node\n\nAny\n\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\nThe argument value that was retrieved.\n\nAny\n\nRun `module` via interpretation and return the result.\n\nThe value returned from executing the Module\n\nAny\n\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\nn (Node) \u2013 The Node to execute\n\nThe result of executing `n`\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "torch.fx", "text": "\nExecute a `call_function` node and return the result.\n\nAny: The value returned by the function invocation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "torch.fx", "text": "\nExecute a `call_method` node and return the result.\n\nAny: The value returned by the method invocation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "torch.fx", "text": "\nExecute a `call_module` node and return the result.\n\nAny: The value returned by the module invocation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "torch.fx", "text": "\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\nn (Node) \u2013 The node for which `args` and `kwargs` should be fetched.\n\n`args` and `kwargs` with concrete values for `n`.\n\nTuple[Tuple, Dict]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "torch.fx", "text": "\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch\n\nThe value of the attribute.\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "torch.fx", "text": "\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\nThe value of the attribute that was retrieved\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "torch.fx", "text": "\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "torch.fx", "text": "\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\nThe return value referenced by the output node\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "torch.fx", "text": "\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\nThe argument value that was retrieved.\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "torch.fx", "text": "\nRun `module` via interpretation and return the result.\n\nThe value returned from executing the Module\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "torch.fx", "text": "\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\nn (Node) \u2013 The Node to execute\n\nThe result of executing `n`\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "torch.fx", "text": "\n`Node` is the data structure that represents individual operations within a\n`Graph`. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each `Node` has a function specified by\nits `op` property. The `Node` semantics for each value of `op` are as follows:\n\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\nList of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that\norder.\n\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\nx (Node) \u2013 The node to put after this node. Must be a member of the same\ngraph.\n\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nReturns the next `Node` in the linked list of Nodes.\n\nThe next `Node` in the linked list of Nodes.\n\nInsert x before this node in the list of nodes in the graph. Example:\n\nx (Node) \u2013 The node to put before this node. Must be a member of the same\ngraph.\n\nReturns the previous `Node` in the linked list of Nodes.\n\nThe previous `Node` in the linked list of Nodes.\n\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\nreplace_with (Node) \u2013 The node to replace all uses of `self` with.\n\nThe list of Nodes on which this change was made.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.all_input_nodes()", "path": "fx#torch.fx.Node.all_input_nodes", "type": "torch.fx", "text": "\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\nList of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that\norder.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "torch.fx", "text": "\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\nx (Node) \u2013 The node to put after this node. Must be a member of the same\ngraph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.args()", "path": "fx#torch.fx.Node.args", "type": "torch.fx", "text": "\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.kwargs()", "path": "fx#torch.fx.Node.kwargs", "type": "torch.fx", "text": "\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.next()", "path": "fx#torch.fx.Node.next", "type": "torch.fx", "text": "\nReturns the next `Node` in the linked list of Nodes.\n\nThe next `Node` in the linked list of Nodes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "torch.fx", "text": "\nInsert x before this node in the list of nodes in the graph. Example:\n\nx (Node) \u2013 The node to put before this node. Must be a member of the same\ngraph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.prev()", "path": "fx#torch.fx.Node.prev", "type": "torch.fx", "text": "\nReturns the previous `Node` in the linked list of Nodes.\n\nThe previous `Node` in the linked list of Nodes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "torch.fx", "text": "\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\nreplace_with (Node) \u2013 The node to replace all uses of `self` with.\n\nThe list of Nodes on which this change was made.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "torch.fx", "text": "\n`Proxy` objects are `Node` wrappers that flow through the program during\nsymbolic tracing and record all the operations (`torch` function calls, method\ncalls, operators) that they touch into the growing FX Graph.\n\nIf you\u2019re doing graph transforms, you can wrap your own `Proxy` method around\na raw `Node` so that you can use the overloaded operators to add additional\nthings to a `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "torch.fx", "text": "\nMatches all possible non-overlapping sets of operators and their data\ndependencies (`pattern`) in the Graph of a GraphModule (`gm`), then replaces\neach of these matched subgraphs with another subgraph (`replacement`).\n\nA list of `Match` objects representing the places in the original graph that\n`pattern` was matched to. The list is empty if there are no matches. `Match`\nis defined as:\n\nList[Match]\n\nExamples:\n\nThe above code will first match `pattern` in the `forward` method of\n`traced_module`. Pattern-matching is done based on use-def relationships, not\nnode names. For example, if you had `p = torch.cat([a, b])` in `pattern`, you\ncould match `m = torch.cat([a, b])` in the original `forward` function,\ndespite the variable names being different (`p` vs `m`).\n\nThe `return` statement in `pattern` is matched based on its value only; it may\nor may not match to the `return` statement in the larger graph. In other\nwords, the pattern doesn\u2019t have to extend to the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger function and\nreplaced by `replacement`. If there are multiple matches for `pattern` in the\nlarger function, each non-overlapping match will be replaced. In the case of a\nmatch overlap, the first found match in the set of overlapping matches will be\nreplaced. (\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node is the\nparameter that appears directly after `self`, while the last Node is whatever\nthe function returns.)\n\nOne important thing to note is that the parameters of the `pattern` Callable\nmust be used in the Callable itself, and the parameters of the `replacement`\nCallable must match the pattern. The first rule is why, in the above code\nblock, the `forward` function has parameters `x, w1, w2`, but the `pattern`\nfunction only has parameters `w1, w2`. `pattern` doesn\u2019t use `x`, so it\nshouldn\u2019t specify `x` as a parameter. As an example of the second rule,\nconsider replacing\n\nwith\n\nIn this case, `replacement` needs the same number of parameters as `pattern`\n(both `x` and `y`), even though the parameter `y` isn\u2019t used in `replacement`.\n\nAfter calling `subgraph_rewriter.replace_pattern`, the generated Python code\nlooks like this:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "torch.fx", "text": "\nSymbolic tracing API\n\nGiven an `nn.Module` or function instance `root`, this function will return a\n`GraphModule` constructed by recording operations seen while tracing through\n`root`.\n\na Module created from the recorded operations from `root`.\n\nGraphModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "torch.fx", "text": "\n`Tracer` is the class that implements the symbolic tracing functionality of\n`torch.fx.symbolic_trace`. A call to `symbolic_trace(m)` is equivalent to\n`Tracer().trace(m)`.\n\nTracer can be subclassed to override various behaviors of the tracing process.\nThe different behaviors that can be overridden are described in the docstrings\nof the methods on this class.\n\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\nBy default, the behavior is to check if the called module is a leaf module via\n`is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the\n`Graph`. Otherwise, call the `Module` normally, tracing through the operations\nin its `forward` function.\n\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing across\n`Module` boundaries. `Module` boundaries.\n\nThe return value from the Module call. In the case that a `call_module` node\nwas emitted, this is a `Proxy` value. Otherwise, it is whatever value was\nreturned from the `Module` invocation.\n\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\nBy default, the behavior includes:\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nThis method can be overridden to support more types.\n\na (Any) \u2013 The value to be emitted as an `Argument` in the `Graph`.\n\nThe value `a` converted into the appropriate `Argument`\n\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by\n`call_module` calls. By default, Modules in the PyTorch standard library\nnamespace (torch.nn) are leaf modules. All other modules are traced through\nand their constituent ops are recorded, unless specified otherwise via this\nparameter.\n\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\nmod (str) \u2013 The `Module` to retrieve the qualified name for.\n\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\nNote that after this call, `self.root` may be different from the `root` passed\nin here. For example, when a free function is passed to `trace()`, we will\ncreate an `nn.Module` instance to use as the root and add embedded constants\nto.\n\nroot (Union[Module, Callable]) \u2013 Either a `Module` or a function to be traced\nthrough.\n\nA `Graph` representing the semantics of the passed-in `root`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "torch.fx", "text": "\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\nBy default, the behavior is to check if the called module is a leaf module via\n`is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the\n`Graph`. Otherwise, call the `Module` normally, tracing through the operations\nin its `forward` function.\n\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing across\n`Module` boundaries. `Module` boundaries.\n\nThe return value from the Module call. In the case that a `call_module` node\nwas emitted, this is a `Proxy` value. Otherwise, it is whatever value was\nreturned from the `Module` invocation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "torch.fx", "text": "\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\nBy default, the behavior includes:\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nThis method can be overridden to support more types.\n\na (Any) \u2013 The value to be emitted as an `Argument` in the `Graph`.\n\nThe value `a` converted into the appropriate `Argument`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "torch.fx", "text": "\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "torch.fx", "text": "\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by\n`call_module` calls. By default, Modules in the PyTorch standard library\nnamespace (torch.nn) are leaf modules. All other modules are traced through\nand their constituent ops are recorded, unless specified otherwise via this\nparameter.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "torch.fx", "text": "\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\nmod (str) \u2013 The `Module` to retrieve the qualified name for.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "torch.fx", "text": "\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\nNote that after this call, `self.root` may be different from the `root` passed\nin here. For example, when a free function is passed to `trace()`, we will\ncreate an `nn.Module` instance to use as the root and add embedded constants\nto.\n\nroot (Union[Module, Callable]) \u2013 Either a `Module` or a function to be traced\nthrough.\n\nA `Graph` representing the semantics of the passed-in `root`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "torch.fx", "text": "\n`Transformer` is a special type of interpreter that produces a new `Module`.\nIt exposes a `transform()` method that returns the transformed `Module`.\n`Transformer` does not require arguments to run, as `Interpreter` does.\n`Transformer` works entirely symbolically.\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\n`Transformer` like so:\n\nmodule (GraphModule) \u2013 The `Module` to be transformed.\n\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\nTransform `self.module` and return the transformed `GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "torch.fx", "text": "\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "torch.fx", "text": "\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "torch.fx", "text": "\nTransform `self.module` and return the transformed `GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "torch.fx", "text": "\nThis function can be called at module-level scope to register fn_or_name as a\n\u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in\nthe FX trace instead of being traced through:\n\nThis function can also equivalently be used as a decorator:\n\nA wrapped function can be thought of a \u201cleaf function\u201d, analogous to the\nconcept of \u201cleaf modules\u201d, that is, they are functions that are left as calls\nin the FX trace rather than traced through.\n\nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global\nfunction to insert into the graph when it\u2019s called\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.gather()", "path": "generated/torch.gather#torch.gather", "type": "torch", "text": "\nGathers values along an axis specified by `dim`.\n\nFor a 3-D tensor the output is specified by:\n\n`input` and `index` must have the same number of dimensions. It is also\nrequired that `index.size(d) <= input.size(d)` for all dimensions `d != dim`.\n`out` will have the same shape as `index`. Note that `input` and `index` do\nnot broadcast against each other.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.gcd()", "path": "generated/torch.gcd#torch.gcd", "type": "torch", "text": "\nComputes the element-wise greatest common divisor (GCD) of `input` and\n`other`.\n\nBoth `input` and `other` must have integer types.\n\nNote\n\nThis defines gcd(0,0)=0gcd(0, 0) = 0 .\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ge()", "path": "generated/torch.ge#torch.ge", "type": "torch", "text": "\nComputes input\u2265other\\text{input} \\geq \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is greater than or equal to\n`other` and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator", "path": "generated/torch.generator#torch.Generator", "type": "torch", "text": "\nCreates and returns a generator object that manages the state of the algorithm\nwhich produces pseudo random numbers. Used as a keyword argument in many In-\nplace random sampling functions.\n\ndevice (`torch.device`, optional) \u2013 the desired device for the generator.\n\nAn torch.Generator object.\n\nGenerator\n\nExample:\n\nGenerator.device -> device\n\nGets the current device of the generator.\n\nExample:\n\nReturns the Generator state as a `torch.ByteTensor`.\n\nA `torch.ByteTensor` which contains all the necessary bits to restore a\nGenerator to a specific point in time.\n\nTensor\n\nExample:\n\nReturns the initial seed for generating random numbers.\n\nExample:\n\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject. It is recommended to set a large seed, i.e. a number that has a good\nbalance of 0 and 1 bits. Avoid having many 0 bits in the seed.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\nAn torch.Generator object.\n\nGenerator\n\nExample:\n\nGets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator.\n\nExample:\n\nSets the Generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.device", "path": "generated/torch.generator#torch.Generator.device", "type": "torch", "text": "\nGenerator.device -> device\n\nGets the current device of the generator.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.get_state()", "path": "generated/torch.generator#torch.Generator.get_state", "type": "torch", "text": "\nReturns the Generator state as a `torch.ByteTensor`.\n\nA `torch.ByteTensor` which contains all the necessary bits to restore a\nGenerator to a specific point in time.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.initial_seed()", "path": "generated/torch.generator#torch.Generator.initial_seed", "type": "torch", "text": "\nReturns the initial seed for generating random numbers.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.manual_seed()", "path": "generated/torch.generator#torch.Generator.manual_seed", "type": "torch", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject. It is recommended to set a large seed, i.e. a number that has a good\nbalance of 0 and 1 bits. Avoid having many 0 bits in the seed.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\nAn torch.Generator object.\n\nGenerator\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.seed()", "path": "generated/torch.generator#torch.Generator.seed", "type": "torch", "text": "\nGets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.set_state()", "path": "generated/torch.generator#torch.Generator.set_state", "type": "torch", "text": "\nSets the Generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.geqrf()", "path": "generated/torch.geqrf#torch.geqrf", "type": "torch", "text": "\nThis is a low-level function for calling LAPACK directly. This function\nreturns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf .\n\nYou\u2019ll generally want to use `torch.qr()` instead.\n\nComputes a QR decomposition of `input`, but without constructing QQ and RR as\nexplicit separate matrices.\n\nRather, this directly calls the underlying LAPACK function `?geqrf` which\nproduces a sequence of \u2018elementary reflectors\u2019.\n\nSee LAPACK documentation for geqrf for further details.\n\ninput (Tensor) \u2013 the input matrix\n\nout (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ger()", "path": "generated/torch.ger#torch.ger", "type": "torch", "text": "\nAlias of `torch.outer()`.\n\nWarning\n\nThis function is deprecated and will be removed in a future PyTorch release.\nUse `torch.outer()` instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_default_dtype()", "path": "generated/torch.get_default_dtype#torch.get_default_dtype", "type": "torch", "text": "\nGet the current default floating point `torch.dtype`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_num_interop_threads()", "path": "generated/torch.get_num_interop_threads#torch.get_num_interop_threads", "type": "torch", "text": "\nReturns the number of threads used for inter-op parallelism on CPU (e.g. in\nJIT interpreter)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_num_threads()", "path": "generated/torch.get_num_threads#torch.get_num_threads", "type": "torch", "text": "\nReturns the number of threads used for parallelizing CPU operations\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_rng_state()", "path": "generated/torch.get_rng_state#torch.get_rng_state", "type": "torch", "text": "\nReturns the random number generator state as a `torch.ByteTensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.greater()", "path": "generated/torch.greater#torch.greater", "type": "torch", "text": "\nAlias for `torch.gt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.greater_equal()", "path": "generated/torch.greater_equal#torch.greater_equal", "type": "torch", "text": "\nAlias for `torch.ge()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.gt()", "path": "generated/torch.gt#torch.gt", "type": "torch", "text": "\nComputes input>other\\text{input} > \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is greater than `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hamming_window()", "path": "generated/torch.hamming_window#torch.hamming_window", "type": "torch", "text": "\nHamming window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.hamming_window(L, periodic=True)` equal to `torch.hamming_window(L + 1,\nperiodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nNote\n\nThis is a generalized version of `torch.hann_window()`.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hann_window()", "path": "generated/torch.hann_window#torch.hann_window", "type": "torch", "text": "\nHann window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.hann_window(L, periodic=True)` equal to `torch.hann_window(L + 1,\nperiodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.heaviside()", "path": "generated/torch.heaviside#torch.heaviside", "type": "torch", "text": "\nComputes the Heaviside step function for each element in `input`. The\nHeaviside step function is defined as:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.histc()", "path": "generated/torch.histc#torch.histc", "type": "torch", "text": "\nComputes the histogram of a tensor.\n\nThe elements are sorted into equal width bins between `min` and `max`. If\n`min` and `max` are both zero, the minimum and maximum values of the data are\nused.\n\nElements lower than min and higher than max are ignored.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nHistogram represented as a tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hspmm()", "path": "sparse#torch.hspmm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of a sparse COO matrix `mat1` and a strided\nmatrix `mat2`. The result is a (1 + 1)-dimensional hybrid COO matrix.\n\n{out} \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hstack()", "path": "generated/torch.hstack#torch.hstack", "type": "torch", "text": "\nStack tensors in sequence horizontally (column wise).\n\nThis is equivalent to concatenation along the first axis for 1-D tensors, and\nalong the second axis for all other tensors.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub", "path": "hub", "type": "torch.hub", "text": "\nPytorch Hub is a pre-trained model repository designed to facilitate research\nreproducibility.\n\nPytorch Hub supports publishing pre-trained models(model definitions and pre-\ntrained weights) to a github repository by adding a simple `hubconf.py` file;\n\n`hubconf.py` can have multiple entrypoints. Each entrypoint is defined as a\npython function (example: a pre-trained model you want to publish).\n\nHere is a code snippet specifies an entrypoint for `resnet18` model if we\nexpand the implementation in `pytorch/vision/hubconf.py`. In most case\nimporting the right function in `hubconf.py` is sufficient. Here we just want\nto use the expanded version as an example to show how it works. You can see\nthe full script in pytorch/vision repo\n\nPytorch Hub provides convenient APIs to explore all available models in hub\nthrough `torch.hub.list()`, show docstring and examples through\n`torch.hub.help()` and load the pre-trained models using `torch.hub.load()`.\n\nList all entrypoints available in `github` hubconf.\n\na list of available entrypoint names\n\nentrypoints\n\nShow the docstring of entrypoint `model`.\n\nLoad a model from a github repo or a local directory.\n\nNote: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.\n\nIf `source` is `'github'`, `repo_or_dir` is expected to be of the form\n`repo_owner/repo_name[:tag_name]` with an optional tag/branch.\n\nIf `source` is `'local'`, `repo_or_dir` is expected to be a path to a local\ndirectory.\n\nThe output of the `model` callable when called with the given `*args` and\n`**kwargs`.\n\nDownload object at the given URL to a local path.\n\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it\u2019s deserialized and\nreturned. The default value of `model_dir` is `<hub_dir>/checkpoints` where\n`hub_dir` is the directory returned by `get_dir()`.\n\nNote that `*args` and `**kwargs` in `torch.hub.load()` are used to instantiate\na model. After you have loaded a model, how can you find out what you can do\nwith the model? A suggested workflow is\n\nTo help users explore without referring to documentation back and forth, we\nstrongly recommend repo owners make function help messages clear and succinct.\nIt\u2019s also helpful to include a minimal working example.\n\nThe locations are used in the order of\n\nGet the Torch Hub cache directory used for storing downloaded models &\nweights.\n\nIf `set_dir()` is not called, default path is `$TORCH_HOME/hub` where\nenvironment variable `$TORCH_HOME` defaults to `$XDG_CACHE_HOME/torch`.\n`$XDG_CACHE_HOME` follows the X Design Group specification of the Linux\nfilesystem layout, with a default value `~/.cache` if the environment variable\nis not set.\n\nOptionally set the Torch Hub directory used to save downloaded models &\nweights.\n\nd (string) \u2013 path to a local folder to save downloaded models & weights.\n\nBy default, we don\u2019t clean up files after loading it. Hub uses the cache by\ndefault if it already exists in the directory returned by `get_dir()`.\n\nUsers can force a reload by calling `hub.load(..., force_reload=True)`. This\nwill delete the existing github folder and downloaded weights, reinitialize a\nfresh download. This is useful when updates are published to the same branch,\nusers can keep up with the latest release.\n\nTorch hub works by importing the package as if it was installed. There\u2019re some\nside effects introduced by importing in Python. For example, you can see new\nitems in Python caches `sys.modules` and `sys.path_importer_cache` which is\nnormal Python behavior.\n\nA known limitation that worth mentioning here is user CANNOT load two\ndifferent branches of the same repo in the same python process. It\u2019s just like\ninstalling two packages with the same name in Python, which is not good. Cache\nmight join the party and give you surprises if you actually try that. Of\ncourse it\u2019s totally fine to load them in separate processes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.download_url_to_file()", "path": "hub#torch.hub.download_url_to_file", "type": "torch.hub", "text": "\nDownload object at the given URL to a local path.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.get_dir()", "path": "hub#torch.hub.get_dir", "type": "torch.hub", "text": "\nGet the Torch Hub cache directory used for storing downloaded models &\nweights.\n\nIf `set_dir()` is not called, default path is `$TORCH_HOME/hub` where\nenvironment variable `$TORCH_HOME` defaults to `$XDG_CACHE_HOME/torch`.\n`$XDG_CACHE_HOME` follows the X Design Group specification of the Linux\nfilesystem layout, with a default value `~/.cache` if the environment variable\nis not set.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.help()", "path": "hub#torch.hub.help", "type": "torch.hub", "text": "\nShow the docstring of entrypoint `model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.list()", "path": "hub#torch.hub.list", "type": "torch.hub", "text": "\nList all entrypoints available in `github` hubconf.\n\na list of available entrypoint names\n\nentrypoints\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.load()", "path": "hub#torch.hub.load", "type": "torch.hub", "text": "\nLoad a model from a github repo or a local directory.\n\nNote: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.\n\nIf `source` is `'github'`, `repo_or_dir` is expected to be of the form\n`repo_owner/repo_name[:tag_name]` with an optional tag/branch.\n\nIf `source` is `'local'`, `repo_or_dir` is expected to be a path to a local\ndirectory.\n\nThe output of the `model` callable when called with the given `*args` and\n`**kwargs`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.load_state_dict_from_url()", "path": "hub#torch.hub.load_state_dict_from_url", "type": "torch.hub", "text": "\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it\u2019s deserialized and\nreturned. The default value of `model_dir` is `<hub_dir>/checkpoints` where\n`hub_dir` is the directory returned by `get_dir()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.set_dir()", "path": "hub#torch.hub.set_dir", "type": "torch.hub", "text": "\nOptionally set the Torch Hub directory used to save downloaded models &\nweights.\n\nd (string) \u2013 path to a local folder to save downloaded models & weights.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hypot()", "path": "generated/torch.hypot#torch.hypot", "type": "torch", "text": "\nGiven the legs of a right triangle, return its hypotenuse.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.i0()", "path": "generated/torch.i0#torch.i0", "type": "torch", "text": "\nComputes the zeroth order modified Bessel function of the first kind for each\nelement of `input`.\n\ninput (Tensor) \u2013 the input tensor\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.igamma()", "path": "generated/torch.igamma#torch.igamma", "type": "torch", "text": "\nComputes the regularized lower incomplete gamma function:\n\nwhere both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive\nand at least one is strictly positive. If both are zero or either is negative\nthen outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot) in the equation above\nis the gamma function,\n\nSee `torch.igammac()` and `torch.lgamma()` for related functions.\n\nSupports broadcasting to a common shape and float inputs.\n\nNote\n\nThe backward pass with respect to `input` is not yet supported. Please open an\nissue on PyTorch\u2019s Github to request it.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.igammac()", "path": "generated/torch.igammac#torch.igammac", "type": "torch", "text": "\nComputes the regularized upper incomplete gamma function:\n\nwhere both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive\nand at least one is strictly positive. If both are zero or either is negative\nthen outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot) in the equation above\nis the gamma function,\n\nSee `torch.igamma()` and `torch.lgamma()` for related functions.\n\nSupports broadcasting to a common shape and float inputs.\n\nNote\n\nThe backward pass with respect to `input` is not yet supported. Please open an\nissue on PyTorch\u2019s Github to request it.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.imag()", "path": "generated/torch.imag#torch.imag", "type": "torch", "text": "\nReturns a new tensor containing imaginary values of the `self` tensor. The\nreturned tensor and `self` share the same underlying storage.\n\nWarning\n\n`imag()` is only supported for tensors with complex dtypes.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.index_select()", "path": "generated/torch.index_select#torch.index_select", "type": "torch", "text": "\nReturns a new tensor which indexes the `input` tensor along dimension `dim`\nusing the entries in `index` which is a `LongTensor`.\n\nThe returned tensor has the same number of dimensions as the original tensor\n(`input`). The `dim`th dimension has the same size as the length of `index`;\nother dimensions have the same size as in the original tensor.\n\nNote\n\nThe returned tensor does not use the same storage as the original tensor. If\n`out` has a different shape than expected, we silently change it to the\ncorrect shape, reallocating the underlying storage if necessary.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.initial_seed()", "path": "generated/torch.initial_seed#torch.initial_seed", "type": "torch", "text": "\nReturns the initial seed for generating random numbers as a Python `long`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.inner()", "path": "generated/torch.inner#torch.inner", "type": "torch", "text": "\nComputes the dot product for 1D tensors. For higher dimensions, sums the\nproduct of elements from `input` and `other` along their last dimension.\n\nNote\n\nIf either `input` or `other` is a scalar, the result is equivalent to\n`torch.mul(input, other)`.\n\nIf both `input` and `other` are non-scalars, the size of their last dimension\nmust match and the result is equivalent to `torch.tensordot(input, other,\ndims=([-1], [-1]))`\n\nout (Tensor, optional) \u2013 Optional output tensor to write result into. The\noutput shape is `input.shape[:-1] + other.shape[:-1]`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.inverse()", "path": "generated/torch.inverse#torch.inverse", "type": "torch", "text": "\nTakes the inverse of the square matrix `input`. `input` can be batches of 2D\nsquare tensors, in which case this function would return a tensor composed of\nindividual inverses.\n\nSupports real and complex input.\n\nNote\n\n`torch.inverse()` is deprecated. Please use `torch.linalg.inv()` instead.\n\nNote\n\nIrrespective of the original strides, the returned tensors will be transposed,\ni.e. with strides like `input.contiguous().transpose(-2, -1).stride()`\n\ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n) where `*` is zero\nor more batch dimensions\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isclose()", "path": "generated/torch.isclose#torch.isclose", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is \u201cclose\u201d to the corresponding element of `other`. Closeness is\ndefined as:\n\nwhere `input` and `other` are finite. Where `input` and/or `other` are\nnonfinite they are close if and only if they are equal, with NaNs being\nconsidered equal to each other when `equal_nan` is True.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isfinite()", "path": "generated/torch.isfinite#torch.isfinite", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element is\n`finite` or not.\n\nReal values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite.\n\ninput (Tensor): the input tensor.\n\nA boolean tensor that is True where `input` is finite and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isinf()", "path": "generated/torch.isinf#torch.isinf", "type": "torch", "text": "\nTests if each element of `input` is infinite (positive or negative infinity)\nor not.\n\nNote\n\nComplex values are infinite when their real or imaginary part is infinite.\n\n{input}\n\nA boolean tensor that is True where `input` is infinite and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isnan()", "path": "generated/torch.isnan#torch.isnan", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is NaN or not. Complex values are considered NaN when either their\nreal and/or imaginary part is NaN.\n\ninput (Tensor) \u2013 the input tensor.\n\nA boolean tensor that is True where `input` is NaN and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isneginf()", "path": "generated/torch.isneginf#torch.isneginf", "type": "torch", "text": "\nTests if each element of `input` is negative infinity or not.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isposinf()", "path": "generated/torch.isposinf#torch.isposinf", "type": "torch", "text": "\nTests if each element of `input` is positive infinity or not.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isreal()", "path": "generated/torch.isreal#torch.isreal", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is real-valued or not. All real-valued types are considered real.\nComplex values are considered real when their imaginary part is 0.\n\ninput (Tensor) \u2013 the input tensor.\n\nA boolean tensor that is True where `input` is real and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.istft()", "path": "generated/torch.istft#torch.istft", "type": "torch", "text": "\nInverse short time Fourier Transform. This is expected to be the inverse of\n`stft()`. It has the same parameters (+ additional optional parameter of\n`length`) and it should return the least squares estimation of the original\nsignal. The algorithm will check using the NOLA condition ( nonzero overlap).\n\nImportant consideration in the parameters `window` and `center` so that the\nenvelop created by the summation of all the windows is never zero at certain\npoint in time. Specifically,\n\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times\nhop\\\\_length] \\cancel{=} 0 .\n\nSince `stft()` discards elements at the end of the signal if they do not fit\nin a frame, `istft` may return a shorter signal than the original signal (can\noccur if `center` is False since the signal isn\u2019t padded).\n\nIf `center` is `True`, then there will be padding e.g. `'constant'`,\n`'reflect'`, etc. Left padding can be trimmed off exactly because they can be\ncalculated but right padding cannot be calculated without additional\ninformation.\n\nExample: Suppose the last window is: `[17, 18, 0, 0, 0]` vs `[18, 0, 0, 0, 0]`\n\nThe `n_fft`, `hop_length`, `win_length` are all the same which prevents the\ncalculation of right padding. These additional values could be zeros or a\nreflection of the signal so providing `length` could be useful. If `length` is\n`None` then padding will be aggressively removed (some loss of signal).\n\n[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time\nFourier transform,\u201d IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.\n\ninput (Tensor) \u2013\n\nThe input tensor. Expected to be output of `stft()`, can either be complex\n(`channel`, `fft_size`, `n_frame`), or real (`channel`, `fft_size`, `n_frame`,\n2) where the `channel` dimension is optional.\n\nDeprecated since version 1.8.0: Real input is deprecated, use complex inputs\nas returned by `stft(..., return_complex=True)` instead.\n\nLeast squares estimation of the original signal of size (\u2026, signal_length)\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_complex()", "path": "generated/torch.is_complex#torch.is_complex", "type": "torch", "text": "\nReturns True if the data type of `input` is a complex data type i.e., one of\n`torch.complex64`, and `torch.complex128`.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_floating_point()", "path": "generated/torch.is_floating_point#torch.is_floating_point", "type": "torch", "text": "\nReturns True if the data type of `input` is a floating point data type i.e.,\none of `torch.float64`, `torch.float32`, `torch.float16`, and\n`torch.bfloat16`.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_nonzero()", "path": "generated/torch.is_nonzero#torch.is_nonzero", "type": "torch", "text": "\nReturns True if the `input` is a single element tensor which is not equal to\nzero after type conversions. i.e. not equal to `torch.tensor([0.])` or\n`torch.tensor([0])` or `torch.tensor([False])`. Throws a `RuntimeError` if\n`torch.numel() != 1` (even in case of sparse tensors).\n\ninput (Tensor) \u2013 the input tensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_storage()", "path": "generated/torch.is_storage#torch.is_storage", "type": "torch", "text": "\nReturns True if `obj` is a PyTorch storage object.\n\nobj (Object) \u2013 Object to test\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_tensor()", "path": "generated/torch.is_tensor#torch.is_tensor", "type": "torch", "text": "\nReturns True if `obj` is a PyTorch tensor.\n\nNote that this function is simply doing `isinstance(obj, Tensor)`. Using that\n`isinstance` check is better for typechecking with mypy, and more explicit -\nso it\u2019s recommended to use that instead of `is_tensor`.\n\nobj (Object) \u2013 Object to test\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.export()", "path": "jit#torch.jit.export", "type": "TorchScript", "text": "\nThis decorator indicates that a method on an `nn.Module` is used as an entry\npoint into a `ScriptModule` and should be compiled.\n\n`forward` implicitly is assumed to be an entry point, so it does not need this\ndecorator. Functions and methods called from `forward` are compiled as they\nare seen by the compiler, so they do not need this decorator either.\n\nExample (using `@torch.jit.export` on a method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.fork()", "path": "generated/torch.jit.fork#torch.jit.fork", "type": "TorchScript", "text": "\nCreates an asynchronous task executing `func` and a reference to the value of\nthe result of this execution. `fork` will return immediately, so the return\nvalue of `func` may not have been computed yet. To force completion of the\ntask and access the return value invoke `torch.jit.wait` on the Future. `fork`\ninvoked with a `func` which returns `T` is typed as `torch.jit.Future[T]`.\n`fork` calls can be arbitrarily nested, and may be invoked with positional and\nkeyword arguments. Asynchronous execution will only occur when run in\nTorchScript. If run in pure python, `fork` will not execute in parallel.\n`fork` will also not execute in parallel when invoked while tracing, however\nthe `fork` and `wait` calls will be captured in the exported IR Graph. ..\nwarning:\n\na reference to the execution of `func`. The value `T` can only be accessed by\nforcing completion of `func` through `torch.jit.wait`.\n\n`torch.jit.Future[T]`\n\nExample (fork a free function):\n\nExample (fork a module method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.freeze()", "path": "generated/torch.jit.freeze#torch.jit.freeze", "type": "TorchScript", "text": "\nFreezing a `ScriptModule` will clone it and attempt to inline the cloned\nmodule\u2019s submodules, parameters, and attributes as constants in the\nTorchScript IR Graph. By default, `forward` will be preserved, as well as\nattributes & methods specified in `preserved_attrs`. Additionally, any\nattribute that is modified within a preserved method will be preserved.\n\nFreezing currently only accepts ScriptModules that are in eval mode.\n\nFrozen `ScriptModule`.\n\nExample (Freezing a simple module with a Parameter):\n\nExample (Freezing a module with preserved attributes)\n\nNote\n\nIf you\u2019re not sure why an attribute is not being inlined as a constant, you\ncan run `dump_alias_db` on frozen_module.forward.graph to see if freezing has\ndetected the attribute is being modified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ignore()", "path": "generated/torch.jit.ignore#torch.jit.ignore", "type": "TorchScript", "text": "\nThis decorator indicates to the compiler that a function or method should be\nignored and left as a Python function. This allows you to leave code in your\nmodel that is not yet TorchScript compatible. If called from TorchScript,\nignored functions will dispatch the call to the Python interpreter. Models\nwith ignored functions cannot be exported; use `@torch.jit.unused` instead.\n\nExample (using `@torch.jit.ignore` on a method):\n\nExample (using `@torch.jit.ignore(drop=True)` on a method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.isinstance()", "path": "generated/torch.jit.isinstance#torch.jit.isinstance", "type": "TorchScript", "text": "\nThis function provides for conatiner type refinement in TorchScript. It can\nrefine parameterized containers of the List, Dict, Tuple, and Optional types.\nE.g. `List[str]`, `Dict[str, List[torch.Tensor]]`,\n`Optional[Tuple[int,str,int]]`. It can also refine basic types such as bools\nand ints that are available in TorchScript.\n\nFalse otherwise with no new type refinement\n\n`bool`\n\nExample (using `torch.jit.isinstance` for type refinement): .. testcode:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.is_scripting()", "path": "jit_language_reference#torch.jit.is_scripting", "type": "TorchScript", "text": "\nFunction that returns True when in compilation and False otherwise. This is\nuseful especially with the @unused decorator to leave code in your model that\nis not yet TorchScript compatible. .. testcode:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.load()", "path": "generated/torch.jit.load#torch.jit.load", "type": "TorchScript", "text": "\nLoad a `ScriptModule` or `ScriptFunction` previously saved with\n`torch.jit.save`\n\nAll previously saved modules, no matter their device, are first loaded onto\nCPU, and then are moved to the devices they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised.\n\nA `ScriptModule` object.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.save()", "path": "generated/torch.jit.save#torch.jit.save", "type": "TorchScript", "text": "\nSave an offline version of this module for use in a separate process. The\nsaved module serializes all of the methods, submodules, parameters, and\nattributes of this module. It can be loaded into the C++ API using\n`torch::jit::load(filename)` or into the Python API with `torch.jit.load`.\n\nTo be able to save a module, it must not make any calls to native Python\nfunctions. This means that all submodules must be subclasses of `ScriptModule`\nas well.\n\nDanger\n\nAll modules, no matter their device, are always loaded onto the CPU during\nloading. This is different from `torch.load()`\u2019s semantics and may change in\nthe future.\n\nNote\n\ntorch.jit.save attempts to preserve the behavior of some operators across\nversions. For example, dividing two integer tensors in PyTorch 1.5 performed\nfloor division, and if the module containing that code is saved in PyTorch 1.5\nand loaded in PyTorch 1.6 its division behavior will be preserved. The same\nmodule saved in PyTorch 1.6 will fail to load in PyTorch 1.5, however, since\nthe behavior of division changed in 1.6, and 1.5 does not know how to\nreplicate the 1.6 behavior.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.script()", "path": "generated/torch.jit.script#torch.jit.script", "type": "TorchScript", "text": "\nScripting a function or `nn.Module` will inspect the source code, compile it\nas TorchScript code using the TorchScript compiler, and return a\n`ScriptModule` or `ScriptFunction`. TorchScript itself is a subset of the\nPython language, so not all features in Python work, but we provide enough\nfunctionality to compute on tensors and do control-dependent operations. For a\ncomplete guide, see the TorchScript Language Reference.\n\n`torch.jit.script` can be used as a function for modules and functions, and as\na decorator `@torch.jit.script` for TorchScript Classes and functions.\n\nobj (callable, class, or `nn.Module`) \u2013 The `nn.Module`, function, or class\ntype to compile.\n\nIf `obj` is `nn.Module`, `script` returns a `ScriptModule` object. The\nreturned `ScriptModule` will have the same set of sub-modules and parameters\nas the original `nn.Module`. If `obj` is a standalone function, a\n`ScriptFunction` will be returned.\n\nThe `@torch.jit.script` decorator will construct a `ScriptFunction` by\ncompiling the body of the function.\n\nExample (scripting a function):\n\nScripting an `nn.Module` by default will compile the `forward` method and\nrecursively compile any methods, submodules, and functions called by\n`forward`. If a `nn.Module` only uses features supported in TorchScript, no\nchanges to the original module code should be necessary. `script` will\nconstruct `ScriptModule` that has copies of the attributes, parameters, and\nmethods of the original module.\n\nExample (scripting a simple module with a Parameter):\n\nExample (scripting a module with traced submodules):\n\nTo compile a method other than `forward` (and recursively compile anything it\ncalls), add the `@torch.jit.export` decorator to the method. To opt out of\ncompilation use `@torch.jit.ignore` or `@torch.jit.unused`.\n\nExample (an exported and ignored method in a module):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction", "type": "TorchScript", "text": "\nFunctionally equivalent to a `ScriptModule`, but represents a single function\nand does not have any attributes or Parameters.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.get_debug_state()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.get_debug_state", "type": "TorchScript", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.save()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save", "type": "TorchScript", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.save_to_buffer()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer", "type": "TorchScript", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule", "type": "TorchScript", "text": "\nA wrapper around C++ `torch::jit::Module`. `ScriptModule`s contain methods,\nattributes, parameters, and constants. These can be accessed the same as on a\nnormal `nn.Module`.\n\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\nReturns a pretty-printed representation (as valid Python syntax) of the\ninternal graph for the `forward` method. See Inspecting Code for details.\n\nReturns a tuple of:\n\n[0] a pretty-printed representation (as valid Python syntax) of the internal\ngraph for the `forward` method. See `code`. [1] a ConstMap following the\nCONSTANT.cN format of the output in [0]. The indices in the [0] output are\nkeys to the underlying constant\u2019s values.\n\nSee Inspecting Code for details.\n\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\nReturns a string representation of the internal graph for the `forward`\nmethod. See Interpreting Graphs for details.\n\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\nReturns a string representation of the internal graph for the `forward`\nmethod. This graph will be preprocessed to inline all function and method\ncalls. See Interpreting Graphs for details.\n\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\nSee `torch.jit.save` for details.\n\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.add_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.add_module", "type": "TorchScript", "text": "\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.apply()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.apply", "type": "TorchScript", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.bfloat16()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.bfloat16", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.buffers", "type": "TorchScript", "text": "\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.children", "type": "TorchScript", "text": "\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.code()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code", "type": "TorchScript", "text": "\nReturns a pretty-printed representation (as valid Python syntax) of the\ninternal graph for the `forward` method. See Inspecting Code for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.code_with_constants()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code_with_constants", "type": "TorchScript", "text": "\nReturns a tuple of:\n\n[0] a pretty-printed representation (as valid Python syntax) of the internal\ngraph for the `forward` method. See `code`. [1] a ConstMap following the\nCONSTANT.cN format of the output in [0]. The indices in the [0] output are\nkeys to the underlying constant\u2019s values.\n\nSee Inspecting Code for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.cpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cpu", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.cuda()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cuda", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.double()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.double", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.eval()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.eval", "type": "TorchScript", "text": "\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.extra_repr()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.extra_repr", "type": "TorchScript", "text": "\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.float()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.float", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.graph", "type": "TorchScript", "text": "\nReturns a string representation of the internal graph for the `forward`\nmethod. See Interpreting Graphs for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.half()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.half", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.inlined_graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.inlined_graph", "type": "TorchScript", "text": "\nReturns a string representation of the internal graph for the `forward`\nmethod. This graph will be preprocessed to inline all function and method\ncalls. See Interpreting Graphs for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.load_state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.load_state_dict", "type": "TorchScript", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.modules", "type": "TorchScript", "text": "\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_buffers", "type": "TorchScript", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_children", "type": "TorchScript", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_modules", "type": "TorchScript", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_parameters", "type": "TorchScript", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.parameters", "type": "TorchScript", "text": "\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_backward_hook", "type": "TorchScript", "text": "\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_buffer", "type": "TorchScript", "text": "\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_forward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_hook", "type": "TorchScript", "text": "\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_forward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_pre_hook", "type": "TorchScript", "text": "\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_full_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_hook", "type": "TorchScript", "text": "\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_parameter", "type": "TorchScript", "text": "\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.requires_grad_()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.requires_grad_", "type": "TorchScript", "text": "\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.save()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.save", "type": "TorchScript", "text": "\nSee `torch.jit.save` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.state_dict", "type": "TorchScript", "text": "\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.to()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to", "type": "TorchScript", "text": "\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.train()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.train", "type": "TorchScript", "text": "\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.type()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.type", "type": "TorchScript", "text": "\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.xpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.xpu", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.zero_grad()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.zero_grad", "type": "TorchScript", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.script_if_tracing()", "path": "generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing", "type": "TorchScript", "text": "\nCompiles `fn` when it is first called during tracing. `torch.jit.script` has a\nnon-negligible start up time when it is first called due to lazy-\ninitializations of many compiler builtins. Therefore you should not use it in\nlibrary code. However, you may want to have parts of your library work in\ntracing even if they use control flow. In these cases, you should use\n`@torch.jit.script_if_tracing` to substitute for `torch.jit.script`.\n\nfn \u2013 A function to compile.\n\nIf called during tracing, a `ScriptFunction` created by `torch.jit.script` is\nreturned. Otherwise, the original function `fn` is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.trace()", "path": "generated/torch.jit.trace#torch.jit.trace", "type": "TorchScript", "text": "\nTrace a function and return an executable or `ScriptFunction` that will be\noptimized using just-in-time compilation. Tracing is ideal for code that\noperates only on `Tensor`s and lists, dictionaries, and tuples of `Tensor`s.\n\nUsing `torch.jit.trace` and `torch.jit.trace_module`, you can turn an existing\nmodule or Python function into a TorchScript `ScriptFunction` or\n`ScriptModule`. You must provide example inputs, and we run the function,\nrecording the operations performed on all the tensors.\n\nThis module also contains any parameters that the original module had as well.\n\nWarning\n\nTracing only correctly records functions and modules which are not data\ndependent (e.g., do not have conditionals on data in tensors) and do not have\nany untracked external dependencies (e.g., perform input/output or access\nglobal variables). Tracing only records operations done when the given\nfunction is run on the given tensors. Therefore, the returned `ScriptModule`\nwill always run the same traced graph on any input. This has some important\nimplications when your module is expected to run different sets of operations,\ndepending on the input and/or the module state. For example,\n\nIn cases like these, tracing would not be appropriate and `scripting` is a\nbetter choice. If you trace such models, you may silently get incorrect\nresults on subsequent invocations of the model. The tracer will try to emit\nwarnings when doing something that may cause an incorrect trace to be\nproduced.\n\nIf `func` is `nn.Module` or `forward` of `nn.Module`, `trace` returns a\n`ScriptModule` object with a single `forward` method containing the traced\ncode. The returned `ScriptModule` will have the same set of sub-modules and\nparameters as the original `nn.Module`. If `func` is a standalone function,\n`trace` returns `ScriptFunction`.\n\nExample (tracing a function):\n\nExample (tracing an existing module):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.trace_module()", "path": "generated/torch.jit.trace_module#torch.jit.trace_module", "type": "TorchScript", "text": "\nTrace a module and return an executable `ScriptModule` that will be optimized\nusing just-in-time compilation. When a module is passed to `torch.jit.trace`,\nonly the `forward` method is run and traced. With `trace_module`, you can\nspecify a dictionary of method names to example inputs to trace (see the\n`inputs`) argument below.\n\nSee `torch.jit.trace` for more information on tracing.\n\nA `ScriptModule` object with a single `forward` method containing the traced\ncode. When `func` is a `torch.nn.Module`, the returned `ScriptModule` will\nhave the same set of sub-modules and parameters as `func`.\n\nExample (tracing a module with multiple methods):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.unused()", "path": "generated/torch.jit.unused#torch.jit.unused", "type": "TorchScript", "text": "\nThis decorator indicates to the compiler that a function or method should be\nignored and replaced with the raising of an exception. This allows you to\nleave code in your model that is not yet TorchScript compatible and still\nexport your model.\n\nExample (using `@torch.jit.unused` on a method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.wait()", "path": "generated/torch.jit.wait#torch.jit.wait", "type": "TorchScript", "text": "\nForces completion of a `torch.jit.Future[T]` asynchronous task, returning the\nresult of the task. See `fork()` for docs and examples. :param func: an\nasynchronous task reference, created through `torch.jit.fork` :type func:\ntorch.jit.Future[T]\n\nthe return value of the the completed task\n\n`T`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.kaiser_window()", "path": "generated/torch.kaiser_window#torch.kaiser_window", "type": "torch", "text": "\nComputes the Kaiser window with window length `window_length` and shape\nparameter `beta`.\n\nLet I_0 be the zeroth order modified Bessel function of the first kind (see\n`torch.i0()`) and `N = L - 1` if `periodic` is False and `L` if `periodic` is\nTrue, where `L` is the `window_length`. This function computes:\n\nCalling `torch.kaiser_window(L, B, periodic=True)` is equivalent to calling\n`torch.kaiser_window(L + 1, B, periodic=False)[:-1])`. The `periodic` argument\nis intended as a helpful shorthand to produce a periodic window as input to\nfunctions like `torch.stft()`.\n\nNote\n\nIf `window_length` is one, then the returned window is a single element tensor\ncontaining a one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.kron()", "path": "generated/torch.kron#torch.kron", "type": "torch", "text": "\nComputes the Kronecker product, denoted by \u2297\\otimes , of `input` and `other`.\n\nIf `input` is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n) tensor\nand `other` is a (b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n) tensor,\nthe result will be a (a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots\n\\times a_n*b_n) tensor with the following entries:\n\nwhere kt=it\u2217bt+jtk_t = i_t * b_t + j_t for 0\u2264t\u2264n0 \\leq t \\leq n . If one\ntensor has fewer dimensions than the other it is unsqueezed until it has the\nsame number of dimensions.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nThis function generalizes the typical definition of the Kronecker product for\ntwo matrices to two tensors, as described above. When `input` is a (m\u00d7n)(m\n\\times n) matrix and `other` is a (p\u00d7q)(p \\times q) matrix, the result will be\na (p\u2217m\u00d7q\u2217n)(p*m \\times q*n) block matrix:\n\nwhere `input` is A\\mathbf{A} and `other` is B\\mathbf{B} .\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.kthvalue()", "path": "generated/torch.kthvalue#torch.kthvalue", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the `k` th smallest\nelement of each row of the `input` tensor in the given dimension `dim`. And\n`indices` is the index location of each element found.\n\nIf `dim` is not given, the last dimension of the `input` is chosen.\n\nIf `keepdim` is `True`, both the `values` and `indices` tensors are the same\nsize as `input`, except in the dimension `dim` where they are of size 1.\nOtherwise, `dim` is squeezed (see `torch.squeeze()`), resulting in both the\n`values` and `indices` tensors having 1 fewer dimension than the `input`\ntensor.\n\nNote\n\nWhen `input` is a CUDA tensor and there are multiple valid `k` th values, this\nfunction may nondeterministically return `indices` for any of them.\n\nout (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) can be\noptionally given to be used as output buffers\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lcm()", "path": "generated/torch.lcm#torch.lcm", "type": "torch", "text": "\nComputes the element-wise least common multiple (LCM) of `input` and `other`.\n\nBoth `input` and `other` must have integer types.\n\nNote\n\nThis defines lcm(0,0)=0lcm(0, 0) = 0 and lcm(0,a)=0lcm(0, a) = 0 .\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ldexp()", "path": "generated/torch.ldexp#torch.ldexp", "type": "torch", "text": "\nMultiplies `input` by 2**:attr:`other`.\n\nTypically this function is used to construct floating point numbers by\nmultiplying mantissas in `input` with integral powers of two created from the\nexponents in :attr:\u2019other\u2019.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.le()", "path": "generated/torch.le#torch.le", "type": "torch", "text": "\nComputes input\u2264other\\text{input} \\leq \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is less than or equal to `other`\nand False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lerp()", "path": "generated/torch.lerp#torch.lerp", "type": "torch", "text": "\nDoes a linear interpolation of two tensors `start` (given by `input`) and\n`end` based on a scalar or tensor `weight` and returns the resulting `out`\ntensor.\n\nThe shapes of `start` and `end` must be broadcastable. If `weight` is a\ntensor, then the shapes of `weight`, `start`, and `end` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.less()", "path": "generated/torch.less#torch.less", "type": "torch", "text": "\nAlias for `torch.lt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.less_equal()", "path": "generated/torch.less_equal#torch.less_equal", "type": "torch", "text": "\nAlias for `torch.le()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lgamma()", "path": "generated/torch.lgamma#torch.lgamma", "type": "torch", "text": "\nComputes the logarithm of the gamma function on `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg", "path": "linalg", "type": "torch.linalg", "text": "\nCommon linear algebra operations.\n\nThis module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details.\n\nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-\nvalued matrices) positive-definite matrix or the Cholesky decompositions for a\nbatch of such matrices. Each decomposition has the form:\n\nwhere LL is a lower-triangular matrix and LHL^H is the conjugate transpose of\nLL , which is just a transpose for the case of real-valued input matrices. In\ncode it translates to `input = L @ L.t()` if `input` is real-valued and `input\n= L @ L.conj().t()` if `input` is complex-valued. The batch of LL matrices is\nreturned.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nLAPACK\u2019s `potrf` is used for CPU inputs, and MAGMA\u2019s `potrf` is used for CUDA\ninputs.\n\nNote\n\nIf `input` is not a Hermitian positive-definite matrix, or if it\u2019s a batch of\nmatrices and one or more of them is not a Hermitian positive-definite matrix,\nthen a RuntimeError will be thrown. If `input` is a batch of matrices, then\nthe error message will include the batch index of the first matrix that is not\nHermitian positive-definite.\n\ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n) consisting of\nHermitian positive-definite n\u00d7nn \\times n matrices, where \u2217* is zero or more\nbatch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nComputes the condition number of a matrix `input`, or of each matrix in a\nbatched `input`, using the matrix norm defined by `p`.\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` this is defined as the matrix\nnorm of `input` times the matrix norm of the inverse of `input` computed using\n`torch.linalg.norm()`. While for norms `{None, 2, -2}` this is defined as the\nratio between the largest and smallest singular values computed using\n`torch.linalg.svd()`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function may synchronize that device\nwith the CPU depending on which norm `p` is used.\n\nNote\n\nFor norms `{None, 2, -2}`, `input` may be a non-square matrix or batch of non-\nsquare matrices. For other norms, however, `input` must be a square matrix or\na batch of square matrices, and if this requirement is not satisfied a\nRuntimeError will be thrown.\n\nNote\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` if `input` is a non-invertible\nmatrix then a tensor containing infinity will be returned. If `input` is a\nbatch of matrices and one or more of them is not invertible then a\nRuntimeError will be thrown.\n\np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nthe type of the matrix norm to use in the computations. inf refers to\n`float('inf')`, numpy\u2019s `inf` object, or any equivalent object. The following\nnorms can be used:\n\np\n\nnorm for matrices\n\nNone\n\nratio of the largest singular value to the smallest singular value\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2019nuc\u2019\n\nnuclear norm\n\ninf\n\nmax(sum(abs(x), dim=1))\n\n-inf\nmin(sum(abs(x), dim=1))\n\n1\n\nmax(sum(abs(x), dim=0))\n\n-1\nmin(sum(abs(x), dim=0))\n\n2\n\nratio of the largest singular value to the smallest singular value\n\n-2\nratio of the smallest singular value to the largest singular value\n\nDefault: `None`\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nThe condition number of `input`. The output dtype is always real valued even\nfor complex inputs (e.g. float if `input` is cfloat).\n\nExamples:\n\nComputes the determinant of a square matrix `input`, or of each square matrix\nin a batched `input`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nBackward through `det` internally uses `torch.linalg.svd()` when `input` is\nnot invertible. In this case, double backward through `det` will be unstable\nwhen `input` doesn\u2019t have distinct singular values. See `torch.linalg.svd()`\nfor more details.\n\ninput (Tensor) \u2013 the input matrix of size `(n, n)` or the batch of matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nExample:\n\nCalculates the sign and natural logarithm of the absolute value of a square\nmatrix\u2019s determinant, or of the absolute values of the determinants of a batch\nof square matrices `input`. The determinant can be computed with `sign *\nexp(logabsdet)`.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nFor matrices that have zero determinant, this returns `(0, -inf)`. If `input`\nis batched then the entries in the result tensors corresponding to matrices\nwith the zero determinant have sign 0 and the natural logarithm of the\nabsolute value of the determinant -inf.\n\ninput (Tensor) \u2013 the input matrix of size (n,n)(n, n) or the batch of matrices\nof size (\u2217,n,n)(*, n, n) where \u2217* is one or more batch dimensions.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to.\n\nA namedtuple (sign, logabsdet) containing the sign of the determinant and the\nnatural logarithm of the absolute value of determinant, respectively.\n\nExample:\n\nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real\nsymmetric) matrix `input`, or of each such matrix in a batched `input`.\n\nFor a single matrix `input`, the tensor of eigenvalues `w` and the tensor of\neigenvectors `V` decompose the `input` such that `input = V diag(w) V\u1d34`, where\n`V\u1d34` is the transpose of `V` for real-valued `input`, or the conjugate\ntranspose of `V` for complex-valued `input`.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues/eigenvectors are computed using LAPACK\u2019s `syevd` and `heevd`\nroutines for CPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA\ninputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThe eigenvectors of matrices are not unique, so any eigenvector multiplied by\na constant remains a valid eigenvector. This function may compute different\neigenvector representations on different device types. Usually the difference\nis only in the sign of the eigenvector.\n\nNote\n\nSee `torch.linalg.eigvalsh()` for a related function that computes only\neigenvalues. However, that function is not differentiable.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to. Default\nis `None`.\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\nThe eigenvalues in ascending order.\n\nThe orthonormal eigenvectors of the `input`.\n\n(Tensor, Tensor)\n\nExamples:\n\nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix\n`input`, or of each such matrix in a batched `input`. The eigenvalues are\nreturned in ascending order.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues are computed using LAPACK\u2019s `syevd` and `heevd` routines for\nCPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA inputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThis function doesn\u2019t support backpropagation, please use\n`torch.linalg.eigh()` instead, which also computes the eigenvectors.\n\nNote\n\nSee `torch.linalg.eigh()` for a related function that computes both\neigenvalues and eigenvectors.\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\nComputes the numerical rank of a matrix `input`, or of each matrix in a\nbatched `input`.\n\nThe matrix rank is computed as the number of singular values (or absolute\neigenvalues when `hermitian` is `True`) that are greater than the specified\n`tol` threshold.\n\nIf `tol` is not specified, `tol` is set to\n`S.max(dim=-1)*max(input.shape[-2:])*eps`, where `S` is the singular values\n(or absolute eigenvalues when `hermitian` is `True`), and `eps` is the epsilon\nvalue for the datatype of `input`. The epsilon value can be obtained using the\n`eps` attribute of `torch.finfo`.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe matrix rank is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation is\ndone by obtaining the eigenvalues (see `torch.linalg.eigvalsh()`).\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\nReturns the matrix norm or vector norm of a given tensor.\n\nThis function can calculate one of eight different types of matrix norms, or\none of an infinite number of vector norms, depending on both the number of\nreduction dimensions and the value of the `ord` parameter.\n\nord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nThe order of norm. inf refers to `float('inf')`, numpy\u2019s `inf` object, or any\nequivalent object. The following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2013 not supported \u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013 not supported \u2013\n\ninf\n\nmax(sum(abs(x), dim=1))\n\nmax(abs(x))\n\n-inf\nmin(sum(abs(x), dim=1))\n\nmin(abs(x))\n\n0\n\n\u2013 not supported \u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), dim=0))\n\nas below\n\n-1\nmin(sum(abs(x), dim=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\nsmallest singular value\n\nas below\n\nother\n\n\u2013 not supported \u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nDefault: `None`\n\nExamples:\n\nUsing the `dim` argument to compute vector norms:\n\nUsing the `dim` argument to compute matrix norms:\n\nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a\nmatrix `input`, or of each matrix in a batched `input`.\n\nThe singular values (or the absolute values of the eigenvalues when\n`hermitian` is `True`) that are below the specified `rcond` threshold are\ntreated as zero and discarded in the computation.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe pseudo-inverse is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation of the\npseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see\n`torch.linalg.eigh()`).\n\nNote\n\nIf singular value decomposition or eigenvalue decomposition algorithms do not\nconverge then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\u201d The singular value decomposition is represented as a\nnamedtuple `(U, S, Vh)`, such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@}\ndiag(S) \\times Vh . If `input` is a batch of tensors, then `U`, `S`, and `Vh`\nare also batched with the same batch dimensions as `input`.\n\nIf `full_matrices` is `False` (default), the method returns the reduced\nsingular value decomposition i.e., if the last two dimensions of `input` are\n`m` and `n`, then the returned `U` and `V` matrices will contain only\nmin(n,m)min(n, m) orthonormal columns.\n\nIf `compute_uv` is `False`, the returned `U` and `Vh` will be empy tensors\nwith no elements and the same device as `input`. The `full_matrices` argument\nhas no effect when `compute_uv` is False.\n\nThe dtypes of `U` and `V` are the same as `input`\u2019s. `S` will always be real-\nvalued, even if `input` is complex.\n\nNote\n\nUnlike NumPy\u2019s `linalg.svd`, this always returns a namedtuple of three\ntensors, even when `compute_uv=False`. This behavior may change in a future\nPyTorch release.\n\nNote\n\nThe singular values are returned in descending order. If `input` is a batch of\nmatrices, then the singular values of each matrix in the batch is returned in\ndescending order.\n\nNote\n\nThe implementation of SVD on CPU uses the LAPACK routine `?gesdd` (a divide-\nand-conquer algorithm) instead of `?gesvd` for speed. Analogously, the SVD on\nGPU uses the cuSOLVER routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243\nand later, and uses the MAGMA routine `gesdd` on earlier versions of CUDA.\n\nNote\n\nThe returned matrix `U` will be transposed, i.e. with strides\n`U.contiguous().transpose(-2, -1).stride()`.\n\nNote\n\nGradients computed using `U` and `Vh` may be unstable if `input` is not full\nrank or has non-unique singular values.\n\nNote\n\nWhen `full_matrices` = `True`, the gradients on `U[..., :, min(m, n):]` and\n`V[..., :, min(m, n):]` will be ignored in backward as those vectors can be\narbitrary bases of the subspaces.\n\nNote\n\nThe `S` tensor can only be used to compute gradients if `compute_uv` is True.\n\nNote\n\nSince `U` and `V` of an SVD is not unique, each vector can be multiplied by an\narbitrary phase factor ei\u03d5e^{i \\phi} while the SVD result is still correct.\nDifferent platforms, like Numpy, or inputs on different device types, may\nproduce different `U` and `V` tensors.\n\nExample:\n\nComputes the solution `x` to the matrix equation `matmul(input, x) = other`\nwith a square matrix, or batches of such matrices, `input` and one or more\nright-hand side vectors `other`. If `input` is batched and `other` is not,\nthen `other` is broadcast to have the same batch dimensions as `input`. The\nresulting tensor has the same shape as the (possibly broadcast) `other`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` dtypes.\n\nNote\n\nIf `input` is a non-square or non-invertible matrix, or a batch containing\nnon-square matrices or one or more non-invertible matrices, then a\nRuntimeError will be thrown.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nBatched input:\n\nComputes a tensor `input_inv` such that `tensordot(input_inv, input, ind) ==\nI_n` (inverse tensor equation), where `I_n` is the n-dimensional identity\ntensor and `n` is equal to `input.ndim`. The resulting tensor `input_inv` has\nshape equal to `input.shape[ind:] + input.shape[:ind]`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` data types.\n\nNote\n\nIf `input` is not invertible or does not satisfy the requirement\n`prod(input.shape[ind:]) == prod(input.shape[:ind])`, then a RuntimeError will\nbe thrown.\n\nNote\n\nWhen `input` is a 2-dimensional tensor and `ind=1`, this function computes the\n(multiplicative) inverse of `input`, equivalent to calling `torch.inverse()`.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nComputes a tensor `x` such that `tensordot(input, x, dims=x.ndim) = other`.\nThe resulting tensor `x` has the same shape as `input[other.ndim:]`.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nIf `input` does not satisfy the requirement `prod(input.shape[other.ndim:]) ==\nprod(input.shape[:other.ndim])` after (optionally) moving the dimensions using\n`dims`, then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nComputes the multiplicative inverse matrix of a square matrix `input`, or of\neach square matrix in a batched `input`. The result satisfies the relation:\n\n`matmul(inv(input),input)` = `matmul(input,inv(input))` =\n`eye(input.shape[0]).expand_as(input)`.\n\nSupports input of float, double, cfloat and cdouble data types.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe inverse matrix is computed using LAPACK\u2019s `getrf` and `getri` routines for\nCPU inputs. For CUDA inputs, cuSOLVER\u2019s `getrf` and `getrs` routines as well\nas cuBLAS\u2019 `getrf` and `getri` routines are used if CUDA version >= 10.1.243,\notherwise MAGMA\u2019s `getrf` and `getri` routines are used instead.\n\nNote\n\nIf `input` is a non-invertible matrix or non-square matrix, or batch with at\nleast one such matrix, then a RuntimeError will be thrown.\n\ninput (Tensor) \u2013 the square `(n, n)` matrix or the batch of such matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nDepending on the value of `mode` this function returns the reduced or complete\nQR factorization. See below for a list of valid modes.\n\nNote\n\nDifferences with `numpy.linalg.qr`:\n\nNote\n\nBackpropagation is not supported for `mode='r'`. Use `mode='reduced'` instead.\n\nBackpropagation is also not supported if the first\nmin\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))\ncolumns of any matrix in `input` are not linearly independent. While no error\nwill be thrown when this occurs the values of the \u201cgradient\u201d produced may be\nanything. This behavior may change in the future.\n\nNote\n\nThis function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may\nproduce different (valid) decompositions on different device types or\ndifferent platforms.\n\nmode (str, optional) \u2013\n\nif `k = min(m, n)` then:\n\nout (tuple, optional) \u2013 tuple of `Q` and `R` tensors. The dimensions of `Q`\nand `R` are detailed in the description of `mode` above.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.cholesky()", "path": "linalg#torch.linalg.cholesky", "type": "torch.linalg", "text": "\nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-\nvalued matrices) positive-definite matrix or the Cholesky decompositions for a\nbatch of such matrices. Each decomposition has the form:\n\nwhere LL is a lower-triangular matrix and LHL^H is the conjugate transpose of\nLL , which is just a transpose for the case of real-valued input matrices. In\ncode it translates to `input = L @ L.t()` if `input` is real-valued and `input\n= L @ L.conj().t()` if `input` is complex-valued. The batch of LL matrices is\nreturned.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nLAPACK\u2019s `potrf` is used for CPU inputs, and MAGMA\u2019s `potrf` is used for CUDA\ninputs.\n\nNote\n\nIf `input` is not a Hermitian positive-definite matrix, or if it\u2019s a batch of\nmatrices and one or more of them is not a Hermitian positive-definite matrix,\nthen a RuntimeError will be thrown. If `input` is a batch of matrices, then\nthe error message will include the batch index of the first matrix that is not\nHermitian positive-definite.\n\ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n) consisting of\nHermitian positive-definite n\u00d7nn \\times n matrices, where \u2217* is zero or more\nbatch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.cond()", "path": "linalg#torch.linalg.cond", "type": "torch.linalg", "text": "\nComputes the condition number of a matrix `input`, or of each matrix in a\nbatched `input`, using the matrix norm defined by `p`.\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` this is defined as the matrix\nnorm of `input` times the matrix norm of the inverse of `input` computed using\n`torch.linalg.norm()`. While for norms `{None, 2, -2}` this is defined as the\nratio between the largest and smallest singular values computed using\n`torch.linalg.svd()`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function may synchronize that device\nwith the CPU depending on which norm `p` is used.\n\nNote\n\nFor norms `{None, 2, -2}`, `input` may be a non-square matrix or batch of non-\nsquare matrices. For other norms, however, `input` must be a square matrix or\na batch of square matrices, and if this requirement is not satisfied a\nRuntimeError will be thrown.\n\nNote\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` if `input` is a non-invertible\nmatrix then a tensor containing infinity will be returned. If `input` is a\nbatch of matrices and one or more of them is not invertible then a\nRuntimeError will be thrown.\n\np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nthe type of the matrix norm to use in the computations. inf refers to\n`float('inf')`, numpy\u2019s `inf` object, or any equivalent object. The following\nnorms can be used:\n\np\n\nnorm for matrices\n\nNone\n\nratio of the largest singular value to the smallest singular value\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2019nuc\u2019\n\nnuclear norm\n\ninf\n\nmax(sum(abs(x), dim=1))\n\n-inf\nmin(sum(abs(x), dim=1))\n\n1\n\nmax(sum(abs(x), dim=0))\n\n-1\nmin(sum(abs(x), dim=0))\n\n2\n\nratio of the largest singular value to the smallest singular value\n\n-2\nratio of the smallest singular value to the largest singular value\n\nDefault: `None`\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nThe condition number of `input`. The output dtype is always real valued even\nfor complex inputs (e.g. float if `input` is cfloat).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.det()", "path": "linalg#torch.linalg.det", "type": "torch.linalg", "text": "\nComputes the determinant of a square matrix `input`, or of each square matrix\nin a batched `input`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nBackward through `det` internally uses `torch.linalg.svd()` when `input` is\nnot invertible. In this case, double backward through `det` will be unstable\nwhen `input` doesn\u2019t have distinct singular values. See `torch.linalg.svd()`\nfor more details.\n\ninput (Tensor) \u2013 the input matrix of size `(n, n)` or the batch of matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.eigh()", "path": "linalg#torch.linalg.eigh", "type": "torch.linalg", "text": "\nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real\nsymmetric) matrix `input`, or of each such matrix in a batched `input`.\n\nFor a single matrix `input`, the tensor of eigenvalues `w` and the tensor of\neigenvectors `V` decompose the `input` such that `input = V diag(w) V\u1d34`, where\n`V\u1d34` is the transpose of `V` for real-valued `input`, or the conjugate\ntranspose of `V` for complex-valued `input`.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues/eigenvectors are computed using LAPACK\u2019s `syevd` and `heevd`\nroutines for CPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA\ninputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThe eigenvectors of matrices are not unique, so any eigenvector multiplied by\na constant remains a valid eigenvector. This function may compute different\neigenvector representations on different device types. Usually the difference\nis only in the sign of the eigenvector.\n\nNote\n\nSee `torch.linalg.eigvalsh()` for a related function that computes only\neigenvalues. However, that function is not differentiable.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to. Default\nis `None`.\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\nThe eigenvalues in ascending order.\n\nThe orthonormal eigenvectors of the `input`.\n\n(Tensor, Tensor)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.eigvalsh()", "path": "linalg#torch.linalg.eigvalsh", "type": "torch.linalg", "text": "\nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix\n`input`, or of each such matrix in a batched `input`. The eigenvalues are\nreturned in ascending order.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues are computed using LAPACK\u2019s `syevd` and `heevd` routines for\nCPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA inputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThis function doesn\u2019t support backpropagation, please use\n`torch.linalg.eigh()` instead, which also computes the eigenvectors.\n\nNote\n\nSee `torch.linalg.eigh()` for a related function that computes both\neigenvalues and eigenvectors.\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.inv()", "path": "linalg#torch.linalg.inv", "type": "torch.linalg", "text": "\nComputes the multiplicative inverse matrix of a square matrix `input`, or of\neach square matrix in a batched `input`. The result satisfies the relation:\n\n`matmul(inv(input),input)` = `matmul(input,inv(input))` =\n`eye(input.shape[0]).expand_as(input)`.\n\nSupports input of float, double, cfloat and cdouble data types.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe inverse matrix is computed using LAPACK\u2019s `getrf` and `getri` routines for\nCPU inputs. For CUDA inputs, cuSOLVER\u2019s `getrf` and `getrs` routines as well\nas cuBLAS\u2019 `getrf` and `getri` routines are used if CUDA version >= 10.1.243,\notherwise MAGMA\u2019s `getrf` and `getri` routines are used instead.\n\nNote\n\nIf `input` is a non-invertible matrix or non-square matrix, or batch with at\nleast one such matrix, then a RuntimeError will be thrown.\n\ninput (Tensor) \u2013 the square `(n, n)` matrix or the batch of such matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.matrix_rank()", "path": "linalg#torch.linalg.matrix_rank", "type": "torch.linalg", "text": "\nComputes the numerical rank of a matrix `input`, or of each matrix in a\nbatched `input`.\n\nThe matrix rank is computed as the number of singular values (or absolute\neigenvalues when `hermitian` is `True`) that are greater than the specified\n`tol` threshold.\n\nIf `tol` is not specified, `tol` is set to\n`S.max(dim=-1)*max(input.shape[-2:])*eps`, where `S` is the singular values\n(or absolute eigenvalues when `hermitian` is `True`), and `eps` is the epsilon\nvalue for the datatype of `input`. The epsilon value can be obtained using the\n`eps` attribute of `torch.finfo`.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe matrix rank is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation is\ndone by obtaining the eigenvalues (see `torch.linalg.eigvalsh()`).\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.norm()", "path": "linalg#torch.linalg.norm", "type": "torch.linalg", "text": "\nReturns the matrix norm or vector norm of a given tensor.\n\nThis function can calculate one of eight different types of matrix norms, or\none of an infinite number of vector norms, depending on both the number of\nreduction dimensions and the value of the `ord` parameter.\n\nord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nThe order of norm. inf refers to `float('inf')`, numpy\u2019s `inf` object, or any\nequivalent object. The following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2013 not supported \u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013 not supported \u2013\n\ninf\n\nmax(sum(abs(x), dim=1))\n\nmax(abs(x))\n\n-inf\nmin(sum(abs(x), dim=1))\n\nmin(abs(x))\n\n0\n\n\u2013 not supported \u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), dim=0))\n\nas below\n\n-1\nmin(sum(abs(x), dim=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\nsmallest singular value\n\nas below\n\nother\n\n\u2013 not supported \u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nDefault: `None`\n\nExamples:\n\nUsing the `dim` argument to compute vector norms:\n\nUsing the `dim` argument to compute matrix norms:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.pinv()", "path": "linalg#torch.linalg.pinv", "type": "torch.linalg", "text": "\nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a\nmatrix `input`, or of each matrix in a batched `input`.\n\nThe singular values (or the absolute values of the eigenvalues when\n`hermitian` is `True`) that are below the specified `rcond` threshold are\ntreated as zero and discarded in the computation.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe pseudo-inverse is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation of the\npseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see\n`torch.linalg.eigh()`).\n\nNote\n\nIf singular value decomposition or eigenvalue decomposition algorithms do not\nconverge then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.qr()", "path": "linalg#torch.linalg.qr", "type": "torch.linalg", "text": "\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nDepending on the value of `mode` this function returns the reduced or complete\nQR factorization. See below for a list of valid modes.\n\nNote\n\nDifferences with `numpy.linalg.qr`:\n\nNote\n\nBackpropagation is not supported for `mode='r'`. Use `mode='reduced'` instead.\n\nBackpropagation is also not supported if the first\nmin\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))\ncolumns of any matrix in `input` are not linearly independent. While no error\nwill be thrown when this occurs the values of the \u201cgradient\u201d produced may be\nanything. This behavior may change in the future.\n\nNote\n\nThis function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may\nproduce different (valid) decompositions on different device types or\ndifferent platforms.\n\nmode (str, optional) \u2013\n\nif `k = min(m, n)` then:\n\nout (tuple, optional) \u2013 tuple of `Q` and `R` tensors. The dimensions of `Q`\nand `R` are detailed in the description of `mode` above.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.slogdet()", "path": "linalg#torch.linalg.slogdet", "type": "torch.linalg", "text": "\nCalculates the sign and natural logarithm of the absolute value of a square\nmatrix\u2019s determinant, or of the absolute values of the determinants of a batch\nof square matrices `input`. The determinant can be computed with `sign *\nexp(logabsdet)`.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nFor matrices that have zero determinant, this returns `(0, -inf)`. If `input`\nis batched then the entries in the result tensors corresponding to matrices\nwith the zero determinant have sign 0 and the natural logarithm of the\nabsolute value of the determinant -inf.\n\ninput (Tensor) \u2013 the input matrix of size (n,n)(n, n) or the batch of matrices\nof size (\u2217,n,n)(*, n, n) where \u2217* is one or more batch dimensions.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to.\n\nA namedtuple (sign, logabsdet) containing the sign of the determinant and the\nnatural logarithm of the absolute value of determinant, respectively.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.solve()", "path": "linalg#torch.linalg.solve", "type": "torch.linalg", "text": "\nComputes the solution `x` to the matrix equation `matmul(input, x) = other`\nwith a square matrix, or batches of such matrices, `input` and one or more\nright-hand side vectors `other`. If `input` is batched and `other` is not,\nthen `other` is broadcast to have the same batch dimensions as `input`. The\nresulting tensor has the same shape as the (possibly broadcast) `other`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` dtypes.\n\nNote\n\nIf `input` is a non-square or non-invertible matrix, or a batch containing\nnon-square matrices or one or more non-invertible matrices, then a\nRuntimeError will be thrown.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nBatched input:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.svd()", "path": "linalg#torch.linalg.svd", "type": "torch.linalg", "text": "\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\u201d The singular value decomposition is represented as a\nnamedtuple `(U, S, Vh)`, such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@}\ndiag(S) \\times Vh . If `input` is a batch of tensors, then `U`, `S`, and `Vh`\nare also batched with the same batch dimensions as `input`.\n\nIf `full_matrices` is `False` (default), the method returns the reduced\nsingular value decomposition i.e., if the last two dimensions of `input` are\n`m` and `n`, then the returned `U` and `V` matrices will contain only\nmin(n,m)min(n, m) orthonormal columns.\n\nIf `compute_uv` is `False`, the returned `U` and `Vh` will be empy tensors\nwith no elements and the same device as `input`. The `full_matrices` argument\nhas no effect when `compute_uv` is False.\n\nThe dtypes of `U` and `V` are the same as `input`\u2019s. `S` will always be real-\nvalued, even if `input` is complex.\n\nNote\n\nUnlike NumPy\u2019s `linalg.svd`, this always returns a namedtuple of three\ntensors, even when `compute_uv=False`. This behavior may change in a future\nPyTorch release.\n\nNote\n\nThe singular values are returned in descending order. If `input` is a batch of\nmatrices, then the singular values of each matrix in the batch is returned in\ndescending order.\n\nNote\n\nThe implementation of SVD on CPU uses the LAPACK routine `?gesdd` (a divide-\nand-conquer algorithm) instead of `?gesvd` for speed. Analogously, the SVD on\nGPU uses the cuSOLVER routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243\nand later, and uses the MAGMA routine `gesdd` on earlier versions of CUDA.\n\nNote\n\nThe returned matrix `U` will be transposed, i.e. with strides\n`U.contiguous().transpose(-2, -1).stride()`.\n\nNote\n\nGradients computed using `U` and `Vh` may be unstable if `input` is not full\nrank or has non-unique singular values.\n\nNote\n\nWhen `full_matrices` = `True`, the gradients on `U[..., :, min(m, n):]` and\n`V[..., :, min(m, n):]` will be ignored in backward as those vectors can be\narbitrary bases of the subspaces.\n\nNote\n\nThe `S` tensor can only be used to compute gradients if `compute_uv` is True.\n\nNote\n\nSince `U` and `V` of an SVD is not unique, each vector can be multiplied by an\narbitrary phase factor ei\u03d5e^{i \\phi} while the SVD result is still correct.\nDifferent platforms, like Numpy, or inputs on different device types, may\nproduce different `U` and `V` tensors.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.tensorinv()", "path": "linalg#torch.linalg.tensorinv", "type": "torch.linalg", "text": "\nComputes a tensor `input_inv` such that `tensordot(input_inv, input, ind) ==\nI_n` (inverse tensor equation), where `I_n` is the n-dimensional identity\ntensor and `n` is equal to `input.ndim`. The resulting tensor `input_inv` has\nshape equal to `input.shape[ind:] + input.shape[:ind]`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` data types.\n\nNote\n\nIf `input` is not invertible or does not satisfy the requirement\n`prod(input.shape[ind:]) == prod(input.shape[:ind])`, then a RuntimeError will\nbe thrown.\n\nNote\n\nWhen `input` is a 2-dimensional tensor and `ind=1`, this function computes the\n(multiplicative) inverse of `input`, equivalent to calling `torch.inverse()`.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.tensorsolve()", "path": "linalg#torch.linalg.tensorsolve", "type": "torch.linalg", "text": "\nComputes a tensor `x` such that `tensordot(input, x, dims=x.ndim) = other`.\nThe resulting tensor `x` has the same shape as `input[other.ndim:]`.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nIf `input` does not satisfy the requirement `prod(input.shape[other.ndim:]) ==\nprod(input.shape[:other.ndim])` after (optionally) moving the dimensions using\n`dims`, then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linspace()", "path": "generated/torch.linspace#torch.linspace", "type": "torch", "text": "\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from `start` to `end`, inclusive. That is, the value are:\n\nWarning\n\nNot providing a value for `steps` is deprecated. For backwards compatibility,\nnot providing a value for `steps` will create a tensor with 100 elements. Note\nthat this behavior is not reflected in the documented function signature and\nshould not be relied on. In a future PyTorch release, failing to provide a\nvalue for `steps` will throw a runtime error.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.load()", "path": "generated/torch.load#torch.load", "type": "torch", "text": "\nLoads an object saved with `torch.save()` from a file.\n\n`torch.load()` uses Python\u2019s unpickling facilities but treats storages, which\nunderlie tensors, specially. They are first deserialized on the CPU and are\nthen moved to the device they were saved from. If this fails (e.g. because the\nrun time system doesn\u2019t have certain devices), an exception is raised.\nHowever, storages can be dynamically remapped to an alternative set of devices\nusing the `map_location` argument.\n\nIf `map_location` is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument will be\nthe initial deserialization of the storage, residing on the CPU. Each\nserialized storage has a location tag associated with it which identifies the\ndevice it was saved from, and this tag is the second argument passed to\n`map_location`. The builtin location tags are `'cpu'` for CPU tensors and\n`'cuda:device_id'` (e.g. `'cuda:2'`) for CUDA tensors. `map_location` should\nreturn either `None` or a storage. If `map_location` returns a storage, it\nwill be used as the final deserialized object, already moved to the right\ndevice. Otherwise, `torch.load()` will fall back to the default behavior, as\nif `map_location` wasn\u2019t specified.\n\nIf `map_location` is a `torch.device` object or a string containing a device\ntag, it indicates the location where all tensors should be loaded.\n\nOtherwise, if `map_location` is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the storages\n(values).\n\nUser extensions can register their own location tags and tagging and\ndeserialization methods using `torch.serialization.register_package()`.\n\nWarning\n\n`torch.load()` uses `pickle` module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary\ncode during unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust.\n\nNote\n\nWhen you call `torch.load()` on a file which contains GPU tensors, those\ntensors will be loaded to GPU by default. You can call `torch.load(..,\nmap_location='cpu')` and then `load_state_dict()` to avoid GPU RAM surge when\nloading a model checkpoint.\n\nNote\n\nBy default, we decode byte strings as `utf-8`. This is to avoid a common error\ncase `UnicodeDecodeError: 'ascii' codec can't decode byte 0x...` when loading\nfiles saved by Python 2 in Python 3. If this default is incorrect, you may use\nan extra `encoding` keyword argument to specify how these objects should be\nloaded, e.g., `encoding='latin1'` decodes them to strings using `latin1`\nencoding, and `encoding='bytes'` keeps them as byte arrays which can be\ndecoded later with `byte_array.decode(...)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lobpcg()", "path": "generated/torch.lobpcg#torch.lobpcg", "type": "torch", "text": "\nFind the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized eigenvalue problem\nusing matrix-free LOBPCG methods.\n\nThis function is a front-end to the following LOBPCG algorithms selectable via\n`method` argument:\n\n`method=\u201dbasic\u201d` \\- the LOBPCG method introduced by Andrew Knyazev, see\n[Knyazev2001]. A less robust method, may fail when Cholesky is applied to\nsingular input.\n\n`method=\u201dortho\u201d` \\- the LOBPCG method with orthogonal basis selection\n[StathopoulosEtal2002]. A robust method.\n\nSupported inputs are dense, sparse, and batches of dense matrices.\n\nNote\n\nIn general, the basic method spends least time per iteration. However, the\nrobust methods converge much faster and are more stable. So, the usage of the\nbasic method is generally not recommended but there exist cases where the\nusage of the basic method may be preferred.\n\nWarning\n\nThe backward method does not support sparse and complex inputs. It works only\nwhen `B` is not provided (i.e. `B == None`). We are actively working on\nextensions, and the details of the algorithms are going to be published\npromptly.\n\nWarning\n\nWhile it is assumed that `A` is symmetric, `A.grad` is not. To make sure that\n`A.grad` is symmetric, so that `A - t * A.grad` is symmetric in first-order\noptimization routines, prior to running `lobpcg` we do the following\nsymmetrization map: `A -> (A + A.t()) / 2`. The map is performed only when the\n`A` requires gradients.\n\ntracker (callable, optional) \u2013\n\na function for tracing the iteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an argument. The LOBPCG instance\nholds the full state of the iteration process in the following attributes:\n\n`iparams`, `fparams`, `bparams` \\- dictionaries of integer, float, and boolean\nvalued input parameters, respectively\n\n`ivars`, `fvars`, `bvars`, `tvars` \\- dictionaries of integer, float, boolean,\nand Tensor valued iteration variables, respectively.\n\n`A`, `B`, `iK` \\- input Tensor arguments.\n\n`E`, `X`, `S`, `R` \\- iteration Tensor variables.\n\nFor instance:\n\n`ivars[\u201cistep\u201d]` \\- the current iteration step `X` \\- the current\napproximation of eigenvectors `E` \\- the current approximation of eigenvalues\n`R` \\- the current residual `ivars[\u201cconverged_count\u201d]` \\- the current number\nof converged eigenpairs `tvars[\u201crerr\u201d]` \\- the current state of convergence\ncriteria\n\nNote that when `tracker` stores Tensor objects from the LOBPCG instance, it\nmust make copies of these.\n\nIf `tracker` sets `bvars[\u201cforce_stop\u201d] = True`, the iteration process will be\nhard-stopped.\n\ntensor of eigenvalues of size (\u2217,k)(*, k)\n\nX (Tensor): tensor of eigenvectors of size (\u2217,m,k)(*, m, k)\n\nE (Tensor)\n\n[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned\nEigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method.\nSIAM J. Sci. Comput., 23(2), 517-541. (25 pages)\nhttps://epubs.siam.org/doi/abs/10.1137/S1064827500366124\n\n[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block\nOrthogonalization Procedure with Constant Synchronization Requirements. SIAM\nJ. Sci. Comput., 23(6), 2165-2182. (18 pages)\nhttps://epubs.siam.org/doi/10.1137/S1064827500370883\n\n[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A\nRobust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5),\nC655-C676. (22 pages) https://epubs.siam.org/doi/abs/10.1137/17M1129830\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log()", "path": "generated/torch.log#torch.log", "type": "torch", "text": "\nReturns a new tensor with the natural logarithm of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log10()", "path": "generated/torch.log10#torch.log10", "type": "torch", "text": "\nReturns a new tensor with the logarithm to the base 10 of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log1p()", "path": "generated/torch.log1p#torch.log1p", "type": "torch", "text": "\nReturns a new tensor with the natural logarithm of (1 + `input`).\n\nNote\n\nThis function is more accurate than `torch.log()` for small values of `input`\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log2()", "path": "generated/torch.log2#torch.log2", "type": "torch", "text": "\nReturns a new tensor with the logarithm to the base 2 of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logaddexp()", "path": "generated/torch.logaddexp#torch.logaddexp", "type": "torch", "text": "\nLogarithm of the sum of exponentiations of the inputs.\n\nCalculates pointwise log\u2061(ex+ey)\\log\\left(e^x + e^y\\right) . This function is\nuseful in statistics where the calculated probabilities of events may be so\nsmall as to exceed the range of normal floating point numbers. In such cases\nthe logarithm of the calculated probability is stored. This function allows\nadding probabilities stored in such a fashion.\n\nThis op should be disambiguated with `torch.logsumexp()` which performs a\nreduction on a single tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logaddexp2()", "path": "generated/torch.logaddexp2#torch.logaddexp2", "type": "torch", "text": "\nLogarithm of the sum of exponentiations of the inputs in base-2.\n\nCalculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right) . See\n`torch.logaddexp()` for more details.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logcumsumexp()", "path": "generated/torch.logcumsumexp#torch.logcumsumexp", "type": "torch", "text": "\nReturns the logarithm of the cumulative summation of the exponentiation of\nelements of `input` in the dimension `dim`.\n\nFor summation index jj given by `dim` and other indices ii , the result is\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logdet()", "path": "generated/torch.logdet#torch.logdet", "type": "torch", "text": "\nCalculates log determinant of a square matrix or batches of square matrices.\n\nNote\n\nResult is `-inf` if `input` has zero log determinant, and is `nan` if `input`\nhas negative determinant.\n\nNote\n\nBackward through `logdet()` internally uses SVD results when `input` is not\ninvertible. In this case, double backward through `logdet()` will be unstable\nin when `input` doesn\u2019t have distinct singular values. See `svd()` for\ndetails.\n\ninput (Tensor) \u2013 the input tensor of size `(*, n, n)` where `*` is zero or\nmore batch dimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_and()", "path": "generated/torch.logical_and#torch.logical_and", "type": "torch", "text": "\nComputes the element-wise logical AND of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_not()", "path": "generated/torch.logical_not#torch.logical_not", "type": "torch", "text": "\nComputes the element-wise logical NOT of the given input tensor. If not\nspecified, the output tensor will have the bool dtype. If the input tensor is\nnot a bool tensor, zeros are treated as `False` and non-zeros are treated as\n`True`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_or()", "path": "generated/torch.logical_or#torch.logical_or", "type": "torch", "text": "\nComputes the element-wise logical OR of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_xor()", "path": "generated/torch.logical_xor#torch.logical_xor", "type": "torch", "text": "\nComputes the element-wise logical XOR of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logit()", "path": "generated/torch.logit#torch.logit", "type": "torch", "text": "\nReturns a new tensor with the logit of the elements of `input`. `input` is\nclamped to [eps, 1 - eps] when eps is not None. When eps is None and `input` <\n0 or `input` > 1, the function will yields NaN.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logspace()", "path": "generated/torch.logspace#torch.logspace", "type": "torch", "text": "\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}} to\nbaseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale\nwith base `base`. That is, the values are:\n\nWarning\n\nNot providing a value for `steps` is deprecated. For backwards compatibility,\nnot providing a value for `steps` will create a tensor with 100 elements. Note\nthat this behavior is not reflected in the documented function signature and\nshould not be relied on. In a future PyTorch release, failing to provide a\nvalue for `steps` will throw a runtime error.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logsumexp()", "path": "generated/torch.logsumexp#torch.logsumexp", "type": "torch", "text": "\nReturns the log of summed exponentials of each row of the `input` tensor in\nthe given dimension `dim`. The computation is numerically stabilized.\n\nFor summation index jj given by `dim` and other indices ii , the result is\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lstsq()", "path": "generated/torch.lstsq#torch.lstsq", "type": "torch", "text": "\nComputes the solution to the least squares and least norm problems for a full\nrank matrix AA of size (m\u00d7n)(m \\times n) and a matrix BB of size (m\u00d7k)(m\n\\times k) .\n\nIf m\u2265nm \\geq n , `lstsq()` solves the least-squares problem:\n\nIf m<nm < n , `lstsq()` solves the least-norm problem:\n\nReturned tensor XX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k) . The first nn\nrows of XX contains the solution. If m\u2265nm \\geq n , the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in\nthe remaining m\u2212nm - n rows of that column.\n\nNote\n\nThe case when m<nm < n is not supported on the GPU.\n\nout (tuple, optional) \u2013 the optional destination tensor\n\nA namedtuple (solution, QR) containing:\n\n(Tensor, Tensor)\n\nNote\n\nThe returned matrices will always be transposed, irrespective of the strides\nof the input matrices. That is, they will have stride `(1, m)` instead of `(m,\n1)`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lt()", "path": "generated/torch.lt#torch.lt", "type": "torch", "text": "\nComputes input<other\\text{input} < \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is less than `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lu()", "path": "generated/torch.lu#torch.lu", "type": "torch", "text": "\nComputes the LU factorization of a matrix or batches of matrices `A`. Returns\na tuple containing the LU factorization and pivots of `A`. Pivoting is done if\n`pivot` is set to `True`.\n\nNote\n\nThe pivots returned by the function are 1-indexed. If `pivot` is `False`, then\nthe returned pivots is a tensor filled with zeros of the appropriate size.\n\nNote\n\nLU factorization with `pivot` = `False` is not available for CPU, and\nattempting to do so will throw an error. However, LU factorization with\n`pivot` = `False` is available for CUDA.\n\nNote\n\nThis function does not check if the factorization was successful or not if\n`get_infos` is `True` since the status of the factorization is present in the\nthird element of the return tuple.\n\nNote\n\nIn the case of batches of square matrices with size less or equal to 32 on a\nCUDA device, the LU factorization is repeated for singular matrices due to the\nbug in the MAGMA library (see magma issue 13).\n\nNote\n\n`L`, `U`, and `P` can be derived using `torch.lu_unpack()`.\n\nWarning\n\nThe LU factorization does have backward support, but only for square inputs of\nfull rank.\n\nA tuple of tensors containing\n\n(Tensor, IntTensor, IntTensor (optional))\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lu_solve()", "path": "generated/torch.lu_solve#torch.lu_solve", "type": "torch", "text": "\nReturns the LU solve of the linear system Ax=bAx = b using the partially\npivoted LU factorization of A from `torch.lu()`.\n\nThis function supports `float`, `double`, `cfloat` and `cdouble` dtypes for\n`input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lu_unpack()", "path": "generated/torch.lu_unpack#torch.lu_unpack", "type": "torch", "text": "\nUnpacks the data and pivots from a LU factorization of a tensor.\n\nReturns a tuple of tensors as `(the pivots, the L tensor, the U tensor)`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.manual_seed()", "path": "generated/torch.manual_seed#torch.manual_seed", "type": "torch", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.masked_select()", "path": "generated/torch.masked_select#torch.masked_select", "type": "torch", "text": "\nReturns a new 1-D tensor which indexes the `input` tensor according to the\nboolean mask `mask` which is a `BoolTensor`.\n\nThe shapes of the `mask` tensor and the `input` tensor don\u2019t need to match,\nbut they must be broadcastable.\n\nNote\n\nThe returned tensor does not use the same storage as the original tensor\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matmul()", "path": "generated/torch.matmul#torch.matmul", "type": "torch", "text": "\nMatrix product of two tensors.\n\nThe behavior depends on the dimensionality of the tensors as follows:\n\nIf both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned. If\nthe first argument is 1-dimensional, a 1 is prepended to its dimension for the\npurpose of the batched matrix multiply and removed after. If the second\nargument is 1-dimensional, a 1 is appended to its dimension for the purpose of\nthe batched matrix multiple and removed after. The non-matrix (i.e. batch)\ndimensions are broadcasted (and thus must be broadcastable). For example, if\n`input` is a (j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n) tensor and `other` is a\n(k\u00d7n\u00d7n)(k \\times n \\times n) tensor, `out` will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k\n\\times n \\times n) tensor.\n\nNote that the broadcasting logic only looks at the batch dimensions when\ndetermining if the inputs are broadcastable, and not the matrix dimensions.\nFor example, if `input` is a (j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m) tensor\nand `other` is a (k\u00d7m\u00d7p)(k \\times m \\times p) tensor, these inputs are valid\nfor broadcasting even though the final two dimensions (i.e. the matrix\ndimensions) are different. `out` will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n\n\\times p) tensor.\n\nThis operator supports TensorFloat32.\n\nNote\n\nThe 1-dimensional dot product version of this function does not support an\n`out` parameter.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matrix_exp()", "path": "generated/torch.matrix_exp#torch.matrix_exp", "type": "torch", "text": "\nReturns the matrix exponential. Supports batched input. For a matrix `A`, the\nmatrix exponential is defined as\n\nThe implementation is based on:\n\nBader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an\nOptimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matrix_power()", "path": "generated/torch.matrix_power#torch.matrix_power", "type": "torch", "text": "\nReturns the matrix raised to the power `n` for square matrices. For batch of\nmatrices, each individual matrix is raised to the power `n`.\n\nIf `n` is negative, then the inverse of the matrix (if invertible) is raised\nto the power `n`. For a batch of matrices, the batched inverse (if invertible)\nis raised to the power `n`. If `n` is 0, then an identity matrix is returned.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matrix_rank()", "path": "generated/torch.matrix_rank#torch.matrix_rank", "type": "torch", "text": "\nReturns the numerical rank of a 2-D tensor. The method to compute the matrix\nrank is done using SVD by default. If `symmetric` is `True`, then `input` is\nassumed to be symmetric, and the computation of the rank is done by obtaining\nthe eigenvalues.\n\n`tol` is the threshold below which the singular values (or the eigenvalues\nwhen `symmetric` is `True`) are considered to be 0. If `tol` is not specified,\n`tol` is set to `S.max() * max(S.size()) * eps` where `S` is the singular\nvalues (or the eigenvalues when `symmetric` is `True`), and `eps` is the\nepsilon value for the datatype of `input`.\n\nNote\n\n`torch.matrix_rank()` is deprecated. Please use `torch.linalg.matrix_rank()`\ninstead. The parameter `symmetric` was renamed in `torch.linalg.matrix_rank()`\nto `hermitian`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.max()", "path": "generated/torch.max#torch.max", "type": "torch", "text": "\nReturns the maximum value of all elements in the `input` tensor.\n\nWarning\n\nThis function produces deterministic (sub)gradients unlike `max(dim=0)`\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` is the maximum value\nof each row of the `input` tensor in the given dimension `dim`. And `indices`\nis the index location of each maximum value found (argmax).\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensors having 1\nfewer dimension than `input`.\n\nNote\n\nIf there are multiple maximal values in a reduced row then the indices of the\nfirst maximal value are returned.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (max,\nmax_indices)\n\nExample:\n\nSee `torch.maximum()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.maximum()", "path": "generated/torch.maximum#torch.maximum", "type": "torch", "text": "\nComputes the element-wise maximum of `input` and `other`.\n\nNote\n\nIf one of the elements being compared is a NaN, then that element is returned.\n`maximum()` is not supported for tensors with complex dtypes.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mean()", "path": "generated/torch.mean#torch.mean", "type": "torch", "text": "\nReturns the mean value of all elements in the `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns the mean value of each row of the `input` tensor in the given\ndimension `dim`. If `dim` is a list of dimensions, reduce over all of them.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.median()", "path": "generated/torch.median#torch.median", "type": "torch", "text": "\nReturns the median of the values in `input`.\n\nNote\n\nThe median is not unique for `input` tensors with an even number of elements.\nIn this case the lower of the two medians is returned. To compute the mean of\nboth medians, use `torch.quantile()` with `q=0.5` instead.\n\nWarning\n\nThis function produces deterministic (sub)gradients unlike `median(dim=0)`\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` contains the median of\neach row of `input` in the dimension `dim`, and `indices` contains the index\nof the median values found in the dimension `dim`.\n\nBy default, `dim` is the last dimension of the `input` tensor.\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the outputs tensor having 1\nfewer dimension than `input`.\n\nNote\n\nThe median is not unique for `input` tensors with an even number of elements\nin the dimension `dim`. In this case the lower of the two medians is returned.\nTo compute the mean of both medians in `input`, use `torch.quantile()` with\n`q=0.5` instead.\n\nWarning\n\n`indices` does not necessarily contain the first occurrence of each median\nvalue found, unless it is unique. The exact implementation details are device-\nspecific. Do not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic.\n\nout ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the\nmedian values and the second tensor, which must have dtype long, with their\nindices in the dimension `dim` of `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.meshgrid()", "path": "generated/torch.meshgrid#torch.meshgrid", "type": "torch", "text": "\nTake NN tensors, each of which can be either scalar or 1-dimensional vector,\nand create NN N-dimensional grids, where the ii th grid is defined by\nexpanding the ii th input over dimensions defined by other inputs.\n\ntensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars\nwill be treated as tensors of size (1,)(1,) automatically\n\nIf the input has kk tensors of size (N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots\n, (N_k,) , then the output would also have kk tensors, where all tensors are\nof size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k) .\n\nseq (sequence of Tensors)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.min()", "path": "generated/torch.min#torch.min", "type": "torch", "text": "\nReturns the minimum value of all elements in the `input` tensor.\n\nWarning\n\nThis function produces deterministic (sub)gradients unlike `min(dim=0)`\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` is the minimum value\nof each row of the `input` tensor in the given dimension `dim`. And `indices`\nis the index location of each minimum value found (argmin).\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensors having 1\nfewer dimension than `input`.\n\nNote\n\nIf there are multiple minimal values in a reduced row then the indices of the\nfirst minimal value are returned.\n\nout (tuple, optional) \u2013 the tuple of two output tensors (min, min_indices)\n\nExample:\n\nSee `torch.minimum()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.minimum()", "path": "generated/torch.minimum#torch.minimum", "type": "torch", "text": "\nComputes the element-wise minimum of `input` and `other`.\n\nNote\n\nIf one of the elements being compared is a NaN, then that element is returned.\n`minimum()` is not supported for tensors with complex dtypes.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mm()", "path": "generated/torch.mm#torch.mm", "type": "torch", "text": "\nPerforms a matrix multiplication of the matrices `input` and `mat2`.\n\nIf `input` is a (n\u00d7m)(n \\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p)\ntensor, `out` will be a (n\u00d7p)(n \\times p) tensor.\n\nNote\n\nThis function does not broadcast. For broadcasting matrix products, see\n`torch.matmul()`.\n\nSupports strided and sparse 2-D tensors as inputs, autograd with respect to\nstrided inputs.\n\nThis operator supports TensorFloat32.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mode()", "path": "generated/torch.mode#torch.mode", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the mode value of\neach row of the `input` tensor in the given dimension `dim`, i.e. a value\nwhich appears most often in that row, and `indices` is the index location of\neach mode value found.\n\nBy default, `dim` is the last dimension of the `input` tensor.\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensors having 1\nfewer dimension than `input`.\n\nNote\n\nThis function is not defined for `torch.cuda.Tensor` yet.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (values,\nindices)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.moveaxis()", "path": "generated/torch.moveaxis#torch.moveaxis", "type": "torch", "text": "\nAlias for `torch.movedim()`.\n\nThis function is equivalent to NumPy\u2019s moveaxis function.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.movedim()", "path": "generated/torch.movedim#torch.movedim", "type": "torch", "text": "\nMoves the dimension(s) of `input` at the position(s) in `source` to the\nposition(s) in `destination`.\n\nOther dimensions of `input` that are not explicitly moved remain in their\noriginal order and appear at the positions not specified in `destination`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.msort()", "path": "generated/torch.msort#torch.msort", "type": "torch", "text": "\nSorts the elements of the `input` tensor along its first dimension in\nascending order by value.\n\nNote\n\n`torch.msort(t)` is equivalent to `torch.sort(t, dim=0)[0]`. See also\n`torch.sort()`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mul()", "path": "generated/torch.mul#torch.mul", "type": "torch", "text": "\nMultiplies each element of the input `input` with the scalar `other` and\nreturns a new resulting tensor.\n\nIf `input` is of type `FloatTensor` or `DoubleTensor`, `other` should be a\nreal number, otherwise it should be an integer\n\n{out} \u2013\n\nExample:\n\nEach element of the tensor `input` is multiplied by the corresponding element\nof the Tensor `other`. The resulting tensor is returned.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multinomial()", "path": "generated/torch.multinomial#torch.multinomial", "type": "torch", "text": "\nReturns a tensor where each row contains `num_samples` indices sampled from\nthe multinomial probability distribution located in the corresponding row of\ntensor `input`.\n\nNote\n\nThe rows of `input` do not need to sum to one (in which case we use the values\nas weights), but must be non-negative, finite and have a non-zero sum.\n\nIndices are ordered from left to right according to when each was sampled\n(first samples are placed in first column).\n\nIf `input` is a vector, `out` is a vector of size `num_samples`.\n\nIf `input` is a matrix with `m` rows, `out` is an matrix of shape\n(m\u00d7num_samples)(m \\times \\text{num\\\\_samples}) .\n\nIf replacement is `True`, samples are drawn with replacement.\n\nIf not, they are drawn without replacement, which means that when a sample\nindex is drawn for a row, it cannot be drawn again for that row.\n\nNote\n\nWhen drawn without replacement, `num_samples` must be lower than number of\nnon-zero elements in `input` (or the min number of non-zero elements in each\nrow of `input` if it is a matrix).\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiply()", "path": "generated/torch.multiply#torch.multiply", "type": "torch", "text": "\nAlias for `torch.mul()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing", "path": "multiprocessing", "type": "torch.multiprocessing", "text": "\ntorch.multiprocessing is a wrapper around the native `multiprocessing` module.\nIt registers custom reducers, that use shared memory to provide shared views\non the same data in different processes. Once the tensor/storage is moved to\nshared_memory (see `share_memory_()`), it will be possible to send it to other\nprocesses without making any copies.\n\nThe API is 100% compatible with the original module - it\u2019s enough to change\n`import multiprocessing` to `import torch.multiprocessing` to have all the\ntensors sent through the queues or shared via other mechanisms, moved to\nshared memory.\n\nBecause of the similarity of APIs we do not document most of this package\ncontents, and we recommend referring to very good docs of the original module.\n\nWarning\n\nIf the main process exits abruptly (e.g. because of an incoming signal),\nPython\u2019s `multiprocessing` sometimes fails to clean up its children. It\u2019s a\nknown caveat, so if you\u2019re seeing any resource leaks after interrupting the\ninterpreter, it probably means that this has just happened to you.\n\nReturns a set of sharing strategies supported on a current system.\n\nReturns the current strategy for sharing CPU tensors.\n\nSets the strategy for sharing CPU tensors.\n\nnew_strategy (str) \u2013 Name of the selected strategy. Should be one of the\nvalues returned by `get_all_sharing_strategies()`.\n\nSharing CUDA tensors between processes is supported only in Python 3, using a\n`spawn` or `forkserver` start methods.\n\nUnlike CPU tensors, the sending process is required to keep the original\ntensor as long as the receiving process retains a copy of the tensor. The\nrefcounting is implemented under the hood but requires users to follow the\nnext best practices.\n\nWarning\n\nIf the consumer process dies abnormally to a fatal signal, the shared tensor\ncould be forever kept in memory as long as the sending process is running.\n\n2\\. Keep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\nThis section provides a brief overview into how different sharing strategies\nwork. Note that it applies only to CPU tensor - CUDA tensors will always use\nthe CUDA API, as that\u2019s the only way they can be shared.\n\nNote\n\nThis is the default strategy (except for macOS and OS X where it\u2019s not\nsupported).\n\nThis strategy will use file descriptors as shared memory handles. Whenever a\nstorage is moved to shared memory, a file descriptor obtained from `shm_open`\nis cached with the object, and when it\u2019s going to be sent to other processes,\nthe file descriptor will be transferred (e.g. via UNIX sockets) to it. The\nreceiver will also cache the file descriptor and `mmap` it, to obtain a shared\nview onto the storage data.\n\nNote that if there will be a lot of tensors shared, this strategy will keep a\nlarge number of file descriptors open most of the time. If your system has low\nlimits for the number of open file descriptors, and you can\u2019t raise them, you\nshould use the `file_system` strategy.\n\nThis strategy will use file names given to `shm_open` to identify the shared\nmemory regions. This has a benefit of not requiring the implementation to\ncache the file descriptors obtained from it, but at the same time is prone to\nshared memory leaks. The file can\u2019t be deleted right after its creation,\nbecause other processes need to access it to open their views. If the\nprocesses fatally crash, or are killed, and don\u2019t call the storage\ndestructors, the files will remain in the system. This is very serious,\nbecause they keep using up the memory until the system is restarted, or\nthey\u2019re freed manually.\n\nTo counter the problem of shared memory file leaks, `torch.multiprocessing`\nwill spawn a daemon named `torch_shm_manager` that will isolate itself from\nthe current process group, and will keep track of all shared memory\nallocations. Once all processes connected to it exit, it will wait a moment to\nensure there will be no new connections, and will iterate over all shared\nmemory files allocated by the group. If it finds that any of them still exist,\nthey will be deallocated. We\u2019ve tested this method and it proved to be robust\nto various failures. Still, if your system has high enough limits, and\n`file_descriptor` is a supported strategy, we do not recommend switching to\nthis one.\n\nNote\n\nAvailable for Python >= 3.4.\n\nThis depends on the `spawn` start method in Python\u2019s `multiprocessing`\npackage.\n\nSpawning a number of subprocesses to perform some function can be done by\ncreating `Process` instances and calling `join` to wait for their completion.\nThis approach works fine when dealing with a single subprocess but presents\npotential issues when dealing with multiple processes.\n\nNamely, joining processes sequentially implies they will terminate\nsequentially. If they don\u2019t, and the first process does not terminate, the\nprocess termination will go unnoticed. Also, there are no native facilities\nfor error propagation.\n\nThe `spawn` function below addresses these concerns and takes care of error\npropagation, out of order termination, and will actively terminate processes\nupon detecting an error in one of them.\n\nSpawns `nprocs` processes that run `fn` with `args`.\n\nIf one of the processes exits with a non-zero exit status, the remaining\nprocesses are killed and an exception is raised with the cause of termination.\nIn the case an exception was caught in the child process, it is forwarded and\nits traceback is included in the exception raised in the parent process.\n\nfn (function) \u2013\n\nFunction is called as the entrypoint of the spawned process. This function\nmust be defined at the top level of a module so it can be pickled and spawned.\nThis is a requirement imposed by multiprocessing.\n\nThe function is called as `fn(i, *args)`, where `i` is the process index and\n`args` is the passed through tuple of arguments.\n\nNone if `join` is `True`, `ProcessContext` if `join` is `False`\n\nReturned by `spawn()` when called with `join=False`.\n\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\nReturns `True` if all processes have been joined successfully, `False` if\nthere are more processes that need to be joined.\n\ntimeout (float) \u2013 Wait this long before giving up on waiting.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.get_all_sharing_strategies()", "path": "multiprocessing#torch.multiprocessing.get_all_sharing_strategies", "type": "torch.multiprocessing", "text": "\nReturns a set of sharing strategies supported on a current system.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.get_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.get_sharing_strategy", "type": "torch.multiprocessing", "text": "\nReturns the current strategy for sharing CPU tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.set_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.set_sharing_strategy", "type": "torch.multiprocessing", "text": "\nSets the strategy for sharing CPU tensors.\n\nnew_strategy (str) \u2013 Name of the selected strategy. Should be one of the\nvalues returned by `get_all_sharing_strategies()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.spawn()", "path": "multiprocessing#torch.multiprocessing.spawn", "type": "torch.multiprocessing", "text": "\nSpawns `nprocs` processes that run `fn` with `args`.\n\nIf one of the processes exits with a non-zero exit status, the remaining\nprocesses are killed and an exception is raised with the cause of termination.\nIn the case an exception was caught in the child process, it is forwarded and\nits traceback is included in the exception raised in the parent process.\n\nfn (function) \u2013\n\nFunction is called as the entrypoint of the spawned process. This function\nmust be defined at the top level of a module so it can be pickled and spawned.\nThis is a requirement imposed by multiprocessing.\n\nThe function is called as `fn(i, *args)`, where `i` is the process index and\n`args` is the passed through tuple of arguments.\n\nNone if `join` is `True`, `ProcessContext` if `join` is `False`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.SpawnContext", "path": "multiprocessing#torch.multiprocessing.SpawnContext", "type": "torch.multiprocessing", "text": "\nReturned by `spawn()` when called with `join=False`.\n\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\nReturns `True` if all processes have been joined successfully, `False` if\nthere are more processes that need to be joined.\n\ntimeout (float) \u2013 Wait this long before giving up on waiting.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.SpawnContext.join()", "path": "multiprocessing#torch.multiprocessing.SpawnContext.join", "type": "torch.multiprocessing", "text": "\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\nReturns `True` if all processes have been joined successfully, `False` if\nthere are more processes that need to be joined.\n\ntimeout (float) \u2013 Wait this long before giving up on waiting.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mv()", "path": "generated/torch.mv#torch.mv", "type": "torch", "text": "\nPerforms a matrix-vector product of the matrix `input` and the vector `vec`.\n\nIf `input` is a (n\u00d7m)(n \\times m) tensor, `vec` is a 1-D tensor of size mm ,\n`out` will be 1-D of size nn .\n\nNote\n\nThis function does not broadcast.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mvlgamma()", "path": "generated/torch.mvlgamma#torch.mvlgamma", "type": "torch", "text": "\nComputes the multivariate log-gamma function) with dimension pp element-wise,\ngiven by\n\nwhere C=log\u2061(\u03c0)\u00d7p(p\u22121)4C = \\log(\\pi) \\times \\frac{p (p - 1)}{4} and\n\u0393(\u22c5)\\Gamma(\\cdot) is the Gamma function.\n\nAll elements must be greater than p\u221212\\frac{p - 1}{2} , otherwise an error\nwould be thrown.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nanmedian()", "path": "generated/torch.nanmedian#torch.nanmedian", "type": "torch", "text": "\nReturns the median of the values in `input`, ignoring `NaN` values.\n\nThis function is identical to `torch.median()` when there are no `NaN` values\nin `input`. When `input` has one or more `NaN` values, `torch.median()` will\nalways return `NaN`, while this function will return the median of the\nnon-`NaN` elements in `input`. If all the elements in `input` are `NaN` it\nwill also return `NaN`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` contains the median of\neach row of `input` in the dimension `dim`, ignoring `NaN` values, and\n`indices` contains the index of the median values found in the dimension\n`dim`.\n\nThis function is identical to `torch.median()` when there are no `NaN` values\nin a reduced row. When a reduced row has one or more `NaN` values,\n`torch.median()` will always reduce it to `NaN`, while this function will\nreduce it to the median of the non-`NaN` elements. If all the elements in a\nreduced row are `NaN` then it will be reduced to `NaN`, too.\n\nout ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the\nmedian values and the second tensor, which must have dtype long, with their\nindices in the dimension `dim` of `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nanquantile()", "path": "generated/torch.nanquantile#torch.nanquantile", "type": "torch", "text": "\nThis is a variant of `torch.quantile()` that \u201cignores\u201d `NaN` values, computing\nthe quantiles `q` as if `NaN` values in `input` did not exist. If all values\nin a reduced row are `NaN` then the quantiles for that reduction will be\n`NaN`. See the documentation for `torch.quantile()`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nansum()", "path": "generated/torch.nansum#torch.nansum", "type": "torch", "text": "\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\n\ninput (Tensor) \u2013 the input tensor.\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\nReturns the sum of each row of the `input` tensor in the given dimension\n`dim`, treating Not a Numbers (NaNs) as zero. If `dim` is a list of\ndimensions, reduce over all of them.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nan_to_num()", "path": "generated/torch.nan_to_num#torch.nan_to_num", "type": "torch", "text": "\nReplaces `NaN`, positive infinity, and negative infinity values in `input`\nwith the values specified by `nan`, `posinf`, and `neginf`, respectively. By\ndefault, `NaN`s are replaced with zero, positive infinity is replaced with the\ngreatest finite value representable by :attr:`input`\u2019s dtype, and negative\ninfinity is replaced with the least finite value representable by `input`\u2019s\ndtype.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.narrow()", "path": "generated/torch.narrow#torch.narrow", "type": "torch", "text": "\nReturns a new tensor that is a narrowed version of `input` tensor. The\ndimension `dim` is input from `start` to `start + length`. The returned tensor\nand `input` tensor share the same underlying storage.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ne()", "path": "generated/torch.ne#torch.ne", "type": "torch", "text": "\nComputes input\u2260other\\text{input} \\neq \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is not equal to `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.neg()", "path": "generated/torch.neg#torch.neg", "type": "torch", "text": "\nReturns a new tensor with the negative of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.negative()", "path": "generated/torch.negative#torch.negative", "type": "torch", "text": "\nAlias for `torch.neg()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nextafter()", "path": "generated/torch.nextafter#torch.nextafter", "type": "torch", "text": "\nReturn the next floating-point value after `input` towards `other`,\nelementwise.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn", "path": "nn", "type": "torch.nn", "text": "\nThese are the basic building block for graphs\n\ntorch.nn\n\nA kind of Tensor that is to be considered a module parameter.\n\nA parameter that is not initialized.\n\nBase class for all neural network modules.\n\nA sequential container.\n\nHolds submodules in a list.\n\nHolds submodules in a dictionary.\n\nHolds parameters in a list.\n\nHolds parameters in a dictionary.\n\nGlobal Hooks For Module\n\nRegisters a forward pre-hook common to all modules.\n\nRegisters a global forward hook for all the modules\n\nRegisters a backward hook common to all the modules.\n\n`nn.Conv1d`\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n`nn.Conv2d`\n\nApplies a 2D convolution over an input signal composed of several input\nplanes.\n\n`nn.Conv3d`\n\nApplies a 3D convolution over an input signal composed of several input\nplanes.\n\n`nn.ConvTranspose1d`\n\nApplies a 1D transposed convolution operator over an input image composed of\nseveral input planes.\n\n`nn.ConvTranspose2d`\n\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes.\n\n`nn.ConvTranspose3d`\n\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes.\n\n`nn.LazyConv1d`\n\nA `torch.nn.Conv1d` module with lazy initialization of the `in_channels`\nargument of the `Conv1d` that is inferred from the `input.size(1)`.\n\n`nn.LazyConv2d`\n\nA `torch.nn.Conv2d` module with lazy initialization of the `in_channels`\nargument of the `Conv2d` that is inferred from the `input.size(1)`.\n\n`nn.LazyConv3d`\n\nA `torch.nn.Conv3d` module with lazy initialization of the `in_channels`\nargument of the `Conv3d` that is inferred from the `input.size(1)`.\n\n`nn.LazyConvTranspose1d`\n\nA `torch.nn.ConvTranspose1d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose1d` that is inferred from the\n`input.size(1)`.\n\n`nn.LazyConvTranspose2d`\n\nA `torch.nn.ConvTranspose2d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose2d` that is inferred from the\n`input.size(1)`.\n\n`nn.LazyConvTranspose3d`\n\nA `torch.nn.ConvTranspose3d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose3d` that is inferred from the\n`input.size(1)`.\n\n`nn.Unfold`\n\nExtracts sliding local blocks from a batched input tensor.\n\n`nn.Fold`\n\nCombines an array of sliding local blocks into a large containing tensor.\n\n`nn.MaxPool1d`\n\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\n`nn.MaxPool2d`\n\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\n`nn.MaxPool3d`\n\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\n`nn.MaxUnpool1d`\n\nComputes a partial inverse of `MaxPool1d`.\n\n`nn.MaxUnpool2d`\n\nComputes a partial inverse of `MaxPool2d`.\n\n`nn.MaxUnpool3d`\n\nComputes a partial inverse of `MaxPool3d`.\n\n`nn.AvgPool1d`\n\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\n`nn.AvgPool2d`\n\nApplies a 2D average pooling over an input signal composed of several input\nplanes.\n\n`nn.AvgPool3d`\n\nApplies a 3D average pooling over an input signal composed of several input\nplanes.\n\n`nn.FractionalMaxPool2d`\n\nApplies a 2D fractional max pooling over an input signal composed of several\ninput planes.\n\n`nn.LPPool1d`\n\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes.\n\n`nn.LPPool2d`\n\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveMaxPool1d`\n\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveMaxPool2d`\n\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveMaxPool3d`\n\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveAvgPool1d`\n\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveAvgPool2d`\n\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveAvgPool3d`\n\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\n`nn.ReflectionPad1d`\n\nPads the input tensor using the reflection of the input boundary.\n\n`nn.ReflectionPad2d`\n\nPads the input tensor using the reflection of the input boundary.\n\n`nn.ReplicationPad1d`\n\nPads the input tensor using replication of the input boundary.\n\n`nn.ReplicationPad2d`\n\nPads the input tensor using replication of the input boundary.\n\n`nn.ReplicationPad3d`\n\nPads the input tensor using replication of the input boundary.\n\n`nn.ZeroPad2d`\n\nPads the input tensor boundaries with zero.\n\n`nn.ConstantPad1d`\n\nPads the input tensor boundaries with a constant value.\n\n`nn.ConstantPad2d`\n\nPads the input tensor boundaries with a constant value.\n\n`nn.ConstantPad3d`\n\nPads the input tensor boundaries with a constant value.\n\n`nn.ELU`\n\nApplies the element-wise function:\n\n`nn.Hardshrink`\n\nApplies the hard shrinkage function element-wise:\n\n`nn.Hardsigmoid`\n\nApplies the element-wise function:\n\n`nn.Hardtanh`\n\nApplies the HardTanh function element-wise\n\n`nn.Hardswish`\n\nApplies the hardswish function, element-wise, as described in the paper:\n\n`nn.LeakyReLU`\n\nApplies the element-wise function:\n\n`nn.LogSigmoid`\n\nApplies the element-wise function:\n\n`nn.MultiheadAttention`\n\nAllows the model to jointly attend to information from different\nrepresentation subspaces.\n\n`nn.PReLU`\n\nApplies the element-wise function:\n\n`nn.ReLU`\n\nApplies the rectified linear unit function element-wise:\n\n`nn.ReLU6`\n\nApplies the element-wise function:\n\n`nn.RReLU`\n\nApplies the randomized leaky rectified liner unit function, element-wise, as\ndescribed in the paper:\n\n`nn.SELU`\n\nApplied element-wise, as:\n\n`nn.CELU`\n\nApplies the element-wise function:\n\n`nn.GELU`\n\nApplies the Gaussian Error Linear Units function:\n\n`nn.Sigmoid`\n\nApplies the element-wise function:\n\n`nn.SiLU`\n\nApplies the silu function, element-wise.\n\n`nn.Softplus`\n\nApplies the element-wise function:\n\n`nn.Softshrink`\n\nApplies the soft shrinkage function elementwise:\n\n`nn.Softsign`\n\nApplies the element-wise function:\n\n`nn.Tanh`\n\nApplies the element-wise function:\n\n`nn.Tanhshrink`\n\nApplies the element-wise function:\n\n`nn.Threshold`\n\nThresholds each element of the input Tensor.\n\n`nn.Softmin`\n\nApplies the Softmin function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range `[0,\n1]` and sum to 1.\n\n`nn.Softmax`\n\nApplies the Softmax function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range [0,1]\nand sum to 1.\n\n`nn.Softmax2d`\n\nApplies SoftMax over features to each spatial location.\n\n`nn.LogSoftmax`\n\nApplies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an\nn-dimensional input Tensor.\n\n`nn.AdaptiveLogSoftmaxWithLoss`\n\nEfficient softmax approximation as described in Efficient softmax\napproximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David\nGrangier, and Herv\u00e9 J\u00e9gou.\n\n`nn.BatchNorm1d`\n\nApplies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs\nwith optional additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\n`nn.BatchNorm2d`\n\nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n`nn.BatchNorm3d`\n\nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n`nn.GroupNorm`\n\nApplies Group Normalization over a mini-batch of inputs as described in the\npaper Group Normalization\n\n`nn.SyncBatchNorm`\n\nApplies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D\ninputs with additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\n`nn.InstanceNorm1d`\n\nApplies Instance Normalization over a 3D input (a mini-batch of 1D inputs with\noptional additional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n`nn.InstanceNorm2d`\n\nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n`nn.InstanceNorm3d`\n\nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n`nn.LayerNorm`\n\nApplies Layer Normalization over a mini-batch of inputs as described in the\npaper Layer Normalization\n\n`nn.LocalResponseNorm`\n\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension.\n\n`nn.RNNBase`\n\n`nn.RNN`\n\nApplies a multi-layer Elman RNN with tanh\u2061\\tanh or ReLU\\text{ReLU} non-\nlinearity to an input sequence.\n\n`nn.LSTM`\n\nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\n`nn.GRU`\n\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n`nn.RNNCell`\n\nAn Elman RNN cell with tanh or ReLU non-linearity.\n\n`nn.LSTMCell`\n\nA long short-term memory (LSTM) cell.\n\n`nn.GRUCell`\n\nA gated recurrent unit (GRU) cell\n\n`nn.Transformer`\n\nA transformer model.\n\n`nn.TransformerEncoder`\n\nTransformerEncoder is a stack of N encoder layers\n\n`nn.TransformerDecoder`\n\nTransformerDecoder is a stack of N decoder layers\n\n`nn.TransformerEncoderLayer`\n\nTransformerEncoderLayer is made up of self-attn and feedforward network.\n\n`nn.TransformerDecoderLayer`\n\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and\nfeedforward network.\n\n`nn.Identity`\n\nA placeholder identity operator that is argument-insensitive.\n\n`nn.Linear`\n\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b\n\n`nn.Bilinear`\n\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\n`nn.LazyLinear`\n\nA `torch.nn.Linear` module with lazy initialization.\n\n`nn.Dropout`\n\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\n`nn.Dropout2d`\n\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ).\n\n`nn.Dropout3d`\n\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ).\n\n`nn.AlphaDropout`\n\nApplies Alpha Dropout over the input.\n\n`nn.Embedding`\n\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\n`nn.EmbeddingBag`\n\nComputes sums or means of \u2018bags\u2019 of embeddings, without instantiating the\nintermediate embeddings.\n\n`nn.CosineSimilarity`\n\nReturns cosine similarity between x1x_1 and x2x_2 , computed along dim.\n\n`nn.PairwiseDistance`\n\nComputes the batchwise pairwise distance between vectors v1v_1 , v2v_2 using\nthe p-norm:\n\n`nn.L1Loss`\n\nCreates a criterion that measures the mean absolute error (MAE) between each\nelement in the input xx and target yy .\n\n`nn.MSELoss`\n\nCreates a criterion that measures the mean squared error (squared L2 norm)\nbetween each element in the input xx and target yy .\n\n`nn.CrossEntropyLoss`\n\nThis criterion combines `LogSoftmax` and `NLLLoss` in one single class.\n\n`nn.CTCLoss`\n\nThe Connectionist Temporal Classification loss.\n\n`nn.NLLLoss`\n\nThe negative log likelihood loss.\n\n`nn.PoissonNLLLoss`\n\nNegative log likelihood loss with Poisson distribution of target.\n\n`nn.GaussianNLLLoss`\n\nGaussian negative log likelihood loss.\n\n`nn.KLDivLoss`\n\nThe Kullback-Leibler divergence loss measure\n\n`nn.BCELoss`\n\nCreates a criterion that measures the Binary Cross Entropy between the target\nand the output:\n\n`nn.BCEWithLogitsLoss`\n\nThis loss combines a `Sigmoid` layer and the `BCELoss` in one single class.\n\n`nn.MarginRankingLoss`\n\nCreates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D\nmini-batch `Tensors`, and a label 1D mini-batch tensor yy (containing 1 or\n-1).\n\n`nn.HingeEmbeddingLoss`\n\nMeasures the loss given an input tensor xx and a labels tensor yy (containing\n1 or -1).\n\n`nn.MultiLabelMarginLoss`\n\nCreates a criterion that optimizes a multi-class multi-classification hinge\nloss (margin-based loss) between input xx (a 2D mini-batch `Tensor`) and\noutput yy (which is a 2D `Tensor` of target class indices).\n\n`nn.SmoothL1Loss`\n\nCreates a criterion that uses a squared term if the absolute element-wise\nerror falls below beta and an L1 term otherwise.\n\n`nn.SoftMarginLoss`\n\nCreates a criterion that optimizes a two-class classification logistic loss\nbetween input tensor xx and target tensor yy (containing 1 or -1).\n\n`nn.MultiLabelSoftMarginLoss`\n\nCreates a criterion that optimizes a multi-label one-versus-all loss based on\nmax-entropy, between input xx and target yy of size (N,C)(N, C) .\n\n`nn.CosineEmbeddingLoss`\n\nCreates a criterion that measures the loss given input tensors x1x_1 , x2x_2\nand a `Tensor` label yy with values 1 or -1.\n\n`nn.MultiMarginLoss`\n\nCreates a criterion that optimizes a multi-class classification hinge loss\n(margin-based loss) between input xx (a 2D mini-batch `Tensor`) and output yy\n(which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq\n\\text{x.size}(1)-1 ):\n\n`nn.TripletMarginLoss`\n\nCreates a criterion that measures the triplet loss given an input tensors x1x1\n, x2x2 , x3x3 and a margin with a value greater than 00 .\n\n`nn.TripletMarginWithDistanceLoss`\n\nCreates a criterion that measures the triplet loss given input tensors aa , pp\n, and nn (representing anchor, positive, and negative examples, respectively),\nand a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute\nthe relationship between the anchor and positive example (\u201cpositive distance\u201d)\nand the anchor and negative example (\u201cnegative distance\u201d).\n\n`nn.PixelShuffle`\n\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nan upscale factor.\n\n`nn.PixelUnshuffle`\n\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.\n\n`nn.Upsample`\n\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric)\ndata.\n\n`nn.UpsamplingNearest2d`\n\nApplies a 2D nearest neighbor upsampling to an input signal composed of\nseveral input channels.\n\n`nn.UpsamplingBilinear2d`\n\nApplies a 2D bilinear upsampling to an input signal composed of several input\nchannels.\n\n`nn.ChannelShuffle`\n\nDivide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups\nand rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the\noriginal tensor shape.\n\n`nn.DataParallel`\n\nImplements data parallelism at the module level.\n\n`nn.parallel.DistributedDataParallel`\n\nImplements distributed data parallelism that is based on `torch.distributed`\npackage at the module level.\n\nFrom the `torch.nn.utils` module\n\nClips gradient norm of an iterable of parameters.\n\nClips gradient of an iterable of parameters at specified value.\n\nConvert parameters to one vector\n\nConvert one vector to the parameters\n\n`prune.BasePruningMethod`\n\nAbstract base class for creation of new pruning techniques.\n\n`prune.PruningContainer`\n\nContainer holding a sequence of pruning methods for iterative pruning.\n\n`prune.Identity`\n\nUtility pruning method that does not prune any units but generates the pruning\nparametrization with a mask of ones.\n\n`prune.RandomUnstructured`\n\nPrune (currently unpruned) units in a tensor at random.\n\n`prune.L1Unstructured`\n\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the\nlowest L1-norm.\n\n`prune.RandomStructured`\n\nPrune entire (currently unpruned) channels in a tensor at random.\n\n`prune.LnStructured`\n\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\n\n`prune.CustomFromMask`\n\n`prune.identity`\n\nApplies pruning reparametrization to the tensor corresponding to the parameter\ncalled `name` in `module` without actually pruning any units.\n\n`prune.random_unstructured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units selected at random.\n\n`prune.l1_unstructured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units with the lowest L1-norm.\n\n`prune.random_structured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` selected at random.\n\n`prune.ln_structured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` with the lowest L``n``-norm.\n\n`prune.global_unstructured`\n\nGlobally prunes tensors corresponding to all parameters in `parameters` by\napplying the specified `pruning_method`.\n\n`prune.custom_from_mask`\n\nPrunes tensor corresponding to parameter called `name` in `module` by applying\nthe pre-computed mask in `mask`.\n\n`prune.remove`\n\nRemoves the pruning reparameterization from a module and the pruning method\nfrom the forward hook.\n\n`prune.is_pruned`\n\nCheck whether `module` is pruned by looking for `forward_pre_hooks` in its\nmodules that inherit from the `BasePruningMethod`.\n\nApplies weight normalization to a parameter in the given module.\n\nRemoves the weight normalization reparameterization from a module.\n\nApplies spectral normalization to a parameter in the given module.\n\nRemoves the spectral normalization reparameterization from a module.\n\nUtility functions in other modules\n\n`nn.utils.rnn.PackedSequence`\n\nHolds the data and list of `batch_sizes` of a packed sequence.\n\n`nn.utils.rnn.pack_padded_sequence`\n\nPacks a Tensor containing padded sequences of variable length.\n\n`nn.utils.rnn.pad_packed_sequence`\n\nPads a packed batch of variable length sequences.\n\n`nn.utils.rnn.pad_sequence`\n\nPad a list of variable length Tensors with `padding_value`\n\n`nn.utils.rnn.pack_sequence`\n\nPacks a list of variable length Tensors\n\n`nn.Flatten`\n\nFlattens a contiguous range of dims into a tensor.\n\n`nn.Unflatten`\n\nUnflattens a tensor dim expanding it to a desired shape.\n\nQuantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. PyTorch supports\nboth per tensor and per channel asymmetric linear quantization. To learn more\nhow to use quantized functions in PyTorch, please refer to the Quantization\ndocumentation.\n\n`nn.modules.lazy.LazyModuleMixin`\n\nA mixin for modules that lazily initialize parameters, also known as \u201clazy\nmodules.\u201d\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool1d", "path": "generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d", "type": "torch.nn", "text": "\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\nThe output size is H, for any input size. The number of output features is\nequal to the number of input planes.\n\noutput_size \u2013 the target output size H\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool2d", "path": "generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d", "type": "torch.nn", "text": "\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\nThe output is of size H x W, for any input size. The number of output features\nis equal to the number of input planes.\n\noutput_size \u2013 the target output size of the image of the form H x W. Can be a\ntuple (H, W) or a single H for a square image H x H. H and W can be either a\n`int`, or `None` which means the size will be the same as that of the input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool3d", "path": "generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d", "type": "torch.nn", "text": "\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\nThe output is of size D x H x W, for any input size. The number of output\nfeatures is equal to the number of input planes.\n\noutput_size \u2013 the target output size of the form D x H x W. Can be a tuple (D,\nH, W) or a single number D for a cube D x D x D. D, H and W can be either a\n`int`, or `None` which means the size will be the same as that of the input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "torch.nn", "text": "\nEfficient softmax approximation as described in Efficient softmax\napproximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David\nGrangier, and Herv\u00e9 J\u00e9gou.\n\nAdaptive softmax is an approximate strategy for training models with large\noutput spaces. It is most effective when the label distribution is highly\nimbalanced, for example in natural language modelling, where the word\nfrequency distribution approximately follows the Zipf\u2019s law.\n\nAdaptive softmax partitions the labels into several clusters, according to\ntheir frequency. These clusters may contain different number of targets each.\nAdditionally, clusters containing less frequent labels assign lower\ndimensional embeddings to those labels, which speeds up the computation. For\neach minibatch, only clusters for which at least one target is present are\nevaluated.\n\nThe idea is that the clusters which are accessed frequently (like the first\none, containing most frequent labels), should also be cheap to compute \u2013 that\nis, contain a small number of assigned labels.\n\nWe highly recommend taking a look at the original paper for more details.\n\nWarning\n\nLabels passed as inputs to this module should be sorted according to their\nfrequency. This means that the most frequent label should be represented by\nthe index `0`, and the least frequent label should be represented by the index\n`n_classes - 1`.\n\nNote\n\nThis module returns a `NamedTuple` with `output` and `loss` fields. See\nfurther documentation for details.\n\nNote\n\nTo compute log-probabilities for all classes, the `log_prob` method can be\nused.\n\n`NamedTuple` with `output` and `loss` fields\n\nComputes log probabilities for all n_classes\\texttt{n\\\\_classes}\n\ninput (Tensor) \u2013 a minibatch of examples\n\nlog-probabilities of for each class cc in range 0<=c<=n_classes0 <= c <=\n\\texttt{n\\\\_classes} , where n_classes\\texttt{n\\\\_classes} is a parameter\npassed to `AdaptiveLogSoftmaxWithLoss` constructor.\n\nThis is equivalent to `self.log_pob(input).argmax(dim=1)`, but is more\nefficient in some cases.\n\ninput (Tensor) \u2013 a minibatch of examples\n\na class with the highest probability for each example\n\noutput (Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "torch.nn", "text": "\nComputes log probabilities for all n_classes\\texttt{n\\\\_classes}\n\ninput (Tensor) \u2013 a minibatch of examples\n\nlog-probabilities of for each class cc in range 0<=c<=n_classes0 <= c <=\n\\texttt{n\\\\_classes} , where n_classes\\texttt{n\\\\_classes} is a parameter\npassed to `AdaptiveLogSoftmaxWithLoss` constructor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "torch.nn", "text": "\nThis is equivalent to `self.log_pob(input).argmax(dim=1)`, but is more\nefficient in some cases.\n\ninput (Tensor) \u2013 a minibatch of examples\n\na class with the highest probability for each example\n\noutput (Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool1d", "path": "generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d", "type": "torch.nn", "text": "\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\nThe output size is H, for any input size. The number of output features is\nequal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool2d", "path": "generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\nThe output is of size H x W, for any input size. The number of output features\nis equal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool3d", "path": "generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d", "type": "torch.nn", "text": "\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\nThe output is of size D x H x W, for any input size. The number of output\nfeatures is equal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AlphaDropout", "path": "generated/torch.nn.alphadropout#torch.nn.AlphaDropout", "type": "torch.nn", "text": "\nApplies Alpha Dropout over the input.\n\nAlpha Dropout is a type of Dropout that maintains the self-normalizing\nproperty. For an input with zero mean and unit standard deviation, the output\nof Alpha Dropout maintains the original mean and standard deviation of the\ninput. Alpha Dropout goes hand-in-hand with SELU activation function, which\nensures that the outputs have zero mean and unit standard deviation.\n\nDuring training, it randomly masks some of the elements of the input tensor\nwith probability p using samples from a bernoulli distribution. The elements\nto masked are randomized on every forward call, and scaled and shifted to\nmaintain zero mean and unit standard deviation.\n\nDuring evaluation the module simply computes an identity function.\n\nMore details can be found in the paper Self-Normalizing Neural Networks .\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AvgPool1d", "path": "generated/torch.nn.avgpool1d#torch.nn.AvgPool1d", "type": "torch.nn", "text": "\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size (N,C,L)(N,\nC, L) , output (N,C,Lout)(N, C, L_{out}) and `kernel_size` kk can be precisely\ndescribed as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on both\nsides for `padding` number of points.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride`, `padding` can each be an `int` or a\none-element tuple.\n\nOutput: (N,C,Lout)(N, C, L_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AvgPool2d", "path": "generated/torch.nn.avgpool2d#torch.nn.AvgPool2d", "type": "torch.nn", "text": "\nApplies a 2D average pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) and\n`kernel_size` (kH,kW)(kH, kW) can be precisely described as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on both\nsides for `padding` number of points.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride`, `padding` can either be:\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AvgPool3d", "path": "generated/torch.nn.avgpool3d#torch.nn.AvgPool3d", "type": "torch.nn", "text": "\nApplies a 3D average pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out},\nH_{out}, W_{out}) and `kernel_size` (kD,kH,kW)(kD, kH, kW) can be precisely\ndescribed as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on all\nthree sides for `padding` number of points.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride` can either be:\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm1d", "path": "generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs\nwith optional additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over the mini-\nbatches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size `C`\n(where `C` is the input size). By default, the elements of \u03b3\\gamma are set to\n1 and the elements of \u03b2\\beta are set to 0. The standard-deviation is\ncalculated via the biased estimator, equivalent to `torch.var(input,\nunbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing\nstatistics on `(N, L)` slices, it\u2019s common terminology to call this Temporal\nBatch Normalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm2d", "path": "generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over the mini-\nbatches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size `C`\n(where `C` is the input size). By default, the elements of \u03b3\\gamma are set to\n1 and the elements of \u03b2\\beta are set to 0. The standard-deviation is\ncalculated via the biased estimator, equivalent to `torch.var(input,\nunbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing\nstatistics on `(N, H, W)` slices, it\u2019s common terminology to call this Spatial\nBatch Normalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm3d", "path": "generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over the mini-\nbatches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size `C`\n(where `C` is the input size). By default, the elements of \u03b3\\gamma are set to\n1 and the elements of \u03b2\\beta are set to 0. The standard-deviation is\ncalculated via the biased estimator, equivalent to `torch.var(input,\nunbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing\nstatistics on `(N, D, H, W)` slices, it\u2019s common terminology to call this\nVolumetric Batch Normalization or Spatio-temporal Batch Normalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BCELoss", "path": "generated/torch.nn.bceloss#torch.nn.BCELoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the Binary Cross Entropy between the target\nand the output:\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen\n\nThis is used for measuring the error of a reconstruction in for example an\nauto-encoder. Note that the targets yy should be numbers between 0 and 1.\n\nNotice that if xnx_n is either 0 or 1, one of the log terms would be\nmathematically undefined in the above loss equation. PyTorch chooses to set\nlog\u2061(0)=\u2212\u221e\\log (0) = -\\infty , since lim\u2061x\u21920log\u2061(x)=\u2212\u221e\\lim_{x\\to 0} \\log (x) =\n-\\infty . However, an infinite term in the loss equation is not desirable for\nseveral reasons.\n\nFor one, if either yn=0y_n = 0 or (1\u2212yn)=0(1 - y_n) = 0 , then we would be\nmultiplying 0 with infinity. Secondly, if we have an infinite loss value, then\nwe would also have an infinite term in our gradient, since\nlim\u2061x\u21920ddxlog\u2061(x)=\u221e\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty . This would\nmake BCELoss\u2019s backward method nonlinear with respect to xnx_n , and using it\nfor things like linear regression would not be straight-forward.\n\nOur solution is that BCELoss clamps its log function outputs to be greater\nthan or equal to -100. This way, we can always have a finite loss value and a\nlinear backward method.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BCEWithLogitsLoss", "path": "generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss", "type": "torch.nn", "text": "\nThis loss combines a `Sigmoid` layer and the `BCELoss` in one single class.\nThis version is more numerically stable than using a plain `Sigmoid` followed\nby a `BCELoss` as, by combining the operations into one layer, we take\nadvantage of the log-sum-exp trick for numerical stability.\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen\n\nThis is used for measuring the error of a reconstruction in for example an\nauto-encoder. Note that the targets `t[i]` should be numbers between 0 and 1.\n\nIt\u2019s possible to trade off recall and precision by adding weights to positive\nexamples. In the case of multi-label classification the loss can be described\nas:\n\nwhere cc is the class number (c>1c > 1 for multi-label binary classification,\nc=1c = 1 for single-label binary classification), nn is the number of the\nsample in the batch and pcp_c is the weight of the positive answer for the\nclass cc .\n\npc>1p_c > 1 increases the recall, pc<1p_c < 1 increases the precision.\n\nFor example, if a dataset contains 100 positive and 300 negative examples of a\nsingle class, then `pos_weight` for the class should be equal to\n300100=3\\frac{300}{100}=3 . The loss would act as if the dataset contains\n3\u00d7100=3003\\times 100=300 positive examples.\n\nExamples:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Bilinear", "path": "generated/torch.nn.bilinear#torch.nn.Bilinear", "type": "torch.nn", "text": "\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CELU", "path": "generated/torch.nn.celu#torch.nn.CELU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nMore details can be found in the paper Continuously Differentiable Exponential\nLinear Units .\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ChannelShuffle", "path": "generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle", "type": "torch.nn", "text": "\nDivide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups\nand rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the\noriginal tensor shape.\n\ngroups (int) \u2013 number of groups to divide channels in.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad1d", "path": "generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in both boundaries. If a 2-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} )\n\nOutput: (N,C,Wout)(N, C, W_{out}) where\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad2d", "path": "generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 4-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} )\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) where\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad3d", "path": "generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 6-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} ,\npadding_front\\text{padding\\\\_front} , padding_back\\text{padding\\\\_back} )\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) where\n\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\\\_front} +\n\\text{padding\\\\_back}\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Conv1d", "path": "generated/torch.nn.conv1d#torch.nn.Conv1d", "type": "torch.nn", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,Cin,L)(N, C_{\\text{in}}, L) and output (N,Cout,Lout)(N, C_{\\text{out}},\nL_{\\text{out}}) can be precisely described as:\n\nwhere \u22c6\\star is the valid cross-correlation operator, NN is a batch size, CC\ndenotes a number of channels, LL is a length of signal sequence.\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nNote\n\nWhen `groups == in_channels` and `out_channels == K * in_channels`, where `K`\nis a positive integer, this operation is also known as a \u201cdepthwise\nconvolution\u201d.\n\nIn other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a\ndepthwise convolution with a depthwise multiplier `K` can be performed with\nthe arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in},\nC_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out}) where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Conv2d", "path": "generated/torch.nn.conv2d#torch.nn.Conv2d", "type": "torch.nn", "text": "\nApplies a 2D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,Cin,H,W)(N, C_{\\text{in}}, H, W) and output (N,Cout,Hout,Wout)(N,\nC_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}}) can be precisely described as:\n\nwhere \u22c6\\star is the valid 2D cross-correlation operator, NN is a batch size,\nCC denotes a number of channels, HH is a height of input planes in pixels, and\nWW is width in pixels.\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n\nNote\n\nWhen `groups == in_channels` and `out_channels == K * in_channels`, where `K`\nis a positive integer, this operation is also known as a \u201cdepthwise\nconvolution\u201d.\n\nIn other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a\ndepthwise convolution with a depthwise multiplier `K` can be performed with\nthe arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in},\nC_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out}) where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Conv3d", "path": "generated/torch.nn.conv3d#torch.nn.Conv3d", "type": "torch.nn", "text": "\nApplies a 3D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,Cin,D,H,W)(N, C_{in}, D, H, W) and output (N,Cout,Dout,Hout,Wout)(N,\nC_{out}, D_{out}, H_{out}, W_{out}) can be precisely described as:\n\nwhere \u22c6\\star is the valid 3D cross-correlation operator\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n\nNote\n\nWhen `groups == in_channels` and `out_channels == K * in_channels`, where `K`\nis a positive integer, this operation is also known as a \u201cdepthwise\nconvolution\u201d.\n\nIn other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a\ndepthwise convolution with a depthwise multiplier `K` can be performed with\nthe arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in},\nC_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out}) where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose1d", "path": "generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d", "type": "torch.nn", "text": "\nApplies a 1D transposed convolution operator over an input image composed of\nseveral input planes.\n\nThis module can be seen as the gradient of Conv1d with respect to its input.\nIt is also known as a fractionally-strided convolution or a deconvolution\n(although it is not an actual deconvolution operation).\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nNote\n\nThe `padding` argument effectively adds `dilation * (kernel_size - 1) -\npadding` amount of zero padding to both sizes of the input. This is set so\nthat when a `Conv1d` and a `ConvTranspose1d` are initialized with same\nparameters, they are inverses of each other in regard to the input and output\nshapes. However, when `stride > 1`, `Conv1d` maps multiple input shapes to the\nsame output shape. `output_padding` is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note that\n`output_padding` is only used to find output shape, but does not actually add\nzero-padding to output.\n\nNote\n\nIn some circumstances when using the CUDA backend with CuDNN, this operator\nmay select a nondeterministic algorithm to increase performance. If this is\nundesirable, you can try to make the operation deterministic (potentially at a\nperformance cost) by setting `torch.backends.cudnn.deterministic = True`.\nPlease see the notes on Reproducibility for background.\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out}) where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose2d", "path": "generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d", "type": "torch.nn", "text": "\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes.\n\nThis module can be seen as the gradient of Conv2d with respect to its input.\nIt is also known as a fractionally-strided convolution or a deconvolution\n(although it is not an actual deconvolution operation).\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `output_padding` can either\nbe:\n\nNote\n\nThe `padding` argument effectively adds `dilation * (kernel_size - 1) -\npadding` amount of zero padding to both sizes of the input. This is set so\nthat when a `Conv2d` and a `ConvTranspose2d` are initialized with same\nparameters, they are inverses of each other in regard to the input and output\nshapes. However, when `stride > 1`, `Conv2d` maps multiple input shapes to the\nsame output shape. `output_padding` is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note that\n`output_padding` is only used to find output shape, but does not actually add\nzero-padding to output.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose3d", "path": "generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d", "type": "torch.nn", "text": "\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes. The transposed convolution operator multiplies each\ninput value element-wise by a learnable kernel, and sums over the outputs from\nall input feature planes.\n\nThis module can be seen as the gradient of Conv3d with respect to its input.\nIt is also known as a fractionally-strided convolution or a deconvolution\n(although it is not an actual deconvolution operation).\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `output_padding` can either\nbe:\n\nNote\n\nThe `padding` argument effectively adds `dilation * (kernel_size - 1) -\npadding` amount of zero padding to both sizes of the input. This is set so\nthat when a `Conv3d` and a `ConvTranspose3d` are initialized with same\nparameters, they are inverses of each other in regard to the input and output\nshapes. However, when `stride > 1`, `Conv3d` maps multiple input shapes to the\nsame output shape. `output_padding` is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note that\n`output_padding` is only used to find output shape, but does not actually add\nzero-padding to output.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CosineEmbeddingLoss", "path": "generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the loss given input tensors x1x_1 , x2x_2\nand a `Tensor` label yy with values 1 or -1. This is used for measuring\nwhether two inputs are similar or dissimilar, using the cosine distance, and\nis typically used for learning nonlinear embeddings or semi-supervised\nlearning.\n\nThe loss function for each sample is:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CosineSimilarity", "path": "generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity", "type": "torch.nn", "text": "\nReturns cosine similarity between x1x_1 and x2x_2 , computed along dim.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CrossEntropyLoss", "path": "generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss", "type": "torch.nn", "text": "\nThis criterion combines `LogSoftmax` and `NLLLoss` in one single class.\n\nIt is useful when training a classification problem with `C` classes. If\nprovided, the optional argument `weight` should be a 1D `Tensor` assigning\nweight to each of the classes. This is particularly useful when you have an\nunbalanced training set.\n\nThe `input` is expected to contain raw, unnormalized scores for each class.\n\n`input` has to be a Tensor of size either (minibatch,C)(minibatch, C) or\n(minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K) with K\u22651K \\geq 1\nfor the `K`-dimensional case (described later).\n\nThis criterion expects a class index in the range [0,C\u22121][0, C-1] as the\n`target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`\nis specified, this criterion also accepts this class index (this index may not\nnecessarily be in the class range).\n\nThe loss can be described as:\n\nor in the case of the `weight` argument being specified:\n\nThe losses are averaged across observations for each minibatch. If the\n`weight` argument is specified then this is a weighted average:\n\nCan also be used for higher dimension inputs, such as 2D images, by providing\nan input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)\nwith K\u22651K \\geq 1 , where KK is the number of dimensions, and a target of\nappropriate shape (see below).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CTCLoss", "path": "generated/torch.nn.ctcloss#torch.nn.CTCLoss", "type": "torch.nn", "text": "\nThe Connectionist Temporal Classification loss.\n\nCalculates loss between a continuous (unsegmented) time series and a target\nsequence. CTCLoss sums over the probability of possible alignments of input to\ntarget, producing a loss value which is differentiable with respect to each\ninput node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d,\nwhich limits the length of the target sequence such that it must be \u2264\\leq the\ninput length.\n\nExamples:\n\nA. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented\nSequence Data with Recurrent Neural Networks:\nhttps://www.cs.toronto.edu/~graves/icml_2006.pdf\n\nNote\n\nIn order to use CuDNN, the following must be satisfied: `targets` must be in\nconcatenated format, all `input_lengths` must be `T`. blank=0blank=0 ,\n`target_lengths` \u2264256\\leq 256 , the integer arguments must be of dtype\n`torch.int32`.\n\nThe regular implementation uses the (more common in PyTorch) `torch.long`\ndtype.\n\nNote\n\nIn some circumstances when using the CUDA backend with CuDNN, this operator\nmay select a nondeterministic algorithm to increase performance. If this is\nundesirable, you can try to make the operation deterministic (potentially at a\nperformance cost) by setting `torch.backends.cudnn.deterministic = True`.\nPlease see the notes on Reproducibility for background.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.DataParallel", "path": "generated/torch.nn.dataparallel#torch.nn.DataParallel", "type": "torch.nn", "text": "\nImplements data parallelism at the module level.\n\nThis container parallelizes the application of the given `module` by splitting\nthe input across the specified devices by chunking in the batch dimension\n(other objects will be copied once per device). In the forward pass, the\nmodule is replicated on each device, and each replica handles a portion of the\ninput. During the backwards pass, gradients from each replica are summed into\nthe original module.\n\nThe batch size should be larger than the number of GPUs used.\n\nWarning\n\nIt is recommended to use `DistributedDataParallel`, instead of this class, to\ndo multi-GPU training, even if there is only a single node. See: Use\nnn.parallel.DistributedDataParallel instead of multiprocessing or\nnn.DataParallel and Distributed Data Parallel.\n\nArbitrary positional and keyword inputs are allowed to be passed into\nDataParallel but some types are specially handled. tensors will be scattered\non dim specified (default 0). tuple, list and dict types will be shallow\ncopied. The other types will be shared among different threads and can be\ncorrupted if written to in the model\u2019s forward pass.\n\nThe parallelized `module` must have its parameters and buffers on\n`device_ids[0]` before running this `DataParallel` module.\n\nWarning\n\nIn each forward, `module` is replicated on each device, so any updates to the\nrunning module in `forward` will be lost. For example, if `module` has a\ncounter attribute that is incremented in each `forward`, it will always stay\nat the initial value because the update is done on the replicas which are\ndestroyed after `forward`. However, `DataParallel` guarantees that the replica\non `device[0]` will have its parameters and buffers sharing storage with the\nbase parallelized `module`. So in-place updates to the parameters or buffers\non `device[0]` will be recorded. E.g., `BatchNorm2d` and `spectral_norm()`\nrely on this behavior to update the buffers.\n\nWarning\n\nForward and backward hooks defined on `module` and its submodules will be\ninvoked `len(device_ids)` times, each with inputs located on a particular\ndevice. Particularly, the hooks are only guaranteed to be executed in correct\norder with respect to operations on corresponding devices. For example, it is\nnot guaranteed that hooks set via `register_forward_pre_hook()` be executed\nbefore `all` `len(device_ids)` `forward()` calls, but that each such hook be\nexecuted before the corresponding `forward()` call of that device.\n\nWarning\n\nWhen `module` returns a scalar (i.e., 0-dimensional tensor) in `forward()`,\nthis wrapper will return a vector of length equal to number of devices used in\ndata parallelism, containing the result from each device.\n\nNote\n\nThere is a subtlety in using the `pack sequence -> recurrent network -> unpack\nsequence` pattern in a `Module` wrapped in `DataParallel`. See My recurrent\nnetwork doesn\u2019t work with data parallelism section in FAQ for details.\n\n~DataParallel.module (Module) \u2013 the module to be parallelized\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Dropout", "path": "generated/torch.nn.dropout#torch.nn.Dropout", "type": "torch.nn", "text": "\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution. Each channel will\nbe zeroed out independently on every forward call.\n\nThis has proven to be an effective technique for regularization and preventing\nthe co-adaptation of neurons as described in the paper Improving neural\nnetworks by preventing co-adaptation of feature detectors .\n\nFurthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p} during\ntraining. This means that during evaluation the module simply computes an\nidentity function.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Dropout2d", "path": "generated/torch.nn.dropout2d#torch.nn.Dropout2d", "type": "torch.nn", "text": "\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently\non every forward call with probability `p` using samples from a Bernoulli\ndistribution.\n\nUsually the input comes from `nn.Conv2d` modules.\n\nAs described in the paper Efficient Object Localization Using Convolutional\nNetworks , if adjacent pixels within feature maps are strongly correlated (as\nis normally the case in early convolution layers) then i.i.d. dropout will not\nregularize the activations and will otherwise just result in an effective\nlearning rate decrease.\n\nIn this case, `nn.Dropout2d()` will help promote independence between feature\nmaps and should be used instead.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Dropout3d", "path": "generated/torch.nn.dropout3d#torch.nn.Dropout3d", "type": "torch.nn", "text": "\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently\non every forward call with probability `p` using samples from a Bernoulli\ndistribution.\n\nUsually the input comes from `nn.Conv3d` modules.\n\nAs described in the paper Efficient Object Localization Using Convolutional\nNetworks , if adjacent pixels within feature maps are strongly correlated (as\nis normally the case in early convolution layers) then i.i.d. dropout will not\nregularize the activations and will otherwise just result in an effective\nlearning rate decrease.\n\nIn this case, `nn.Dropout3d()` will help promote independence between feature\nmaps and should be used instead.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ELU", "path": "generated/torch.nn.elu#torch.nn.ELU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Embedding", "path": "generated/torch.nn.embedding#torch.nn.Embedding", "type": "torch.nn", "text": "\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\nThis module is often used to store word embeddings and retrieve them using\nindices. The input to the module is a list of indices, and the output is the\ncorresponding word embeddings.\n\n~Embedding.weight (Tensor) \u2013 the learnable weights of the module of shape\n(num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)\n\nNote\n\nKeep in mind that only a limited number of optimizers support sparse\ngradients: currently it\u2019s `optim.SGD` (`CUDA` and `CPU`), `optim.SparseAdam`\n(`CUDA` and `CPU`) and `optim.Adagrad` (`CPU`)\n\nNote\n\nWith `padding_idx` set, the embedding vector at `padding_idx` is initialized\nto all zeros. However, note that this vector can be modified afterwards, e.g.,\nusing a customized initialization method, and thus changing the vector used to\npad the output. The gradient for this vector from `Embedding` is always zero.\n\nNote\n\nWhen `max_norm` is not `None`, `Embedding`\u2019s forward method will modify the\n`weight` tensor in-place. Since tensors needed for gradient computations\ncannot be modified in-place, performing a differentiable operation on\n`Embedding.weight` before calling `Embedding`\u2019s forward method requires\ncloning `Embedding.weight` when `max_norm` is not `None`. For example:\n\nExamples:\n\nCreates Embedding instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Embedding.from_pretrained()", "path": "generated/torch.nn.embedding#torch.nn.Embedding.from_pretrained", "type": "torch.nn", "text": "\nCreates Embedding instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.EmbeddingBag", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag", "type": "torch.nn", "text": "\nComputes sums or means of \u2018bags\u2019 of embeddings, without instantiating the\nintermediate embeddings.\n\nFor bags of constant length and no `per_sample_weights` and 2D inputs, this\nclass\n\nHowever, `EmbeddingBag` is much more time and memory efficient than using a\nchain of these operations.\n\nEmbeddingBag also supports per-sample weights as an argument to the forward\npass. This scales the output of the Embedding before performing a weighted\nreduction as specified by `mode`. If `per_sample_weights`` is passed, the only\nsupported `mode` is `\"sum\"`, which computes a weighted sum according to\n`per_sample_weights`.\n\n~EmbeddingBag.weight (Tensor) \u2013 the learnable weights of the module of shape\n`(num_embeddings, embedding_dim)` initialized from N(0,1)\\mathcal{N}(0, 1) .\n\n`per_index_weights` (Tensor, optional)\n\nIf `input` is 2D of shape `(B, N)`,\n\nit will be treated as `B` bags (sequences) each of fixed length `N`, and this\nwill return `B` values aggregated in a way depending on the `mode`. `offsets`\nis ignored and required to be `None` in this case.\n\nIf `input` is 1D of shape `(N)`,\n\nit will be treated as a concatenation of multiple bags (sequences). `offsets`\nis required to be a 1D tensor containing the starting index positions of each\nbag in `input`. Therefore, for `offsets` of shape `(B)`, `input` will be\nviewed as having `B` bags. Empty bags (i.e., having 0-length) will have\nreturned vectors filled by zeros.\n\nto indicate all weights should be taken to be `1`. If specified,\n`per_sample_weights` must have exactly the same shape as input and is treated\nas having the same `offsets`, if those are not `None`. Only supported for\n`mode='sum'`.\n\nOutput shape: `(B, embedding_dim)`\n\nExamples:\n\nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.EmbeddingBag.from_pretrained()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.from_pretrained", "type": "torch.nn", "text": "\nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten", "path": "generated/torch.nn.flatten#torch.nn.Flatten", "type": "torch.nn", "text": "\nFlattens a contiguous range of dims into a tensor. For use with `Sequential`.\n\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.add_module()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.apply()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.bfloat16()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.cpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.cuda()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.double()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.eval()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.float()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.half()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.load_state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_buffer()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_forward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_forward_pre_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_full_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_parameter()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.requires_grad_()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.to()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.train()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.type()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.xpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.zero_grad()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Fold", "path": "generated/torch.nn.fold#torch.nn.Fold", "type": "torch.nn", "text": "\nCombines an array of sliding local blocks into a large containing tensor.\n\nConsider a batched `input` tensor containing sliding local blocks, e.g.,\npatches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times\n\\prod(\\text{kernel\\\\_size}), L) , where NN is batch dimension,\nC\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\\\_size}) is the number of values\nwithin a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\\\_size}) spatial\nlocations each containing a CC -channeled vector), and LL is the total number\nof blocks. (This is exactly the same specification as the output shape of\n`Unfold`.) This operation combines these local blocks into the large `output`\ntensor of shape (N,C,output_size[0],output_size[1],\u2026)(N, C,\n\\text{output\\\\_size}[0], \\text{output\\\\_size}[1], \\dots) by summing the\noverlapping values. Similar to `Unfold`, the arguments must satisfy\n\nwhere dd is over all spatial dimensions.\n\nThe `padding`, `stride` and `dilation` arguments specify how the sliding\nblocks are retrieved.\n\nNote\n\n`Fold` calculates each combined value in the resulting large tensor by summing\nall values from all containing blocks. `Unfold` extracts the values in the\nlocal blocks by copying from the large tensor. So, if the blocks overlap, they\nare not inverses of each other.\n\nIn general, folding and unfolding operations are related as follows. Consider\n`Fold` and `Unfold` instances created with the same parameters:\n\nThen for any (supported) `input` tensor the following equality holds:\n\nwhere `divisor` is a tensor that depends only on the shape and dtype of the\n`input`:\n\nWhen the `divisor` tensor contains no zero elements, then `fold` and `unfold`\noperations are inverses of each other (up to constant divisor).\n\nWarning\n\nCurrently, only 4-D output tensors (batched image-like tensors) are supported.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.FractionalMaxPool2d", "path": "generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D fractional max pooling over an input signal composed of several\ninput planes.\n\nFractional MaxPooling is described in detail in the paper Fractional\nMaxPooling by Ben Graham\n\nThe max-pooling operation is applied in kH\u00d7kWkH \\times kW regions by a\nstochastic step size determined by the target output size. The number of\noutput features is equal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional", "path": "nn.functional", "type": "torch.nn.functional", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 2D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 3D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 1D transposed convolution operator over an input signal composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nExtracts sliding local blocks from a batched input tensor.\n\nWarning\n\nCurrently, only 4-D input tensors (batched image-like tensors) are supported.\n\nWarning\n\nMore than one element of the unfolded tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensor, please clone it first.\n\nSee `torch.nn.Unfold` for details\n\nCombines an array of sliding local blocks into a large containing tensor.\n\nWarning\n\nCurrently, only 3-D output tensors (unfolded batched image-like tensors) are\nsupported.\n\nSee `torch.nn.Fold` for details\n\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\nSee `AvgPool1d` for details and output shape.\n\nExamples:\n\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\nSee `AvgPool2d` for details and output shape.\n\nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions\nby step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output\nfeatures is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input\nplanes}}{sT}\\rfloor .\n\nSee `AvgPool3d` for details and output shape.\n\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool1d` for details.\n\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool2d` for details.\n\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool3d` for details.\n\nComputes a partial inverse of `MaxPool1d`.\n\nSee `MaxUnpool1d` for details.\n\nComputes a partial inverse of `MaxPool2d`.\n\nSee `MaxUnpool2d` for details.\n\nComputes a partial inverse of `MaxPool3d`.\n\nSee `MaxUnpool3d` for details.\n\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool1d` for details.\n\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool2d` for details.\n\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool1d` for details and output shape.\n\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool2d` for details and output shape.\n\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool3d` for details and output shape.\n\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool1d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer)\n\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool2d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or double-integer tuple)\n\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool3d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or triple-integer tuple)\n\nThresholds each element of the input Tensor.\n\nSee `Threshold` for more details.\n\nIn-place version of `threshold()`.\n\nApplies the rectified linear unit function element-wise. See `ReLU` for more\ndetails.\n\nIn-place version of `relu()`.\n\nApplies the HardTanh function element-wise. See `Hardtanh` for more details.\n\nIn-place version of `hardtanh()`.\n\nApplies the hardswish function, element-wise, as described in the paper:\n\nSearching for MobileNetV3.\n\nSee `Hardswish` for more details.\n\nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) =\n\\min(\\max(0,x), 6) .\n\nSee `ReLU6` for more details.\n\nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) =\n\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .\n\nSee `ELU` for more details.\n\nIn-place version of `elu()`.\n\nApplies element-wise,\nSELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale *\n(\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with\n\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\nand\nscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946\n.\n\nSee `SELU` for more details.\n\nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x)\n= \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .\n\nSee `CELU` for more details.\n\nApplies element-wise,\nLeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0,\nx) + \\text{negative\\\\_slope} * \\min(0, x)\n\nSee `LeakyReLU` for more details.\n\nIn-place version of `leaky_relu()`.\n\nApplies element-wise the function\nPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight}\n* \\min(0,x) where weight is a learnable parameter.\n\nSee `PReLU` for more details.\n\nRandomized leaky ReLU.\n\nSee `RReLU` for more details.\n\nIn-place version of `rrelu()`.\n\nThe gated linear unit. Computes:\n\nwhere `input` is split in half along `dim` to form `a` and `b`, \u03c3\\sigma is the\nsigmoid function and \u2297\\otimes is the element-wise product between matrices.\n\nSee Language Modeling with Gated Convolutional Networks.\n\nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)\n\nwhere \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian\nDistribution.\n\nSee Gaussian Error Linear Units (GELUs).\n\nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) =\n\\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)\n\nSee `LogSigmoid` for more details.\n\nApplies the hard shrinkage function element-wise\n\nSee `Hardshrink` for more details.\n\nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x -\n\\text{Tanh}(x)\n\nSee `Tanhshrink` for more details.\n\nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) =\n\\frac{x}{1 + |x|}\n\nSee `Softsign` for more details.\n\nApplies element-wise, the function\nSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1\n+ \\exp(\\beta * x)) .\n\nFor numerical stability the implementation reverts to the linear function when\ninput\u00d7\u03b2>thresholdinput \\times \\beta > threshold .\n\nSee `Softplus` for more details.\n\nApplies a softmin function.\n\nNote that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See\nsoftmax definition for mathematical formula.\n\nSee `Softmin` for more details.\n\nApplies a softmax function.\n\nSoftmax is defined as:\n\nSoftmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j\n\\exp(x_j)}\n\nIt is applied to all slices along dim, and will re-scale them so that the\nelements lie in the range `[0, 1]` and sum to 1.\n\nSee `Softmax` for more details.\n\nNote\n\nThis function doesn\u2019t work directly with NLLLoss, which expects the Log to be\ncomputed between the Softmax and itself. Use log_softmax instead (it\u2019s faster\nand has better numerical properties).\n\nApplies the soft shrinkage function elementwise\n\nSee `Softshrink` for more details.\n\nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally\ndiscretizes.\n\nSampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\nIf `hard=True`, the returned samples will be one-hot, otherwise they will be\nprobability distributions that sum to 1 across `dim`.\n\nNote\n\nThis function is here for legacy reasons, may be removed from nn.Functional in\nthe future.\n\nNote\n\nThe main trick for `hard` is to do `y_hard - y_soft.detach() + y_soft`\n\nIt achieves two things: - makes the output value exactly one-hot (since we add\nthen subtract y_soft value) - makes the gradient equal to y_soft gradient\n(since we strip all other gradients)\n\nApplies a softmax followed by a logarithm.\n\nWhile mathematically equivalent to log(softmax(x)), doing these two operations\nseparately is slower, and numerically unstable. This function uses an\nalternative formulation to compute the output and gradient correctly.\n\nSee `LogSoftmax` for more details.\n\nApplies element-wise,\nTanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) =\n\\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n\nSee `Tanh` for more details.\n\nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) =\n\\frac{1}{1 + \\exp(-x)}\n\nSee `Sigmoid` for more details.\n\nApplies the element-wise function\n\ninplace \u2013 If set to `True`, will do this operation in-place. Default: `False`\n\nSee `Hardsigmoid` for more details.\n\nApplies the silu function, element-wise.\n\nNote\n\nSee Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit)\nwas originally coined, and see Sigmoid-Weighted Linear Units for Neural\nNetwork Function Approximation in Reinforcement Learning and Swish: a Self-\nGated Activation Function where the SiLU was experimented with later.\n\nSee `SiLU` for more details.\n\nApplies Batch Normalization for each channel across a batch of data.\n\nSee `BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d` for details.\n\nApplies Instance Normalization for each channel in each data sample in a\nbatch.\n\nSee `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d` for details.\n\nApplies Layer Normalization for last certain number of dimensions.\n\nSee `LayerNorm` for details.\n\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\nSee `LocalResponseNorm` for details.\n\nPerforms LpL_p normalization of inputs over specified dimension.\n\nFor a tensor `input` of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ...,\nn_k) , each ndimn_{dim} -element vector vv along dimension `dim` is\ntransformed as\n\nWith the default arguments it uses the Euclidean norm over vectors along\ndimension 11 for normalization.\n\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b .\n\nThis operator supports TensorFloat32.\n\nShape:\n\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\nShape:\n\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\nSee `Dropout` for details.\n\nApplies alpha dropout to the input.\n\nSee `AlphaDropout` for details.\n\nRandomly masks out entire channels (a channel is a feature map, e.g. the jj\n-th channel of the ii -th sample in the batch input is a tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting\nactivations to zero, as in regular Dropout, the activations are set to the\nnegative saturation value of the SELU activation function.\n\nEach element will be masked independently on every forward call with\nprobability `p` using samples from a Bernoulli distribution. The elements to\nbe masked are randomized on every forward call, and scaled and shifted to\nmaintain zero mean and unit variance.\n\nSee `FeatureAlphaDropout` for details.\n\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout2d` for details.\n\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout3d` for details.\n\nA simple lookup table that looks up embeddings in a fixed dictionary and size.\n\nThis module is often used to retrieve word embeddings using indices. The input\nto the module is a list of indices, and the embedding matrix, and the output\nis the corresponding word embeddings.\n\nSee `torch.nn.Embedding` for more details.\n\nwhere V = maximum index + 1 and embedding_dim = the embedding size\n\nExamples:\n\nComputes sums, means or maxes of `bags` of embeddings, without instantiating\nthe intermediate embeddings.\n\nSee `torch.nn.EmbeddingBag` for more details.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nShape:\n\n`input` (LongTensor) and `offsets` (LongTensor, optional)\n\nIf `input` is 2D of shape `(B, N)`,\n\nit will be treated as `B` bags (sequences) each of fixed length `N`, and this\nwill return `B` values aggregated in a way depending on the `mode`. `offsets`\nis ignored and required to be `None` in this case.\n\nIf `input` is 1D of shape `(N)`,\n\nit will be treated as a concatenation of multiple bags (sequences). `offsets`\nis required to be a 1D tensor containing the starting index positions of each\nbag in `input`. Therefore, for `offsets` of shape `(B)`, `input` will be\nviewed as having `B` bags. Empty bags (i.e., having 0-length) will have\nreturned vectors filled by zeros.\n\nExamples:\n\nTakes LongTensor with index values of shape `(*)` and returns a tensor of\nshape `(*, num_classes)` that have zeros everywhere except where the index of\nlast dimension matches the corresponding value of the input tensor, in which\ncase it will be 1.\n\nSee also One-hot on Wikipedia .\n\nLongTensor that has one more dimension with 1 values at the index of last\ndimension indicated by the input, and 0 everywhere else.\n\nSee `torch.nn.PairwiseDistance` for details\n\nReturns cosine similarity between x1 and x2, computed along dim.\n\nExample:\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape N\u00d7MN \\times M then the output will have shape\n12N(N\u22121)\\frac{1}{2} N (N - 1) .\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n\u2018minkowski\u2019, p=p)` if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0 it is\nequivalent to `scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M`. When p=\u221ep\n= \\infty , the closest scipy function is `scipy.spatial.distance.pdist(xn,\nlambda x, y: np.abs(x - y).max())`.\n\nFunction that measures the Binary Cross Entropy between the target and the\noutput.\n\nSee `BCELoss` for details.\n\nExamples:\n\nFunction that measures Binary Cross Entropy between target and output logits.\n\nSee `BCEWithLogitsLoss` for details.\n\nExamples:\n\nPoisson negative log likelihood loss.\n\nSee `PoissonNLLLoss` for details.\n\nSee `CosineEmbeddingLoss` for details.\n\nThis criterion combines `log_softmax` and `nll_loss` in a single function.\n\nSee `CrossEntropyLoss` for details.\n\nExamples:\n\nThe Connectionist Temporal Classification loss.\n\nSee `CTCLoss` for details.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nExample:\n\nSee `HingeEmbeddingLoss` for details.\n\nThe Kullback-Leibler divergence Loss\n\nSee `KLDivLoss` for details.\n\nNote\n\n`size_average` and `reduce` are in the process of being deprecated, and in the\nmeantime, specifying either of those two args will override `reduction`.\n\nNote\n\n:attr:`reduction` = `'mean'` doesn\u2019t return the true kl divergence value,\nplease use :attr:`reduction` = `'batchmean'` which aligns with KL math\ndefinition. In the next major release, `'mean'` will be changed to be the same\nas \u2018batchmean\u2019.\n\nFunction that takes the mean element-wise absolute value difference.\n\nSee `L1Loss` for details.\n\nMeasures the element-wise mean squared error.\n\nSee `MSELoss` for details.\n\nSee `MarginRankingLoss` for details.\n\nSee `MultiLabelMarginLoss` for details.\n\nSee `MultiLabelSoftMarginLoss` for details.\n\nreduce=None, reduction=\u2019mean\u2019) -> Tensor\n\nSee `MultiMarginLoss` for details.\n\nThe negative log likelihood loss.\n\nSee `NLLLoss` for details.\n\nExample:\n\nFunction that uses a squared term if the absolute element-wise error falls\nbelow beta and an L1 term otherwise.\n\nSee `SmoothL1Loss` for details.\n\nSee `SoftMarginLoss` for details.\n\nSee `TripletMarginLoss` for details\n\nSee `TripletMarginWithDistanceLoss` for details.\n\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nthe `upscale_factor`.\n\nSee `PixelShuffle` for details.\n\nExamples:\n\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the `downscale_factor`.\n\nSee `PixelUnshuffle` for details.\n\nExamples:\n\nPads tensor.\n\nThe padding size by which to pad some dimensions of `input` are described\nstarting from the last dimension and moving forward.\n\u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor dimensions of\n`input` will be padded. For example, to pad only the last dimension of the\ninput tensor, then `pad` has the form\n(padding_left,padding_right)(\\text{padding\\\\_left}, \\text{padding\\\\_right}) ;\nto pad the last 2 dimensions of the input tensor, then use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom)\\text{padding\\\\_top}, \\text{padding\\\\_bottom}) ; to\npad the last 3 dimensions, use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom\\text{padding\\\\_top}, \\text{padding\\\\_bottom}\npadding_front,padding_back)\\text{padding\\\\_front}, \\text{padding\\\\_back}) .\n\nSee `torch.nn.ConstantPad2d`, `torch.nn.ReflectionPad2d`, and\n`torch.nn.ReplicationPad2d` for concrete examples on how each of the padding\nmodes works. Constant padding is implemented for arbitrary dimensions.\nReplicate padding is implemented for padding the last 3 dimensions of 5D input\ntensor, or the last 2 dimensions of 4D input tensor, or the last dimension of\n3D input tensor. Reflect padding is only implemented for padding the last 2\ndimensions of 4D input tensor, or the last dimension of 3D input tensor.\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nExamples:\n\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\nThe algorithm used for interpolation is determined by `mode`.\n\nCurrently temporal, spatial and volumetric sampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for resizing are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\nWarning\n\nWhen scale_factor is specified, if recompute_scale_factor=True, scale_factor\nis used to compute the output_size which will then be used to infer new scales\nfor the interpolation. The default behavior for recompute_scale_factor changed\nto False in 1.6.0, and scale_factor is used in the interpolation calculation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nUpsamples the input to either the given `size` or the given `scale_factor`\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(...)`.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nThe algorithm used for upsampling is determined by `mode`.\n\nCurrently temporal, spatial and volumetric upsampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for upsampling are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='nearest')`.\n\nCurrently spatial and volumetric upsampling are supported (i.e. expected\ninputs are 4 or 5 dimensional).\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nUpsamples the input, using bilinear upsampling.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='bilinear',\nalign_corners=True)`.\n\nExpected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\nvolumetric (5 dimensional) inputs.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nGiven an `input` and a flow-field `grid`, computes the `output` using `input`\nvalues and pixel locations from `grid`.\n\nCurrently, only spatial (4-D) and volumetric (5-D) `input` are supported.\n\nIn the spatial (4-D) case, for `input` with shape (N,C,Hin,Win)(N, C,\nH_\\text{in}, W_\\text{in}) and `grid` with shape (N,Hout,Wout,2)(N,\nH_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N,\nC, H_\\text{out}, W_\\text{out}) .\n\nFor each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h,\nw]` specifies `input` pixel locations `x` and `y`, which are used to\ninterpolate the output value `output[n, :, h, w]`. In the case of 5D inputs,\n`grid[n, d, h, w]` specifies the `x`, `y`, `z` pixel locations for\ninterpolating `output[n, :, d, h, w]`. `mode` argument specifies `nearest` or\n`bilinear` interpolation method to sample the input pixels.\n\n`grid` specifies the sampling pixel locations normalized by the `input`\nspatial dimensions. Therefore, it should have most values in the range of\n`[-1, 1]`. For example, values `x = -1, y = -1` is the left-top pixel of\n`input`, and values `x = 1, y = 1` is the right-bottom pixel of `input`.\n\nIf `grid` has values outside the range of `[-1, 1]`, the corresponding outputs\nare handled as defined by `padding_mode`. Options are\n\nNote\n\nThis function is often used in conjunction with `affine_grid()` to build\nSpatial Transformer Networks .\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nNote\n\nNaN values in `grid` would be interpreted as `-1`.\n\noutput Tensor\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nNote\n\n`mode='bicubic'` is implemented using the cubic convolution algorithm with\n\u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha might be different from packages to\npackages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This\nalgorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example,\nit may produce negative values or values greater than 255 when interpolating\ninput in [0, 255]. Clamp the results with :func: `torch.clamp` to ensure they\nare within the valid range.\n\nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine\nmatrices `theta`.\n\nNote\n\nThis function is often used in conjunction with `grid_sample()` to build\nSpatial Transformer Networks .\n\noutput Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nWarning\n\nWhen `align_corners = True`, 2D affine transforms on 1D data and 3D affine\ntransforms on 2D data (that is, when one of the spatial dimensions has unit\nsize) are ill-defined, and not an intended use case. This is not a problem\nwhen `align_corners = False`. Up to version 1.2.0, all grid points along a\nunit dimension were considered arbitrarily to be at `-1`. From version 1.3.0,\nunder `align_corners = True` all grid points along a unit dimension are\nconsidered to be at ``0` (the center of the input image).\n\nEvaluates module(input) in parallel across the GPUs given in device_ids.\n\nThis is the functional version of the DataParallel module.\n\na Tensor containing the result of module(input) located on output_device\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool1d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool2d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or double-integer tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool3d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or triple-integer tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool1d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool2d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool3d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.affine_grid()", "path": "nn.functional#torch.nn.functional.affine_grid", "type": "torch.nn.functional", "text": "\nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine\nmatrices `theta`.\n\nNote\n\nThis function is often used in conjunction with `grid_sample()` to build\nSpatial Transformer Networks .\n\noutput Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nWarning\n\nWhen `align_corners = True`, 2D affine transforms on 1D data and 3D affine\ntransforms on 2D data (that is, when one of the spatial dimensions has unit\nsize) are ill-defined, and not an intended use case. This is not a problem\nwhen `align_corners = False`. Up to version 1.2.0, all grid points along a\nunit dimension were considered arbitrarily to be at `-1`. From version 1.3.0,\nunder `align_corners = True` all grid points along a unit dimension are\nconsidered to be at ``0` (the center of the input image).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.alpha_dropout()", "path": "nn.functional#torch.nn.functional.alpha_dropout", "type": "torch.nn.functional", "text": "\nApplies alpha dropout to the input.\n\nSee `AlphaDropout` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool1d()", "path": "nn.functional#torch.nn.functional.avg_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\nSee `AvgPool1d` for details and output shape.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool2d()", "path": "nn.functional#torch.nn.functional.avg_pool2d", "type": "torch.nn.functional", "text": "\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\nSee `AvgPool2d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool3d()", "path": "nn.functional#torch.nn.functional.avg_pool3d", "type": "torch.nn.functional", "text": "\nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions\nby step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output\nfeatures is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input\nplanes}}{sT}\\rfloor .\n\nSee `AvgPool3d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.batch_norm()", "path": "nn.functional#torch.nn.functional.batch_norm", "type": "torch.nn.functional", "text": "\nApplies Batch Normalization for each channel across a batch of data.\n\nSee `BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.bilinear()", "path": "nn.functional#torch.nn.functional.bilinear", "type": "torch.nn.functional", "text": "\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\nShape:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.binary_cross_entropy()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy", "type": "torch.nn.functional", "text": "\nFunction that measures the Binary Cross Entropy between the target and the\noutput.\n\nSee `BCELoss` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy_with_logits", "type": "torch.nn.functional", "text": "\nFunction that measures Binary Cross Entropy between target and output logits.\n\nSee `BCEWithLogitsLoss` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.celu()", "path": "nn.functional#torch.nn.functional.celu", "type": "torch.nn.functional", "text": "\nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x)\n= \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .\n\nSee `CELU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv1d()", "path": "nn.functional#torch.nn.functional.conv1d", "type": "torch.nn.functional", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv2d()", "path": "nn.functional#torch.nn.functional.conv2d", "type": "torch.nn.functional", "text": "\nApplies a 2D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv3d()", "path": "nn.functional#torch.nn.functional.conv3d", "type": "torch.nn.functional", "text": "\nApplies a 3D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose1d()", "path": "nn.functional#torch.nn.functional.conv_transpose1d", "type": "torch.nn.functional", "text": "\nApplies a 1D transposed convolution operator over an input signal composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose2d()", "path": "nn.functional#torch.nn.functional.conv_transpose2d", "type": "torch.nn.functional", "text": "\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose3d()", "path": "nn.functional#torch.nn.functional.conv_transpose3d", "type": "torch.nn.functional", "text": "\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.cosine_embedding_loss()", "path": "nn.functional#torch.nn.functional.cosine_embedding_loss", "type": "torch.nn.functional", "text": "\nSee `CosineEmbeddingLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.cosine_similarity()", "path": "nn.functional#torch.nn.functional.cosine_similarity", "type": "torch.nn.functional", "text": "\nReturns cosine similarity between x1 and x2, computed along dim.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.cross_entropy()", "path": "nn.functional#torch.nn.functional.cross_entropy", "type": "torch.nn.functional", "text": "\nThis criterion combines `log_softmax` and `nll_loss` in a single function.\n\nSee `CrossEntropyLoss` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.ctc_loss()", "path": "nn.functional#torch.nn.functional.ctc_loss", "type": "torch.nn.functional", "text": "\nThe Connectionist Temporal Classification loss.\n\nSee `CTCLoss` for details.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout()", "path": "nn.functional#torch.nn.functional.dropout", "type": "torch.nn.functional", "text": "\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\nSee `Dropout` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout2d()", "path": "nn.functional#torch.nn.functional.dropout2d", "type": "torch.nn.functional", "text": "\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout3d()", "path": "nn.functional#torch.nn.functional.dropout3d", "type": "torch.nn.functional", "text": "\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.elu()", "path": "nn.functional#torch.nn.functional.elu", "type": "torch.nn.functional", "text": "\nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) =\n\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .\n\nSee `ELU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.elu_()", "path": "nn.functional#torch.nn.functional.elu_", "type": "torch.nn.functional", "text": "\nIn-place version of `elu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.embedding()", "path": "nn.functional#torch.nn.functional.embedding", "type": "torch.nn.functional", "text": "\nA simple lookup table that looks up embeddings in a fixed dictionary and size.\n\nThis module is often used to retrieve word embeddings using indices. The input\nto the module is a list of indices, and the embedding matrix, and the output\nis the corresponding word embeddings.\n\nSee `torch.nn.Embedding` for more details.\n\nwhere V = maximum index + 1 and embedding_dim = the embedding size\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.embedding_bag()", "path": "nn.functional#torch.nn.functional.embedding_bag", "type": "torch.nn.functional", "text": "\nComputes sums, means or maxes of `bags` of embeddings, without instantiating\nthe intermediate embeddings.\n\nSee `torch.nn.EmbeddingBag` for more details.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nShape:\n\n`input` (LongTensor) and `offsets` (LongTensor, optional)\n\nIf `input` is 2D of shape `(B, N)`,\n\nit will be treated as `B` bags (sequences) each of fixed length `N`, and this\nwill return `B` values aggregated in a way depending on the `mode`. `offsets`\nis ignored and required to be `None` in this case.\n\nIf `input` is 1D of shape `(N)`,\n\nit will be treated as a concatenation of multiple bags (sequences). `offsets`\nis required to be a 1D tensor containing the starting index positions of each\nbag in `input`. Therefore, for `offsets` of shape `(B)`, `input` will be\nviewed as having `B` bags. Empty bags (i.e., having 0-length) will have\nreturned vectors filled by zeros.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.feature_alpha_dropout()", "path": "nn.functional#torch.nn.functional.feature_alpha_dropout", "type": "torch.nn.functional", "text": "\nRandomly masks out entire channels (a channel is a feature map, e.g. the jj\n-th channel of the ii -th sample in the batch input is a tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting\nactivations to zero, as in regular Dropout, the activations are set to the\nnegative saturation value of the SELU activation function.\n\nEach element will be masked independently on every forward call with\nprobability `p` using samples from a Bernoulli distribution. The elements to\nbe masked are randomized on every forward call, and scaled and shifted to\nmaintain zero mean and unit variance.\n\nSee `FeatureAlphaDropout` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.fold()", "path": "nn.functional#torch.nn.functional.fold", "type": "torch.nn.functional", "text": "\nCombines an array of sliding local blocks into a large containing tensor.\n\nWarning\n\nCurrently, only 3-D output tensors (unfolded batched image-like tensors) are\nsupported.\n\nSee `torch.nn.Fold` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.gelu()", "path": "nn.functional#torch.nn.functional.gelu", "type": "torch.nn.functional", "text": "\nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)\n\nwhere \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian\nDistribution.\n\nSee Gaussian Error Linear Units (GELUs).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.glu()", "path": "nn.functional#torch.nn.functional.glu", "type": "torch.nn.functional", "text": "\nThe gated linear unit. Computes:\n\nwhere `input` is split in half along `dim` to form `a` and `b`, \u03c3\\sigma is the\nsigmoid function and \u2297\\otimes is the element-wise product between matrices.\n\nSee Language Modeling with Gated Convolutional Networks.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.grid_sample()", "path": "nn.functional#torch.nn.functional.grid_sample", "type": "torch.nn.functional", "text": "\nGiven an `input` and a flow-field `grid`, computes the `output` using `input`\nvalues and pixel locations from `grid`.\n\nCurrently, only spatial (4-D) and volumetric (5-D) `input` are supported.\n\nIn the spatial (4-D) case, for `input` with shape (N,C,Hin,Win)(N, C,\nH_\\text{in}, W_\\text{in}) and `grid` with shape (N,Hout,Wout,2)(N,\nH_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N,\nC, H_\\text{out}, W_\\text{out}) .\n\nFor each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h,\nw]` specifies `input` pixel locations `x` and `y`, which are used to\ninterpolate the output value `output[n, :, h, w]`. In the case of 5D inputs,\n`grid[n, d, h, w]` specifies the `x`, `y`, `z` pixel locations for\ninterpolating `output[n, :, d, h, w]`. `mode` argument specifies `nearest` or\n`bilinear` interpolation method to sample the input pixels.\n\n`grid` specifies the sampling pixel locations normalized by the `input`\nspatial dimensions. Therefore, it should have most values in the range of\n`[-1, 1]`. For example, values `x = -1, y = -1` is the left-top pixel of\n`input`, and values `x = 1, y = 1` is the right-bottom pixel of `input`.\n\nIf `grid` has values outside the range of `[-1, 1]`, the corresponding outputs\nare handled as defined by `padding_mode`. Options are\n\nNote\n\nThis function is often used in conjunction with `affine_grid()` to build\nSpatial Transformer Networks .\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nNote\n\nNaN values in `grid` would be interpreted as `-1`.\n\noutput Tensor\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nNote\n\n`mode='bicubic'` is implemented using the cubic convolution algorithm with\n\u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha might be different from packages to\npackages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This\nalgorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example,\nit may produce negative values or values greater than 255 when interpolating\ninput in [0, 255]. Clamp the results with :func: `torch.clamp` to ensure they\nare within the valid range.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.gumbel_softmax()", "path": "nn.functional#torch.nn.functional.gumbel_softmax", "type": "torch.nn.functional", "text": "\nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally\ndiscretizes.\n\nSampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\nIf `hard=True`, the returned samples will be one-hot, otherwise they will be\nprobability distributions that sum to 1 across `dim`.\n\nNote\n\nThis function is here for legacy reasons, may be removed from nn.Functional in\nthe future.\n\nNote\n\nThe main trick for `hard` is to do `y_hard - y_soft.detach() + y_soft`\n\nIt achieves two things: - makes the output value exactly one-hot (since we add\nthen subtract y_soft value) - makes the gradient equal to y_soft gradient\n(since we strip all other gradients)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardshrink()", "path": "nn.functional#torch.nn.functional.hardshrink", "type": "torch.nn.functional", "text": "\nApplies the hard shrinkage function element-wise\n\nSee `Hardshrink` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardsigmoid()", "path": "nn.functional#torch.nn.functional.hardsigmoid", "type": "torch.nn.functional", "text": "\nApplies the element-wise function\n\ninplace \u2013 If set to `True`, will do this operation in-place. Default: `False`\n\nSee `Hardsigmoid` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardswish()", "path": "nn.functional#torch.nn.functional.hardswish", "type": "torch.nn.functional", "text": "\nApplies the hardswish function, element-wise, as described in the paper:\n\nSearching for MobileNetV3.\n\nSee `Hardswish` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardtanh()", "path": "nn.functional#torch.nn.functional.hardtanh", "type": "torch.nn.functional", "text": "\nApplies the HardTanh function element-wise. See `Hardtanh` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardtanh_()", "path": "nn.functional#torch.nn.functional.hardtanh_", "type": "torch.nn.functional", "text": "\nIn-place version of `hardtanh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hinge_embedding_loss()", "path": "nn.functional#torch.nn.functional.hinge_embedding_loss", "type": "torch.nn.functional", "text": "\nSee `HingeEmbeddingLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.instance_norm()", "path": "nn.functional#torch.nn.functional.instance_norm", "type": "torch.nn.functional", "text": "\nApplies Instance Normalization for each channel in each data sample in a\nbatch.\n\nSee `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.interpolate()", "path": "nn.functional#torch.nn.functional.interpolate", "type": "torch.nn.functional", "text": "\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\nThe algorithm used for interpolation is determined by `mode`.\n\nCurrently temporal, spatial and volumetric sampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for resizing are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\nWarning\n\nWhen scale_factor is specified, if recompute_scale_factor=True, scale_factor\nis used to compute the output_size which will then be used to infer new scales\nfor the interpolation. The default behavior for recompute_scale_factor changed\nto False in 1.6.0, and scale_factor is used in the interpolation calculation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.kl_div()", "path": "nn.functional#torch.nn.functional.kl_div", "type": "torch.nn.functional", "text": "\nThe Kullback-Leibler divergence Loss\n\nSee `KLDivLoss` for details.\n\nNote\n\n`size_average` and `reduce` are in the process of being deprecated, and in the\nmeantime, specifying either of those two args will override `reduction`.\n\nNote\n\n:attr:`reduction` = `'mean'` doesn\u2019t return the true kl divergence value,\nplease use :attr:`reduction` = `'batchmean'` which aligns with KL math\ndefinition. In the next major release, `'mean'` will be changed to be the same\nas \u2018batchmean\u2019.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.l1_loss()", "path": "nn.functional#torch.nn.functional.l1_loss", "type": "torch.nn.functional", "text": "\nFunction that takes the mean element-wise absolute value difference.\n\nSee `L1Loss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.layer_norm()", "path": "nn.functional#torch.nn.functional.layer_norm", "type": "torch.nn.functional", "text": "\nApplies Layer Normalization for last certain number of dimensions.\n\nSee `LayerNorm` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.leaky_relu()", "path": "nn.functional#torch.nn.functional.leaky_relu", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nLeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0,\nx) + \\text{negative\\\\_slope} * \\min(0, x)\n\nSee `LeakyReLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.leaky_relu_()", "path": "nn.functional#torch.nn.functional.leaky_relu_", "type": "torch.nn.functional", "text": "\nIn-place version of `leaky_relu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.linear()", "path": "nn.functional#torch.nn.functional.linear", "type": "torch.nn.functional", "text": "\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b .\n\nThis operator supports TensorFloat32.\n\nShape:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.local_response_norm()", "path": "nn.functional#torch.nn.functional.local_response_norm", "type": "torch.nn.functional", "text": "\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\nSee `LocalResponseNorm` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.logsigmoid()", "path": "nn.functional#torch.nn.functional.logsigmoid", "type": "torch.nn.functional", "text": "\nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) =\n\\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)\n\nSee `LogSigmoid` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.log_softmax()", "path": "nn.functional#torch.nn.functional.log_softmax", "type": "torch.nn.functional", "text": "\nApplies a softmax followed by a logarithm.\n\nWhile mathematically equivalent to log(softmax(x)), doing these two operations\nseparately is slower, and numerically unstable. This function uses an\nalternative formulation to compute the output and gradient correctly.\n\nSee `LogSoftmax` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.lp_pool1d()", "path": "nn.functional#torch.nn.functional.lp_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool1d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.lp_pool2d()", "path": "nn.functional#torch.nn.functional.lp_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.margin_ranking_loss()", "path": "nn.functional#torch.nn.functional.margin_ranking_loss", "type": "torch.nn.functional", "text": "\nSee `MarginRankingLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool1d()", "path": "nn.functional#torch.nn.functional.max_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool1d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool2d()", "path": "nn.functional#torch.nn.functional.max_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool3d()", "path": "nn.functional#torch.nn.functional.max_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool1d()", "path": "nn.functional#torch.nn.functional.max_unpool1d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool1d`.\n\nSee `MaxUnpool1d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool2d()", "path": "nn.functional#torch.nn.functional.max_unpool2d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool2d`.\n\nSee `MaxUnpool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool3d()", "path": "nn.functional#torch.nn.functional.max_unpool3d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool3d`.\n\nSee `MaxUnpool3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.mse_loss()", "path": "nn.functional#torch.nn.functional.mse_loss", "type": "torch.nn.functional", "text": "\nMeasures the element-wise mean squared error.\n\nSee `MSELoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.multilabel_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_margin_loss", "type": "torch.nn.functional", "text": "\nSee `MultiLabelMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.multilabel_soft_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_soft_margin_loss", "type": "torch.nn.functional", "text": "\nSee `MultiLabelSoftMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.multi_margin_loss()", "path": "nn.functional#torch.nn.functional.multi_margin_loss", "type": "torch.nn.functional", "text": "\nreduce=None, reduction=\u2019mean\u2019) -> Tensor\n\nSee `MultiMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.nll_loss()", "path": "nn.functional#torch.nn.functional.nll_loss", "type": "torch.nn.functional", "text": "\nThe negative log likelihood loss.\n\nSee `NLLLoss` for details.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.normalize()", "path": "nn.functional#torch.nn.functional.normalize", "type": "torch.nn.functional", "text": "\nPerforms LpL_p normalization of inputs over specified dimension.\n\nFor a tensor `input` of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ...,\nn_k) , each ndimn_{dim} -element vector vv along dimension `dim` is\ntransformed as\n\nWith the default arguments it uses the Euclidean norm over vectors along\ndimension 11 for normalization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.one_hot()", "path": "nn.functional#torch.nn.functional.one_hot", "type": "torch.nn.functional", "text": "\nTakes LongTensor with index values of shape `(*)` and returns a tensor of\nshape `(*, num_classes)` that have zeros everywhere except where the index of\nlast dimension matches the corresponding value of the input tensor, in which\ncase it will be 1.\n\nSee also One-hot on Wikipedia .\n\nLongTensor that has one more dimension with 1 values at the index of last\ndimension indicated by the input, and 0 everywhere else.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pad()", "path": "nn.functional#torch.nn.functional.pad", "type": "torch.nn.functional", "text": "\nPads tensor.\n\nThe padding size by which to pad some dimensions of `input` are described\nstarting from the last dimension and moving forward.\n\u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor dimensions of\n`input` will be padded. For example, to pad only the last dimension of the\ninput tensor, then `pad` has the form\n(padding_left,padding_right)(\\text{padding\\\\_left}, \\text{padding\\\\_right}) ;\nto pad the last 2 dimensions of the input tensor, then use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom)\\text{padding\\\\_top}, \\text{padding\\\\_bottom}) ; to\npad the last 3 dimensions, use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom\\text{padding\\\\_top}, \\text{padding\\\\_bottom}\npadding_front,padding_back)\\text{padding\\\\_front}, \\text{padding\\\\_back}) .\n\nSee `torch.nn.ConstantPad2d`, `torch.nn.ReflectionPad2d`, and\n`torch.nn.ReplicationPad2d` for concrete examples on how each of the padding\nmodes works. Constant padding is implemented for arbitrary dimensions.\nReplicate padding is implemented for padding the last 3 dimensions of 5D input\ntensor, or the last 2 dimensions of 4D input tensor, or the last dimension of\n3D input tensor. Reflect padding is only implemented for padding the last 2\ndimensions of 4D input tensor, or the last dimension of 3D input tensor.\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pairwise_distance()", "path": "nn.functional#torch.nn.functional.pairwise_distance", "type": "torch.nn.functional", "text": "\nSee `torch.nn.PairwiseDistance` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pdist()", "path": "nn.functional#torch.nn.functional.pdist", "type": "torch.nn.functional", "text": "\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape N\u00d7MN \\times M then the output will have shape\n12N(N\u22121)\\frac{1}{2} N (N - 1) .\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n\u2018minkowski\u2019, p=p)` if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0 it is\nequivalent to `scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M`. When p=\u221ep\n= \\infty , the closest scipy function is `scipy.spatial.distance.pdist(xn,\nlambda x, y: np.abs(x - y).max())`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pixel_shuffle()", "path": "nn.functional#torch.nn.functional.pixel_shuffle", "type": "torch.nn.functional", "text": "\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nthe `upscale_factor`.\n\nSee `PixelShuffle` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pixel_unshuffle()", "path": "nn.functional#torch.nn.functional.pixel_unshuffle", "type": "torch.nn.functional", "text": "\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the `downscale_factor`.\n\nSee `PixelUnshuffle` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.poisson_nll_loss()", "path": "nn.functional#torch.nn.functional.poisson_nll_loss", "type": "torch.nn.functional", "text": "\nPoisson negative log likelihood loss.\n\nSee `PoissonNLLLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.prelu()", "path": "nn.functional#torch.nn.functional.prelu", "type": "torch.nn.functional", "text": "\nApplies element-wise the function\nPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight}\n* \\min(0,x) where weight is a learnable parameter.\n\nSee `PReLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.relu()", "path": "nn.functional#torch.nn.functional.relu", "type": "torch.nn.functional", "text": "\nApplies the rectified linear unit function element-wise. See `ReLU` for more\ndetails.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.relu6()", "path": "nn.functional#torch.nn.functional.relu6", "type": "torch.nn.functional", "text": "\nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) =\n\\min(\\max(0,x), 6) .\n\nSee `ReLU6` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.relu_()", "path": "nn.functional#torch.nn.functional.relu_", "type": "torch.nn.functional", "text": "\nIn-place version of `relu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.rrelu()", "path": "nn.functional#torch.nn.functional.rrelu", "type": "torch.nn.functional", "text": "\nRandomized leaky ReLU.\n\nSee `RReLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.rrelu_()", "path": "nn.functional#torch.nn.functional.rrelu_", "type": "torch.nn.functional", "text": "\nIn-place version of `rrelu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.selu()", "path": "nn.functional#torch.nn.functional.selu", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nSELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale *\n(\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with\n\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\nand\nscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946\n.\n\nSee `SELU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.sigmoid()", "path": "nn.functional#torch.nn.functional.sigmoid", "type": "torch.nn.functional", "text": "\nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) =\n\\frac{1}{1 + \\exp(-x)}\n\nSee `Sigmoid` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.silu()", "path": "nn.functional#torch.nn.functional.silu", "type": "torch.nn.functional", "text": "\nApplies the silu function, element-wise.\n\nNote\n\nSee Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit)\nwas originally coined, and see Sigmoid-Weighted Linear Units for Neural\nNetwork Function Approximation in Reinforcement Learning and Swish: a Self-\nGated Activation Function where the SiLU was experimented with later.\n\nSee `SiLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.smooth_l1_loss()", "path": "nn.functional#torch.nn.functional.smooth_l1_loss", "type": "torch.nn.functional", "text": "\nFunction that uses a squared term if the absolute element-wise error falls\nbelow beta and an L1 term otherwise.\n\nSee `SmoothL1Loss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softmax()", "path": "nn.functional#torch.nn.functional.softmax", "type": "torch.nn.functional", "text": "\nApplies a softmax function.\n\nSoftmax is defined as:\n\nSoftmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j\n\\exp(x_j)}\n\nIt is applied to all slices along dim, and will re-scale them so that the\nelements lie in the range `[0, 1]` and sum to 1.\n\nSee `Softmax` for more details.\n\nNote\n\nThis function doesn\u2019t work directly with NLLLoss, which expects the Log to be\ncomputed between the Softmax and itself. Use log_softmax instead (it\u2019s faster\nand has better numerical properties).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softmin()", "path": "nn.functional#torch.nn.functional.softmin", "type": "torch.nn.functional", "text": "\nApplies a softmin function.\n\nNote that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See\nsoftmax definition for mathematical formula.\n\nSee `Softmin` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softplus()", "path": "nn.functional#torch.nn.functional.softplus", "type": "torch.nn.functional", "text": "\nApplies element-wise, the function\nSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1\n+ \\exp(\\beta * x)) .\n\nFor numerical stability the implementation reverts to the linear function when\ninput\u00d7\u03b2>thresholdinput \\times \\beta > threshold .\n\nSee `Softplus` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softshrink()", "path": "nn.functional#torch.nn.functional.softshrink", "type": "torch.nn.functional", "text": "\nApplies the soft shrinkage function elementwise\n\nSee `Softshrink` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softsign()", "path": "nn.functional#torch.nn.functional.softsign", "type": "torch.nn.functional", "text": "\nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) =\n\\frac{x}{1 + |x|}\n\nSee `Softsign` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.soft_margin_loss()", "path": "nn.functional#torch.nn.functional.soft_margin_loss", "type": "torch.nn.functional", "text": "\nSee `SoftMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.tanh()", "path": "nn.functional#torch.nn.functional.tanh", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nTanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) =\n\\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n\nSee `Tanh` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.tanhshrink()", "path": "nn.functional#torch.nn.functional.tanhshrink", "type": "torch.nn.functional", "text": "\nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x -\n\\text{Tanh}(x)\n\nSee `Tanhshrink` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.threshold()", "path": "nn.functional#torch.nn.functional.threshold", "type": "torch.nn.functional", "text": "\nThresholds each element of the input Tensor.\n\nSee `Threshold` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.threshold_()", "path": "nn.functional#torch.nn.functional.threshold_", "type": "torch.nn.functional", "text": "\nIn-place version of `threshold()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.triplet_margin_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_loss", "type": "torch.nn.functional", "text": "\nSee `TripletMarginLoss` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_with_distance_loss", "type": "torch.nn.functional", "text": "\nSee `TripletMarginWithDistanceLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.unfold()", "path": "nn.functional#torch.nn.functional.unfold", "type": "torch.nn.functional", "text": "\nExtracts sliding local blocks from a batched input tensor.\n\nWarning\n\nCurrently, only 4-D input tensors (batched image-like tensors) are supported.\n\nWarning\n\nMore than one element of the unfolded tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensor, please clone it first.\n\nSee `torch.nn.Unfold` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample()", "path": "nn.functional#torch.nn.functional.upsample", "type": "torch.nn.functional", "text": "\nUpsamples the input to either the given `size` or the given `scale_factor`\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(...)`.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nThe algorithm used for upsampling is determined by `mode`.\n\nCurrently temporal, spatial and volumetric upsampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for upsampling are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample_bilinear()", "path": "nn.functional#torch.nn.functional.upsample_bilinear", "type": "torch.nn.functional", "text": "\nUpsamples the input, using bilinear upsampling.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='bilinear',\nalign_corners=True)`.\n\nExpected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\nvolumetric (5 dimensional) inputs.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample_nearest()", "path": "nn.functional#torch.nn.functional.upsample_nearest", "type": "torch.nn.functional", "text": "\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='nearest')`.\n\nCurrently spatial and volumetric upsampling are supported (i.e. expected\ninputs are 4 or 5 dimensional).\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GaussianNLLLoss", "path": "generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss", "type": "torch.nn", "text": "\nGaussian negative log likelihood loss.\n\nThe targets are treated as samples from Gaussian distributions with\nexpectations and variances predicted by the neural network. For a\nD-dimensional `target` tensor modelled as having heteroscedastic Gaussian\ndistributions with a D-dimensional tensor of expectations `input` and a\nD-dimensional tensor of positive variances `var` the loss is:\n\nwhere `eps` is used for stability. By default, the constant term of the loss\nfunction is omitted unless `full` is `True`. If `var` is a scalar (implying\n`target` tensor has homoscedastic Gaussian distributions) it is broadcasted to\nbe the same size as the input.\n\nExamples:\n\nNote\n\nThe clamping of `var` is ignored with respect to autograd, and so the\ngradients are unaffected by it.\n\nNix, D. A. and Weigend, A. S., \u201cEstimating the mean and variance of the target\nprobability distribution\u201d, Proceedings of 1994 IEEE International Conference\non Neural Networks (ICNN\u201994), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi:\n10.1109/ICNN.1994.374138.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GELU", "path": "generated/torch.nn.gelu#torch.nn.GELU", "type": "torch.nn", "text": "\nApplies the Gaussian Error Linear Units function:\n\nwhere \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian\nDistribution.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GroupNorm", "path": "generated/torch.nn.groupnorm#torch.nn.GroupNorm", "type": "torch.nn", "text": "\nApplies Group Normalization over a mini-batch of inputs as described in the\npaper Group Normalization\n\nThe input channels are separated into `num_groups` groups, each containing\n`num_channels / num_groups` channels. The mean and standard-deviation are\ncalculated separately over the each group. \u03b3\\gamma and \u03b2\\beta are learnable\nper-channel affine transform parameter vectors of size `num_channels` if\n`affine` is `True`. The standard-deviation is calculated via the biased\nestimator, equivalent to `torch.var(input, unbiased=False)`.\n\nThis layer uses statistics computed from input data in both training and\nevaluation modes.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GRU", "path": "generated/torch.nn.gru#torch.nn.GRU", "type": "torch.nn", "text": "\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\nFor each element in the input sequence, each layer computes the following\nfunction:\n\nwhere hth_t is the hidden state at time `t`, xtx_t is the input at time `t`,\nh(t\u22121)h_{(t-1)} is the hidden state of the layer at time `t-1` or the initial\nhidden state at time `0`, and rtr_t , ztz_t , ntn_t are the reset, update, and\nnew gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2217* is the\nHadamard product.\n\nIn a multilayer GRU, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2\n) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by\ndropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a\nBernoulli random variable which is 00 with probability `dropout`.\n\noutput of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\ncontaining the output features h_t from the last layer of the GRU, for each\n`t`. If a `torch.nn.utils.rnn.PackedSequence` has been given as the input, the\noutput will also be a packed sequence. For the unpacked case, the directions\ncan be separated using `output.view(seq_len, batch, num_directions,\nhidden_size)`, with forward and backward being direction `0` and `1`\nrespectively.\n\nSimilarly, the directions can be separated in the packed case.\n\nh_n of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\ncontaining the hidden state for `t = seq_len`\n\nLike output, the layers can be separated using `h_n.view(num_layers,\nnum_directions, batch, hidden_size)`.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nNote\n\nIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data\nis on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5)\ninput data is not in `PackedSequence` format persistent algorithm can be\nselected to improve performance.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GRUCell", "path": "generated/torch.nn.grucell#torch.nn.GRUCell", "type": "torch.nn", "text": "\nA gated recurrent unit (GRU) cell\n\nwhere \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardshrink", "path": "generated/torch.nn.hardshrink#torch.nn.Hardshrink", "type": "torch.nn", "text": "\nApplies the hard shrinkage function element-wise:\n\nlambd \u2013 the \u03bb\\lambda value for the Hardshrink formulation. Default: 0.5\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardsigmoid", "path": "generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardswish", "path": "generated/torch.nn.hardswish#torch.nn.Hardswish", "type": "torch.nn", "text": "\nApplies the hardswish function, element-wise, as described in the paper:\n\nSearching for MobileNetV3.\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardtanh", "path": "generated/torch.nn.hardtanh#torch.nn.Hardtanh", "type": "torch.nn", "text": "\nApplies the HardTanh function element-wise\n\nHardTanh is defined as:\n\nThe range of the linear region [\u22121,1][-1, 1] can be adjusted using `min_val`\nand `max_val`.\n\nKeyword arguments `min_value` and `max_value` have been deprecated in favor of\n`min_val` and `max_val`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.HingeEmbeddingLoss", "path": "generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss", "type": "torch.nn", "text": "\nMeasures the loss given an input tensor xx and a labels tensor yy (containing\n1 or -1). This is usually used for measuring whether two inputs are similar or\ndissimilar, e.g. using the L1 pairwise distance as xx , and is typically used\nfor learning nonlinear embeddings or semi-supervised learning.\n\nThe loss function for nn -th sample in the mini-batch is\n\nand the total loss functions is\n\nwhere L={l1,\u2026,lN}\u22a4L = \\\\{l_1,\\dots,l_N\\\\}^\\top .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.identity#torch.nn.Identity", "type": "torch.nn", "text": "\nA placeholder identity operator that is argument-insensitive.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init", "path": "nn.init", "type": "torch.nn.init", "text": "\nReturn the recommended gain value for the given nonlinearity function. The\nvalues are as follows:\n\nnonlinearity\n\ngain\n\nLinear / Identity\n\n11\n\nConv{1,2,3}D\n\n11\n\nSigmoid\n\n11\n\nTanh\n\n53\\frac{5}{3}\n\nReLU\n\n2\\sqrt{2}\n\nLeaky Relu\n\n21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\\\_slope}^2}}\n\nSELU\n\n34\\frac{3}{4}\n\nFills the input Tensor with values drawn from the uniform distribution\nU(a,b)\\mathcal{U}(a, b) .\n\nFills the input Tensor with values drawn from the normal distribution\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .\n\nFills the input Tensor with the value val\\text{val} .\n\nFills the input Tensor with the scalar value `1`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\nFills the input Tensor with the scalar value `0`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\nFills the 2-dimensional input `Tensor` with the identity matrix. Preserves the\nidentity of the inputs in `Linear` layers, where as many inputs are preserved\nas possible.\n\ntensor \u2013 a 2-dimensional `torch.Tensor`\n\nFills the {3, 4, 5}-dimensional input `Tensor` with the Dirac delta function.\nPreserves the identity of the inputs in `Convolutional` layers, where as many\ninput channels are preserved as possible. In case of groups>1, each group of\nchannels preserves identity\n\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting\ntensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where\n\nAlso known as Glorot initialization.\n\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting\ntensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)\nwhere\n\nAlso known as Glorot initialization.\n\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a uniform distribution. The\nresulting tensor will have values sampled from\nU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where\n\nAlso known as He initialization.\n\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a normal distribution. The\nresulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0,\n\\text{std}^2) where\n\nAlso known as He initialization.\n\nFills the input `Tensor` with a (semi) orthogonal matrix, as described in\n`Exact solutions to the nonlinear dynamics of learning in deep linear neural\nnetworks` \\- Saxe, A. et al. (2013). The input tensor must have at least 2\ndimensions, and for tensors with more than 2 dimensions the trailing\ndimensions are flattened.\n\nFills the 2D input `Tensor` as a sparse matrix, where the non-zero elements\nwill be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as\ndescribed in `Deep learning via Hessian-free optimization` \\- Martens, J.\n(2010).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.calculate_gain()", "path": "nn.init#torch.nn.init.calculate_gain", "type": "torch.nn.init", "text": "\nReturn the recommended gain value for the given nonlinearity function. The\nvalues are as follows:\n\nnonlinearity\n\ngain\n\nLinear / Identity\n\n11\n\nConv{1,2,3}D\n\n11\n\nSigmoid\n\n11\n\nTanh\n\n53\\frac{5}{3}\n\nReLU\n\n2\\sqrt{2}\n\nLeaky Relu\n\n21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\\\_slope}^2}}\n\nSELU\n\n34\\frac{3}{4}\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.constant_()", "path": "nn.init#torch.nn.init.constant_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the value val\\text{val} .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.dirac_()", "path": "nn.init#torch.nn.init.dirac_", "type": "torch.nn.init", "text": "\nFills the {3, 4, 5}-dimensional input `Tensor` with the Dirac delta function.\nPreserves the identity of the inputs in `Convolutional` layers, where as many\ninput channels are preserved as possible. In case of groups>1, each group of\nchannels preserves identity\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.eye_()", "path": "nn.init#torch.nn.init.eye_", "type": "torch.nn.init", "text": "\nFills the 2-dimensional input `Tensor` with the identity matrix. Preserves the\nidentity of the inputs in `Linear` layers, where as many inputs are preserved\nas possible.\n\ntensor \u2013 a 2-dimensional `torch.Tensor`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.kaiming_normal_()", "path": "nn.init#torch.nn.init.kaiming_normal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a normal distribution. The\nresulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0,\n\\text{std}^2) where\n\nAlso known as He initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.kaiming_uniform_()", "path": "nn.init#torch.nn.init.kaiming_uniform_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a uniform distribution. The\nresulting tensor will have values sampled from\nU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where\n\nAlso known as He initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.normal_()", "path": "nn.init#torch.nn.init.normal_", "type": "torch.nn.init", "text": "\nFills the input Tensor with values drawn from the normal distribution\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.ones_()", "path": "nn.init#torch.nn.init.ones_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the scalar value `1`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.orthogonal_()", "path": "nn.init#torch.nn.init.orthogonal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with a (semi) orthogonal matrix, as described in\n`Exact solutions to the nonlinear dynamics of learning in deep linear neural\nnetworks` \\- Saxe, A. et al. (2013). The input tensor must have at least 2\ndimensions, and for tensors with more than 2 dimensions the trailing\ndimensions are flattened.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.sparse_()", "path": "nn.init#torch.nn.init.sparse_", "type": "torch.nn.init", "text": "\nFills the 2D input `Tensor` as a sparse matrix, where the non-zero elements\nwill be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as\ndescribed in `Deep learning via Hessian-free optimization` \\- Martens, J.\n(2010).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.uniform_()", "path": "nn.init#torch.nn.init.uniform_", "type": "torch.nn.init", "text": "\nFills the input Tensor with values drawn from the uniform distribution\nU(a,b)\\mathcal{U}(a, b) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.xavier_normal_()", "path": "nn.init#torch.nn.init.xavier_normal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting\ntensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)\nwhere\n\nAlso known as Glorot initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.xavier_uniform_()", "path": "nn.init#torch.nn.init.xavier_uniform_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting\ntensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where\n\nAlso known as Glorot initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.zeros_()", "path": "nn.init#torch.nn.init.zeros_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the scalar value `0`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm1d", "path": "generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 3D input (a mini-batch of 1D inputs with\noptional additional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\nThe mean and standard-deviation are calculated per-dimension separately for\neach object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size `C` (where `C` is the input size) if `affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\n\nIf `track_running_stats` is set to `True`, during training this layer keeps\nrunning estimates of its computed mean and variance, which are then used for\nnormalization during evaluation. The running estimates are kept with a default\n`momentum` of 0.1.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nNote\n\n`InstanceNorm1d` and `LayerNorm` are very similar, but have some subtle\ndifferences. `InstanceNorm1d` is applied on each channel of channeled data\nlike multidimensional time series, but `LayerNorm` is usually applied on\nentire sample and often in NLP tasks. Additionally, `LayerNorm` applies\nelementwise affine transform, while `InstanceNorm1d` usually don\u2019t apply\naffine transform.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm2d", "path": "generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\nThe mean and standard-deviation are calculated per-dimension separately for\neach object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size `C` (where `C` is the input size) if `affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\n\nIf `track_running_stats` is set to `True`, during training this layer keeps\nrunning estimates of its computed mean and variance, which are then used for\nnormalization during evaluation. The running estimates are kept with a default\n`momentum` of 0.1.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nNote\n\n`InstanceNorm2d` and `LayerNorm` are very similar, but have some subtle\ndifferences. `InstanceNorm2d` is applied on each channel of channeled data\nlike RGB images, but `LayerNorm` is usually applied on entire sample and often\nin NLP tasks. Additionally, `LayerNorm` applies elementwise affine transform,\nwhile `InstanceNorm2d` usually don\u2019t apply affine transform.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm3d", "path": "generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\nThe mean and standard-deviation are calculated per-dimension separately for\neach object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size C (where C is the input size) if `affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\n\nIf `track_running_stats` is set to `True`, during training this layer keeps\nrunning estimates of its computed mean and variance, which are then used for\nnormalization during evaluation. The running estimates are kept with a default\n`momentum` of 0.1.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nNote\n\n`InstanceNorm3d` and `LayerNorm` are very similar, but have some subtle\ndifferences. `InstanceNorm3d` is applied on each channel of channeled data\nlike 3D models with RGB color, but `LayerNorm` is usually applied on entire\nsample and often in NLP tasks. Additionally, `LayerNorm` applies elementwise\naffine transform, while `InstanceNorm3d` usually don\u2019t apply affine transform.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBn1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 1d and Batch Norm 1d\nmodules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBn2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 2d and Batch Norm 2d\nmodules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBnReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and\nReLU modules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBnReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and\nReLU modules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv1d and ReLU modules. During\nquantization this will be replaced with the corresponding fused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv2d and ReLU modules. During\nquantization this will be replaced with the corresponding fused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvBn2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": "\nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with\nFakeQuantize modules for weight, used in quantization aware training.\n\nWe combined the interface of `torch.nn.Conv2d` and `torch.nn.BatchNorm2d`.\n\nSimilar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to\ndefault.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvBnReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": "\nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU,\nattached with FakeQuantize modules for weight, used in quantization aware\ntraining.\n\nWe combined the interface of `torch.nn.Conv2d` and `torch.nn.BatchNorm2d` and\n`torch.nn.ReLU`.\n\nSimilar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to\ndefault.\n\n~ConvBnReLU2d.weight_fake_quant \u2013 fake quant module for weight\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": "\nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\nFakeQuantize modules for weight for quantization aware training.\n\nWe combined the interface of `Conv2d` and `BatchNorm2d`.\n\n~ConvReLU2d.weight_fake_quant \u2013 fake quant module for weight\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.LinearReLU", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": "\nA LinearReLU module fused from Linear and ReLU modules, attached with\nFakeQuantize modules for weight, used in quantization aware training.\n\nWe adopt the same interface as `torch.nn.Linear`.\n\nSimilar to `torch.nn.intrinsic.LinearReLU`, with FakeQuantize modules\ninitialized to default.\n\n~LinearReLU.weight \u2013 fake quant module for weight\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU2d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": "\nA ConvReLU2d module is a fused module of Conv2d and ReLU\n\nWe adopt the same interface as `torch.nn.quantized.Conv2d`.\n\nas torch.nn.quantized.Conv2d (Same) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU3d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": "\nA ConvReLU3d module is a fused module of Conv3d and ReLU\n\nWe adopt the same interface as `torch.nn.quantized.Conv3d`.\n\nAttributes: Same as torch.nn.quantized.Conv3d\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.LinearReLU", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": "\nA LinearReLU module fused from Linear and ReLU modules\n\nWe adopt the same interface as `torch.nn.quantized.Linear`.\n\nas torch.nn.quantized.Linear (Same) \u2013\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.KLDivLoss", "path": "generated/torch.nn.kldivloss#torch.nn.KLDivLoss", "type": "torch.nn", "text": "\nThe Kullback-Leibler divergence loss measure\n\nKullback-Leibler divergence is a useful distance measure for continuous\ndistributions and is often useful when performing direct regression over the\nspace of (discretely sampled) continuous output distributions.\n\nAs with `NLLLoss`, the `input` given is expected to contain log-probabilities\nand is not restricted to a 2D Tensor. The targets are interpreted as\nprobabilities by default, but could be considered as log-probabilities with\n`log_target` set to `True`.\n\nThis criterion expects a `target` `Tensor` of the same size as the `input`\n`Tensor`.\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere the index NN spans all dimensions of `input` and LL has the same shape\nas `input`. If `reduction` is not `'none'` (default `'mean'`), then:\n\nIn default `reduction` mode `'mean'`, the losses are averaged for each\nminibatch over observations as well as over dimensions. `'batchmean'` mode\ngives the correct KL divergence where losses are averaged over batch dimension\nonly. `'mean'` mode\u2019s behavior will be changed to the same as `'batchmean'` in\nthe next major release.\n\nNote\n\n`size_average` and `reduce` are in the process of being deprecated, and in the\nmeantime, specifying either of those two args will override `reduction`.\n\nNote\n\n`reduction` = `'mean'` doesn\u2019t return the true kl divergence value, please use\n`reduction` = `'batchmean'` which aligns with KL math definition. In the next\nmajor release, `'mean'` will be changed to be the same as `'batchmean'`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.L1Loss", "path": "generated/torch.nn.l1loss#torch.nn.L1Loss", "type": "torch.nn", "text": "\nCreates a criterion that measures the mean absolute error (MAE) between each\nelement in the input xx and target yy .\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen:\n\nxx and yy are tensors of arbitrary shapes with a total of nn elements each.\n\nThe sum operation still operates over all the elements, and divides by nn .\n\nThe division by nn can be avoided if one sets `reduction = 'sum'`.\n\nSupports real-valued and complex-valued inputs.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LayerNorm", "path": "generated/torch.nn.layernorm#torch.nn.LayerNorm", "type": "torch.nn", "text": "\nApplies Layer Normalization over a mini-batch of inputs as described in the\npaper Layer Normalization\n\nThe mean and standard-deviation are calculated separately over the last\ncertain number dimensions which have to be of the shape specified by\n`normalized_shape`. \u03b3\\gamma and \u03b2\\beta are learnable affine transform\nparameters of `normalized_shape` if `elementwise_affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nNote\n\nUnlike Batch Normalization and Instance Normalization, which applies scalar\nscale and bias for each entire channel/plane with the `affine` option, Layer\nNormalization applies per-element scale and bias with `elementwise_affine`.\n\nThis layer uses statistics computed from input data in both training and\nevaluation modes.\n\nnormalized_shape (int or list or torch.Size) \u2013\n\ninput shape from an expected input of size\n\nIf a single integer is used, it is treated as a singleton list, and this\nmodule will normalize over the last dimension which is expected to be of that\nspecific size.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv1d", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d", "type": "torch.nn", "text": "\nA `torch.nn.Conv1d` module with lazy initialization of the `in_channels`\nargument of the `Conv1d` that is inferred from the `input.size(1)`.\n\nSee also\n\n`torch.nn.Conv1d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `Conv1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv1d.cls_to_become", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv2d", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d", "type": "torch.nn", "text": "\nA `torch.nn.Conv2d` module with lazy initialization of the `in_channels`\nargument of the `Conv2d` that is inferred from the `input.size(1)`.\n\nSee also\n\n`torch.nn.Conv2d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `Conv2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv2d.cls_to_become", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv3d", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d", "type": "torch.nn", "text": "\nA `torch.nn.Conv3d` module with lazy initialization of the `in_channels`\nargument of the `Conv3d` that is inferred from the `input.size(1)`.\n\nSee also\n\n`torch.nn.Conv3d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `Conv3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv3d.cls_to_become", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose1d", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose1d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose1d` that is inferred from the\n`input.size(1)`.\n\nSee also\n\n`torch.nn.ConvTranspose1d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `ConvTranspose1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose1d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose2d", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose2d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose2d` that is inferred from the\n`input.size(1)`.\n\nSee also\n\n`torch.nn.ConvTranspose2d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `ConvTranspose2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose2d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose3d", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose3d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose3d` that is inferred from the\n`input.size(1)`.\n\nSee also\n\n`torch.nn.ConvTranspose3d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `ConvTranspose3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose3d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyLinear", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear", "type": "torch.nn", "text": "\nA `torch.nn.Linear` module with lazy initialization.\n\nIn this module, the `weight` and `bias` are of\n`torch.nn.UninitializedParameter` class. They will be initialized after the\nfirst call to `forward` is done and the module will become a regular\n`torch.nn.Linear` module.\n\nCheck the `torch.nn.modules.lazy.LazyModuleMixin` for further documentation on\nlazy modules and their limitations.\n\nalias of `Linear`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyLinear.cls_to_become", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear.cls_to_become", "type": "torch.nn", "text": "\nalias of `Linear`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LeakyReLU", "path": "generated/torch.nn.leakyrelu#torch.nn.LeakyReLU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nor\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Linear", "path": "generated/torch.nn.linear#torch.nn.Linear", "type": "torch.nn", "text": "\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b\n\nThis module supports TensorFloat32.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LocalResponseNorm", "path": "generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm", "type": "torch.nn", "text": "\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LogSigmoid", "path": "generated/torch.nn.logsigmoid#torch.nn.LogSigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LogSoftmax", "path": "generated/torch.nn.logsoftmax#torch.nn.LogSoftmax", "type": "torch.nn", "text": "\nApplies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an\nn-dimensional input Tensor. The LogSoftmax formulation can be simplified as:\n\ndim (int) \u2013 A dimension along which LogSoftmax will be computed.\n\na Tensor of the same dimension and shape as the input with values in the range\n[-inf, 0)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LPPool1d", "path": "generated/torch.nn.lppool1d#torch.nn.LPPool1d", "type": "torch.nn", "text": "\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes.\n\nOn each window, the function computed is:\n\nNote\n\nIf the sum to the power of `p` is zero, the gradient of this function is not\ndefined. This implementation will set the gradient to zero in this case.\n\nOutput: (N,C,Lout)(N, C, L_{out}) , where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LPPool2d", "path": "generated/torch.nn.lppool2d#torch.nn.LPPool2d", "type": "torch.nn", "text": "\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes.\n\nOn each window, the function computed is:\n\nThe parameters `kernel_size`, `stride` can either be:\n\nNote\n\nIf the sum to the power of `p` is zero, the gradient of this function is not\ndefined. This implementation will set the gradient to zero in this case.\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LSTM", "path": "generated/torch.nn.lstm#torch.nn.LSTM", "type": "torch.nn", "text": "\nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\nFor each element in the input sequence, each layer computes the following\nfunction:\n\nwhere hth_t is the hidden state at time `t`, ctc_t is the cell state at time\n`t`, xtx_t is the input at time `t`, ht\u22121h_{t-1} is the hidden state of the\nlayer at time `t-1` or the initial hidden state at time `0`, and iti_t , ftf_t\n, gtg_t , oto_t are the input, forget, cell, and output gates, respectively.\n\u03c3\\sigma is the sigmoid function, and \u2299\\odot is the Hadamard product.\n\nIn a multilayer LSTM, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2\n) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by\ndropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a\nBernoulli random variable which is 00 with probability `dropout`.\n\nIf `proj_size > 0` is specified, LSTM with projections will be used. This\nchanges the LSTM cell in the following way. First, the dimension of hth_t will\nbe changed from `hidden_size` to `proj_size` (dimensions of WhiW_{hi} will be\nchanged accordingly). Second, the output hidden state of each layer will be\nmultiplied by a learnable projection matrix: ht=Whrhth_t = W_{hr}h_t . Note\nthat as a consequence of this, the output of LSTM network will be of different\nshape as well. See Inputs/Outputs sections below for exact dimensions of all\nvariables. You can find more details in https://arxiv.org/abs/1402.1128.\n\nc_0 of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\ncontaining the initial cell state for each element in the batch.\n\nIf `(h_0, c_0)` is not provided, both h_0 and c_0 default to zero.\n\noutput of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\ncontaining the output features `(h_t)` from the last layer of the LSTM, for\neach `t`. If a `torch.nn.utils.rnn.PackedSequence` has been given as the\ninput, the output will also be a packed sequence. If `proj_size > 0` was\nspecified, output shape will be `(seq_len, batch, num_directions *\nproj_size)`.\n\nFor the unpacked case, the directions can be separated using\n`output.view(seq_len, batch, num_directions, hidden_size)`, with forward and\nbackward being direction `0` and `1` respectively. Similarly, the directions\ncan be separated in the packed case.\n\nh_n of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\ncontaining the hidden state for `t = seq_len`. If `proj_size > 0` was\nspecified, `h_n` shape will be `(num_layers * num_directions, batch,\nproj_size)`.\n\nLike output, the layers can be separated using `h_n.view(num_layers,\nnum_directions, batch, hidden_size)` and similarly for c_n.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nWarning\n\nThere are known non-determinism issues for RNN functions on some versions of\ncuDNN and CUDA. You can enforce deterministic behavior by setting the\nfollowing environment variables:\n\nOn CUDA 10.1, set environment variable `CUDA_LAUNCH_BLOCKING=1`. This may\naffect performance.\n\nOn CUDA 10.2 or later, set environment variable (note the leading colon\nsymbol) `CUBLAS_WORKSPACE_CONFIG=:16:8` or `CUBLAS_WORKSPACE_CONFIG=:4096:2`.\n\nSee the cuDNN 8 Release Notes for more information.\n\nNote\n\nIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data\nis on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5)\ninput data is not in `PackedSequence` format persistent algorithm can be\nselected to improve performance.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LSTMCell", "path": "generated/torch.nn.lstmcell#torch.nn.LSTMCell", "type": "torch.nn", "text": "\nA long short-term memory (LSTM) cell.\n\nwhere \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.\n\nc_0 of shape `(batch, hidden_size)`: tensor containing the initial cell state\nfor each element in the batch.\n\nIf `(h_0, c_0)` is not provided, both h_0 and c_0 default to zero.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MarginRankingLoss", "path": "generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D\nmini-batch `Tensors`, and a label 1D mini-batch tensor yy (containing 1 or\n-1).\n\nIf y=1y = 1 then it assumed the first input should be ranked higher (have a\nlarger value) than the second input, and vice-versa for y=\u22121y = -1 .\n\nThe loss function for each pair of samples in the mini-batch is:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MaxPool1d", "path": "generated/torch.nn.maxpool1d#torch.nn.MaxPool1d", "type": "torch.nn", "text": "\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size (N,C,L)(N,\nC, L) and output (N,C,Lout)(N, C, L_{out}) can be precisely described as:\n\nIf `padding` is non-zero, then the input is implicitly padded with negative\ninfinity on both sides for `padding` number of points. `dilation` is the\nstride between the elements within the sliding window. This link has a nice\nvisualization of the pooling parameters.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nOutput: (N,C,Lout)(N, C, L_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MaxPool2d", "path": "generated/torch.nn.maxpool2d#torch.nn.MaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) and\n`kernel_size` (kH,kW)(kH, kW) can be precisely described as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on both\nsides for `padding` number of points. `dilation` controls the spacing between\nthe kernel points. It is harder to describe, but this link has a nice\nvisualization of what `dilation` does.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MaxPool3d", "path": "generated/torch.nn.maxpool3d#torch.nn.MaxPool3d", "type": "torch.nn", "text": "\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out},\nH_{out}, W_{out}) and `kernel_size` (kD,kH,kW)(kD, kH, kW) can be precisely\ndescribed as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on both\nsides for `padding` number of points. `dilation` controls the spacing between\nthe kernel points. It is harder to describe, but this link has a nice\nvisualization of what `dilation` does.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MaxUnpool1d", "path": "generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d", "type": "torch.nn", "text": "\nComputes a partial inverse of `MaxPool1d`.\n\n`MaxPool1d` is not fully invertible, since the non-maximal values are lost.\n\n`MaxUnpool1d` takes in as input the output of `MaxPool1d` including the\nindices of the maximal values and computes a partial inverse in which all non-\nmaximal values are set to zero.\n\nNote\n\n`MaxPool1d` can map several input sizes to the same output sizes. Hence, the\ninversion process can get ambiguous. To accommodate this, you can provide the\nneeded output size as an additional argument `output_size` in the forward\ncall. See the Inputs and Example below.\n\nOutput: (N,C,Hout)(N, C, H_{out}) , where\n\nor as given by `output_size` in the call operator\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MaxUnpool2d", "path": "generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d", "type": "torch.nn", "text": "\nComputes a partial inverse of `MaxPool2d`.\n\n`MaxPool2d` is not fully invertible, since the non-maximal values are lost.\n\n`MaxUnpool2d` takes in as input the output of `MaxPool2d` including the\nindices of the maximal values and computes a partial inverse in which all non-\nmaximal values are set to zero.\n\nNote\n\n`MaxPool2d` can map several input sizes to the same output sizes. Hence, the\ninversion process can get ambiguous. To accommodate this, you can provide the\nneeded output size as an additional argument `output_size` in the forward\ncall. See the Inputs and Example below.\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where\n\nor as given by `output_size` in the call operator\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MaxUnpool3d", "path": "generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d", "type": "torch.nn", "text": "\nComputes a partial inverse of `MaxPool3d`.\n\n`MaxPool3d` is not fully invertible, since the non-maximal values are lost.\n`MaxUnpool3d` takes in as input the output of `MaxPool3d` including the\nindices of the maximal values and computes a partial inverse in which all non-\nmaximal values are set to zero.\n\nNote\n\n`MaxPool3d` can map several input sizes to the same output sizes. Hence, the\ninversion process can get ambiguous. To accommodate this, you can provide the\nneeded output size as an additional argument `output_size` in the forward\ncall. See the Inputs section below.\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where\n\nor as given by `output_size` in the call operator\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module", "path": "generated/torch.nn.module#torch.nn.Module", "type": "torch.nn", "text": "\nBase class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing to nest them in a tree\nstructure. You can assign the submodules as regular attributes:\n\nSubmodules assigned in this way will be registered, and will have their\nparameters converted too when you call `to()`, etc.\n\ntraining (bool) \u2013 Boolean represents whether this module is in training or\nevaluation mode.\n\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\nThis allows better BC support for `load_state_dict()`. In `state_dict()`, the\nversion number will be saved as in the attribute `_metadata` of the returned\nstate dict, and thus pickled. `_metadata` is a dictionary with keys that\nfollow the naming convention of state dict. See `_load_from_state_dict` on how\nto use this information in loading.\n\nIf new parameters/buffers are added/removed from a module, this number shall\nbe bumped, and the module\u2019s `_load_from_state_dict` method can compare the\nversion number and do appropriate changes if the state dict is from before the\nchange.\n\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote\n\nAlthough the recipe for forward pass needs to be defined within this function,\none should call the `Module` instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently\nignores them.\n\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.add_module()", "path": "generated/torch.nn.module#torch.nn.Module.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.apply()", "path": "generated/torch.nn.module#torch.nn.Module.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.bfloat16()", "path": "generated/torch.nn.module#torch.nn.Module.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.buffers()", "path": "generated/torch.nn.module#torch.nn.Module.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.children()", "path": "generated/torch.nn.module#torch.nn.Module.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.cpu()", "path": "generated/torch.nn.module#torch.nn.Module.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.cuda()", "path": "generated/torch.nn.module#torch.nn.Module.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.double()", "path": "generated/torch.nn.module#torch.nn.Module.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.dump_patches", "path": "generated/torch.nn.module#torch.nn.Module.dump_patches", "type": "torch.nn", "text": "\nThis allows better BC support for `load_state_dict()`. In `state_dict()`, the\nversion number will be saved as in the attribute `_metadata` of the returned\nstate dict, and thus pickled. `_metadata` is a dictionary with keys that\nfollow the naming convention of state dict. See `_load_from_state_dict` on how\nto use this information in loading.\n\nIf new parameters/buffers are added/removed from a module, this number shall\nbe bumped, and the module\u2019s `_load_from_state_dict` method can compare the\nversion number and do appropriate changes if the state dict is from before the\nchange.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.eval()", "path": "generated/torch.nn.module#torch.nn.Module.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.extra_repr()", "path": "generated/torch.nn.module#torch.nn.Module.extra_repr", "type": "torch.nn", "text": "\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.float()", "path": "generated/torch.nn.module#torch.nn.Module.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.forward()", "path": "generated/torch.nn.module#torch.nn.Module.forward", "type": "torch.nn", "text": "\nDefines the computation performed at every call.\n\nShould be overridden by all subclasses.\n\nNote\n\nAlthough the recipe for forward pass needs to be defined within this function,\none should call the `Module` instance afterwards instead of this since the\nformer takes care of running the registered hooks while the latter silently\nignores them.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.half()", "path": "generated/torch.nn.module#torch.nn.Module.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.load_state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.modules()", "path": "generated/torch.nn.module#torch.nn.Module.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.named_buffers()", "path": "generated/torch.nn.module#torch.nn.Module.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.named_children()", "path": "generated/torch.nn.module#torch.nn.Module.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.named_modules()", "path": "generated/torch.nn.module#torch.nn.Module.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.named_parameters()", "path": "generated/torch.nn.module#torch.nn.Module.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.parameters()", "path": "generated/torch.nn.module#torch.nn.Module.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.register_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.register_buffer()", "path": "generated/torch.nn.module#torch.nn.Module.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.register_forward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.register_forward_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.register_full_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.register_parameter()", "path": "generated/torch.nn.module#torch.nn.Module.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.requires_grad_()", "path": "generated/torch.nn.module#torch.nn.Module.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.to()", "path": "generated/torch.nn.module#torch.nn.Module.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.train()", "path": "generated/torch.nn.module#torch.nn.Module.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.type()", "path": "generated/torch.nn.module#torch.nn.Module.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.xpu()", "path": "generated/torch.nn.module#torch.nn.Module.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Module.zero_grad()", "path": "generated/torch.nn.module#torch.nn.Module.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict", "type": "torch.nn", "text": "\nHolds submodules in a dictionary.\n\n`ModuleDict` can be indexed like a regular Python dictionary, but modules it\ncontains are properly registered, and will be visible by all `Module` methods.\n\n`ModuleDict` is an ordered dictionary that respects\n\nNote that `update()` with other unordered mapping types (e.g., Python\u2019s plain\n`dict` before Python version 3.6) does not preserve the order of the merged\nmapping.\n\nmodules (iterable, optional) \u2013 a mapping (dictionary) of (string: module) or\nan iterable of key-value pairs of type (string, module)\n\nExample:\n\nRemove all items from the ModuleDict.\n\nReturn an iterable of the ModuleDict key/value pairs.\n\nReturn an iterable of the ModuleDict keys.\n\nRemove key from the ModuleDict and return its module.\n\nkey (string) \u2013 key to pop from the ModuleDict\n\nUpdate the `ModuleDict` with the key-value pairs from a mapping or an\niterable, overwriting existing keys.\n\nNote\n\nIf `modules` is an `OrderedDict`, a `ModuleDict`, or an iterable of key-value\npairs, the order of new elements in it is preserved.\n\nmodules (iterable) \u2013 a mapping (dictionary) from string to `Module`, or an\niterable of key-value pairs of type (string, `Module`)\n\nReturn an iterable of the ModuleDict values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.clear()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.clear", "type": "torch.nn", "text": "\nRemove all items from the ModuleDict.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.items()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.items", "type": "torch.nn", "text": "\nReturn an iterable of the ModuleDict key/value pairs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.keys()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.keys", "type": "torch.nn", "text": "\nReturn an iterable of the ModuleDict keys.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.pop()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.pop", "type": "torch.nn", "text": "\nRemove key from the ModuleDict and return its module.\n\nkey (string) \u2013 key to pop from the ModuleDict\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.update()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.update", "type": "torch.nn", "text": "\nUpdate the `ModuleDict` with the key-value pairs from a mapping or an\niterable, overwriting existing keys.\n\nNote\n\nIf `modules` is an `OrderedDict`, a `ModuleDict`, or an iterable of key-value\npairs, the order of new elements in it is preserved.\n\nmodules (iterable) \u2013 a mapping (dictionary) from string to `Module`, or an\niterable of key-value pairs of type (string, `Module`)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.values()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.values", "type": "torch.nn", "text": "\nReturn an iterable of the ModuleDict values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleList", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList", "type": "torch.nn", "text": "\nHolds submodules in a list.\n\n`ModuleList` can be indexed like a regular Python list, but modules it\ncontains are properly registered, and will be visible by all `Module` methods.\n\nmodules (iterable, optional) \u2013 an iterable of modules to add\n\nExample:\n\nAppends a given module to the end of the list.\n\nmodule (nn.Module) \u2013 module to append\n\nAppends modules from a Python iterable to the end of the list.\n\nmodules (iterable) \u2013 iterable of modules to append\n\nInsert a given module before a given index in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleList.append()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.append", "type": "torch.nn", "text": "\nAppends a given module to the end of the list.\n\nmodule (nn.Module) \u2013 module to append\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleList.extend()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.extend", "type": "torch.nn", "text": "\nAppends modules from a Python iterable to the end of the list.\n\nmodules (iterable) \u2013 iterable of modules to append\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ModuleList.insert()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.insert", "type": "torch.nn", "text": "\nInsert a given module before a given index in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin", "type": "torch.nn", "text": "\nA mixin for modules that lazily initialize parameters, also known as \u201clazy\nmodules.\u201d\n\nModules that lazily initialize parameters, or \u201clazy modules\u201d, derive the\nshapes of their parameters from the first input(s) to their forward method.\nUntil that first forward they contain `torch.nn.UninitializedParameter`s that\nshould not be accessed or used, and afterward they contain regular\n:class:`torch.nn.Parameter`s. Lazy modules are convenient since they don't\nrequire computing some module arguments, like the `in_features` argument of a\ntypical `torch.nn.Linear`.\n\nAfter construction, networks with lazy modules should first be converted to\nthe desired dtype and placed on the desired device. The lazy modules should\nthen be initialized with one or more \u201cdry runs\u201d. These \u201cdry runs\u201d send inputs\nof the correct size, dtype, and device through the network and to each one of\nits lazy modules. After this the network can be used as usual.\n\nA final caveat when using lazy modules is that the order of initialization of\na network\u2019s parameters may change, since the lazy modules are always\ninitialized after other modules. This can cause the parameters of a network\nusing lazy modules to be initialized differently than the parameters of a\nnetwork without lazy modules. For example, if the LazyMLP class defined above\nhad a `torch.nn.LazyLinear` module first and then a regular `torch.nn.Linear`\nsecond, the second module would be initialized on construction and the first\nmodule would be initialized during the first dry run.\n\nLazy modules can be serialized with a state dict like other modules. For\nexample:\n\nLazy modules can also load regular `torch.nn.Parameter` s, which replace their\n`torch.nn.UninitializedParameter` s:\n\nNote, however, that lazy modules cannot validate that the shape of parameters\nthey load is correct.\n\nCheck if a module has parameters that are not initialized\n\nInitialize parameters according to the input batch properties. This adds an\ninterface to isolate parameter initialization from the forward pass when doing\nparameter shape inference.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params", "type": "torch.nn", "text": "\nCheck if a module has parameters that are not initialized\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters", "type": "torch.nn", "text": "\nInitialize parameters according to the input batch properties. This adds an\ninterface to isolate parameter initialization from the forward pass when doing\nparameter shape inference.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.modules.module.register_module_backward_hook()", "path": "generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook common to all the modules.\n\nThis function is deprecated in favor of\n`nn.module.register_module_full_backward_hook()` and the behavior of this\nfunction will change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.modules.module.register_module_forward_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook", "type": "torch.nn", "text": "\nRegisters a global forward hook for all the modules\n\nWarning\n\nThis adds global state to the `nn.module` module and it is only intended for\ndebugging/profiling purposes.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nThis hook will be executed before specific module hooks registered with\n`register_forward_hook`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.modules.module.register_module_forward_pre_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook common to all modules.\n\nWarning\n\nThis adds global state to the `nn.module` module and it is only intended for\ndebugging/profiling purposes.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\nThis hook has precedence over the specific module hooks registered with\n`register_forward_pre_hook`.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MSELoss", "path": "generated/torch.nn.mseloss#torch.nn.MSELoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the mean squared error (squared L2 norm)\nbetween each element in the input xx and target yy .\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen:\n\nxx and yy are tensors of arbitrary shapes with a total of nn elements each.\n\nThe mean operation still operates over all the elements, and divides by nn .\n\nThe division by nn can be avoided if one sets `reduction = 'sum'`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MultiheadAttention", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention", "type": "torch.nn", "text": "\nAllows the model to jointly attend to information from different\nrepresentation subspaces. See Attention Is All You Need\n\nwhere headi=Attention(QWiQ,KWiK,VWiV)head_i = \\text{Attention}(QW_i^Q, KW_i^K,\nVW_i^V) .\n\nNote that if `kdim` and `vdim` are None, they will be set to `embed_dim` such\nthat query, key, and value have the same number of features.\n\nExamples:\n\nattn_mask: if a 2D mask: (L,S)(L, S) where L is the target sequence length, S\nis the source sequence length.\n\nIf a 3D mask: (N\u22c5num_heads,L,S)(N\\cdot\\text{num\\\\_heads}, L, S) where N is the\nbatch size, L is the target sequence length, S is the source sequence length.\n`attn_mask` ensure that position i is allowed to attend the unmasked\npositions. If a ByteTensor is provided, the non-zero positions are not allowed\nto attend while the zero positions will be unchanged. If a BoolTensor is\nprovided, positions with `True` is not allowed to attend while `False` values\nwill be unchanged. If a FloatTensor is provided, it will be added to the\nattention weight.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MultiheadAttention.forward()", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention.forward", "type": "torch.nn", "text": "\nattn_mask: if a 2D mask: (L,S)(L, S) where L is the target sequence length, S\nis the source sequence length.\n\nIf a 3D mask: (N\u22c5num_heads,L,S)(N\\cdot\\text{num\\\\_heads}, L, S) where N is the\nbatch size, L is the target sequence length, S is the source sequence length.\n`attn_mask` ensure that position i is allowed to attend the unmasked\npositions. If a ByteTensor is provided, the non-zero positions are not allowed\nto attend while the zero positions will be unchanged. If a BoolTensor is\nprovided, positions with `True` is not allowed to attend while `False` values\nwill be unchanged. If a FloatTensor is provided, it will be added to the\nattention weight.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MultiLabelMarginLoss", "path": "generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a multi-class multi-classification hinge\nloss (margin-based loss) between input xx (a 2D mini-batch `Tensor`) and\noutput yy (which is a 2D `Tensor` of target class indices). For each sample in\nthe mini-batch:\n\nwhere x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\\\{0, \\; \\cdots , \\; \\text{x.size}(0) -\n1\\right\\\\} , y\u2208{0,\u22ef,y.size(0)\u22121}y \\in \\left\\\\{0, \\; \\cdots , \\;\n\\text{y.size}(0) - 1\\right\\\\} , 0\u2264y[j]\u2264x.size(0)\u221210 \\leq y[j] \\leq\n\\text{x.size}(0)-1 , and i\u2260y[j]i \\neq y[j] for all ii and jj .\n\nyy and xx must have the same size.\n\nThe criterion only considers a contiguous block of non-negative targets that\nstarts at the front.\n\nThis allows for different samples to have variable amounts of target classes.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MultiLabelSoftMarginLoss", "path": "generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a multi-label one-versus-all loss based on\nmax-entropy, between input xx and target yy of size (N,C)(N, C) . For each\nsample in the minibatch:\n\nwhere i\u2208{0,\u22ef,x.nElement()\u22121}i \\in \\left\\\\{0, \\; \\cdots , \\;\n\\text{x.nElement}() - 1\\right\\\\} , y[i]\u2208{0,1}y[i] \\in \\left\\\\{0, \\; 1\\right\\\\}\n.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.MultiMarginLoss", "path": "generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a multi-class classification hinge loss\n(margin-based loss) between input xx (a 2D mini-batch `Tensor`) and output yy\n(which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq\n\\text{x.size}(1)-1 ):\n\nFor each mini-batch sample, the loss in terms of the 1D input xx and scalar\noutput yy is:\n\nwhere x\u2208{0,\u22ef,x.size(0)\u22121}x \\in \\left\\\\{0, \\; \\cdots , \\; \\text{x.size}(0) -\n1\\right\\\\} and i\u2260yi \\neq y .\n\nOptionally, you can give non-equal weighting on the classes by passing a 1D\n`weight` tensor into the constructor.\n\nThe loss function then becomes:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.NLLLoss", "path": "generated/torch.nn.nllloss#torch.nn.NLLLoss", "type": "torch.nn", "text": "\nThe negative log likelihood loss. It is useful to train a classification\nproblem with `C` classes.\n\nIf provided, the optional argument `weight` should be a 1D Tensor assigning\nweight to each of the classes. This is particularly useful when you have an\nunbalanced training set.\n\nThe `input` given through a forward call is expected to contain log-\nprobabilities of each class. `input` has to be a Tensor of size either\n(minibatch,C)(minibatch, C) or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1,\nd_2, ..., d_K) with K\u22651K \\geq 1 for the `K`-dimensional case (described\nlater).\n\nObtaining log-probabilities in a neural network is easily achieved by adding a\n`LogSoftmax` layer in the last layer of your network. You may use\n`CrossEntropyLoss` instead, if you prefer not to add an extra layer.\n\nThe `target` that this loss expects should be a class index in the range\n[0,C\u22121][0, C-1] where `C = number of classes`; if `ignore_index` is specified,\nthis loss also accepts this class index (this index may not necessarily be in\nthe class range).\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere xx is the input, yy is the target, ww is the weight, and NN is the batch\nsize. If `reduction` is not `'none'` (default `'mean'`), then\n\nCan also be used for higher dimension inputs, such as 2D images, by providing\nan input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)\nwith K\u22651K \\geq 1 , where KK is the number of dimensions, and a target of\nappropriate shape (see below). In the case of images, it computes NLL loss\nper-pixel.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.PairwiseDistance", "path": "generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance", "type": "torch.nn", "text": "\nComputes the batchwise pairwise distance between vectors v1v_1 , v2v_2 using\nthe p-norm:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parallel.data_parallel()", "path": "nn.functional#torch.nn.parallel.data_parallel", "type": "torch.nn.functional", "text": "\nEvaluates module(input) in parallel across the GPUs given in device_ids.\n\nThis is the functional version of the DataParallel module.\n\na Tensor containing the result of module(input) located on output_device\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel", "type": "torch.nn", "text": "\nImplements distributed data parallelism that is based on `torch.distributed`\npackage at the module level.\n\nThis container parallelizes the application of the given module by splitting\nthe input across the specified devices by chunking in the batch dimension. The\nmodule is replicated on each machine and each device, and each such replica\nhandles a portion of the input. During the backwards pass, gradients from each\nnode are averaged.\n\nThe batch size should be larger than the number of GPUs used locally.\n\nSee also: Basics and Use nn.parallel.DistributedDataParallel instead of\nmultiprocessing or nn.DataParallel. The same constraints on input as in\n`torch.nn.DataParallel` apply.\n\nCreation of this class requires that `torch.distributed` to be already\ninitialized, by calling `torch.distributed.init_process_group()`.\n\n`DistributedDataParallel` is proven to be significantly faster than\n`torch.nn.DataParallel` for single-node multi-GPU data parallel training.\n\nTo use `DistributedDataParallel` on a host with N GPUs, you should spawn up\n`N` processes, ensuring that each process exclusively works on a single GPU\nfrom 0 to N-1. This can be done by either setting `CUDA_VISIBLE_DEVICES` for\nevery process or by calling:\n\nwhere i is from 0 to N-1. In each process, you should refer the following to\nconstruct this module:\n\nIn order to spawn up multiple processes per node, you can use either\n`torch.distributed.launch` or `torch.multiprocessing.spawn`.\n\nNote\n\nPlease refer to PyTorch Distributed Overview for a brief introduction to all\nfeatures related to distributed training.\n\nNote\n\n`nccl` backend is currently the fastest and highly recommended backend when\nusing GPUs. This applies to both single-node and multi-node distributed\ntraining.\n\nNote\n\nThis module also supports mixed-precision distributed training. This means\nthat your model can have different types of parameters such as mixed types of\n`fp16` and `fp32`, the gradient reduction on these mixed types of parameters\nwill just work fine.\n\nNote\n\nIf you use `torch.save` on one process to checkpoint the module, and\n`torch.load` on some other processes to recover it, make sure that\n`map_location` is configured properly for every process. Without\n`map_location`, `torch.load` would recover the module to devices where the\nmodule was saved from.\n\nNote\n\nWhen a model is trained on `M` nodes with `batch=N`, the gradient will be `M`\ntimes smaller when compared to the same model trained on a single node with\n`batch=M*N` if the loss is summed (NOT averaged as usual) across instances in\na batch (because the gradients between different nodes are averaged). You\nshould take this into consideration when you want to obtain a mathematically\nequivalent training process compared to the local training counterpart. But in\nmost cases, you can just treat a DistributedDataParallel wrapped model, a\nDataParallel wrapped model and an ordinary model on a single GPU as the same\n(E.g. using the same learning rate for equivalent batch size).\n\nNote\n\nParameters are never broadcast between processes. The module performs an all-\nreduce step on gradients and assumes that they will be modified by the\noptimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are\nbroadcast from the module in process of rank 0, to all other replicas in the\nsystem in every iteration.\n\nNote\n\nIf you are using DistributedDataParallel in conjunction with the Distributed\nRPC Framework, you should always use `torch.distributed.autograd.backward()`\nto compute gradients and `torch.distributed.optim.DistributedOptimizer` for\noptimizing parameters.\n\nExample:\n\nWarning\n\nConstructor, forward method, and differentiation of the output (or a function\nof the output of this module) are distributed synchronization points. Take\nthat into account in case different processes might be executing different\ncode.\n\nWarning\n\nThis module assumes all parameters are registered in the model by the time it\nis created. No parameters should be added nor removed later. Same applies to\nbuffers.\n\nWarning\n\nThis module assumes all parameters are registered in the model of each\ndistributed processes are in the same order. The module itself will conduct\ngradient `allreduce` following the reverse order of the registered parameters\nof the model. In other words, it is users\u2019 responsibility to ensure that each\ndistributed process has the exact same model and thus the exact same parameter\nregistration order.\n\nWarning\n\nThis module allows parameters with non-rowmajor-contiguous strides. For\nexample, your model may contain some parameters whose `torch.memory_format` is\n`torch.contiguous_format` and others whose format is `torch.channels_last`.\nHowever, corresponding parameters in different processes must have the same\nstrides.\n\nWarning\n\nThis module doesn\u2019t work with `torch.autograd.grad()` (i.e. it will only work\nif gradients are to be accumulated in `.grad` attributes of parameters).\n\nWarning\n\nIf you plan on using this module with a `nccl` backend or a `gloo` backend\n(that uses Infiniband), together with a DataLoader that uses multiple workers,\nplease change the multiprocessing start method to `forkserver` (Python 3 only)\nor `spawn`. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork\nsafe, and you will likely experience deadlocks if you don\u2019t change this\nsetting.\n\nWarning\n\nForward and backward hooks defined on `module` and its submodules won\u2019t be\ninvoked anymore, unless the hooks are initialized in the `forward()` method.\n\nWarning\n\nYou should never try to change your model\u2019s parameters after wrapping up your\nmodel with `DistributedDataParallel`. Because, when wrapping up your model\nwith `DistributedDataParallel`, the constructor of `DistributedDataParallel`\nwill register the additional gradient reduction functions on all the\nparameters of the model itself at the time of construction. If you change the\nmodel\u2019s parameters afterwards, gradient redunction functions no longer match\nthe correct set of parameters.\n\nWarning\n\nUsing `DistributedDataParallel` in conjunction with the Distributed RPC\nFramework is experimental and subject to change.\n\nWarning\n\nThe `gradient_as_bucket_view` mode does not yet work with Automatic Mixed\nPrecision (AMP). AMP maintains stashed gradients that are used for unscaling\ngradients. With `gradient_as_bucket_view=True`, these stashed gradients will\npoint to communication buckets in the first iteration. In the next iteration,\nthe communication buckets are mutated and thus these stashed gradients will be\nunexpectedly mutated as well, which might lead to wrong results.\n\n~DistributedDataParallel.module (Module) \u2013 the module to be parallelized.\n\nExample:\n\nA context manager to be used in conjunction with an instance of\n`torch.nn.parallel.DistributedDataParallel` to be able to train with uneven\ninputs across participating processes.\n\nThis context manager will keep track of already-joined DDP processes, and\n\u201cshadow\u201d the forward and backward passes by inserting collective communication\noperations to match with the ones created by non-joined DDP processes. This\nwill ensure each collective call has a corresponding call by already-joined\nDDP processes, preventing hangs or errors that would otherwise happen when\ntraining with uneven inputs across processes.\n\nOnce all DDP processes have joined, the context manager will broadcast the\nmodel corresponding to the last joined process to all processes to ensure the\nmodel is the same across all processes (which is guaranteed by DDP).\n\nTo use this to enable training with uneven inputs across processes, simply\nwrap this context manager around your training loop. No further modifications\nto the model or data loading is required.\n\nWarning\n\nThis module works only with the multi-process, single-device usage of\n`torch.nn.parallel.DistributedDataParallel`, which means that a single process\nworks on a single GPU.\n\nWarning\n\nThis module currently does not support custom distributed collective\noperations in the forward pass, such as `SyncBatchNorm` or other custom\ndefined collectives in the model\u2019s forward pass.\n\nExample:\n\nA context manager to disable gradient synchronizations across DDP processes.\nWithin this context, gradients will be accumulated on module variables, which\nwill later be synchronized in the first forward-backward pass exiting the\ncontext.\n\nExample:\n\nRegisters a communication hook which is an enhancement that provides a\nflexible hook to users where they can specify how DDP aggregates gradients\nacross multiple workers.\n\nThis hook would be very useful for researchers to try out new ideas. For\nexample, this hook can be used to implement several algorithms like GossipGrad\nand gradient compression which involve different communication strategies for\nparameter syncs while running Distributed DataParallel training.\n\nstate (object) \u2013\n\nPassed to the hook to maintain any state information during the training\nprocess. Examples include error feedback in gradient compression, peers to\ncommunicate with next in GossipGrad, etc.\n\nIt is locally stored by each worker and shared by all the gradient tensors on\nthe worker.\n\nhook (callable) \u2013\n\nAverages gradient tensors across workers and defined as: `hook(state: object,\nbucket: dist._GradBucket) -> torch.futures.Future`:\n\nThis function is called once the bucket is ready. The hook can perform\nwhatever processing is needed and return a Future indicating completion of any\nasync work (ex: allreduce). If the hook doesn\u2019t perform any communication, it\ncan also just return a completed Future. The Future should hold the new value\nof grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this\nhook and use the tensors returned by the Future and copy grads to individual\nparameters.\n\nWe also provide an API called `get_future` to retrieve a Future associated\nwith the completion of `c10d.ProcessGroup.work`.\n\nWarning\n\nGrad bucket\u2019s tensors will not be predivided by world_size. User is\nresponsible to divide by the world_size in case of operations like allreduce.\n\nWarning\n\nDDP communication hook can only be registered once and should be registered\nbefore calling backward.\n\nWarning\n\nThe Future object that hook returns should contain a result that has the same\nshape with the tensors inside grad bucket.\n\nWarning\n\nDDP communication hook does not support single-process multiple-device mode.\nGradbucket tensors should consist of only a single tensor.\n\nWarning\n\n`get_future` API supports only NCCL backend and will return a\n`torch._C.Future` which is an internal type and should be used with caution.\nIt can still be used by `register_comm_hook` API, but it is subject to some\nsubtle differences compared to `torch.futures.Future`.\n\nWarning\n\nDDP communication hook is experimental and subject to change.\n\nBelow is an example of a noop hook that returns the same tensors.\n\nBelow is an example of a Parallel SGD algorithm where gradients are encoded\nbefore allreduce, and then decoded after allreduce.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel.join()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.join", "type": "torch.nn", "text": "\nA context manager to be used in conjunction with an instance of\n`torch.nn.parallel.DistributedDataParallel` to be able to train with uneven\ninputs across participating processes.\n\nThis context manager will keep track of already-joined DDP processes, and\n\u201cshadow\u201d the forward and backward passes by inserting collective communication\noperations to match with the ones created by non-joined DDP processes. This\nwill ensure each collective call has a corresponding call by already-joined\nDDP processes, preventing hangs or errors that would otherwise happen when\ntraining with uneven inputs across processes.\n\nOnce all DDP processes have joined, the context manager will broadcast the\nmodel corresponding to the last joined process to all processes to ensure the\nmodel is the same across all processes (which is guaranteed by DDP).\n\nTo use this to enable training with uneven inputs across processes, simply\nwrap this context manager around your training loop. No further modifications\nto the model or data loading is required.\n\nWarning\n\nThis module works only with the multi-process, single-device usage of\n`torch.nn.parallel.DistributedDataParallel`, which means that a single process\nworks on a single GPU.\n\nWarning\n\nThis module currently does not support custom distributed collective\noperations in the forward pass, such as `SyncBatchNorm` or other custom\ndefined collectives in the model\u2019s forward pass.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel.no_sync()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.no_sync", "type": "torch.nn", "text": "\nA context manager to disable gradient synchronizations across DDP processes.\nWithin this context, gradients will be accumulated on module variables, which\nwill later be synchronized in the first forward-backward pass exiting the\ncontext.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel.register_comm_hook()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.register_comm_hook", "type": "torch.nn", "text": "\nRegisters a communication hook which is an enhancement that provides a\nflexible hook to users where they can specify how DDP aggregates gradients\nacross multiple workers.\n\nThis hook would be very useful for researchers to try out new ideas. For\nexample, this hook can be used to implement several algorithms like GossipGrad\nand gradient compression which involve different communication strategies for\nparameter syncs while running Distributed DataParallel training.\n\nstate (object) \u2013\n\nPassed to the hook to maintain any state information during the training\nprocess. Examples include error feedback in gradient compression, peers to\ncommunicate with next in GossipGrad, etc.\n\nIt is locally stored by each worker and shared by all the gradient tensors on\nthe worker.\n\nhook (callable) \u2013\n\nAverages gradient tensors across workers and defined as: `hook(state: object,\nbucket: dist._GradBucket) -> torch.futures.Future`:\n\nThis function is called once the bucket is ready. The hook can perform\nwhatever processing is needed and return a Future indicating completion of any\nasync work (ex: allreduce). If the hook doesn\u2019t perform any communication, it\ncan also just return a completed Future. The Future should hold the new value\nof grad bucket\u2019s tensors. Once a bucket is ready, c10d reducer would call this\nhook and use the tensors returned by the Future and copy grads to individual\nparameters.\n\nWe also provide an API called `get_future` to retrieve a Future associated\nwith the completion of `c10d.ProcessGroup.work`.\n\nWarning\n\nGrad bucket\u2019s tensors will not be predivided by world_size. User is\nresponsible to divide by the world_size in case of operations like allreduce.\n\nWarning\n\nDDP communication hook can only be registered once and should be registered\nbefore calling backward.\n\nWarning\n\nThe Future object that hook returns should contain a result that has the same\nshape with the tensors inside grad bucket.\n\nWarning\n\nDDP communication hook does not support single-process multiple-device mode.\nGradbucket tensors should consist of only a single tensor.\n\nWarning\n\n`get_future` API supports only NCCL backend and will return a\n`torch._C.Future` which is an internal type and should be used with caution.\nIt can still be used by `register_comm_hook` API, but it is subject to some\nsubtle differences compared to `torch.futures.Future`.\n\nWarning\n\nDDP communication hook is experimental and subject to change.\n\nBelow is an example of a noop hook that returns the same tensors.\n\nBelow is an example of a Parallel SGD algorithm where gradients are encoded\nbefore allreduce, and then decoded after allreduce.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parameter.Parameter", "path": "generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter", "type": "torch.nn", "text": "\nA kind of Tensor that is to be considered a module parameter.\n\nParameters are `Tensor` subclasses, that have a very special property when\nused with `Module` s - when they\u2019re assigned as Module attributes they are\nautomatically added to the list of its parameters, and will appear e.g. in\n`parameters()` iterator. Assigning a Tensor doesn\u2019t have such effect. This is\nbecause one might want to cache some temporary state, like last hidden state\nof the RNN, in the model. If there was no such class as `Parameter`, these\ntemporaries would get registered too.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parameter.UninitializedParameter", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter", "type": "torch.nn", "text": "\nA parameter that is not initialized.\n\nUnitialized Parameters are a a special case of `torch.nn.Parameter` where the\nshape of the data is still unknown.\n\nUnlikely a `torch.nn.Parameter`, uninitialized parameters hold no data and\nattempting to access some properties, like their shape, will throw a runtime\nerror. The only operations that can be performed on a uninitialized parameter\nare changing its datatype, moving it to a different device and converting it\nto a regular `torch.nn.Parameter`.\n\nCreate a Parameter with the same properties of the uninitialized one. Given a\nshape, it materializes a parameter in the same device and with the same\n`dtype` as the current one or the specified ones in the arguments.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.parameter.UninitializedParameter.materialize()", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter.materialize", "type": "torch.nn", "text": "\nCreate a Parameter with the same properties of the uninitialized one. Given a\nshape, it materializes a parameter in the same device and with the same\n`dtype` as the current one or the specified ones in the arguments.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict", "type": "torch.nn", "text": "\nHolds parameters in a dictionary.\n\nParameterDict can be indexed like a regular Python dictionary, but parameters\nit contains are properly registered, and will be visible by all Module\nmethods.\n\n`ParameterDict` is an ordered dictionary that respects\n\nNote that `update()` with other unordered mapping types (e.g., Python\u2019s plain\n`dict`) does not preserve the order of the merged mapping.\n\nparameters (iterable, optional) \u2013 a mapping (dictionary) of (string :\n`Parameter`) or an iterable of key-value pairs of type (string, `Parameter`)\n\nExample:\n\nRemove all items from the ParameterDict.\n\nReturn an iterable of the ParameterDict key/value pairs.\n\nReturn an iterable of the ParameterDict keys.\n\nRemove key from the ParameterDict and return its parameter.\n\nkey (string) \u2013 key to pop from the ParameterDict\n\nUpdate the `ParameterDict` with the key-value pairs from a mapping or an\niterable, overwriting existing keys.\n\nNote\n\nIf `parameters` is an `OrderedDict`, a `ParameterDict`, or an iterable of key-\nvalue pairs, the order of new elements in it is preserved.\n\nparameters (iterable) \u2013 a mapping (dictionary) from string to `Parameter`, or\nan iterable of key-value pairs of type (string, `Parameter`)\n\nReturn an iterable of the ParameterDict values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.clear()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.clear", "type": "torch.nn", "text": "\nRemove all items from the ParameterDict.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.items()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.items", "type": "torch.nn", "text": "\nReturn an iterable of the ParameterDict key/value pairs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.keys()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.keys", "type": "torch.nn", "text": "\nReturn an iterable of the ParameterDict keys.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.pop()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.pop", "type": "torch.nn", "text": "\nRemove key from the ParameterDict and return its parameter.\n\nkey (string) \u2013 key to pop from the ParameterDict\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.update()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.update", "type": "torch.nn", "text": "\nUpdate the `ParameterDict` with the key-value pairs from a mapping or an\niterable, overwriting existing keys.\n\nNote\n\nIf `parameters` is an `OrderedDict`, a `ParameterDict`, or an iterable of key-\nvalue pairs, the order of new elements in it is preserved.\n\nparameters (iterable) \u2013 a mapping (dictionary) from string to `Parameter`, or\nan iterable of key-value pairs of type (string, `Parameter`)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.values()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.values", "type": "torch.nn", "text": "\nReturn an iterable of the ParameterDict values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterList", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList", "type": "torch.nn", "text": "\nHolds parameters in a list.\n\n`ParameterList` can be indexed like a regular Python list, but parameters it\ncontains are properly registered, and will be visible by all `Module` methods.\n\nparameters (iterable, optional) \u2013 an iterable of `Parameter` to add\n\nExample:\n\nAppends a given parameter at the end of the list.\n\nparameter (nn.Parameter) \u2013 parameter to append\n\nAppends parameters from a Python iterable to the end of the list.\n\nparameters (iterable) \u2013 iterable of parameters to append\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterList.append()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.append", "type": "torch.nn", "text": "\nAppends a given parameter at the end of the list.\n\nparameter (nn.Parameter) \u2013 parameter to append\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ParameterList.extend()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.extend", "type": "torch.nn", "text": "\nAppends parameters from a Python iterable to the end of the list.\n\nparameters (iterable) \u2013 iterable of parameters to append\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.PixelShuffle", "path": "generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle", "type": "torch.nn", "text": "\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nan upscale factor.\n\nThis is useful for implementing efficient sub-pixel convolution with a stride\nof 1/r1/r .\n\nSee the paper: Real-Time Single Image and Video Super-Resolution Using an\nEfficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more\ndetails.\n\nupscale_factor (int) \u2013 factor to increase spatial resolution by\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.PixelUnshuffle", "path": "generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle", "type": "torch.nn", "text": "\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.\n\nSee the paper: Real-Time Single Image and Video Super-Resolution Using an\nEfficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more\ndetails.\n\ndownscale_factor (int) \u2013 factor to decrease spatial resolution by\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.PoissonNLLLoss", "path": "generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss", "type": "torch.nn", "text": "\nNegative log likelihood loss with Poisson distribution of target.\n\nThe loss can be described as:\n\nThe last term can be omitted or approximated with Stirling formula. The\napproximation is used for target values more than 1. For targets less or equal\nto 1 zeros are added to the loss.\n\nfull (bool, optional) \u2013\n\nwhether to compute full loss, i. e. to add the Stirling approximation term\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.PReLU", "path": "generated/torch.nn.prelu#torch.nn.PReLU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nor\n\nHere aa is a learnable parameter. When called without arguments, `nn.PReLU()`\nuses a single parameter aa across all input channels. If called with\n`nn.PReLU(nChannels)`, a separate aa is used for each input channel.\n\nNote\n\nweight decay should not be used when learning aa for good performance.\n\nNote\n\nChannel dim is the 2nd dim of input. When input has dims < 2, then there is no\nchannel dim and the number of channels = 1.\n\n~PReLU.weight (Tensor) \u2013 the learnable weights of shape (`num_parameters`).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.qat.Conv2d", "path": "torch.nn.qat#torch.nn.qat.Conv2d", "type": "Quantization", "text": "\nA Conv2d module attached with FakeQuantize modules for weight, used for\nquantization aware training.\n\nWe adopt the same interface as `torch.nn.Conv2d`, please see\nhttps://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for\ndocumentation.\n\nSimilar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to\ndefault.\n\n~Conv2d.weight_fake_quant \u2013 fake quant module for weight\n\nCreate a qat module from a float module or qparams_dict\n\nArgs: `mod` a float module, either produced by torch.quantization utilities or\ndirectly from user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.qat.Conv2d.from_float()", "path": "torch.nn.qat#torch.nn.qat.Conv2d.from_float", "type": "Quantization", "text": "\nCreate a qat module from a float module or qparams_dict\n\nArgs: `mod` a float module, either produced by torch.quantization utilities or\ndirectly from user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.qat.Linear", "path": "torch.nn.qat#torch.nn.qat.Linear", "type": "Quantization", "text": "\nA linear module attached with FakeQuantize modules for weight, used for\nquantization aware training.\n\nWe adopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\nSimilar to `torch.nn.Linear`, with FakeQuantize modules initialized to\ndefault.\n\n~Linear.weight \u2013 fake quant module for weight\n\nCreate a qat module from a float module or qparams_dict\n\nArgs: `mod` a float module, either produced by torch.quantization utilities or\ndirectly from user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.qat.Linear.from_float()", "path": "torch.nn.qat#torch.nn.qat.Linear.from_float", "type": "Quantization", "text": "\nCreate a qat module from a float module or qparams_dict\n\nArgs: `mod` a float module, either produced by torch.quantization utilities or\ndirectly from user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.BatchNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm2d", "type": "Quantization", "text": "\nThis is the quantized version of `BatchNorm2d`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.BatchNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm3d", "type": "Quantization", "text": "\nThis is the quantized version of `BatchNorm3d`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv1d", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d", "type": "Quantization", "text": "\nApplies a 1D convolution over a quantized input signal composed of several\nquantized input planes.\n\nFor details on input arguments, parameters, and implementation see `Conv1d`.\n\nNote\n\nOnly `zeros` is supported for the `padding_mode` argument.\n\nNote\n\nOnly `torch.quint8` is supported for the input data type.\n\nSee `Conv1d` for other attributes.\n\nExamples:\n\nCreates a quantized module from a float module or qparams_dict.\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv1d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": "\nCreates a quantized module from a float module or qparams_dict.\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv2d", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d", "type": "Quantization", "text": "\nApplies a 2D convolution over a quantized input signal composed of several\nquantized input planes.\n\nFor details on input arguments, parameters, and implementation see `Conv2d`.\n\nNote\n\nOnly `zeros` is supported for the `padding_mode` argument.\n\nNote\n\nOnly `torch.quint8` is supported for the input data type.\n\nSee `Conv2d` for other attributes.\n\nExamples:\n\nCreates a quantized module from a float module or qparams_dict.\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv2d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": "\nCreates a quantized module from a float module or qparams_dict.\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv3d", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d", "type": "Quantization", "text": "\nApplies a 3D convolution over a quantized input signal composed of several\nquantized input planes.\n\nFor details on input arguments, parameters, and implementation see `Conv3d`.\n\nNote\n\nOnly `zeros` is supported for the `padding_mode` argument.\n\nNote\n\nOnly `torch.quint8` is supported for the input data type.\n\nSee `Conv3d` for other attributes.\n\nExamples:\n\nCreates a quantized module from a float module or qparams_dict.\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv3d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": "\nCreates a quantized module from a float module or qparams_dict.\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.DeQuantize", "path": "torch.nn.quantized#torch.nn.quantized.DeQuantize", "type": "Quantization", "text": "\nDequantizes an incoming tensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic", "path": "torch.nn.quantized.dynamic", "type": "torch.nn.quantized.dynamic", "text": "\nA dynamic quantized linear module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\nSimilar to `torch.nn.Linear`, attributes will be randomly initialized at\nmodule creation time and will be overwritten later\n\nExamples:\n\nCreate a dynamic quantized module from a float module or qparams_dict\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\nA dynamic quantized LSTM module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.LSTM`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.\n\nExamples:\n\nA long short-term memory (LSTM) cell.\n\nA dynamic quantized LSTMCell module with floating point tensor as inputs and\noutputs. Weights are quantized to 8 bits. We adopt the same interface as\n`torch.nn.LSTMCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.\n\nExamples:\n\nA gated recurrent unit (GRU) cell\n\nA dynamic quantized GRUCell module with floating point tensor as inputs and\noutputs. Weights are quantized to 8 bits. We adopt the same interface as\n`torch.nn.GRUCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.\n\nExamples:\n\nAn Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell\nmodule with floating point tensor as inputs and outputs. Weights are quantized\nto 8 bits. We adopt the same interface as `torch.nn.RNNCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.GRUCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.GRUCell", "type": "torch.nn.quantized.dynamic", "text": "\nA gated recurrent unit (GRU) cell\n\nA dynamic quantized GRUCell module with floating point tensor as inputs and\noutputs. Weights are quantized to 8 bits. We adopt the same interface as\n`torch.nn.GRUCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.Linear", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear", "type": "torch.nn.quantized.dynamic", "text": "\nA dynamic quantized linear module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\nSimilar to `torch.nn.Linear`, attributes will be randomly initialized at\nmodule creation time and will be overwritten later\n\nExamples:\n\nCreate a dynamic quantized module from a float module or qparams_dict\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.Linear.from_float()", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear.from_float", "type": "torch.nn.quantized.dynamic", "text": "\nCreate a dynamic quantized module from a float module or qparams_dict\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.LSTM", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTM", "type": "torch.nn.quantized.dynamic", "text": "\nA dynamic quantized LSTM module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.LSTM`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.LSTMCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTMCell", "type": "torch.nn.quantized.dynamic", "text": "\nA long short-term memory (LSTM) cell.\n\nA dynamic quantized LSTMCell module with floating point tensor as inputs and\noutputs. Weights are quantized to 8 bits. We adopt the same interface as\n`torch.nn.LSTMCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.RNNCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.RNNCell", "type": "torch.nn.quantized.dynamic", "text": "\nAn Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell\nmodule with floating point tensor as inputs and outputs. Weights are quantized\nto 8 bits. We adopt the same interface as `torch.nn.RNNCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.ELU", "path": "torch.nn.quantized#torch.nn.quantized.ELU", "type": "Quantization", "text": "\nThis is the quantized equivalent of `ELU`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Embedding", "path": "torch.nn.quantized#torch.nn.quantized.Embedding", "type": "Quantization", "text": "\nA quantized Embedding module with quantized packed weights as inputs. We adopt\nthe same interface as `torch.nn.Embedding`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.\n\nSimilar to `Embedding`, attributes will be randomly initialized at module\ncreation time and will be overwritten later\n\n~Embedding.weight (Tensor) \u2013 the non-learnable quantized weights of the module\nof shape (num_embeddings,embedding_dim)(\\text{num\\\\_embeddings},\n\\text{embedding\\\\_dim}) .\n\nCreate a quantized embedding module from a float module\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Embedding.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Embedding.from_float", "type": "Quantization", "text": "\nCreate a quantized embedding module from a float module\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.EmbeddingBag", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag", "type": "Quantization", "text": "\nA quantized EmbeddingBag module with quantized packed weights as inputs. We\nadopt the same interface as `torch.nn.EmbeddingBag`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for\ndocumentation.\n\nSimilar to `EmbeddingBag`, attributes will be randomly initialized at module\ncreation time and will be overwritten later\n\n~EmbeddingBag.weight (Tensor) \u2013 the non-learnable quantized weights of the\nmodule of shape (num_embeddings,embedding_dim)(\\text{num\\\\_embeddings},\n\\text{embedding\\\\_dim}) .\n\nCreate a quantized embedding_bag module from a float module\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.EmbeddingBag.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": "\nCreate a quantized embedding_bag module from a float module\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.FloatFunctional", "path": "torch.nn.quantized#torch.nn.quantized.FloatFunctional", "type": "Quantization", "text": "\nState collector class for float operations.\n\nThe instance of this class can be used instead of the `torch.` prefix for some\noperations. See example usage below.\n\nNote\n\nThis class does not provide a `forward` hook. Instead, you must use one of the\nunderlying functions (e.g. `add`).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.adaptive_avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": "\nApplies a 2D adaptive average pooling over a quantized input signal composed\nof several quantized input planes.\n\nNote\n\nThe input quantization parameters propagate to the output.\n\nSee `AdaptiveAvgPool2d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or double-integer tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": "\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\nNote\n\nThe input quantization parameters propagate to the output.\n\nSee `AvgPool2d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.conv1d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv1d", "type": "Quantization", "text": "\nApplies a 1D convolution over a quantized 1D input composed of several input\nplanes.\n\nSee `Conv1d` for details and output shape.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.conv2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv2d", "type": "Quantization", "text": "\nApplies a 2D convolution over a quantized 2D input composed of several input\nplanes.\n\nSee `Conv2d` for details and output shape.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.conv3d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv3d", "type": "Quantization", "text": "\nApplies a 3D convolution over a quantized 3D input composed of several input\nplanes.\n\nSee `Conv3d` for details and output shape.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.hardswish()", "path": "torch.nn.quantized#torch.nn.quantized.functional.hardswish", "type": "Quantization", "text": "\nThis is the quantized version of `hardswish()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.interpolate()", "path": "torch.nn.quantized#torch.nn.quantized.functional.interpolate", "type": "Quantization", "text": "\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\nSee `torch.nn.functional.interpolate()` for implementation details.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nNote\n\nThe input quantization parameters propagate to the output.\n\nNote\n\nOnly 2D/3D input is supported for quantized inputs\n\nNote\n\nOnly the following modes are supported for the quantized inputs:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.linear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.linear", "type": "Quantization", "text": "\nApplies a linear transformation to the incoming quantized data: y=xAT+by =\nxA^T + b . See `Linear`\n\nNote\n\nCurrent implementation packs weights on every call, which has penalty on\nperformance. If you want to avoid the overhead, use `Linear`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.max_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": "\nApplies a 2D max pooling over a quantized input signal composed of several\nquantized input planes.\n\nNote\n\nThe input quantization parameters are propagated to the output.\n\nSee `MaxPool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.upsample()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample", "type": "Quantization", "text": "\nUpsamples the input to either the given `size` or the given `scale_factor`\n\nWarning\n\nThis function is deprecated in favor of\n`torch.nn.quantized.functional.interpolate()`. This is equivalent with\n`nn.quantized.functional.interpolate(...)`.\n\nSee `torch.nn.functional.interpolate()` for implementation details.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nNote\n\nThe input quantization parameters propagate to the output.\n\nNote\n\nOnly 2D input is supported for quantized inputs\n\nNote\n\nOnly the following modes are supported for the quantized inputs:\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`bilinear`)\ndon\u2019t proportionally align the output and input pixels, and thus the output\nvalues can depend on the input size. This was the default behavior for these\nmodes up to version 0.3.1. Since then, the default behavior is `align_corners\n= False`. See `Upsample` for concrete examples on how this affects the\noutputs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.upsample_bilinear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": "\nUpsamples the input, using bilinear upsampling.\n\nWarning\n\nThis function is deprecated in favor of\n`torch.nn.quantized.functional.interpolate()`. This is equivalent with\n`nn.quantized.functional.interpolate(..., mode='bilinear',\nalign_corners=True)`.\n\nNote\n\nThe input quantization parameters propagate to the output.\n\nNote\n\nOnly 2D inputs are supported\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.upsample_nearest()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": "\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\nWarning\n\nThis function is deprecated in favor of\n`torch.nn.quantized.functional.interpolate()`. This is equivalent with\n`nn.quantized.functional.interpolate(..., mode='nearest')`.\n\nNote\n\nThe input quantization parameters propagate to the output.\n\nNote\n\nOnly 2D inputs are supported\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.GroupNorm", "path": "torch.nn.quantized#torch.nn.quantized.GroupNorm", "type": "Quantization", "text": "\nThis is the quantized version of `GroupNorm`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Hardswish", "path": "torch.nn.quantized#torch.nn.quantized.Hardswish", "type": "Quantization", "text": "\nThis is the quantized version of `Hardswish`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.InstanceNorm1d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": "\nThis is the quantized version of `InstanceNorm1d`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.InstanceNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": "\nThis is the quantized version of `InstanceNorm2d`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.InstanceNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": "\nThis is the quantized version of `InstanceNorm3d`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.LayerNorm", "path": "torch.nn.quantized#torch.nn.quantized.LayerNorm", "type": "Quantization", "text": "\nThis is the quantized version of `LayerNorm`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Linear", "path": "torch.nn.quantized#torch.nn.quantized.Linear", "type": "Quantization", "text": "\nA quantized linear module with quantized tensor as inputs and outputs. We\nadopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\nSimilar to `Linear`, attributes will be randomly initialized at module\ncreation time and will be overwritten later\n\nExamples:\n\nCreate a quantized module from a float module or qparams_dict\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Linear.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Linear.from_float", "type": "Quantization", "text": "\nCreate a quantized module from a float module or qparams_dict\n\nmod (Module) \u2013 a float module, either produced by torch.quantization utilities\nor provided by the user\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.QFunctional", "path": "torch.nn.quantized#torch.nn.quantized.QFunctional", "type": "Quantization", "text": "\nWrapper class for quantized operations.\n\nThe instance of this class can be used instead of the `torch.ops.quantized`\nprefix. See example usage below.\n\nNote\n\nThis class does not provide a `forward` hook. Instead, you must use one of the\nunderlying functions (e.g. `add`).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.Quantize", "path": "torch.nn.quantized#torch.nn.quantized.Quantize", "type": "Quantization", "text": "\nQuantizes an incoming tensor\n\nzero_point, dtype (`scale`,) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.quantized.ReLU6", "path": "torch.nn.quantized#torch.nn.quantized.ReLU6", "type": "Quantization", "text": "\nApplies the element-wise function:\n\nReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6)) ,\nwhere x0x_0 is the zero_point, and q(6)q(6) is the quantized representation of\nnumber 6.\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReflectionPad1d", "path": "generated/torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d", "type": "torch.nn", "text": "\nPads the input tensor using the reflection of the input boundary.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 2-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} )\n\nOutput: (N,C,Wout)(N, C, W_{out}) where\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReflectionPad2d", "path": "generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d", "type": "torch.nn", "text": "\nPads the input tensor using the reflection of the input boundary.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 4-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} )\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) where\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReLU", "path": "generated/torch.nn.relu#torch.nn.ReLU", "type": "torch.nn", "text": "\nApplies the rectified linear unit function element-wise:\n\nReLU(x)=(x)+=max\u2061(0,x)\\text{ReLU}(x) = (x)^+ = \\max(0, x)\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReLU6", "path": "generated/torch.nn.relu6#torch.nn.ReLU6", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReplicationPad1d", "path": "generated/torch.nn.replicationpad1d#torch.nn.ReplicationPad1d", "type": "torch.nn", "text": "\nPads the input tensor using replication of the input boundary.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 2-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} )\n\nOutput: (N,C,Wout)(N, C, W_{out}) where\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReplicationPad2d", "path": "generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d", "type": "torch.nn", "text": "\nPads the input tensor using replication of the input boundary.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 4-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} )\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) where\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ReplicationPad3d", "path": "generated/torch.nn.replicationpad3d#torch.nn.ReplicationPad3d", "type": "torch.nn", "text": "\nPads the input tensor using replication of the input boundary.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 6-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} ,\npadding_front\\text{padding\\\\_front} , padding_back\\text{padding\\\\_back} )\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) where\n\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\\\_front} +\n\\text{padding\\\\_back}\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.RNN", "path": "generated/torch.nn.rnn#torch.nn.RNN", "type": "torch.nn", "text": "\nApplies a multi-layer Elman RNN with tanh\u2061\\tanh or ReLU\\text{ReLU} non-\nlinearity to an input sequence.\n\nFor each element in the input sequence, each layer computes the following\nfunction:\n\nwhere hth_t is the hidden state at time `t`, xtx_t is the input at time `t`,\nand h(t\u22121)h_{(t-1)} is the hidden state of the previous layer at time `t-1` or\nthe initial hidden state at time `0`. If `nonlinearity` is `'relu'`, then\nReLU\\text{ReLU} is used instead of tanh\u2061\\tanh .\n\noutput of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\ncontaining the output features (`h_t`) from the last layer of the RNN, for\neach `t`. If a `torch.nn.utils.rnn.PackedSequence` has been given as the\ninput, the output will also be a packed sequence.\n\nFor the unpacked case, the directions can be separated using\n`output.view(seq_len, batch, num_directions, hidden_size)`, with forward and\nbackward being direction `0` and `1` respectively. Similarly, the directions\ncan be separated in the packed case.\n\nh_n of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\ncontaining the hidden state for `t = seq_len`.\n\nLike output, the layers can be separated using `h_n.view(num_layers,\nnum_directions, batch, hidden_size)`.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nWarning\n\nThere are known non-determinism issues for RNN functions on some versions of\ncuDNN and CUDA. You can enforce deterministic behavior by setting the\nfollowing environment variables:\n\nOn CUDA 10.1, set environment variable `CUDA_LAUNCH_BLOCKING=1`. This may\naffect performance.\n\nOn CUDA 10.2 or later, set environment variable (note the leading colon\nsymbol) `CUBLAS_WORKSPACE_CONFIG=:16:8` or `CUBLAS_WORKSPACE_CONFIG=:4096:2`.\n\nSee the cuDNN 8 Release Notes for more information.\n\nNote\n\nIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data\nis on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5)\ninput data is not in `PackedSequence` format persistent algorithm can be\nselected to improve performance.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.RNNBase", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase", "type": "torch.nn", "text": "\nResets parameter data pointer so that they can use faster code paths.\n\nRight now, this works only if the module is on the GPU and cuDNN is enabled.\nOtherwise, it\u2019s a no-op.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.RNNBase.flatten_parameters()", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase.flatten_parameters", "type": "torch.nn", "text": "\nResets parameter data pointer so that they can use faster code paths.\n\nRight now, this works only if the module is on the GPU and cuDNN is enabled.\nOtherwise, it\u2019s a no-op.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.RNNCell", "path": "generated/torch.nn.rnncell#torch.nn.RNNCell", "type": "torch.nn", "text": "\nAn Elman RNN cell with tanh or ReLU non-linearity.\n\nIf `nonlinearity` is `\u2018relu\u2019`, then ReLU is used in place of tanh.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.RReLU", "path": "generated/torch.nn.rrelu#torch.nn.RReLU", "type": "torch.nn", "text": "\nApplies the randomized leaky rectified liner unit function, element-wise, as\ndescribed in the paper:\n\nEmpirical Evaluation of Rectified Activations in Convolutional Network.\n\nThe function is defined as:\n\nwhere aa is randomly sampled from uniform distribution\nU(lower,upper)\\mathcal{U}(\\text{lower}, \\text{upper}) .\n\nSee: https://arxiv.org/pdf/1505.00853.pdf\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.SELU", "path": "generated/torch.nn.selu#torch.nn.SELU", "type": "torch.nn", "text": "\nApplied element-wise, as:\n\nwith \u03b1=1.6732632423543772848170429916717\\alpha =\n1.6732632423543772848170429916717 and\nscale=1.0507009873554804934193349852946\\text{scale} =\n1.0507009873554804934193349852946 .\n\nMore details can be found in the paper Self-Normalizing Neural Networks .\n\ninplace (bool, optional) \u2013 can optionally do the operation in-place. Default:\n`False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Sequential", "path": "generated/torch.nn.sequential#torch.nn.Sequential", "type": "torch.nn", "text": "\nA sequential container. Modules will be added to it in the order they are\npassed in the constructor. Alternatively, an ordered dict of modules can also\nbe passed in.\n\nTo make it easier to understand, here is a small example:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Sigmoid", "path": "generated/torch.nn.sigmoid#torch.nn.Sigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.SiLU", "path": "generated/torch.nn.silu#torch.nn.SiLU", "type": "torch.nn", "text": "\nApplies the silu function, element-wise.\n\nNote\n\nSee Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit)\nwas originally coined, and see Sigmoid-Weighted Linear Units for Neural\nNetwork Function Approximation in Reinforcement Learning and Swish: a Self-\nGated Activation Function where the SiLU was experimented with later.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.SmoothL1Loss", "path": "generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss", "type": "torch.nn", "text": "\nCreates a criterion that uses a squared term if the absolute element-wise\nerror falls below beta and an L1 term otherwise. It is less sensitive to\noutliers than the `torch.nn.MSELoss` and in some cases prevents exploding\ngradients (e.g. see `Fast R-CNN` paper by Ross Girshick). Omitting a scaling\nfactor of `beta`, this loss is also known as the Huber loss:\n\nwhere ziz_{i} is given by:\n\nxx and yy arbitrary shapes with a total of nn elements each the sum operation\nstill operates over all the elements, and divides by nn .\n\n`beta` is an optional parameter that defaults to 1.\n\nNote: When `beta` is set to 0, this is equivalent to `L1Loss`. Passing a\nnegative value in for `beta` will result in an exception.\n\nThe division by nn can be avoided if sets `reduction = 'sum'`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.SoftMarginLoss", "path": "generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a two-class classification logistic loss\nbetween input tensor xx and target tensor yy (containing 1 or -1).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Softmax", "path": "generated/torch.nn.softmax#torch.nn.Softmax", "type": "torch.nn", "text": "\nApplies the Softmax function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range [0,1]\nand sum to 1.\n\nSoftmax is defined as:\n\nWhen the input Tensor is a sparse tensor then the unspecifed values are\ntreated as `-inf`.\n\na Tensor of the same dimension and shape as the input with values in the range\n[0, 1]\n\ndim (int) \u2013 A dimension along which Softmax will be computed (so every slice\nalong dim will sum to 1).\n\nNote\n\nThis module doesn\u2019t work directly with NLLLoss, which expects the Log to be\ncomputed between the Softmax and itself. Use `LogSoftmax` instead (it\u2019s faster\nand has better numerical properties).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Softmax2d", "path": "generated/torch.nn.softmax2d#torch.nn.Softmax2d", "type": "torch.nn", "text": "\nApplies SoftMax over features to each spatial location.\n\nWhen given an image of `Channels x Height x Width`, it will apply `Softmax` to\neach location (Channels,hi,wj)(Channels, h_i, w_j)\n\na Tensor of the same dimension and shape as the input with values in the range\n[0, 1]\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Softmin", "path": "generated/torch.nn.softmin#torch.nn.Softmin", "type": "torch.nn", "text": "\nApplies the Softmin function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range `[0,\n1]` and sum to 1.\n\nSoftmin is defined as:\n\ndim (int) \u2013 A dimension along which Softmin will be computed (so every slice\nalong dim will sum to 1).\n\na Tensor of the same dimension and shape as the input, with values in the\nrange [0, 1]\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Softplus", "path": "generated/torch.nn.softplus#torch.nn.Softplus", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nSoftPlus is a smooth approximation to the ReLU function and can be used to\nconstrain the output of a machine to always be positive.\n\nFor numerical stability the implementation reverts to the linear function when\ninput\u00d7\u03b2>thresholdinput \\times \\beta > threshold .\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Softshrink", "path": "generated/torch.nn.softshrink#torch.nn.Softshrink", "type": "torch.nn", "text": "\nApplies the soft shrinkage function elementwise:\n\nlambd \u2013 the \u03bb\\lambda (must be no less than zero) value for the Softshrink\nformulation. Default: 0.5\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Softsign", "path": "generated/torch.nn.softsign#torch.nn.Softsign", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.SyncBatchNorm", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm", "type": "torch.nn", "text": "\nApplies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D\ninputs with additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over all mini-\nbatches of the same process groups. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size `C` (where `C` is the input size). By default, the elements of\n\u03b3\\gamma are sampled from U(0,1)\\mathcal{U}(0, 1) and the elements of \u03b2\\beta\nare set to 0. The standard-deviation is calculated via the biased estimator,\nequivalent to `torch.var(input, unbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done for each channel in the `C` dimension,\ncomputing statistics on `(N, +)` slices, it\u2019s common terminology to call this\nVolumetric Batch Normalization or Spatio-temporal Batch Normalization.\n\nCurrently `SyncBatchNorm` only supports `DistributedDataParallel` (DDP) with\nsingle GPU per process. Use `torch.nn.SyncBatchNorm.convert_sync_batchnorm()`\nto convert `BatchNorm*D` layer to `SyncBatchNorm` before wrapping Network with\nDDP.\n\nExamples:\n\nHelper function to convert all `BatchNorm*D` layers in the model to\n`torch.nn.SyncBatchNorm` layers.\n\nThe original `module` with the converted `torch.nn.SyncBatchNorm` layers. If\nthe original `module` is a `BatchNorm*D` layer, a new `torch.nn.SyncBatchNorm`\nlayer object will be returned instead.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm()", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm.convert_sync_batchnorm", "type": "torch.nn", "text": "\nHelper function to convert all `BatchNorm*D` layers in the model to\n`torch.nn.SyncBatchNorm` layers.\n\nThe original `module` with the converted `torch.nn.SyncBatchNorm` layers. If\nthe original `module` is a `BatchNorm*D` layer, a new `torch.nn.SyncBatchNorm`\nlayer object will be returned instead.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Tanh", "path": "generated/torch.nn.tanh#torch.nn.Tanh", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Tanhshrink", "path": "generated/torch.nn.tanhshrink#torch.nn.Tanhshrink", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Threshold", "path": "generated/torch.nn.threshold#torch.nn.Threshold", "type": "torch.nn", "text": "\nThresholds each element of the input Tensor.\n\nThreshold is defined as:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Transformer", "path": "generated/torch.nn.transformer#torch.nn.Transformer", "type": "torch.nn", "text": "\nA transformer model. User is able to modify the attributes as needed. The\narchitecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish\nVaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\nIn Advances in Neural Information Processing Systems, pages 6000-6010. Users\ncan build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding\nparameters.\n\nNote: A full example to apply nn.Transformer module for the word language\nmodel is available in\nhttps://github.com/pytorch/examples/tree/master/word_language_model\n\nTake in and process masked source/target sequences.\n\nNote: [src/tgt/memory]_mask ensures that position i is allowed to attend the\nunmasked positions. If a ByteTensor is provided, the non-zero positions are\nnot allowed to attend while the zero positions will be unchanged. If a\nBoolTensor is provided, positions with `True` are not allowed to attend while\n`False` values will be unchanged. If a FloatTensor is provided, it will be\nadded to the attention weight. [src/tgt/memory]_key_padding_mask provides\nspecified elements in the key to be ignored by the attention. If a ByteTensor\nis provided, the non-zero positions will be ignored while the zero positions\nwill be unchanged. If a BoolTensor is provided, the positions with the value\nof `True` will be ignored while the position with the value of `False` will be\nunchanged.\n\nNote: Due to the multi-head attention architecture in the transformer model,\nthe output sequence length of a transformer is same as the input sequence\n(i.e. target) length of the decode.\n\nwhere S is the source sequence length, T is the target sequence length, N is\nthe batch size, E is the feature number\n\nGenerate a square mask for the sequence. The masked positions are filled with\nfloat(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Transformer.forward()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.forward", "type": "torch.nn", "text": "\nTake in and process masked source/target sequences.\n\nNote: [src/tgt/memory]_mask ensures that position i is allowed to attend the\nunmasked positions. If a ByteTensor is provided, the non-zero positions are\nnot allowed to attend while the zero positions will be unchanged. If a\nBoolTensor is provided, positions with `True` are not allowed to attend while\n`False` values will be unchanged. If a FloatTensor is provided, it will be\nadded to the attention weight. [src/tgt/memory]_key_padding_mask provides\nspecified elements in the key to be ignored by the attention. If a ByteTensor\nis provided, the non-zero positions will be ignored while the zero positions\nwill be unchanged. If a BoolTensor is provided, the positions with the value\nof `True` will be ignored while the position with the value of `False` will be\nunchanged.\n\nNote: Due to the multi-head attention architecture in the transformer model,\nthe output sequence length of a transformer is same as the input sequence\n(i.e. target) length of the decode.\n\nwhere S is the source sequence length, T is the target sequence length, N is\nthe batch size, E is the feature number\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Transformer.generate_square_subsequent_mask()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.generate_square_subsequent_mask", "type": "torch.nn", "text": "\nGenerate a square mask for the sequence. The masked positions are filled with\nfloat(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoder", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder", "type": "torch.nn", "text": "\nTransformerDecoder is a stack of N decoder layers\n\nPass the inputs (and mask) through the decoder layer in turn.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoder.forward()", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder.forward", "type": "torch.nn", "text": "\nPass the inputs (and mask) through the decoder layer in turn.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoderLayer", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer", "type": "torch.nn", "text": "\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and\nfeedforward network. This standard decoder layer is based on the paper\n\u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n2017. Attention is all you need. In Advances in Neural Information Processing\nSystems, pages 6000-6010. Users may modify or implement in a different way\nduring application.\n\nPass the inputs (and mask) through the decoder layer.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoderLayer.forward()", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer.forward", "type": "torch.nn", "text": "\nPass the inputs (and mask) through the decoder layer.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoder", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder", "type": "torch.nn", "text": "\nTransformerEncoder is a stack of N encoder layers\n\nPass the input through the encoder layers in turn.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoder.forward()", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder.forward", "type": "torch.nn", "text": "\nPass the input through the encoder layers in turn.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoderLayer", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer", "type": "torch.nn", "text": "\nTransformerEncoderLayer is made up of self-attn and feedforward network. This\nstandard encoder layer is based on the paper \u201cAttention Is All You Need\u201d.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\nN Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\nIn Advances in Neural Information Processing Systems, pages 6000-6010. Users\nmay modify or implement in a different way during application.\n\nPass the input through the encoder layer.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoderLayer.forward()", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer.forward", "type": "torch.nn", "text": "\nPass the input through the encoder layer.\n\nsee the docs in Transformer class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TripletMarginLoss", "path": "generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the triplet loss given an input tensors x1x1\n, x2x2 , x3x3 and a margin with a value greater than 00 . This is used for\nmeasuring a relative similarity between samples. A triplet is composed by `a`,\n`p` and `n` (i.e., `anchor`, `positive examples` and `negative examples`\nrespectively). The shapes of all input tensors should be (N,D)(N, D) .\n\nThe distance swap is described in detail in the paper Learning shallow\nconvolutional feature descriptors with triplet losses by V. Balntas, E. Riba\net al.\n\nThe loss function for each sample in the mini-batch is:\n\nwhere\n\nSee also `TripletMarginWithDistanceLoss`, which computes the triplet margin\nloss for input tensors using a custom distance function.\n\notherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.TripletMarginWithDistanceLoss", "path": "generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the triplet loss given input tensors aa , pp\n, and nn (representing anchor, positive, and negative examples, respectively),\nand a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute\nthe relationship between the anchor and positive example (\u201cpositive distance\u201d)\nand the anchor and negative example (\u201cnegative distance\u201d).\n\nThe unreduced loss (i.e., with `reduction` set to `'none'`) can be described\nas:\n\nwhere NN is the batch size; dd is a nonnegative, real-valued function\nquantifying the closeness of two tensors, referred to as the\n`distance_function`; and marginmargin is a nonnegative margin representing the\nminimum difference between the positive and negative distances that is\nrequired for the loss to be 0. The input tensors have NN elements each and can\nbe of any shape that the distance function can handle.\n\nIf `reduction` is not `'none'` (default `'mean'`), then:\n\nSee also `TripletMarginLoss`, which computes the triplet loss for input\ntensors using the lpl_p distance as the distance function.\n\nExamples:\n\nV. Balntas, et al.: Learning shallow convolutional feature descriptors with\ntriplet losses: http://www.bmva.org/bmvc/2016/papers/paper119/index.html\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten", "type": "torch.nn", "text": "\nUnflattens a tensor dim expanding it to a desired shape. For use with\n`Sequential`.\n\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.add_module()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.apply()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.bfloat16()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.cpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.cuda()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.double()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.eval()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.float()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.half()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.load_state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_buffer()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_forward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_forward_pre_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_full_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_parameter()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.requires_grad_()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.to()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.train()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.type()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.xpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.zero_grad()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Unfold", "path": "generated/torch.nn.unfold#torch.nn.Unfold", "type": "torch.nn", "text": "\nExtracts sliding local blocks from a batched input tensor.\n\nConsider a batched `input` tensor of shape (N,C,\u2217)(N, C, *) , where NN is the\nbatch dimension, CC is the channel dimension, and \u2217* represent arbitrary\nspatial dimensions. This operation flattens each sliding `kernel_size`-sized\nblock within the spatial dimensions of `input` into a column (i.e., last\ndimension) of a 3-D `output` tensor of shape (N,C\u00d7\u220f(kernel_size),L)(N, C\n\\times \\prod(\\text{kernel\\\\_size}), L) , where C\u00d7\u220f(kernel_size)C \\times\n\\prod(\\text{kernel\\\\_size}) is the total number of values within each block (a\nblock has \u220f(kernel_size)\\prod(\\text{kernel\\\\_size}) spatial locations each\ncontaining a CC -channeled vector), and LL is the total number of such blocks:\n\nwhere spatial_size\\text{spatial\\\\_size} is formed by the spatial dimensions of\n`input` (\u2217* above), and dd is over all spatial dimensions.\n\nTherefore, indexing `output` at the last dimension (column dimension) gives\nall values within a certain block.\n\nThe `padding`, `stride` and `dilation` arguments specify how the sliding\nblocks are retrieved.\n\nNote\n\n`Fold` calculates each combined value in the resulting large tensor by summing\nall values from all containing blocks. `Unfold` extracts the values in the\nlocal blocks by copying from the large tensor. So, if the blocks overlap, they\nare not inverses of each other.\n\nIn general, folding and unfolding operations are related as follows. Consider\n`Fold` and `Unfold` instances created with the same parameters:\n\nThen for any (supported) `input` tensor the following equality holds:\n\nwhere `divisor` is a tensor that depends only on the shape and dtype of the\n`input`:\n\nWhen the `divisor` tensor contains no zero elements, then `fold` and `unfold`\noperations are inverses of each other (up to constant divisor).\n\nWarning\n\nCurrently, only 4-D input tensors (batched image-like tensors) are supported.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Upsample", "path": "generated/torch.nn.upsample#torch.nn.Upsample", "type": "torch.nn", "text": "\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric)\ndata.\n\nThe input data is assumed to be of the form `minibatch x channels x [optional\ndepth] x [optional height] x width`. Hence, for spatial inputs, we expect a 4D\nTensor and for volumetric inputs, we expect a 5D Tensor.\n\nThe algorithms available for upsampling are nearest neighbor and linear,\nbilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively.\n\nOne can either give a `scale_factor` or the target output `size` to calculate\nthe output size. (You cannot give both, as it is ambiguous)\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, `bicubic`, and `trilinear`) don\u2019t proportionally align the output\nand input pixels, and thus the output values can depend on the input size.\nThis was the default behavior for these modes up to version 0.3.1. Since then,\nthe default behavior is `align_corners = False`. See below for concrete\nexamples on how this affects the outputs.\n\nNote\n\nIf you want downsampling/general resizing, you should use `interpolate()`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.UpsamplingBilinear2d", "path": "generated/torch.nn.upsamplingbilinear2d#torch.nn.UpsamplingBilinear2d", "type": "torch.nn", "text": "\nApplies a 2D bilinear upsampling to an input signal composed of several input\nchannels.\n\nTo specify the scale, it takes either the `size` or the `scale_factor` as it\u2019s\nconstructor argument.\n\nWhen `size` is given, it is the output size of the image `(h, w)`.\n\nWarning\n\nThis class is deprecated in favor of `interpolate()`. It is equivalent to\n`nn.functional.interpolate(..., mode='bilinear', align_corners=True)`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.UpsamplingNearest2d", "path": "generated/torch.nn.upsamplingnearest2d#torch.nn.UpsamplingNearest2d", "type": "torch.nn", "text": "\nApplies a 2D nearest neighbor upsampling to an input signal composed of\nseveral input channels.\n\nTo specify the scale, it takes either the `size` or the `scale_factor` as it\u2019s\nconstructor argument.\n\nWhen `size` is given, it is the output size of the image `(h, w)`.\n\nWarning\n\nThis class is deprecated in favor of `interpolate()`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.clip_grad_norm_()", "path": "generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_", "type": "torch.nn", "text": "\nClips gradient norm of an iterable of parameters.\n\nThe norm is computed over all gradients together, as if they were concatenated\ninto a single vector. Gradients are modified in-place.\n\nTotal norm of the parameters (viewed as a single vector).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.clip_grad_value_()", "path": "generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_", "type": "torch.nn", "text": "\nClips gradient of an iterable of parameters at specified value.\n\nGradients are modified in-place.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.parameters_to_vector()", "path": "generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector", "type": "torch.nn", "text": "\nConvert parameters to one vector\n\nparameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters\nof a model.\n\nThe parameters represented by a single vector\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod", "type": "torch.nn", "text": "\nAbstract base class for creation of new pruning techniques.\n\nProvides a skeleton for customization requiring the overriding of methods such\nas `compute_mask()` and `apply()`.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a random mask to apply on top of the `default_mask`\naccording to the specific pruning method recipe.\n\nmask to apply to `t`, of same dims as `t`\n\nmask (torch.Tensor)\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.compute_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.compute_mask", "type": "torch.nn", "text": "\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a random mask to apply on top of the `default_mask`\naccording to the specific pruning method recipe.\n\nmask to apply to `t`, of same dims as `t`\n\nmask (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.prune()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.remove()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.apply()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.apply_mask()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.prune()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.remove()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.custom_from_mask()", "path": "generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by applying\nthe pre-computed mask in `mask`. Modifies module in place (and also return the\nmodified module) by: 1) adding a named buffer called `name+'_mask'`\ncorresponding to the binary mask applied to the parameter `name` by the\npruning method. 2) replacing the parameter `name` by its pruned version, while\nthe original (unpruned) parameter is stored in a new parameter named\n`name+'_orig'`.\n\nmodified (i.e. pruned) version of the input module\n\nmodule (nn.Module)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.global_unstructured()", "path": "generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured", "type": "torch.nn", "text": "\nGlobally prunes tensors corresponding to all parameters in `parameters` by\napplying the specified `pruning_method`. Modifies modules in place by: 1)\nadding a named buffer called `name+'_mask'` corresponding to the binary mask\napplied to the parameter `name` by the pruning method. 2) replacing the\nparameter `name` by its pruned version, while the original (unpruned)\nparameter is stored in a new parameter named `name+'_orig'`.\n\nTypeError \u2013 if `PRUNING_TYPE != 'unstructured'`\n\nNote\n\nSince global structured pruning doesn\u2019t make much sense unless the norm is\nnormalized by the size of the parameter, we now limit the scope of global\npruning to unstructured methods.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity", "type": "torch.nn", "text": "\nUtility pruning method that does not prune any units but generates the pruning\nparametrization with a mask of ones.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.apply()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.apply_mask()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.prune()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.remove()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.is_pruned()", "path": "generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned", "type": "torch.nn", "text": "\nCheck whether `module` is pruned by looking for `forward_pre_hooks` in its\nmodules that inherit from the `BasePruningMethod`.\n\nmodule (nn.Module) \u2013 object that is either pruned or unpruned\n\nbinary answer to whether `module` is pruned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured", "type": "torch.nn", "text": "\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the\nlowest L1-norm.\n\namount (int or float) \u2013 quantity of parameters to prune. If `float`, should be\nbetween 0.0 and 1.0 and represent the fraction of parameters to prune. If\n`int`, it represents the absolute number of parameters to prune.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.apply()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.prune()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.remove()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.l1_unstructured()", "path": "generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units with the lowest L1-norm.\nModifies module in place (and also return the modified module) by: 1) adding a\nnamed buffer called `name+'_mask'` corresponding to the binary mask applied to\nthe parameter `name` by the pruning method. 2) replacing the parameter `name`\nby its pruned version, while the original (unpruned) parameter is stored in a\nnew parameter named `name+'_orig'`.\n\nmodified (i.e. pruned) version of the input module\n\nmodule (nn.Module)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured", "type": "torch.nn", "text": "\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a mask to apply on top of the `default_mask` by zeroing\nout the channels along the specified dim with the lowest Ln-norm.\n\nmask to apply to `t`, of same dims as `t`\n\nmask (torch.Tensor)\n\nIndexError \u2013 if `self.dim >= len(t.shape)`\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.apply()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.compute_mask", "type": "torch.nn", "text": "\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a mask to apply on top of the `default_mask` by zeroing\nout the channels along the specified dim with the lowest Ln-norm.\n\nmask to apply to `t`, of same dims as `t`\n\nmask (torch.Tensor)\n\nIndexError \u2013 if `self.dim >= len(t.shape)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.prune()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.remove()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.ln_structured()", "path": "generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` with the lowest L``n``-norm. Modifies module in place (and also return\nthe modified module) by: 1) adding a named buffer called `name+'_mask'`\ncorresponding to the binary mask applied to the parameter `name` by the\npruning method. 2) replacing the parameter `name` by its pruned version, while\nthe original (unpruned) parameter is stored in a new parameter named\n`name+'_orig'`.\n\nmodified (i.e. pruned) version of the input module\n\nmodule (nn.Module)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer", "type": "torch.nn", "text": "\nContainer holding a sequence of pruning methods for iterative pruning. Keeps\ntrack of the order in which pruning methods are applied and handles combining\nsuccessive pruning calls.\n\nAccepts as argument an instance of a BasePruningMethod or an iterable of them.\n\nAdds a child pruning `method` to the container.\n\nmethod (subclass of BasePruningMethod) \u2013 child pruning method to be added to\nthe container.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nApplies the latest `method` by computing the new partial masks and returning\nits combination with the `default_mask`. The new partial mask should be\ncomputed on the entries or channels that were not zeroed out by the\n`default_mask`. Which portions of the tensor `t` the new mask will be\ncalculated from depends on the `PRUNING_TYPE` (handled by the type handler):\n\nnew mask that combines the effects of the `default_mask` and the new mask from\nthe current pruning `method` (of same dimensions as `default_mask` and `t`).\n\nmask (torch.Tensor)\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.add_pruning_method()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.add_pruning_method", "type": "torch.nn", "text": "\nAdds a child pruning `method` to the container.\n\nmethod (subclass of BasePruningMethod) \u2013 child pruning method to be added to\nthe container.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.apply()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.apply_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.compute_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.compute_mask", "type": "torch.nn", "text": "\nApplies the latest `method` by computing the new partial masks and returning\nits combination with the `default_mask`. The new partial mask should be\ncomputed on the entries or channels that were not zeroed out by the\n`default_mask`. Which portions of the tensor `t` the new mask will be\ncalculated from depends on the `PRUNING_TYPE` (handled by the type handler):\n\nnew mask that combines the effects of the `default_mask` and the new mask from\nthe current pruning `method` (of same dimensions as `default_mask` and `t`).\n\nmask (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.prune()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.remove()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured", "type": "torch.nn", "text": "\nPrune entire (currently unpruned) channels in a tensor at random.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a random mask to apply on top of the `default_mask` by\nrandomly zeroing out channels along the specified dim of the tensor.\n\nmask to apply to `t`, of same dims as `t`\n\nmask (torch.Tensor)\n\nIndexError \u2013 if `self.dim >= len(t.shape)`\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.apply()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.compute_mask", "type": "torch.nn", "text": "\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a random mask to apply on top of the `default_mask` by\nrandomly zeroing out channels along the specified dim of the tensor.\n\nmask to apply to `t`, of same dims as `t`\n\nmask (torch.Tensor)\n\nIndexError \u2013 if `self.dim >= len(t.shape)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.prune()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.remove()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured", "type": "torch.nn", "text": "\nPrune (currently unpruned) units in a tensor at random.\n\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\nmodule (nn.Module) \u2013 module containing the tensor to prune\n\npruned version of the input tensor\n\npruned_tensor (torch.Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.prune()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\npruned version of tensor `t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.remove()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.random_structured()", "path": "generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` selected at random. Modifies module in place (and also return the\nmodified module) by: 1) adding a named buffer called `name+'_mask'`\ncorresponding to the binary mask applied to the parameter `name` by the\npruning method. 2) replacing the parameter `name` by its pruned version, while\nthe original (unpruned) parameter is stored in a new parameter named\n`name+'_orig'`.\n\nmodified (i.e. pruned) version of the input module\n\nmodule (nn.Module)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.random_unstructured()", "path": "generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units selected at random.\nModifies module in place (and also return the modified module) by: 1) adding a\nnamed buffer called `name+'_mask'` corresponding to the binary mask applied to\nthe parameter `name` by the pruning method. 2) replacing the parameter `name`\nby its pruned version, while the original (unpruned) parameter is stored in a\nnew parameter named `name+'_orig'`.\n\nmodified (i.e. pruned) version of the input module\n\nmodule (nn.Module)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.remove()", "path": "generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module and the pruning method\nfrom the forward hook. The pruned parameter named `name` remains permanently\npruned, and the parameter named `name+'_orig'` is removed from the parameter\nlist. Similarly, the buffer named `name+'_mask'` is removed from the buffers.\n\nNote\n\nPruning itself is NOT undone or reversed!\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.remove_spectral_norm()", "path": "generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm", "type": "torch.nn", "text": "\nRemoves the spectral normalization reparameterization from a module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.remove_weight_norm()", "path": "generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm", "type": "torch.nn", "text": "\nRemoves the weight normalization reparameterization from a module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence", "type": "torch.nn", "text": "\nHolds the data and list of `batch_sizes` of a packed sequence.\n\nAll RNN modules accept packed sequences as inputs.\n\nNote\n\nInstances of this class should never be created manually. They are meant to be\ninstantiated by functions like `pack_padded_sequence()`.\n\nBatch sizes represent the number elements at each sequence step in the batch,\nnot the varying sequence lengths passed to `pack_padded_sequence()`. For\ninstance, given data `abc` and `x` the `PackedSequence` would contain data\n`axbc` with `batch_sizes=[2,1,1]`.\n\nNote\n\n`data` can be on arbitrary device and of arbitrary dtype. `sorted_indices` and\n`unsorted_indices` must be `torch.int64` tensors on the same device as `data`.\n\nHowever, `batch_sizes` should always be a CPU `torch.int64` tensor.\n\nThis invariant is maintained throughout `PackedSequence` class, and all\nfunctions that construct a `:class:PackedSequence` in PyTorch (i.e., they only\npass in tensors conforming to this constraint).\n\nAlias for field number 1\n\nReturn number of occurrences of value.\n\nAlias for field number 0\n\nReturn first index of value.\n\nRaises ValueError if the value is not present.\n\nReturns true if `self.data` stored on a gpu\n\nReturns true if `self.data` stored on in pinned memory\n\nAlias for field number 2\n\nPerforms dtype and/or device conversion on `self.data`.\n\nIt has similar signature as `torch.Tensor.to()`, except optional arguments\nlike `non_blocking` and `copy` should be passed as kwargs, not args, or they\nwill not apply to the index tensors.\n\nNote\n\nIf the `self.data` Tensor already has the correct `torch.dtype` and\n`torch.device`, then `self` is returned. Otherwise, returns a copy with the\ndesired configuration.\n\nAlias for field number 3\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.batch_sizes()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.batch_sizes", "type": "torch.nn", "text": "\nAlias for field number 1\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.count()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.count", "type": "torch.nn", "text": "\nReturn number of occurrences of value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.data()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.data", "type": "torch.nn", "text": "\nAlias for field number 0\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.index()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.index", "type": "torch.nn", "text": "\nReturn first index of value.\n\nRaises ValueError if the value is not present.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.is_cuda()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_cuda", "type": "torch.nn", "text": "\nReturns true if `self.data` stored on a gpu\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.is_pinned()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_pinned", "type": "torch.nn", "text": "\nReturns true if `self.data` stored on in pinned memory\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.sorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.sorted_indices", "type": "torch.nn", "text": "\nAlias for field number 2\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.to()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.to", "type": "torch.nn", "text": "\nPerforms dtype and/or device conversion on `self.data`.\n\nIt has similar signature as `torch.Tensor.to()`, except optional arguments\nlike `non_blocking` and `copy` should be passed as kwargs, not args, or they\nwill not apply to the index tensors.\n\nNote\n\nIf the `self.data` Tensor already has the correct `torch.dtype` and\n`torch.device`, then `self` is returned. Otherwise, returns a copy with the\ndesired configuration.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.unsorted_indices", "type": "torch.nn", "text": "\nAlias for field number 3\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pack_padded_sequence()", "path": "generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence", "type": "torch.nn", "text": "\nPacks a Tensor containing padded sequences of variable length.\n\n`input` can be of size `T x B x *` where `T` is the length of the longest\nsequence (equal to `lengths[0]`), `B` is the batch size, and `*` is any number\nof dimensions (including 0). If `batch_first` is `True`, `B x T x *` `input`\nis expected.\n\nFor unsorted sequences, use `enforce_sorted = False`. If `enforce_sorted` is\n`True`, the sequences should be sorted by length in a decreasing order, i.e.\n`input[:,0]` should be the longest sequence, and `input[:,B-1]` the shortest\none. `enforce_sorted = True` is only necessary for ONNX export.\n\nNote\n\nThis function accepts any input that has at least two dimensions. You can\napply it to pack the labels, and use the output of the RNN with them to\ncompute the loss directly. A Tensor can be retrieved from a `PackedSequence`\nobject by accessing its `.data` attribute.\n\na `PackedSequence` object\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pack_sequence()", "path": "generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence", "type": "torch.nn", "text": "\nPacks a list of variable length Tensors\n\n`sequences` should be a list of Tensors of size `L x *`, where `L` is the\nlength of a sequence and `*` is any number of trailing dimensions, including\nzero.\n\nFor unsorted sequences, use `enforce_sorted = False`. If `enforce_sorted` is\n`True`, the sequences should be sorted in the order of decreasing length.\n`enforce_sorted = True` is only necessary for ONNX export.\n\na `PackedSequence` object\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pad_packed_sequence()", "path": "generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence", "type": "torch.nn", "text": "\nPads a packed batch of variable length sequences.\n\nIt is an inverse operation to `pack_padded_sequence()`.\n\nThe returned Tensor\u2019s data will be of size `T x B x *`, where `T` is the\nlength of the longest sequence and `B` is the batch size. If `batch_first` is\nTrue, the data will be transposed into `B x T x *` format.\n\nNote\n\n`total_length` is useful to implement the `pack sequence -> recurrent network\n-> unpack sequence` pattern in a `Module` wrapped in `DataParallel`. See this\nFAQ section for details.\n\nTuple of Tensor containing the padded sequence, and a Tensor containing the\nlist of lengths of each sequence in the batch. Batch elements will be re-\nordered as they were ordered originally when the batch was passed to\n`pack_padded_sequence` or `pack_sequence`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pad_sequence()", "path": "generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence", "type": "torch.nn", "text": "\nPad a list of variable length Tensors with `padding_value`\n\n`pad_sequence` stacks a list of Tensors along a new dimension, and pads them\nto equal length. For example, if the input is list of sequences with size `L x\n*` and if batch_first is False, and `T x B x *` otherwise.\n\n`B` is batch size. It is equal to the number of elements in `sequences`. `T`\nis length of the longest sequence. `L` is length of the sequence. `*` is any\nnumber of trailing dimensions, including none.\n\nNote\n\nThis function returns a Tensor of size `T x B x *` or `B x T x *` where `T` is\nthe length of the longest sequence. This function assumes trailing dimensions\nand type of all the Tensors in sequences are same.\n\nTensor of size `T x B x *` if `batch_first` is `False`. Tensor of size `B x T\nx *` otherwise\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.spectral_norm()", "path": "generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm", "type": "torch.nn", "text": "\nApplies spectral normalization to a parameter in the given module.\n\nSpectral normalization stabilizes the training of discriminators (critics) in\nGenerative Adversarial Networks (GANs) by rescaling the weight tensor with\nspectral norm \u03c3\\sigma of the weight matrix calculated using power iteration\nmethod. If the dimension of the weight tensor is greater than 2, it is\nreshaped to 2D in power iteration method to get spectral norm. This is\nimplemented via a hook that calculates spectral norm and rescales weight\nbefore every `forward()` call.\n\nSee Spectral Normalization for Generative Adversarial Networks .\n\nThe original module with the spectral norm hook\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.vector_to_parameters()", "path": "generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters", "type": "torch.nn", "text": "\nConvert one vector to the parameters\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.utils.weight_norm()", "path": "generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm", "type": "torch.nn", "text": "\nApplies weight normalization to a parameter in the given module.\n\nWeight normalization is a reparameterization that decouples the magnitude of a\nweight tensor from its direction. This replaces the parameter specified by\n`name` (e.g. `'weight'`) with two parameters: one specifying the magnitude\n(e.g. `'weight_g'`) and one specifying the direction (e.g. `'weight_v'`).\nWeight normalization is implemented via a hook that recomputes the weight\ntensor from the magnitude and direction before every `forward()` call.\n\nBy default, with `dim=0`, the norm is computed independently per output\nchannel/plane. To compute a norm over the entire weight tensor, use\n`dim=None`.\n\nSee https://arxiv.org/abs/1602.07868\n\nThe original module with the weight norm hook\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ZeroPad2d", "path": "generated/torch.nn.zeropad2d#torch.nn.ZeroPad2d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with zero.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 4-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} )\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) where\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nonzero()", "path": "generated/torch.nonzero#torch.nonzero", "type": "torch", "text": "\nNote\n\n`torch.nonzero(..., as_tuple=False)` (default) returns a 2-D tensor where each\nrow is the index for a nonzero value.\n\n`torch.nonzero(..., as_tuple=True)` returns a tuple of 1-D index tensors,\nallowing for advanced indexing, so `x[x.nonzero(as_tuple=True)]` gives all\nnonzero values of tensor `x`. Of the returned tuple, each index tensor\ncontains nonzero indices for a certain dimension.\n\nSee below for more details on the two behaviors.\n\nWhen `input` is on CUDA, `torch.nonzero()` causes host-device synchronization.\n\nWhen `as_tuple` is ``False`` (default):\n\nReturns a tensor containing the indices of all non-zero elements of `input`.\nEach row in the result contains the indices of a non-zero element in `input`.\nThe result is sorted lexicographically, with the last index changing the\nfastest (C-style).\n\nIf `input` has nn dimensions, then the resulting indices tensor `out` is of\nsize (z\u00d7n)(z \\times n) , where zz is the total number of non-zero elements in\nthe `input` tensor.\n\nWhen `as_tuple` is ``True``:\n\nReturns a tuple of 1-D tensors, one for each dimension in `input`, each\ncontaining the indices (in that dimension) of all non-zero elements of `input`\n.\n\nIf `input` has nn dimensions, then the resulting tuple contains nn tensors of\nsize zz , where zz is the total number of non-zero elements in the `input`\ntensor.\n\nAs a special case, when `input` has zero dimensions and a nonzero scalar\nvalue, it is treated as a one-dimensional tensor with one element.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (LongTensor, optional) \u2013 the output tensor containing indices\n\nIf `as_tuple` is `False`, the output tensor containing indices. If `as_tuple`\nis `True`, one 1-D tensor for each dimension, containing the indices of each\nnonzero element along that dimension.\n\nLongTensor or tuple of LongTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.norm()", "path": "generated/torch.norm#torch.norm", "type": "torch", "text": "\nReturns the matrix norm or vector norm of a given tensor.\n\nWarning\n\ntorch.norm is deprecated and may be removed in a future PyTorch release. Use\n`torch.linalg.norm()` instead, but note that `torch.linalg.norm()` has a\ndifferent signature and slightly different behavior that is more consistent\nwith NumPy\u2019s numpy.linalg.norm.\n\np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nthe order of norm. Default: `'fro'` The following norms can be calculated:\n\nord\n\nmatrix norm\n\nvector norm\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013\n\nNumber\n\n\u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nThe vector norm can be calculated across any number of dimensions. The\ncorresponding dimensions of `input` are flattened into one dimension, and the\nnorm is calculated on the flattened dimension.\n\nFrobenius norm produces the same result as `p=2` in all cases except when\n`dim` is a list of three or more dims, in which case Frobenius norm throws an\nerror.\n\nNuclear norm can only be calculated across exactly two dimensions.\n\nNote\n\nEven though `p='fro'` supports any number of dimensions, the true mathematical\ndefinition of Frobenius norm only applies to tensors with exactly two\ndimensions. `torch.linalg.norm()` with `ord='fro'` aligns with the\nmathematical definition, since it can only be applied across exactly two\ndimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.normal()", "path": "generated/torch.normal#torch.normal", "type": "torch", "text": "\nReturns a tensor of random numbers drawn from separate normal distributions\nwhose mean and standard deviation are given.\n\nThe `mean` is a tensor with the mean of each output element\u2019s normal\ndistribution\n\nThe `std` is a tensor with the standard deviation of each output element\u2019s\nnormal distribution\n\nThe shapes of `mean` and `std` don\u2019t need to match, but the total number of\nelements in each tensor need to be the same.\n\nNote\n\nWhen the shapes do not match, the shape of `mean` is used as the shape for the\nreturned output tensor\n\nExample:\n\nSimilar to the function above, but the means are shared among all drawn\nelements.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nSimilar to the function above, but the standard-deviations are shared among\nall drawn elements.\n\nout (Tensor, optional) \u2013 the output tensor\n\nExample:\n\nSimilar to the function above, but the means and standard deviations are\nshared among all drawn elements. The resulting tensor has size given by\n`size`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.not_equal()", "path": "generated/torch.not_equal#torch.not_equal", "type": "torch", "text": "\nAlias for `torch.ne()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.no_grad", "path": "generated/torch.no_grad#torch.no_grad", "type": "torch", "text": "\nContext-manager that disabled gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call `Tensor.backward()`. It will reduce memory consumption for\ncomputations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have `requires_grad=False`,\neven when the inputs have `requires_grad=True`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.numel()", "path": "generated/torch.numel#torch.numel", "type": "torch", "text": "\nReturns the total number of elements in the `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ones()", "path": "generated/torch.ones#torch.ones", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `1`, with the shape defined by\nthe variable argument `size`.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor. Can be a variable number of arguments or a collection like a list or\ntuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ones_like()", "path": "generated/torch.ones_like#torch.ones_like", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `1`, with the same size as\n`input`. `torch.ones_like(input)` is equivalent to `torch.ones(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\nWarning\n\nAs of 0.4, this function does not support an `out` keyword. As an alternative,\nthe old `torch.ones_like(input, out=output)` is equivalent to\n`torch.ones(input.size(), out=output)`.\n\ninput (Tensor) \u2013 the size of `input` will determine size of the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx", "path": "onnx", "type": "torch.onnx", "text": "\nIndexing\n\nAdding support for operators\n\nOperator Export Type\n\nHere is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX. It runs a single round of inference and then saves the\nresulting traced model to `alexnet.onnx`:\n\nThe resulting `alexnet.onnx` is a binary protobuf file which contains both the\nnetwork structure and parameters of the model you exported (in this case,\nAlexNet). The keyword argument `verbose=True` causes the exporter to print out\na human-readable representation of the network:\n\nYou can also verify the protobuf using the ONNX library. You can install\n`ONNX` with conda:\n\nThen, you can run:\n\nTo run the exported script with caffe2, you will need to install `caffe2`: If\nyou don\u2019t have one already, Please follow the install instructions.\n\nOnce these are installed, you can use the backend for Caffe2:\n\nYou can also run the exported model with ONNX Runtime, you will need to\ninstall `ONNX Runtime`: please follow these instructions.\n\nOnce these are installed, you can use the backend for ONNX Runtime:\n\nHere is another tutorial of exporting the SuperResolution model to ONNX..\n\nIn the future, there will be backends for other frameworks as well.\n\nThe ONNX exporter can be both trace-based and script-based exporter.\n\nWe allow mixing tracing and scripting. You can compose tracing and scripting\nto suit the particular requirements of a part of a model. Checkout this\nexample:\n\nWith trace-based exporter, we get the result ONNX graph which unrolls the for\nloop:\n\nTo utilize script-based exporter for capturing the dynamic loop, we can write\nthe loop in script, and call it from the regular nn.Module:\n\nNow the exported ONNX graph becomes:\n\nThe dynamic control flow is captured correctly. We can verify in backends with\ndifferent loop range.\n\nTo avoid exporting a variable scalar tensor as a fixed value constant as part\nof the ONNX model, please avoid use of `torch.Tensor.item()`. Torch supports\nimplicit cast of single-element tensors to numbers. E.g.:\n\nPyTorch models can be written using numpy manipulations, but this is not\nproper when we convert to the ONNX model. For the trace-based exporter,\ntracing treats the numpy values as the constant node, therefore it calculates\nthe wrong result if we change the input. So the PyTorch model need implement\nusing torch operators. For example, do not use numpy operators on numpy\ntensors:\n\ndo not convert to numpy types:\n\nAlways use torch tensors and torch operators: torch.concat, etc. In addition,\nDropout layer need defined in init function so that inferencing can handle it\nproperly, i.e.,\n\nThere are two ways to handle models which consist of named parameters or\nkeyword arguments as inputs:\n\nFor example, in the model:\n\nThere are two ways of exporting the model:\n\nNot using a dictionary for the keyword arguments and passing all the inputs in\nthe same order as required by the model\n\nUsing a dictionary to represent the keyword arguments. This dictionary is\nalways passed in addition to the non-keyword arguments and is always the last\nargument in the args tuple.\n\nFor cases in which there are no keyword arguments, models can be exported with\neither an empty or no dictionary. For example,\n\nAn exception to this rule are cases in which the last input is also of a\ndictionary type. In these cases it is mandatory to have an empty dictionary as\nthe last argument in the args tuple. For example,\n\nWithout the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named\narguments. In order to prevent this from being an issue a constraint is placed\nto provide an empty dictionary as the last input in the tuple args in such\ncases. The new call would look like this.\n\nTensor indexing in PyTorch is very flexible and complicated. There are two\ncategories of indexing. Both are largely supported in exporting today. If you\nare experiencing issues exporting indexing that belongs to the supported\npatterns below, please double check that you are exporting with the latest\nopset (opset_version=12).\n\nThis type of indexing occurs on the RHS. Export is supported for ONNX opset\nversion >= 9. E.g.:\n\nBelow is the list of supported patterns for RHS indexing.\n\nAnd below is the list of unsupported patterns for RHS indexing.\n\nIn code, this type of indexing occurs on the LHS. Export is supported for ONNX\nopset version >= 11. E.g.:\n\nBelow is the list of supported patterns for LHS indexing.\n\nAnd below is the list of unsupported patterns for LHS indexing.\n\nIf you are experiencing issues exporting indexing that belongs to the above\nsupported patterns, please double check that you are exporting with the latest\nopset (opset_version=12).\n\nAll TorchVision models, except for quantized versions, are exportable to ONNX.\nMore details can be found in TorchVision.\n\nThe following operators are supported:\n\nThe operator set above is sufficient to export the following models:\n\nAdding export support for operators is an advance usage.\n\nTo achieve this, developers need to touch the source code of PyTorch. Please\nfollow the instructions for installing PyTorch from source. If the wanted\noperator is standardized in ONNX, it should be easy to add support for\nexporting such operator (adding a symbolic function for the operator). To\nconfirm whether the operator is standardized or not, please check the ONNX\noperator list.\n\nIf the operator is an ATen operator, which means you can find the declaration\nof the function in `torch/csrc/autograd/generated/VariableType.h` (available\nin generated code in PyTorch install dir), you should add the symbolic\nfunction in `torch/onnx/symbolic_opset<version>.py` and follow the\ninstructions listed as below:\n\nIf the operator is a non-ATen operator, the symbolic function has to be added\nin the corresponding PyTorch Function class. Please read the following\ninstructions:\n\nSymbolic functions should be implemented in Python. All of these functions\ninteract with Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:\n\nThe ONNX graph C++ definition is in `torch/csrc/jit/ir/ir.h`.\n\nHere is an example of handling missing symbolic function for `elu` operator.\nWe try to export the model and see the error message as below:\n\nThe export fails because PyTorch does not support exporting `elu` operator. We\nfind `virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace)\nconst override;` in `VariableType.h`. This means `elu` is an ATen operator. We\ncheck the ONNX operator list, and confirm that `Elu` is standardized in ONNX.\nWe add the following lines to `symbolic_opset9.py`:\n\nNow PyTorch is able to export `elu` operator.\n\nThere are more examples in symbolic_opset9.py, symbolic_opset10.py.\n\nThe interface for specifying operator definitions is experimental; adventurous\nusers should note that the APIs will probably change in a future interface.\n\nFollowing this tutorial Extending TorchScript with Custom C++ Operators, you\ncan create and register your own custom ops implementation in PyTorch. Here\u2019s\nhow to export such model to ONNX.:\n\nDepending on the custom operator, you can export it as one or a combination of\nexisting ONNX ops. You can also export it as a custom op in ONNX as well. In\nthat case, you can specify the custom domain and version (custom opset) using\nthe `custom_opsets` dictionary at export. If not explicitly specified, the\ncustom opset version is set to 1 by default. Using custom ONNX ops, you will\nneed to extend the backend of your choice with matching custom ops\nimplementation, e.g. Caffe2 custom ops, ONNX Runtime custom ops.\n\nExporting models with unsupported ONNX operators can be achieved using the\n`operator_export_type` flag in export API. This flag is useful when users try\nto export ATen and non-ATen operators that are not registered and supported in\nONNX.\n\nThis mode is used to export all operators as regular ONNX operators. This is\nthe default `operator_export_type` mode.\n\nThis mode is used to export all operators as ATen ops, and avoid conversion to\nONNX.\n\nTo fallback on unsupported ATen operators in ONNX. Supported operators are\nexported to ONNX regularly. In the following example, aten::triu is not\nsupported in ONNX. Exporter falls back on this operator.\n\nTo export a raw ir.\n\nThis mode can be used to export any operator (ATen or non-ATen) that is not\nregistered and supported in ONNX. Exported falls through and exports the\noperator as is, as custom op. Exporting custom operators enables users to\nregister and implement the operator as part of their runtime backend.\n\nQ: I have exported my lstm model, but its input size seems to be fixed?\n\nThe tracer records the example inputs shape in the graph. In case the model\nshould accept inputs of dynamic shape, you can utilize the parameter\n`dynamic_axes` in export api.\n\nQ: How to export models with loops in it?\n\nPlease checkout Tracing vs Scripting.\n\nQ: Does ONNX support implicit scalar datatype casting?\n\nNo, but the exporter will try to handle that part. Scalars are converted to\nconstant tensors in ONNX. The exporter will try to figure out the right\ndatatype for scalars. However for cases that it failed to do so, you will need\nto manually provide the datatype information. This often happens with scripted\nmodels, where the datatypes are not recorded. We are trying to improve the\ndatatype propagation in the exporter such that manual changes are not required\nin the future.\n\nQ: Is tensor in-place indexed assignment like `data[index] = new_data`\nsupported?\n\nYes, this is supported for ONNX opset version >= 11. Please checkout Indexing.\n\nQ: Is tensor list exportable to ONNX?\n\nYes, this is supported now for ONNX opset version >= 11. ONNX introduced the\nconcept of Sequence in opset 11. Similar to list, Sequence is a data type that\ncontains arbitrary number of Tensors. Associated operators are also introduced\nin ONNX, such as SequenceInsert, SequenceAt, etc. However, in-place list\nappend within loops is not exportable to ONNX. To implement this, please use\ninplace add operator. E.g.:\n\n`use_external_data_format` argument in export API enables export of models in\nONNX external data format. With this option enabled, the exporter stores some\nmodel parameters in external binary files, rather than the ONNX file itself.\nThese external binary files are stored in the same location as the ONNX file.\nArgument \u2018f\u2019 must be a string specifying the location of the model.\n\nThis argument enables export of large models to ONNX. Models larger than 2GB\ncannot be exported in one file because of the protobuf size limit. Users\nshould set `use_external_data_format` to `True` to successfully export such\nmodels.\n\n`Training` argument in export API allows users to export models in a training-\nfriendly mode. `TrainingMode.TRAINING` exports model in a training-friendly\nmode that avoids certain model optimizations which might interfere with model\nparameter training. `TrainingMode.PRESERVE` exports the model in inference\nmode if `model.training` is `False`. Otherwise, it exports the model in a\ntraining-friendly mode. The default mode for this argument is\n`TrainingMode.EVAL` which exports the model in inference mode.\n\nExport a model into ONNX format. This exporter runs your model once in order\nto get a trace of its execution to be exported; at the moment, it supports a\nlimited set of dynamic models (e.g., RNNs.)\n\nargs (tuple of arguments or torch.Tensor, a dictionary consisting of named\narguments (optional)) \u2013\n\na dictionary to specify the input to the corresponding named parameter: - KEY:\nstr, named parameter - VALUE: corresponding input args can be structured\neither as:\n\nONLY A TUPLE OF ARGUMENTS or torch.Tensor:\n\nThe inputs to the model, e.g., such that `model(*args)` is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported\nmodel; any Tensor arguments will become inputs of the exported model, in the\norder they occur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor.\n\nA TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:\n\nThe inputs to the model are structured as a tuple consisting of non-keyword\narguments and the last value of this tuple being a dictionary consisting of\nnamed parameters and the corresponding inputs as key-value pairs. If certain\nnamed argument is not present in the dictionary, it is assigned the default\nvalue, or None if default value is not provided.\n\nCases in which an dictionary input is the last input of the args tuple would\ncause a conflict when a dictionary of named parameters is used. The model\nbelow provides such an example.\n\n\u2026 return x\n\nm = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)}\n\nIn the previous iteration, the call to export API would look like\n\ntorch.onnx.export(model, (k, x), \u2018test.onnx\u2019)\n\nThis would work as intended. However, the export function would now assume\nthat the \u2018x\u2019 input is intended to represent the optional dictionary consisting\nof named arguments. In order to prevent this from being an issue a constraint\nis placed to provide an empty dictionary as the last input in the tuple args\nin such cases. The new call would look like this.\n\ntorch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)\n\noperator_export_type (enum, default OperatorExportTypes.ONNX) \u2013\n\nOperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX\nnamespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op\nis not supported in ONNX or its symbolic is missing, fall back on ATen op.\nRegistered ops are exported to ONNX regularly. Example graph:\n\nis exported as:\n\nIn the above example, aten::triu is not supported in ONNX, hence exporter\nfalls back on this op. OperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall\nthrough and export the operator as is, as a custom ONNX op. Using this mode,\nthe op can be exported and implemented by the user for their runtime backend.\nExample graph:\n\nis exported as:\n\nIn the above example, prim::ListConstruct is not supported, hence exporter\nfalls through.\n\ndynamic_axes (dict<string, dict<python:int, string>> or dict<string,\nlist(int)>, default empty dict) \u2013\n\na dictionary to specify dynamic axes of input/output, such that: - KEY: input\nand/or output names - VALUE: index of dynamic axes for given key and\npotentially the name to be used for exported dynamic axes. In general the\nvalue is defined according to one of the following ways or a combination of\nboth: (1). A list of integers specifying the dynamic axes of provided input.\nIn this scenario automated names will be generated and applied to dynamic axes\nof provided input/output during export. OR (2). An inner dictionary that\nspecifies a mapping FROM the index of dynamic axis in corresponding\ninput/output TO the name that is desired to be applied on such axis of such\ninput/output during export.\n\nExample. if we have the following shape for inputs and outputs:\n\nThen `dynamic axes` can be defined either as:\n\nONLY INDICES:\n\nINDICES WITH CORRESPONDING NAMES:\n\nMIXED MODE OF (1) and (2):\n\nkeep_initializers_as_inputs (bool, default None) \u2013\n\nIf True, all the initializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False, then\ninitializers are not added as inputs to the graph, and only the non-parameter\ninputs are added as inputs.\n\nThis may allow for better optimizations (such as constant folding etc.) by\nbackends/runtimes that execute these graphs. If unspecified (default None),\nthen the behavior is chosen automatically as follows. If operator_export_type\nis OperatorExportTypes.ONNX, the behavior is equivalent to setting this\nargument to False. For other values of operator_export_type, the behavior is\nequivalent to setting this argument to True. Note that for ONNX opset version\n< 9, initializers MUST be part of graph inputs. Therefore, if opset_version\nargument is set to a 8 or lower, this argument will be ignored.\n\nA context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019,\nresetting it when we exit the with-block. A no-op if mode is None.\n\nIn version 1.6 changed to this from set_training\n\nCheck whether it\u2019s in the middle of the ONNX export. This function returns\nTrue in the middle of torch.onnx.export(). torch.onnx.export should be\nexecuted with single thread.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx.export()", "path": "onnx#torch.onnx.export", "type": "torch.onnx", "text": "\nExport a model into ONNX format. This exporter runs your model once in order\nto get a trace of its execution to be exported; at the moment, it supports a\nlimited set of dynamic models (e.g., RNNs.)\n\nargs (tuple of arguments or torch.Tensor, a dictionary consisting of named\narguments (optional)) \u2013\n\na dictionary to specify the input to the corresponding named parameter: - KEY:\nstr, named parameter - VALUE: corresponding input args can be structured\neither as:\n\nONLY A TUPLE OF ARGUMENTS or torch.Tensor:\n\nThe inputs to the model, e.g., such that `model(*args)` is a valid invocation\nof the model. Any non-Tensor arguments will be hard-coded into the exported\nmodel; any Tensor arguments will become inputs of the exported model, in the\norder they occur in args. If args is a Tensor, this is equivalent to having\ncalled it with a 1-ary tuple of that Tensor.\n\nA TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:\n\nThe inputs to the model are structured as a tuple consisting of non-keyword\narguments and the last value of this tuple being a dictionary consisting of\nnamed parameters and the corresponding inputs as key-value pairs. If certain\nnamed argument is not present in the dictionary, it is assigned the default\nvalue, or None if default value is not provided.\n\nCases in which an dictionary input is the last input of the args tuple would\ncause a conflict when a dictionary of named parameters is used. The model\nbelow provides such an example.\n\n\u2026 return x\n\nm = Model() k = torch.randn(2, 3) x = {torch.tensor(1.): torch.randn(2, 3)}\n\nIn the previous iteration, the call to export API would look like\n\ntorch.onnx.export(model, (k, x), \u2018test.onnx\u2019)\n\nThis would work as intended. However, the export function would now assume\nthat the \u2018x\u2019 input is intended to represent the optional dictionary consisting\nof named arguments. In order to prevent this from being an issue a constraint\nis placed to provide an empty dictionary as the last input in the tuple args\nin such cases. The new call would look like this.\n\ntorch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)\n\noperator_export_type (enum, default OperatorExportTypes.ONNX) \u2013\n\nOperatorExportTypes.ONNX: All ops are exported as regular ONNX ops (with ONNX\nnamespace). OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops\n(with aten namespace). OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op\nis not supported in ONNX or its symbolic is missing, fall back on ATen op.\nRegistered ops are exported to ONNX regularly. Example graph:\n\nis exported as:\n\nIn the above example, aten::triu is not supported in ONNX, hence exporter\nfalls back on this op. OperatorExportTypes.RAW: Export raw ir.\nOperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported in ONNX, fall\nthrough and export the operator as is, as a custom ONNX op. Using this mode,\nthe op can be exported and implemented by the user for their runtime backend.\nExample graph:\n\nis exported as:\n\nIn the above example, prim::ListConstruct is not supported, hence exporter\nfalls through.\n\ndynamic_axes (dict<string, dict<python:int, string>> or dict<string,\nlist(int)>, default empty dict) \u2013\n\na dictionary to specify dynamic axes of input/output, such that: - KEY: input\nand/or output names - VALUE: index of dynamic axes for given key and\npotentially the name to be used for exported dynamic axes. In general the\nvalue is defined according to one of the following ways or a combination of\nboth: (1). A list of integers specifying the dynamic axes of provided input.\nIn this scenario automated names will be generated and applied to dynamic axes\nof provided input/output during export. OR (2). An inner dictionary that\nspecifies a mapping FROM the index of dynamic axis in corresponding\ninput/output TO the name that is desired to be applied on such axis of such\ninput/output during export.\n\nExample. if we have the following shape for inputs and outputs:\n\nThen `dynamic axes` can be defined either as:\n\nONLY INDICES:\n\nINDICES WITH CORRESPONDING NAMES:\n\nMIXED MODE OF (1) and (2):\n\nkeep_initializers_as_inputs (bool, default None) \u2013\n\nIf True, all the initializers (typically corresponding to parameters) in the\nexported graph will also be added as inputs to the graph. If False, then\ninitializers are not added as inputs to the graph, and only the non-parameter\ninputs are added as inputs.\n\nThis may allow for better optimizations (such as constant folding etc.) by\nbackends/runtimes that execute these graphs. If unspecified (default None),\nthen the behavior is chosen automatically as follows. If operator_export_type\nis OperatorExportTypes.ONNX, the behavior is equivalent to setting this\nargument to False. For other values of operator_export_type, the behavior is\nequivalent to setting this argument to True. Note that for ONNX opset version\n< 9, initializers MUST be part of graph inputs. Therefore, if opset_version\nargument is set to a 8 or lower, this argument will be ignored.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx.export_to_pretty_string()", "path": "onnx#torch.onnx.export_to_pretty_string", "type": "torch.onnx", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx.is_in_onnx_export()", "path": "onnx#torch.onnx.is_in_onnx_export", "type": "torch.onnx", "text": "\nCheck whether it\u2019s in the middle of the ONNX export. This function returns\nTrue in the middle of torch.onnx.export(). torch.onnx.export should be\nexecuted with single thread.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx.operators.shape_as_tensor()", "path": "onnx#torch.onnx.operators.shape_as_tensor", "type": "torch.onnx", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx.register_custom_op_symbolic()", "path": "onnx#torch.onnx.register_custom_op_symbolic", "type": "torch.onnx", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.onnx.select_model_mode_for_export()", "path": "onnx#torch.onnx.select_model_mode_for_export", "type": "torch.onnx", "text": "\nA context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019,\nresetting it when we exit the with-block. A no-op if mode is None.\n\nIn version 1.6 changed to this from set_training\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim", "path": "optim", "type": "torch.optim", "text": "\n`torch.optim` is a package implementing various optimization algorithms. Most\ncommonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture.\n\nTo use `torch.optim` you have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed\ngradients.\n\nTo construct an `Optimizer` you have to give it an iterable containing the\nparameters (all should be `Variable` s) to optimize. Then, you can specify\noptimizer-specific options such as the learning rate, weight decay, etc.\n\nNote\n\nIf you need to move a model to GPU via `.cuda()`, please do so before\nconstructing optimizers for it. Parameters of a model after `.cuda()` will be\ndifferent objects with those before the call.\n\nIn general, you should make sure that optimized parameters live in consistent\nlocations when optimizers are constructed and used.\n\nExample:\n\n`Optimizer` s also support specifying per-parameter options. To do this,\ninstead of passing an iterable of `Variable` s, pass in an iterable of `dict`\ns. Each of them will define a separate parameter group, and should contain a\n`params` key, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be\nused as optimization options for this group.\n\nNote\n\nYou can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent between\nparameter groups.\n\nFor example, this is very useful when one wants to specify per-layer learning\nrates:\n\nThis means that `model.base`\u2019s parameters will use the default learning rate\nof `1e-2`, `model.classifier`\u2019s parameters will use a learning rate of `1e-3`,\nand a momentum of `0.9` will be used for all parameters.\n\nAll optimizers implement a `step()` method, that updates the parameters. It\ncan be used in two ways:\n\nThis is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g. `backward()`.\n\nExample:\n\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it.\n\nExample:\n\nBase class for all optimizers.\n\nWarning\n\nParameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.\n\nAdd a param group to the `Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can\nbe made trainable and added to the `Optimizer` as training progresses.\n\nLoads the optimizer state.\n\nstate_dict (dict) \u2013 optimizer state. Should be an object returned from a call\nto `state_dict()`.\n\nReturns the state of the optimizer as a `dict`.\n\nIt contains two entries:\n\ndiffers between optimizer classes.\n\nPerforms a single optimization step (parameter update).\n\nclosure (callable) \u2013 A closure that reevaluates the model and returns the\nloss. Optional for most optimizers.\n\nNote\n\nUnless otherwise specified, this function should not modify the `.grad` field\nof the parameters.\n\nSets the gradients of all optimized `torch.Tensor` s to zero.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This\nwill in general have lower memory footprint, and can modestly improve\nperformance. However, it changes certain behaviors. For example: 1. When the\nuser tries to access a gradient and perform manual ops on it, a None attribute\nor a Tensor full of 0s will behave differently. 2. If the user requests\n`zero_grad(set_to_none=True)` followed by a backward pass, `.grad`s are\nguaranteed to be None for params that did not receive a gradient. 3.\n`torch.optim` optimizers have a different behavior if the gradient is 0 or\nNone (in one case it does the step with a gradient of 0 and in the other it\nskips the step altogether).\n\nImplements Adadelta algorithm.\n\nIt has been proposed in ADADELTA: An Adaptive Learning Rate Method.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements Adagrad algorithm.\n\nIt has been proposed in Adaptive Subgradient Methods for Online Learning and\nStochastic Optimization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements Adam algorithm.\n\nIt has been proposed in Adam: A Method for Stochastic Optimization. The\nimplementation of the L2 penalty follows changes proposed in Decoupled Weight\nDecay Regularization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements AdamW algorithm.\n\nThe original Adam algorithm was proposed in Adam: A Method for Stochastic\nOptimization. The AdamW variant was proposed in Decoupled Weight Decay\nRegularization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements lazy version of Adam algorithm suitable for sparse tensors.\n\nIn this variant, only moments that show up in the gradient get updated, and\nonly those portions of the gradient get applied to the parameters.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements Adamax algorithm (a variant of Adam based on infinity norm).\n\nIt has been proposed in Adam: A Method for Stochastic Optimization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements Averaged Stochastic Gradient Descent.\n\nIt has been proposed in Acceleration of stochastic approximation by averaging.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements L-BFGS algorithm, heavily inspired by `minFunc\n<https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>`.\n\nWarning\n\nThis optimizer doesn\u2019t support per-parameter options and parameter groups\n(there can be only one).\n\nWarning\n\nRight now all parameters have to be on a single device. This will be improved\nin the future.\n\nNote\n\nThis is a very memory intensive optimizer (it requires additional `param_bytes\n* (history_size + 1)` bytes). If it doesn\u2019t fit in memory try reducing the\nhistory size, or use a different algorithm.\n\nPerforms a single optimization step.\n\nclosure (callable) \u2013 A closure that reevaluates the model and returns the\nloss.\n\nImplements RMSprop algorithm.\n\nProposed by G. Hinton in his course.\n\nThe centered version first appears in Generating Sequences With Recurrent\nNeural Networks.\n\nThe implementation here takes the square root of the gradient average before\nadding epsilon (note that TensorFlow interchanges these two operations). The\neffective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon) where\n\u03b1\\alpha is the scheduled learning rate and vv is the weighted moving average\nof the squared gradient.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements the resilient backpropagation algorithm.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\nImplements stochastic gradient descent (optionally with momentum).\n\nNesterov momentum is based on the formula from On the importance of\ninitialization and momentum in deep learning.\n\nNote\n\nThe implementation of SGD with Momentum/Nesterov subtly differs from Sutskever\net. al. and implementations in some other frameworks.\n\nConsidering the specific case of Momentum, the update can be written as\n\nwhere pp , gg , vv and \u03bc\\mu denote the parameters, gradient, velocity, and\nmomentum respectively.\n\nThis is in contrast to Sutskever et. al. and other frameworks which employ an\nupdate of the form\n\nThe Nesterov version is analogously modified.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n`torch.optim.lr_scheduler` provides several methods to adjust the learning\nrate based on the number of epochs.\n`torch.optim.lr_scheduler.ReduceLROnPlateau` allows dynamic learning rate\nreducing based on some validation measurements.\n\nLearning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way:\n\nWarning\n\nPrior to PyTorch 1.1.0, the learning rate scheduler was expected to be called\nbefore the optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking\nway. If you use the learning rate scheduler (calling `scheduler.step()`)\nbefore the optimizer\u2019s update (calling `optimizer.step()`), this will skip the\nfirst value of the learning rate schedule. If you are unable to reproduce\nresults after upgrading to PyTorch 1.1.0, please check if you are calling\n`scheduler.step()` at the wrong time.\n\nSets the learning rate of each parameter group to the initial lr times a given\nfunction. When last_epoch=-1, sets initial lr as lr.\n\nLoads the schedulers state.\n\nWhen saving or loading the scheduler, please make sure to also save or load\nthe state of the optimizer.\n\nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call\nto `state_dict()`.\n\nReturns the state of the scheduler as a `dict`.\n\nIt contains an entry for every variable in self.__dict__ which is not the\noptimizer. The learning rate lambda functions will only be saved if they are\ncallable objects and not if they are functions or lambdas.\n\nWhen saving or loading the scheduler, please make sure to also save or load\nthe state of the optimizer.\n\nMultiply the learning rate of each parameter group by the factor given in the\nspecified function. When last_epoch=-1, sets initial lr as lr.\n\nLoads the schedulers state.\n\nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call\nto `state_dict()`.\n\nReturns the state of the scheduler as a `dict`.\n\nIt contains an entry for every variable in self.__dict__ which is not the\noptimizer. The learning rate lambda functions will only be saved if they are\ncallable objects and not if they are functions or lambdas.\n\nDecays the learning rate of each parameter group by gamma every step_size\nepochs. Notice that such decay can happen simultaneously with other changes to\nthe learning rate from outside this scheduler. When last_epoch=-1, sets\ninitial lr as lr.\n\nDecays the learning rate of each parameter group by gamma once the number of\nepoch reaches one of the milestones. Notice that such decay can happen\nsimultaneously with other changes to the learning rate from outside this\nscheduler. When last_epoch=-1, sets initial lr as lr.\n\nDecays the learning rate of each parameter group by gamma every epoch. When\nlast_epoch=-1, sets initial lr as lr.\n\nSet the learning rate of each parameter group using a cosine annealing\nschedule, where \u03b7max\\eta_{max} is set to the initial lr and TcurT_{cur} is the\nnumber of epochs since the last restart in SGDR:\n\nWhen last_epoch=-1, sets initial lr as lr. Notice that because the schedule is\ndefined recursively, the learning rate can be simultaneously modified outside\nthis scheduler by other operators. If the learning rate is set solely by this\nscheduler, the learning rate at each step becomes:\n\nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.\nNote that this only implements the cosine annealing part of SGDR, and not the\nrestarts.\n\nReduce learning rate when a metric has stopped improving. Models often benefit\nfrom reducing the learning rate by a factor of 2-10 once learning stagnates.\nThis scheduler reads a metrics quantity and if no improvement is seen for a\n\u2018patience\u2019 number of epochs, the learning rate is reduced.\n\nSets the learning rate of each parameter group according to cyclical learning\nrate policy (CLR). The policy cycles the learning rate between two boundaries\nwith a constant frequency, as detailed in the paper Cyclical Learning Rates\nfor Training Neural Networks. The distance between the two boundaries can be\nscaled on a per-iteration or per-cycle basis.\n\nCyclical learning rate policy changes the learning rate after every batch.\n`step` should be called after a batch has been used for training.\n\nThis class has three built-in policies, as put forth in the paper:\n\nThis implementation was adapted from the github repo: bckenstler/CLR\n\nCalculates the learning rate at batch index. This function treats\n`self.last_epoch` as the last batch index.\n\nIf `self.cycle_momentum` is `True`, this function has a side effect of\nupdating the optimizer\u2019s momentum.\n\nSets the learning rate of each parameter group according to the 1cycle\nlearning rate policy. The 1cycle policy anneals the learning rate from an\ninitial learning rate to some maximum learning rate and then from that maximum\nlearning rate to some minimum learning rate much lower than the initial\nlearning rate. This policy was initially described in the paper Super-\nConvergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n\nThe 1cycle learning rate policy changes the learning rate after every batch.\n`step` should be called after a batch has been used for training.\n\nThis scheduler is not chainable.\n\nNote also that the total number of steps in the cycle can be determined in one\nof two ways (listed in order of precedence):\n\nYou must either provide a value for total_steps or provide a value for both\nepochs and steps_per_epoch.\n\nThe default behaviour of this scheduler follows the fastai implementation of\n1cycle, which claims that \u201cunpublished work has shown even better results by\nusing only two phases\u201d. To mimic the behaviour of the original paper instead,\nset `three_phase=True`.\n\nSet the learning rate of each parameter group using a cosine annealing\nschedule, where \u03b7max\\eta_{max} is set to the initial lr, TcurT_{cur} is the\nnumber of epochs since the last restart and TiT_{i} is the number of epochs\nbetween two warm restarts in SGDR:\n\nWhen Tcur=TiT_{cur}=T_{i} , set \u03b7t=\u03b7min\\eta_t = \\eta_{min} . When\nTcur=0T_{cur}=0 after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max} .\n\nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.\n\nStep could be called after every batch update\n\nThis function can be called in an interleaved way.\n\n`torch.optim.swa_utils` implements Stochastic Weight Averaging (SWA). In\nparticular, `torch.optim.swa_utils.AveragedModel` class implements SWA models,\n`torch.optim.swa_utils.SWALR` implements the SWA learning rate scheduler and\n`torch.optim.swa_utils.update_bn()` is a utility function used to update SWA\nbatch normalization statistics at the end of training.\n\nSWA has been proposed in Averaging Weights Leads to Wider Optima and Better\nGeneralization.\n\n`AveragedModel` class serves to compute the weights of the SWA model. You can\ncreate an averaged model by running:\n\nHere the model `model` can be an arbitrary `torch.nn.Module` object.\n`swa_model` will keep track of the running averages of the parameters of the\n`model`. To update these averages, you can use the `update_parameters()`\nfunction:\n\nTypically, in SWA the learning rate is set to a high constant value. `SWALR`\nis a learning rate scheduler that anneals the learning rate to a fixed value,\nand then keeps it constant. For example, the following code creates a\nscheduler that linearly anneals the learning rate from its initial value to\n0.05 in 5 epochs within each parameter group:\n\nYou can also use cosine annealing to a fixed value instead of linear annealing\nby setting `anneal_strategy=\"cos\"`.\n\n`update_bn()` is a utility function that allows to compute the batchnorm\nstatistics for the SWA model on a given dataloader `loader` at the end of\ntraining:\n\n`update_bn()` applies the `swa_model` to every element in the dataloader and\ncomputes the activation statistics for each batch normalization layer in the\nmodel.\n\nWarning\n\n`update_bn()` assumes that each batch in the dataloader `loader` is either a\ntensors or a list of tensors where the first element is the tensor that the\nnetwork `swa_model` should be applied to. If your dataloader has a different\nstructure, you can update the batch normalization statistics of the\n`swa_model` by doing a forward pass with the `swa_model` on each element of\nthe dataset.\n\nBy default, `torch.optim.swa_utils.AveragedModel` computes a running equal\naverage of the parameters that you provide, but you can also use custom\naveraging functions with the `avg_fn` parameter. In the following example\n`ema_model` computes an exponential moving average.\n\nExample:\n\nIn the example below, `swa_model` is the SWA model that accumulates the\naverages of the weights. We train the model for a total of 300 epochs and we\nswitch to the SWA learning rate schedule and start to collect SWA averages of\nthe parameters at epoch 160:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adadelta", "path": "optim#torch.optim.Adadelta", "type": "torch.optim", "text": "\nImplements Adadelta algorithm.\n\nIt has been proposed in ADADELTA: An Adaptive Learning Rate Method.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adadelta.step()", "path": "optim#torch.optim.Adadelta.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adagrad", "path": "optim#torch.optim.Adagrad", "type": "torch.optim", "text": "\nImplements Adagrad algorithm.\n\nIt has been proposed in Adaptive Subgradient Methods for Online Learning and\nStochastic Optimization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adagrad.step()", "path": "optim#torch.optim.Adagrad.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adam", "path": "optim#torch.optim.Adam", "type": "torch.optim", "text": "\nImplements Adam algorithm.\n\nIt has been proposed in Adam: A Method for Stochastic Optimization. The\nimplementation of the L2 penalty follows changes proposed in Decoupled Weight\nDecay Regularization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adam.step()", "path": "optim#torch.optim.Adam.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adamax", "path": "optim#torch.optim.Adamax", "type": "torch.optim", "text": "\nImplements Adamax algorithm (a variant of Adam based on infinity norm).\n\nIt has been proposed in Adam: A Method for Stochastic Optimization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Adamax.step()", "path": "optim#torch.optim.Adamax.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.AdamW", "path": "optim#torch.optim.AdamW", "type": "torch.optim", "text": "\nImplements AdamW algorithm.\n\nThe original Adam algorithm was proposed in Adam: A Method for Stochastic\nOptimization. The AdamW variant was proposed in Decoupled Weight Decay\nRegularization.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.AdamW.step()", "path": "optim#torch.optim.AdamW.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.ASGD", "path": "optim#torch.optim.ASGD", "type": "torch.optim", "text": "\nImplements Averaged Stochastic Gradient Descent.\n\nIt has been proposed in Acceleration of stochastic approximation by averaging.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.ASGD.step()", "path": "optim#torch.optim.ASGD.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.LBFGS", "path": "optim#torch.optim.LBFGS", "type": "torch.optim", "text": "\nImplements L-BFGS algorithm, heavily inspired by `minFunc\n<https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>`.\n\nWarning\n\nThis optimizer doesn\u2019t support per-parameter options and parameter groups\n(there can be only one).\n\nWarning\n\nRight now all parameters have to be on a single device. This will be improved\nin the future.\n\nNote\n\nThis is a very memory intensive optimizer (it requires additional `param_bytes\n* (history_size + 1)` bytes). If it doesn\u2019t fit in memory try reducing the\nhistory size, or use a different algorithm.\n\nPerforms a single optimization step.\n\nclosure (callable) \u2013 A closure that reevaluates the model and returns the\nloss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.LBFGS.step()", "path": "optim#torch.optim.LBFGS.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable) \u2013 A closure that reevaluates the model and returns the\nloss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingLR", "type": "torch.optim", "text": "\nSet the learning rate of each parameter group using a cosine annealing\nschedule, where \u03b7max\\eta_{max} is set to the initial lr and TcurT_{cur} is the\nnumber of epochs since the last restart in SGDR:\n\nWhen last_epoch=-1, sets initial lr as lr. Notice that because the schedule is\ndefined recursively, the learning rate can be simultaneously modified outside\nthis scheduler by other operators. If the learning rate is set solely by this\nscheduler, the learning rate at each step becomes:\n\nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.\nNote that this only implements the cosine annealing part of SGDR, and not the\nrestarts.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "type": "torch.optim", "text": "\nSet the learning rate of each parameter group using a cosine annealing\nschedule, where \u03b7max\\eta_{max} is set to the initial lr, TcurT_{cur} is the\nnumber of epochs since the last restart and TiT_{i} is the number of epochs\nbetween two warm restarts in SGDR:\n\nWhen Tcur=TiT_{cur}=T_{i} , set \u03b7t=\u03b7min\\eta_t = \\eta_{min} . When\nTcur=0T_{cur}=0 after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max} .\n\nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.\n\nStep could be called after every batch update\n\nThis function can be called in an interleaved way.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step()", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step", "type": "torch.optim", "text": "\nStep could be called after every batch update\n\nThis function can be called in an interleaved way.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CyclicLR", "path": "optim#torch.optim.lr_scheduler.CyclicLR", "type": "torch.optim", "text": "\nSets the learning rate of each parameter group according to cyclical learning\nrate policy (CLR). The policy cycles the learning rate between two boundaries\nwith a constant frequency, as detailed in the paper Cyclical Learning Rates\nfor Training Neural Networks. The distance between the two boundaries can be\nscaled on a per-iteration or per-cycle basis.\n\nCyclical learning rate policy changes the learning rate after every batch.\n`step` should be called after a batch has been used for training.\n\nThis class has three built-in policies, as put forth in the paper:\n\nThis implementation was adapted from the github repo: bckenstler/CLR\n\nCalculates the learning rate at batch index. This function treats\n`self.last_epoch` as the last batch index.\n\nIf `self.cycle_momentum` is `True`, this function has a side effect of\nupdating the optimizer\u2019s momentum.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CyclicLR.get_lr()", "path": "optim#torch.optim.lr_scheduler.CyclicLR.get_lr", "type": "torch.optim", "text": "\nCalculates the learning rate at batch index. This function treats\n`self.last_epoch` as the last batch index.\n\nIf `self.cycle_momentum` is `True`, this function has a side effect of\nupdating the optimizer\u2019s momentum.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.ExponentialLR", "path": "optim#torch.optim.lr_scheduler.ExponentialLR", "type": "torch.optim", "text": "\nDecays the learning rate of each parameter group by gamma every epoch. When\nlast_epoch=-1, sets initial lr as lr.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.LambdaLR", "path": "optim#torch.optim.lr_scheduler.LambdaLR", "type": "torch.optim", "text": "\nSets the learning rate of each parameter group to the initial lr times a given\nfunction. When last_epoch=-1, sets initial lr as lr.\n\nLoads the schedulers state.\n\nWhen saving or loading the scheduler, please make sure to also save or load\nthe state of the optimizer.\n\nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call\nto `state_dict()`.\n\nReturns the state of the scheduler as a `dict`.\n\nIt contains an entry for every variable in self.__dict__ which is not the\noptimizer. The learning rate lambda functions will only be saved if they are\ncallable objects and not if they are functions or lambdas.\n\nWhen saving or loading the scheduler, please make sure to also save or load\nthe state of the optimizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.LambdaLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.load_state_dict", "type": "torch.optim", "text": "\nLoads the schedulers state.\n\nWhen saving or loading the scheduler, please make sure to also save or load\nthe state of the optimizer.\n\nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call\nto `state_dict()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.LambdaLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.state_dict", "type": "torch.optim", "text": "\nReturns the state of the scheduler as a `dict`.\n\nIt contains an entry for every variable in self.__dict__ which is not the\noptimizer. The learning rate lambda functions will only be saved if they are\ncallable objects and not if they are functions or lambdas.\n\nWhen saving or loading the scheduler, please make sure to also save or load\nthe state of the optimizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR", "type": "torch.optim", "text": "\nMultiply the learning rate of each parameter group by the factor given in the\nspecified function. When last_epoch=-1, sets initial lr as lr.\n\nLoads the schedulers state.\n\nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call\nto `state_dict()`.\n\nReturns the state of the scheduler as a `dict`.\n\nIt contains an entry for every variable in self.__dict__ which is not the\noptimizer. The learning rate lambda functions will only be saved if they are\ncallable objects and not if they are functions or lambdas.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict", "type": "torch.optim", "text": "\nLoads the schedulers state.\n\nstate_dict (dict) \u2013 scheduler state. Should be an object returned from a call\nto `state_dict()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.state_dict", "type": "torch.optim", "text": "\nReturns the state of the scheduler as a `dict`.\n\nIt contains an entry for every variable in self.__dict__ which is not the\noptimizer. The learning rate lambda functions will only be saved if they are\ncallable objects and not if they are functions or lambdas.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiStepLR", "path": "optim#torch.optim.lr_scheduler.MultiStepLR", "type": "torch.optim", "text": "\nDecays the learning rate of each parameter group by gamma once the number of\nepoch reaches one of the milestones. Notice that such decay can happen\nsimultaneously with other changes to the learning rate from outside this\nscheduler. When last_epoch=-1, sets initial lr as lr.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.OneCycleLR", "path": "optim#torch.optim.lr_scheduler.OneCycleLR", "type": "torch.optim", "text": "\nSets the learning rate of each parameter group according to the 1cycle\nlearning rate policy. The 1cycle policy anneals the learning rate from an\ninitial learning rate to some maximum learning rate and then from that maximum\nlearning rate to some minimum learning rate much lower than the initial\nlearning rate. This policy was initially described in the paper Super-\nConvergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n\nThe 1cycle learning rate policy changes the learning rate after every batch.\n`step` should be called after a batch has been used for training.\n\nThis scheduler is not chainable.\n\nNote also that the total number of steps in the cycle can be determined in one\nof two ways (listed in order of precedence):\n\nYou must either provide a value for total_steps or provide a value for both\nepochs and steps_per_epoch.\n\nThe default behaviour of this scheduler follows the fastai implementation of\n1cycle, which claims that \u201cunpublished work has shown even better results by\nusing only two phases\u201d. To mimic the behaviour of the original paper instead,\nset `three_phase=True`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.ReduceLROnPlateau", "path": "optim#torch.optim.lr_scheduler.ReduceLROnPlateau", "type": "torch.optim", "text": "\nReduce learning rate when a metric has stopped improving. Models often benefit\nfrom reducing the learning rate by a factor of 2-10 once learning stagnates.\nThis scheduler reads a metrics quantity and if no improvement is seen for a\n\u2018patience\u2019 number of epochs, the learning rate is reduced.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.StepLR", "path": "optim#torch.optim.lr_scheduler.StepLR", "type": "torch.optim", "text": "\nDecays the learning rate of each parameter group by gamma every step_size\nepochs. Notice that such decay can happen simultaneously with other changes to\nthe learning rate from outside this scheduler. When last_epoch=-1, sets\ninitial lr as lr.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Optimizer", "path": "optim#torch.optim.Optimizer", "type": "torch.optim", "text": "\nBase class for all optimizers.\n\nWarning\n\nParameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.\n\nAdd a param group to the `Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can\nbe made trainable and added to the `Optimizer` as training progresses.\n\nLoads the optimizer state.\n\nstate_dict (dict) \u2013 optimizer state. Should be an object returned from a call\nto `state_dict()`.\n\nReturns the state of the optimizer as a `dict`.\n\nIt contains two entries:\n\ndiffers between optimizer classes.\n\nPerforms a single optimization step (parameter update).\n\nclosure (callable) \u2013 A closure that reevaluates the model and returns the\nloss. Optional for most optimizers.\n\nNote\n\nUnless otherwise specified, this function should not modify the `.grad` field\nof the parameters.\n\nSets the gradients of all optimized `torch.Tensor` s to zero.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This\nwill in general have lower memory footprint, and can modestly improve\nperformance. However, it changes certain behaviors. For example: 1. When the\nuser tries to access a gradient and perform manual ops on it, a None attribute\nor a Tensor full of 0s will behave differently. 2. If the user requests\n`zero_grad(set_to_none=True)` followed by a backward pass, `.grad`s are\nguaranteed to be None for params that did not receive a gradient. 3.\n`torch.optim` optimizers have a different behavior if the gradient is 0 or\nNone (in one case it does the step with a gradient of 0 and in the other it\nskips the step altogether).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.add_param_group()", "path": "optim#torch.optim.Optimizer.add_param_group", "type": "torch.optim", "text": "\nAdd a param group to the `Optimizer` s `param_groups`.\n\nThis can be useful when fine tuning a pre-trained network as frozen layers can\nbe made trainable and added to the `Optimizer` as training progresses.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.load_state_dict()", "path": "optim#torch.optim.Optimizer.load_state_dict", "type": "torch.optim", "text": "\nLoads the optimizer state.\n\nstate_dict (dict) \u2013 optimizer state. Should be an object returned from a call\nto `state_dict()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.state_dict()", "path": "optim#torch.optim.Optimizer.state_dict", "type": "torch.optim", "text": "\nReturns the state of the optimizer as a `dict`.\n\nIt contains two entries:\n\ndiffers between optimizer classes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.step()", "path": "optim#torch.optim.Optimizer.step", "type": "torch.optim", "text": "\nPerforms a single optimization step (parameter update).\n\nclosure (callable) \u2013 A closure that reevaluates the model and returns the\nloss. Optional for most optimizers.\n\nNote\n\nUnless otherwise specified, this function should not modify the `.grad` field\nof the parameters.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.zero_grad()", "path": "optim#torch.optim.Optimizer.zero_grad", "type": "torch.optim", "text": "\nSets the gradients of all optimized `torch.Tensor` s to zero.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. This\nwill in general have lower memory footprint, and can modestly improve\nperformance. However, it changes certain behaviors. For example: 1. When the\nuser tries to access a gradient and perform manual ops on it, a None attribute\nor a Tensor full of 0s will behave differently. 2. If the user requests\n`zero_grad(set_to_none=True)` followed by a backward pass, `.grad`s are\nguaranteed to be None for params that did not receive a gradient. 3.\n`torch.optim` optimizers have a different behavior if the gradient is 0 or\nNone (in one case it does the step with a gradient of 0 and in the other it\nskips the step altogether).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.RMSprop", "path": "optim#torch.optim.RMSprop", "type": "torch.optim", "text": "\nImplements RMSprop algorithm.\n\nProposed by G. Hinton in his course.\n\nThe centered version first appears in Generating Sequences With Recurrent\nNeural Networks.\n\nThe implementation here takes the square root of the gradient average before\nadding epsilon (note that TensorFlow interchanges these two operations). The\neffective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon) where\n\u03b1\\alpha is the scheduled learning rate and vv is the weighted moving average\nof the squared gradient.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.RMSprop.step()", "path": "optim#torch.optim.RMSprop.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Rprop", "path": "optim#torch.optim.Rprop", "type": "torch.optim", "text": "\nImplements the resilient backpropagation algorithm.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.Rprop.step()", "path": "optim#torch.optim.Rprop.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.SGD", "path": "optim#torch.optim.SGD", "type": "torch.optim", "text": "\nImplements stochastic gradient descent (optionally with momentum).\n\nNesterov momentum is based on the formula from On the importance of\ninitialization and momentum in deep learning.\n\nNote\n\nThe implementation of SGD with Momentum/Nesterov subtly differs from Sutskever\net. al. and implementations in some other frameworks.\n\nConsidering the specific case of Momentum, the update can be written as\n\nwhere pp , gg , vv and \u03bc\\mu denote the parameters, gradient, velocity, and\nmomentum respectively.\n\nThis is in contrast to Sutskever et. al. and other frameworks which employ an\nupdate of the form\n\nThe Nesterov version is analogously modified.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.SGD.step()", "path": "optim#torch.optim.SGD.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.SparseAdam", "path": "optim#torch.optim.SparseAdam", "type": "torch.optim", "text": "\nImplements lazy version of Adam algorithm suitable for sparse tensors.\n\nIn this variant, only moments that show up in the gradient get updated, and\nonly those portions of the gradient get applied to the parameters.\n\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.optim.SparseAdam.step()", "path": "optim#torch.optim.SparseAdam.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\nclosure (callable, optional) \u2013 A closure that reevaluates the model and\nreturns the loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.orgqr()", "path": "generated/torch.orgqr#torch.orgqr", "type": "torch", "text": "\nComputes the orthogonal matrix `Q` of a QR factorization, from the `(input,\ninput2)` tuple returned by `torch.geqrf()`.\n\nThis directly calls the underlying LAPACK function `?orgqr`. See LAPACK\ndocumentation for orgqr for further details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ormqr()", "path": "generated/torch.ormqr#torch.ormqr", "type": "torch", "text": "\nMultiplies `mat` (given by `input3`) by the orthogonal `Q` matrix of the QR\nfactorization formed by `torch.geqrf()` that is represented by `(a, tau)`\n(given by (`input`, `input2`)).\n\nThis directly calls the underlying LAPACK function `?ormqr`. See LAPACK\ndocumentation for ormqr for further details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.outer()", "path": "generated/torch.outer#torch.outer", "type": "torch", "text": "\nOuter product of `input` and `vec2`. If `input` is a vector of size nn and\n`vec2` is a vector of size mm , then `out` must be a matrix of size (n\u00d7m)(n\n\\times m) .\n\nNote\n\nThis function does not broadcast.\n\nout (Tensor, optional) \u2013 optional output matrix\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides", "path": "torch.overrides", "type": "torch.overrides", "text": "\nThis module exposes various helper functions for the `__torch_function__`\nprotocol. See Extending torch for more detail on the `__torch_function__`\nprotocol.\n\nReturn public functions that cannot be overridden by `__torch_function__`.\n\nA tuple of functions that are publicly available in the torch API but cannot\nbe overridden with `__torch_function__`. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes.\n\nSet[Callable]\n\nList functions that are overridable via __torch_function__\n\nA dictionary that maps namespaces that contain overridable functions to\nfunctions in that namespace that can be overridden.\n\nDict[Any, List[Callable]]\n\nReturn a dict containing dummy overrides for all overridable functions\n\nA dictionary that maps overridable functions in the PyTorch API to lambda\nfunctions that have the same signature as the real function and\nunconditionally return -1. These lambda functions are useful for testing API\ncoverage for a type that defines `__torch_function__`.\n\nDict[Callable, Callable]\n\nImplement a function with checks for `__torch_function__` overrides.\n\nSee torch::autograd::handle_torch_function for the equivalent of this function\nin the C++ implementation.\n\nResult from calling `implementation` or an `__torch_function__` method, as\nappropriate.\n\nobject\n\n:raises TypeError : if no implementation is found.:\n\nCheck for __torch_function__ implementations in the elements of an iterable.\nConsiders exact `Tensor` s and `Parameter` s non-dispatchable. :param\nrelevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable\n\nTrue if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise.\n\nbool\n\nSee also\n\nChecks if something is a Tensor-like, including an exact `Tensor`.\n\nReturns `True` if the passed-in input is a Tensor-like.\n\nCurrently, this occurs whenever there\u2019s a `__torch_function__` attribute on\nthe type of the input.\n\nA subclass of tensor is generally a Tensor-like.\n\nBuilt-in or user types aren\u2019t usually Tensor-like.\n\nBut, they can be made Tensor-like by implementing __torch_function__.\n\nReturns True if the function passed in is a handler for a method or property\nbelonging to `torch.Tensor`, as passed into `__torch_function__`.\n\nNote\n\nFor properties, their `__get__` method must be passed in.\n\nThis may be needed, in particular, for the following reasons:\n\nWraps a given function with `__torch_function__` -related functionality.\n\ndispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes\npassed into the function.\n\nNote\n\nThis decorator may reduce the performance of your code. Generally, it\u2019s enough\nto express your code as a series of functions that, themselves, support\n__torch_function__. If you find yourself in the rare situation where this is\nnot the case, e.g. if you\u2019re wrapping a low-level library and you also need it\nto work for Tensor-likes, then this function is available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.get_ignored_functions()", "path": "torch.overrides#torch.overrides.get_ignored_functions", "type": "torch.overrides", "text": "\nReturn public functions that cannot be overridden by `__torch_function__`.\n\nA tuple of functions that are publicly available in the torch API but cannot\nbe overridden with `__torch_function__`. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes.\n\nSet[Callable]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.get_overridable_functions()", "path": "torch.overrides#torch.overrides.get_overridable_functions", "type": "torch.overrides", "text": "\nList functions that are overridable via __torch_function__\n\nA dictionary that maps namespaces that contain overridable functions to\nfunctions in that namespace that can be overridden.\n\nDict[Any, List[Callable]]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.get_testing_overrides()", "path": "torch.overrides#torch.overrides.get_testing_overrides", "type": "torch.overrides", "text": "\nReturn a dict containing dummy overrides for all overridable functions\n\nA dictionary that maps overridable functions in the PyTorch API to lambda\nfunctions that have the same signature as the real function and\nunconditionally return -1. These lambda functions are useful for testing API\ncoverage for a type that defines `__torch_function__`.\n\nDict[Callable, Callable]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.handle_torch_function()", "path": "torch.overrides#torch.overrides.handle_torch_function", "type": "torch.overrides", "text": "\nImplement a function with checks for `__torch_function__` overrides.\n\nSee torch::autograd::handle_torch_function for the equivalent of this function\nin the C++ implementation.\n\nResult from calling `implementation` or an `__torch_function__` method, as\nappropriate.\n\nobject\n\n:raises TypeError : if no implementation is found.:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.has_torch_function()", "path": "torch.overrides#torch.overrides.has_torch_function", "type": "torch.overrides", "text": "\nCheck for __torch_function__ implementations in the elements of an iterable.\nConsiders exact `Tensor` s and `Parameter` s non-dispatchable. :param\nrelevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable\n\nTrue if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise.\n\nbool\n\nSee also\n\nChecks if something is a Tensor-like, including an exact `Tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.is_tensor_like()", "path": "torch.overrides#torch.overrides.is_tensor_like", "type": "torch.overrides", "text": "\nReturns `True` if the passed-in input is a Tensor-like.\n\nCurrently, this occurs whenever there\u2019s a `__torch_function__` attribute on\nthe type of the input.\n\nA subclass of tensor is generally a Tensor-like.\n\nBuilt-in or user types aren\u2019t usually Tensor-like.\n\nBut, they can be made Tensor-like by implementing __torch_function__.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.is_tensor_method_or_property()", "path": "torch.overrides#torch.overrides.is_tensor_method_or_property", "type": "torch.overrides", "text": "\nReturns True if the function passed in is a handler for a method or property\nbelonging to `torch.Tensor`, as passed into `__torch_function__`.\n\nNote\n\nFor properties, their `__get__` method must be passed in.\n\nThis may be needed, in particular, for the following reasons:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.overrides.wrap_torch_function()", "path": "torch.overrides#torch.overrides.wrap_torch_function", "type": "torch.overrides", "text": "\nWraps a given function with `__torch_function__` -related functionality.\n\ndispatcher (Callable) \u2013 A callable that returns an iterable of Tensor-likes\npassed into the function.\n\nNote\n\nThis decorator may reduce the performance of your code. Generally, it\u2019s enough\nto express your code as a series of functions that, themselves, support\n__torch_function__. If you find yourself in the rare situation where this is\nnot the case, e.g. if you\u2019re wrapping a low-level library and you also need it\nto work for Tensor-likes, then this function is available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.pca_lowrank()", "path": "generated/torch.pca_lowrank#torch.pca_lowrank", "type": "torch", "text": "\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix,\nbatches of such matrices, or sparse matrix.\n\nThis function returns a namedtuple `(U, S, V)` which is the nearly optimal\napproximation of a singular value decomposition of a centered matrix AA such\nthat A=Udiag(S)VTA = U diag(S) V^T .\n\nNote\n\nThe relation of `(U, S, V)` to PCA is as follows:\n\nNote\n\nDifferent from the standard SVD, the size of returned matrices depend on the\nspecified rank and q values as follows:\n\nNote\n\nTo obtain repeatable results, reset the seed for the pseudorandom number\ngenerator\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.pinverse()", "path": "generated/torch.pinverse#torch.pinverse", "type": "torch", "text": "\nCalculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a\n2D tensor. Please look at Moore-Penrose inverse for more details\n\nNote\n\n`torch.pinverse()` is deprecated. Please use `torch.linalg.pinv()` instead\nwhich includes new parameters `hermitian` and `out`.\n\nNote\n\nThis method is implemented using the Singular Value Decomposition.\n\nNote\n\nThe pseudo-inverse is not necessarily a continuous function in the elements of\nthe matrix [1]. Therefore, derivatives are not always existent, and exist for\na constant rank only [2]. However, this method is backprop-able due to the\nimplementation by using SVD results, and could be unstable. Double-backward\nwill also be unstable due to the usage of SVD internally. See `svd()` for more\ndetails.\n\nNote\n\nSupports real and complex inputs. Batched version for complex inputs is only\nsupported on the CPU.\n\nThe pseudo-inverse of `input` of dimensions (\u2217,n,m)(*, n, m)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.poisson()", "path": "generated/torch.poisson#torch.poisson", "type": "torch", "text": "\nReturns a tensor of the same size as `input` with each element sampled from a\nPoisson distribution with rate parameter given by the corresponding element in\n`input` i.e.,\n\ninput (Tensor) \u2013 the input tensor containing the rates of the Poisson\ndistribution\n\ngenerator (`torch.Generator`, optional) \u2013 a pseudorandom number generator for\nsampling\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.polar()", "path": "generated/torch.polar#torch.polar", "type": "torch", "text": "\nConstructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value `abs` and angle\n`angle`.\n\nout (Tensor) \u2013 If the inputs are `torch.float32`, must be `torch.complex64`.\nIf the inputs are `torch.float64`, must be `torch.complex128`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.polygamma()", "path": "generated/torch.polygamma#torch.polygamma", "type": "torch", "text": "\nComputes the nthn^{th} derivative of the digamma function on `input`. n\u22650n\n\\geq 0 is called the order of the polygamma function.\n\nNote\n\nThis function is implemented only for nonnegative integers n\u22650n \\geq 0 .\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.pow()", "path": "generated/torch.pow#torch.pow", "type": "torch", "text": "\nTakes the power of each element in `input` with `exponent` and returns a\ntensor with the result.\n\n`exponent` can be either a single `float` number or a `Tensor` with the same\nnumber of elements as `input`.\n\nWhen `exponent` is a scalar value, the operation applied is:\n\nWhen `exponent` is a tensor, the operation applied is:\n\nWhen `exponent` is a tensor, the shapes of `input` and `exponent` must be\nbroadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n`self` is a scalar `float` value, and `exponent` is a tensor. The returned\ntensor `out` is of the same shape as `exponent`\n\nThe operation applied is:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.prod()", "path": "generated/torch.prod#torch.prod", "type": "torch", "text": "\nReturns the product of all elements in the `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\nReturns the product of each row of the `input` tensor in the given dimension\n`dim`.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1\nfewer dimension than `input`.\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.promote_types()", "path": "generated/torch.promote_types#torch.promote_types", "type": "torch", "text": "\nReturns the `torch.dtype` with the smallest size and scalar kind that is not\nsmaller nor of lower kind than either `type1` or `type2`. See type promotion\ndocumentation for more information on the type promotion logic.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.qr()", "path": "generated/torch.qr#torch.qr", "type": "torch", "text": "\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nIf `some` is `True`, then this function returns the thin (reduced) QR\nfactorization. Otherwise, if `some` is `False`, this function returns the\ncomplete QR factorization.\n\nWarning\n\n`torch.qr` is deprecated. Please use `torch.linalg.qr()` instead.\n\nDifferences with `torch.linalg.qr`:\n\n`torch.linalg.qr` takes a string parameter `mode` instead of `some`:\n\nWarning\n\nIf you plan to backpropagate through QR, note that the current backward\nimplementation is only well-defined when the first\nmin\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))\ncolumns of `input` are linearly independent. This behavior will propably\nchange once QR supports pivoting.\n\nNote\n\nThis function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may\nproduce different (valid) decompositions on different device types or\ndifferent platforms.\n\nsome (bool, optional) \u2013\n\nSet to `True` for reduced QR decomposition and `False` for complete QR\ndecomposition. If `k = min(m, n)` then:\n\nout (tuple, optional) \u2013 tuple of `Q` and `R` tensors. The dimensions of `Q`\nand `R` are detailed in the description of `some` above.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantile()", "path": "generated/torch.quantile#torch.quantile", "type": "torch", "text": "\nReturns the q-th quantiles of all elements in the `input` tensor, doing a\nlinear interpolation when the q-th quantile lies between two data points.\n\nExample:\n\nReturns the q-th quantiles of each row of the `input` tensor along the\ndimension `dim`, doing a linear interpolation when the q-th quantile lies\nbetween two data points. By default, `dim` is `None` resulting in the `input`\ntensor being flattened before computation.\n\nIf `keepdim` is `True`, the output dimensions are of the same size as `input`\nexcept in the dimensions being reduced (`dim` or all if `dim` is `None`) where\nthey have size 1. Otherwise, the dimensions being reduced are squeezed (see\n`torch.squeeze()`). If `q` is a 1D tensor, an extra dimension is prepended to\nthe output tensor with the same size as `q` which represents the quantiles.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization", "path": "torch.quantization", "type": "torch.quantization", "text": "\nThis module implements the functions you call directly to convert your model\nfrom FP32 to quantized form. For example the `prepare()` is used in post\ntraining quantization to prepares your model for the calibration step and\n`convert()` actually converts the weights to int8 and replaces the operations\nwith their quantized counterparts. There are other helper functions for things\nlike quantizing the input to your model and performing critical fusions like\nconv+relu.\n\nQuantize the input float model with post training static quantization.\n\nFirst it will prepare the model for calibration, then it calls `run_fn` which\nwill run the calibration step, after that we will convert the model to a\nquantized model.\n\nQuantized model.\n\nConverts a float model to dynamic (i.e. weights-only) quantized model.\n\nReplaces specified modules with dynamic weight-only quantized versions and\noutput the quantized model.\n\nFor simplest usage provide `dtype` argument that can be float16 or qint8.\nWeight-only quantization by default is performed for layers with large weights\nsize - i.e. Linear and RNN variants.\n\nFine grained control is possible with `qconfig` and `mapping` that act\nsimilarly to `quantize()`. If `qconfig` is provided, the `dtype` argument is\nignored.\n\nqconfig_spec \u2013\n\nEither:\n\nDo quantization aware training and output a quantized model\n\nQuantized model.\n\nPrepares a copy of the model for quantization calibration or quantization-\naware training.\n\nQuantization configuration should be assigned preemptively to individual\nsubmodules in `.qconfig` attribute.\n\nThe model will be attached with observer or fake quant modules, and qconfig\nwill be propagated.\n\nPrepares a copy of the model for quantization calibration or quantization-\naware training and converts it to quantized version.\n\nQuantization configuration should be assigned preemptively to individual\nsubmodules in `.qconfig` attribute.\n\nConverts submodules in input module to a different module according to\n`mapping` by calling `from_float` method on the target module class. And\nremove qconfig at the end if remove_qconfig is set to True.\n\nDescribes how to quantize a layer or a part of the network by providing\nsettings (observer classes) for activations and weights respectively.\n\nNote that QConfig needs to contain observer classes (like MinMaxObserver) or a\ncallable that returns instances on invocation, not the concrete observer\ninstances themselves. Quantization preparation function will instantiate\nobservers multiple times for each of the layers.\n\nObserver classes have usually reasonable default arguments, but they can be\noverwritten with `with_args` method (that behaves like functools.partial):\n\nmy_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8),\nweight=default_observer.with_args(dtype=torch.qint8))\n\nDescribes how to dynamically quantize a layer or a part of the network by\nproviding settings (observer classes) for weights.\n\nIt\u2019s like QConfig, but for dynamic quantization.\n\nNote that QConfigDynamic needs to contain observer classes (like\nMinMaxObserver) or a callable that returns instances on invocation, not the\nconcrete observer instances themselves. Quantization function will instantiate\nobservers multiple times for each of the layers.\n\nObserver classes have usually reasonable default arguments, but they can be\noverwritten with `with_args` method (that behaves like functools.partial):\n\nmy_qconfig =\nQConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))\n\nFuses a list of modules into a single module\n\nFuses only the following sequence of modules: conv, bn conv, bn, relu conv,\nrelu linear, relu bn, relu All other sequences are left unchanged. For these\nsequences, replaces the first item in the list with the fused module,\nreplacing the rest of the modules with identity.\n\nmodel with fused modules. A new copy is created if inplace=True.\n\nExamples:\n\nQuantize stub module, before calibration, this is same as an observer, it will\nbe swapped as `nnq.Quantize` in `convert`.\n\nqconfig \u2013 quantization configuration for the tensor, if qconfig is not\nprovided, we will get qconfig from parent modules\n\nDequantize stub module, before calibration, this is same as identity, this\nwill be swapped as `nnq.DeQuantize` in `convert`.\n\nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub\nand surround the call to module with call to quant and dequant modules.\n\nThis is used by the `quantization` utility functions to add the quant and\ndequant modules, before `convert` function `QuantStub` will just be observer,\nit observes the input tensor, after `convert`, `QuantStub` will be swapped to\n`nnq.Quantize` which does actual quantization. Similarly for `DeQuantStub`.\n\nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that\nthis function will modify the children of module inplace and it can return a\nnew module which wraps the input module as well.\n\nEither the inplace modified module with submodules wrapped in `QuantWrapper`\nbased on qconfig or a new `QuantWrapper` module which wraps the input module,\nthe latter case only happens when the input module is a leaf module and we\nwant to quantize it.\n\nAdd observer for the leaf child of the module.\n\nThis function insert observer module to all leaf child module that has a valid\nqconfig attribute.\n\nNone, module is modified inplace with added observer modules and forward_hooks\n\nSwaps the module if it has a quantized counterpart and it has an `observer`\nattached.\n\nThe corresponding quantized module of `mod`\n\nPropagate qconfig through the module hierarchy and assign `qconfig` attribute\non each leaf module\n\nNone, module is modified inplace with qconfig attached\n\nDefault evaluation function takes a torch.utils.data.Dataset or a list of\ninput Tensors and run the model on the dataset\n\nBase observer Module. Any observer implementation should derive from this\nclass.\n\nConcrete observers should follow the same API. In forward, they will update\nthe statistics of the observed Tensor. And they should provide a\n`calculate_qparams` function that computes the quantization parameters given\nthe collected statistics.\n\ndtype \u2013 Quantized data type\n\nWrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances.\n\nExample:\n\nObserver module for computing the quantization parameters based on the running\nmin and max values.\n\nThis observer uses the tensor min/max statistics to compute the quantization\nparameters. The module records the running minimum and maximum of incoming\ntensors, and uses this statistic to compute the quantization parameters.\n\nGiven running min/max as xminx_\\text{min} and xmaxx_\\text{max} , scale ss and\nzero point zz are computed as:\n\nThe running minimum/maximum xmin/maxx_\\text{min/max} is computed as:\n\nwhere XX is the observed tensor.\n\nThe scale ss and zero point zz are then computed as:\n\nwhere QminQ_\\text{min} and QmaxQ_\\text{max} are the minimum and maximum of the\nquantized data type.\n\nWarning\n\nOnly works with `torch.per_tensor_symmetric` quantization scheme\n\nWarning\n\n`dtype` can only take `torch.qint8` or `torch.quint8`.\n\nNote\n\nIf the running minimum equals to the running maximum, the scale and zero_point\nare set to 1.0 and 0.\n\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\nThis observer computes the quantization parameters based on the moving\naverages of minimums and maximums of the incoming tensors. The module records\nthe average minimum and maximum of incoming tensors, and uses this statistic\nto compute the quantization parameters.\n\nThe moving average min/max is computed as follows\n\nwhere xmin/maxx_\\text{min/max} is the running average min/max, XX is is the\nincoming tensor, and cc is the `averaging_constant`.\n\nThe scale and zero point are then computed as in `MinMaxObserver`.\n\nNote\n\nOnly works with `torch.per_tensor_affine` quantization scheme.\n\nNote\n\nIf the running minimum equals to the running maximum, the scale and zero_point\nare set to 1.0 and 0.\n\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\nThe quantization parameters are computed the same way as in `MinMaxObserver`,\nwith the difference that the running min/max values are stored per channel.\nScales and zero points are thus computed per channel as well.\n\nNote\n\nIf the running minimum equals to the running maximum, the scales and\nzero_points are set to 1.0 and 0.\n\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\nThe quantization parameters are computed the same way as in\n`MovingAverageMinMaxObserver`, with the difference that the running min/max\nvalues are stored per channel. Scales and zero points are thus computed per\nchannel as well.\n\nNote\n\nIf the running minimum equals to the running maximum, the scales and\nzero_points are set to 1.0 and 0.\n\nThe module records the running histogram of tensor values along with min/max\nvalues. `calculate_qparams` will calculate scale and zero_point.\n\nThe scale and zero point are computed as follows:\n\nThe histogram is computed continuously, and the ranges per bin change with\nevery new tensor observed.\n\nThe search for the min/max values ensures the minimization of the quantization\nerror with respect to the floating point model.\n\n`MinMaxObserver`\n\nSimulate the quantize and dequantize operations in training time. The output\nof this module is given by\n\nx_out = (clamp(round(x/scale + zero_point), quant_min,\nquant_max)-zero_point)*scale\n\nallowable values are torch.qint8 and torch.quint8. The values of quant_min and\nquant_max should be chosen to be consistent with the dtype\n\n~FakeQuantize.observer (Module) \u2013 User provided module that collects\nstatistics on the input tensor and provides a method to calculate scale and\nzero-point.\n\nObserver that doesn\u2019t do anything and just passes its configuration to the\nquantized module\u2019s `.from_float()`.\n\nPrimarily used for quantization to float16 which doesn\u2019t require determining\nranges.\n\nTraverse the modules and save all observers into dict. This is mainly used for\nquantization accuracy debug :param mod: the top module we want to save all\nobservers :param prefix: the prefix for the current module :param target_dict:\nthe dictionary used to save all the observers\n\nThe module is mainly for debug and records the tensor values during runtime.\n\n`nn.intrinsic`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.add_observer_()", "path": "torch.quantization#torch.quantization.add_observer_", "type": "torch.quantization", "text": "\nAdd observer for the leaf child of the module.\n\nThis function insert observer module to all leaf child module that has a valid\nqconfig attribute.\n\nNone, module is modified inplace with added observer modules and forward_hooks\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.add_quant_dequant()", "path": "torch.quantization#torch.quantization.add_quant_dequant", "type": "torch.quantization", "text": "\nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that\nthis function will modify the children of module inplace and it can return a\nnew module which wraps the input module as well.\n\nEither the inplace modified module with submodules wrapped in `QuantWrapper`\nbased on qconfig or a new `QuantWrapper` module which wraps the input module,\nthe latter case only happens when the input module is a leaf module and we\nwant to quantize it.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.convert()", "path": "torch.quantization#torch.quantization.convert", "type": "torch.quantization", "text": "\nConverts submodules in input module to a different module according to\n`mapping` by calling `from_float` method on the target module class. And\nremove qconfig at the end if remove_qconfig is set to True.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.default_eval_fn()", "path": "torch.quantization#torch.quantization.default_eval_fn", "type": "torch.quantization", "text": "\nDefault evaluation function takes a torch.utils.data.Dataset or a list of\ninput Tensors and run the model on the dataset\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.DeQuantStub", "path": "torch.quantization#torch.quantization.DeQuantStub", "type": "torch.quantization", "text": "\nDequantize stub module, before calibration, this is same as identity, this\nwill be swapped as `nnq.DeQuantize` in `convert`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.FakeQuantize", "path": "torch.quantization#torch.quantization.FakeQuantize", "type": "torch.quantization", "text": "\nSimulate the quantize and dequantize operations in training time. The output\nof this module is given by\n\nx_out = (clamp(round(x/scale + zero_point), quant_min,\nquant_max)-zero_point)*scale\n\nallowable values are torch.qint8 and torch.quint8. The values of quant_min and\nquant_max should be chosen to be consistent with the dtype\n\n~FakeQuantize.observer (Module) \u2013 User provided module that collects\nstatistics on the input tensor and provides a method to calculate scale and\nzero-point.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.fuse_modules()", "path": "torch.quantization#torch.quantization.fuse_modules", "type": "torch.quantization", "text": "\nFuses a list of modules into a single module\n\nFuses only the following sequence of modules: conv, bn conv, bn, relu conv,\nrelu linear, relu bn, relu All other sequences are left unchanged. For these\nsequences, replaces the first item in the list with the fused module,\nreplacing the rest of the modules with identity.\n\nmodel with fused modules. A new copy is created if inplace=True.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.get_observer_dict()", "path": "torch.quantization#torch.quantization.get_observer_dict", "type": "torch.quantization", "text": "\nTraverse the modules and save all observers into dict. This is mainly used for\nquantization accuracy debug :param mod: the top module we want to save all\nobservers :param prefix: the prefix for the current module :param target_dict:\nthe dictionary used to save all the observers\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.HistogramObserver", "path": "torch.quantization#torch.quantization.HistogramObserver", "type": "torch.quantization", "text": "\nThe module records the running histogram of tensor values along with min/max\nvalues. `calculate_qparams` will calculate scale and zero_point.\n\nThe scale and zero point are computed as follows:\n\nThe histogram is computed continuously, and the ranges per bin change with\nevery new tensor observed.\n\nThe search for the min/max values ensures the minimization of the quantization\nerror with respect to the floating point model.\n\n`MinMaxObserver`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.MinMaxObserver", "path": "torch.quantization#torch.quantization.MinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the running\nmin and max values.\n\nThis observer uses the tensor min/max statistics to compute the quantization\nparameters. The module records the running minimum and maximum of incoming\ntensors, and uses this statistic to compute the quantization parameters.\n\nGiven running min/max as xminx_\\text{min} and xmaxx_\\text{max} , scale ss and\nzero point zz are computed as:\n\nThe running minimum/maximum xmin/maxx_\\text{min/max} is computed as:\n\nwhere XX is the observed tensor.\n\nThe scale ss and zero point zz are then computed as:\n\nwhere QminQ_\\text{min} and QmaxQ_\\text{max} are the minimum and maximum of the\nquantized data type.\n\nWarning\n\nOnly works with `torch.per_tensor_symmetric` quantization scheme\n\nWarning\n\n`dtype` can only take `torch.qint8` or `torch.quint8`.\n\nNote\n\nIf the running minimum equals to the running maximum, the scale and zero_point\nare set to 1.0 and 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.MovingAverageMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAverageMinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\nThis observer computes the quantization parameters based on the moving\naverages of minimums and maximums of the incoming tensors. The module records\nthe average minimum and maximum of incoming tensors, and uses this statistic\nto compute the quantization parameters.\n\nThe moving average min/max is computed as follows\n\nwhere xmin/maxx_\\text{min/max} is the running average min/max, XX is is the\nincoming tensor, and cc is the `averaging_constant`.\n\nThe scale and zero point are then computed as in `MinMaxObserver`.\n\nNote\n\nOnly works with `torch.per_tensor_affine` quantization scheme.\n\nNote\n\nIf the running minimum equals to the running maximum, the scale and zero_point\nare set to 1.0 and 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.MovingAveragePerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAveragePerChannelMinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\nThe quantization parameters are computed the same way as in\n`MovingAverageMinMaxObserver`, with the difference that the running min/max\nvalues are stored per channel. Scales and zero points are thus computed per\nchannel as well.\n\nNote\n\nIf the running minimum equals to the running maximum, the scales and\nzero_points are set to 1.0 and 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.NoopObserver", "path": "torch.quantization#torch.quantization.NoopObserver", "type": "torch.quantization", "text": "\nObserver that doesn\u2019t do anything and just passes its configuration to the\nquantized module\u2019s `.from_float()`.\n\nPrimarily used for quantization to float16 which doesn\u2019t require determining\nranges.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.ObserverBase", "path": "torch.quantization#torch.quantization.ObserverBase", "type": "torch.quantization", "text": "\nBase observer Module. Any observer implementation should derive from this\nclass.\n\nConcrete observers should follow the same API. In forward, they will update\nthe statistics of the observed Tensor. And they should provide a\n`calculate_qparams` function that computes the quantization parameters given\nthe collected statistics.\n\ndtype \u2013 Quantized data type\n\nWrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.ObserverBase.with_args()", "path": "torch.quantization#torch.quantization.ObserverBase.with_args", "type": "torch.quantization", "text": "\nWrapper that allows creation of class factories.\n\nThis can be useful when there is a need to create classes with the same\nconstructor arguments, but different instances.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.PerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.PerChannelMinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\nThis observer uses the tensor min/max statistics to compute the per channel\nquantization parameters. The module records the running minimum and maximum of\nincoming tensors, and uses this statistic to compute the quantization\nparameters.\n\nThe quantization parameters are computed the same way as in `MinMaxObserver`,\nwith the difference that the running min/max values are stored per channel.\nScales and zero points are thus computed per channel as well.\n\nNote\n\nIf the running minimum equals to the running maximum, the scales and\nzero_points are set to 1.0 and 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.prepare()", "path": "torch.quantization#torch.quantization.prepare", "type": "torch.quantization", "text": "\nPrepares a copy of the model for quantization calibration or quantization-\naware training.\n\nQuantization configuration should be assigned preemptively to individual\nsubmodules in `.qconfig` attribute.\n\nThe model will be attached with observer or fake quant modules, and qconfig\nwill be propagated.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.prepare_qat()", "path": "torch.quantization#torch.quantization.prepare_qat", "type": "torch.quantization", "text": "\nPrepares a copy of the model for quantization calibration or quantization-\naware training and converts it to quantized version.\n\nQuantization configuration should be assigned preemptively to individual\nsubmodules in `.qconfig` attribute.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.propagate_qconfig_()", "path": "torch.quantization#torch.quantization.propagate_qconfig_", "type": "torch.quantization", "text": "\nPropagate qconfig through the module hierarchy and assign `qconfig` attribute\non each leaf module\n\nNone, module is modified inplace with qconfig attached\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.QConfig", "path": "torch.quantization#torch.quantization.QConfig", "type": "torch.quantization", "text": "\nDescribes how to quantize a layer or a part of the network by providing\nsettings (observer classes) for activations and weights respectively.\n\nNote that QConfig needs to contain observer classes (like MinMaxObserver) or a\ncallable that returns instances on invocation, not the concrete observer\ninstances themselves. Quantization preparation function will instantiate\nobservers multiple times for each of the layers.\n\nObserver classes have usually reasonable default arguments, but they can be\noverwritten with `with_args` method (that behaves like functools.partial):\n\nmy_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8),\nweight=default_observer.with_args(dtype=torch.qint8))\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.QConfigDynamic", "path": "torch.quantization#torch.quantization.QConfigDynamic", "type": "torch.quantization", "text": "\nDescribes how to dynamically quantize a layer or a part of the network by\nproviding settings (observer classes) for weights.\n\nIt\u2019s like QConfig, but for dynamic quantization.\n\nNote that QConfigDynamic needs to contain observer classes (like\nMinMaxObserver) or a callable that returns instances on invocation, not the\nconcrete observer instances themselves. Quantization function will instantiate\nobservers multiple times for each of the layers.\n\nObserver classes have usually reasonable default arguments, but they can be\noverwritten with `with_args` method (that behaves like functools.partial):\n\nmy_qconfig =\nQConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.quantize()", "path": "torch.quantization#torch.quantization.quantize", "type": "torch.quantization", "text": "\nQuantize the input float model with post training static quantization.\n\nFirst it will prepare the model for calibration, then it calls `run_fn` which\nwill run the calibration step, after that we will convert the model to a\nquantized model.\n\nQuantized model.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.quantize_dynamic()", "path": "torch.quantization#torch.quantization.quantize_dynamic", "type": "torch.quantization", "text": "\nConverts a float model to dynamic (i.e. weights-only) quantized model.\n\nReplaces specified modules with dynamic weight-only quantized versions and\noutput the quantized model.\n\nFor simplest usage provide `dtype` argument that can be float16 or qint8.\nWeight-only quantization by default is performed for layers with large weights\nsize - i.e. Linear and RNN variants.\n\nFine grained control is possible with `qconfig` and `mapping` that act\nsimilarly to `quantize()`. If `qconfig` is provided, the `dtype` argument is\nignored.\n\nqconfig_spec \u2013\n\nEither:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.quantize_qat()", "path": "torch.quantization#torch.quantization.quantize_qat", "type": "torch.quantization", "text": "\nDo quantization aware training and output a quantized model\n\nQuantized model.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.QuantStub", "path": "torch.quantization#torch.quantization.QuantStub", "type": "torch.quantization", "text": "\nQuantize stub module, before calibration, this is same as an observer, it will\nbe swapped as `nnq.Quantize` in `convert`.\n\nqconfig \u2013 quantization configuration for the tensor, if qconfig is not\nprovided, we will get qconfig from parent modules\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.QuantWrapper", "path": "torch.quantization#torch.quantization.QuantWrapper", "type": "torch.quantization", "text": "\nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub\nand surround the call to module with call to quant and dequant modules.\n\nThis is used by the `quantization` utility functions to add the quant and\ndequant modules, before `convert` function `QuantStub` will just be observer,\nit observes the input tensor, after `convert`, `QuantStub` will be swapped to\n`nnq.Quantize` which does actual quantization. Similarly for `DeQuantStub`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.RecordingObserver", "path": "torch.quantization#torch.quantization.RecordingObserver", "type": "torch.quantization", "text": "\nThe module is mainly for debug and records the tensor values during runtime.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantization.swap_module()", "path": "torch.quantization#torch.quantization.swap_module", "type": "torch.quantization", "text": "\nSwaps the module if it has a quantized counterpart and it has an `observer`\nattached.\n\nThe corresponding quantized module of `mod`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantize_per_channel()", "path": "generated/torch.quantize_per_channel#torch.quantize_per_channel", "type": "torch", "text": "\nConverts a float tensor to a per-channel quantized tensor with given scales\nand zero points.\n\nA newly quantized tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quantize_per_tensor()", "path": "generated/torch.quantize_per_tensor#torch.quantize_per_tensor", "type": "torch", "text": "\nConverts a float tensor to a quantized tensor with given scale and zero point.\n\nA newly quantized tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine", "type": "torch", "text": "\nThe `torch.quasirandom.SobolEngine` is an engine for generating (scrambled)\nSobol sequences. Sobol sequences are an example of low discrepancy quasi-\nrandom sequences.\n\nThis implementation of an engine for Sobol sequences is capable of sampling\nsequences up to a maximum dimension of 21201. It uses direction numbers from\nhttps://web.maths.unsw.edu.au/~fkuo/sobol/ obtained using the search criterion\nD(6) up to the dimension 21201. This is the recommended choice by the authors.\n\nExamples:\n\nFunction to draw a sequence of `n` points from a Sobol sequence. Note that the\nsamples are dependent on the previous samples. The size of the result is\n(n,dimension)(n, dimension) .\n\nFunction to draw a sequence of `2**m` points from a Sobol sequence. Note that\nthe samples are dependent on the previous samples. The size of the result is\n(2\u2217\u2217m,dimension)(2**m, dimension) .\n\nFunction to fast-forward the state of the `SobolEngine` by `n` steps. This is\nequivalent to drawing `n` samples without using the samples.\n\nn (Int) \u2013 The number of steps to fast-forward by.\n\nFunction to reset the `SobolEngine` to base state.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.draw()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw", "type": "torch", "text": "\nFunction to draw a sequence of `n` points from a Sobol sequence. Note that the\nsamples are dependent on the previous samples. The size of the result is\n(n,dimension)(n, dimension) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.draw_base2()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw_base2", "type": "torch", "text": "\nFunction to draw a sequence of `2**m` points from a Sobol sequence. Note that\nthe samples are dependent on the previous samples. The size of the result is\n(2\u2217\u2217m,dimension)(2**m, dimension) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.fast_forward()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.fast_forward", "type": "torch", "text": "\nFunction to fast-forward the state of the `SobolEngine` by `n` steps. This is\nequivalent to drawing `n` samples without using the samples.\n\nn (Int) \u2013 The number of steps to fast-forward by.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.reset()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.reset", "type": "torch", "text": "\nFunction to reset the `SobolEngine` to base state.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.rad2deg()", "path": "generated/torch.rad2deg#torch.rad2deg", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` converted from\nangles in radians to degrees.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.rand()", "path": "generated/torch.rand#torch.rand", "type": "torch", "text": "\nReturns a tensor filled with random numbers from a uniform distribution on the\ninterval [0,1)[0, 1)\n\nThe shape of the tensor is defined by the variable argument `size`.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor. Can be a variable number of arguments or a collection like a list or\ntuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.randint()", "path": "generated/torch.randint#torch.randint", "type": "torch", "text": "\nReturns a tensor filled with random integers generated uniformly between `low`\n(inclusive) and `high` (exclusive).\n\nThe shape of the tensor is defined by the variable argument `size`.\n\nNote\n\nWith the global dtype default (`torch.float32`), this function returns a\ntensor with dtype `torch.int64`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.randint_like()", "path": "generated/torch.randint_like#torch.randint_like", "type": "torch", "text": "\nReturns a tensor with the same shape as Tensor `input` filled with random\nintegers generated uniformly between `low` (inclusive) and `high` (exclusive).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.randn()", "path": "generated/torch.randn#torch.randn", "type": "torch", "text": "\nReturns a tensor filled with random numbers from a normal distribution with\nmean `0` and variance `1` (also called the standard normal distribution).\n\nThe shape of the tensor is defined by the variable argument `size`.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor. Can be a variable number of arguments or a collection like a list or\ntuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.randn_like()", "path": "generated/torch.randn_like#torch.randn_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a normal distribution with mean 0 and variance 1.\n`torch.randn_like(input)` is equivalent to `torch.randn(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\ninput (Tensor) \u2013 the size of `input` will determine size of the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random", "path": "random", "type": "torch.random", "text": "\nForks the RNG, so that when you return, the RNG is reset to the state that it\nwas previously in.\n\nReturns the random number generator state as a `torch.ByteTensor`.\n\nReturns the initial seed for generating random numbers as a Python `long`.\n\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\nSets the seed for generating random numbers to a non-deterministic random\nnumber. Returns a 64 bit number used to seed the RNG.\n\nSets the random number generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random.fork_rng()", "path": "random#torch.random.fork_rng", "type": "torch.random", "text": "\nForks the RNG, so that when you return, the RNG is reset to the state that it\nwas previously in.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random.get_rng_state()", "path": "random#torch.random.get_rng_state", "type": "torch.random", "text": "\nReturns the random number generator state as a `torch.ByteTensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random.initial_seed()", "path": "random#torch.random.initial_seed", "type": "torch.random", "text": "\nReturns the initial seed for generating random numbers as a Python `long`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random.manual_seed()", "path": "random#torch.random.manual_seed", "type": "torch.random", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random.seed()", "path": "random#torch.random.seed", "type": "torch.random", "text": "\nSets the seed for generating random numbers to a non-deterministic random\nnumber. Returns a 64 bit number used to seed the RNG.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.random.set_rng_state()", "path": "random#torch.random.set_rng_state", "type": "torch.random", "text": "\nSets the random number generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.randperm()", "path": "generated/torch.randperm#torch.randperm", "type": "torch", "text": "\nReturns a random permutation of integers from `0` to `n - 1`.\n\nn (int) \u2013 the upper bound (exclusive)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.rand_like()", "path": "generated/torch.rand_like#torch.rand_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a uniform distribution on the interval [0,1)[0, 1) .\n`torch.rand_like(input)` is equivalent to `torch.rand(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\ninput (Tensor) \u2013 the size of `input` will determine size of the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.range()", "path": "generated/torch.range#torch.range", "type": "torch", "text": "\nReturns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rfloor + 1 with values from `start` to `end`\nwith step `step`. Step is the gap between two values in the tensor.\n\nWarning\n\nThis function is deprecated and will be removed in a future release because\nits behavior is inconsistent with Python\u2019s range builtin. Instead, use\n`torch.arange()`, which produces values in [start, end).\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ravel()", "path": "generated/torch.ravel#torch.ravel", "type": "torch", "text": "\nReturn a contiguous flattened tensor. A copy is made only if needed.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.real()", "path": "generated/torch.real#torch.real", "type": "torch", "text": "\nReturns a new tensor containing real values of the `self` tensor. The returned\ntensor and `self` share the same underlying storage.\n\nWarning\n\n`real()` is only supported for tensors with complex dtypes.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.reciprocal()", "path": "generated/torch.reciprocal#torch.reciprocal", "type": "torch", "text": "\nReturns a new tensor with the reciprocal of the elements of `input`\n\nNote\n\nUnlike NumPy\u2019s reciprocal, torch.reciprocal supports integral inputs. Integral\ninputs to reciprocal are automatically promoted to the default scalar type.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.remainder()", "path": "generated/torch.remainder#torch.remainder", "type": "torch", "text": "\nComputes the element-wise remainder of division.\n\nThe dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor `other`.\n\nSupports broadcasting to a common shape, type promotion, and integer and float\ninputs.\n\nNote\n\nComplex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee `torch.fmod()` for how division by zero is handled.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nSee also\n\n`torch.fmod()`, which computes the element-wise remainder of division\nequivalently to the C library function `fmod()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.renorm()", "path": "generated/torch.renorm#torch.renorm", "type": "torch", "text": "\nReturns a tensor where each sub-tensor of `input` along dimension `dim` is\nnormalized such that the `p`-norm of the sub-tensor is lower than the value\n`maxnorm`\n\nNote\n\nIf the norm of a row is lower than `maxnorm`, the row is unchanged\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.repeat_interleave()", "path": "generated/torch.repeat_interleave#torch.repeat_interleave", "type": "torch", "text": "\nRepeat elements of a tensor.\n\nWarning\n\nThis is different from `torch.Tensor.repeat()` but similar to `numpy.repeat`.\n\nRepeated tensor which has the same shape as input, except along the given\naxis.\n\nTensor\n\nExample:\n\nIf the `repeats` is `tensor([n1, n2, n3, \u2026])`, then the output will be\n`tensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026])` where `0` appears `n1` times, `1`\nappears `n2` times, `2` appears `n3` times, etc.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.reshape()", "path": "generated/torch.reshape#torch.reshape", "type": "torch", "text": "\nReturns a tensor with the same data and number of elements as `input`, but\nwith the specified shape. When possible, the returned tensor will be a view of\n`input`. Otherwise, it will be a copy. Contiguous inputs and inputs with\ncompatible strides can be reshaped without copying, but you should not depend\non the copying vs. viewing behavior.\n\nSee `torch.Tensor.view()` on when it is possible to return a view.\n\nA single dimension may be -1, in which case it\u2019s inferred from the remaining\ndimensions and the number of elements in `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.result_type()", "path": "generated/torch.result_type#torch.result_type", "type": "torch", "text": "\nReturns the `torch.dtype` that would result from performing an arithmetic\noperation on the provided input tensors. See type promotion documentation for\nmore information on the type promotion logic.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.roll()", "path": "generated/torch.roll#torch.roll", "type": "torch", "text": "\nRoll the tensor along the given dimension(s). Elements that are shifted beyond\nthe last position are re-introduced at the first position. If a dimension is\nnot specified, the tensor will be flattened before rolling and then restored\nto the original shape.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.rot90()", "path": "generated/torch.rot90#torch.rot90", "type": "torch", "text": "\nRotate a n-D tensor by 90 degrees in the plane specified by dims axis.\nRotation direction is from the first towards the second axis if k > 0, and\nfrom the second towards the first for k < 0.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.round()", "path": "generated/torch.round#torch.round", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` rounded to the\nclosest integer.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.row_stack()", "path": "generated/torch.row_stack#torch.row_stack", "type": "torch", "text": "\nAlias of `torch.vstack()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.rsqrt()", "path": "generated/torch.rsqrt#torch.rsqrt", "type": "torch", "text": "\nReturns a new tensor with the reciprocal of the square-root of each of the\nelements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.save()", "path": "generated/torch.save#torch.save", "type": "torch", "text": "\nSaves an object to a disk file.\n\nSee also: `saving-loading-tensors`\n\nNote\n\nA common PyTorch convention is to save tensors using .pt file extension.\n\nNote\n\nPyTorch preserves storage sharing across serialization. See `preserve-storage-\nsharing` for more details.\n\nNote\n\nThe 1.6 release of PyTorch switched `torch.save` to use a new zipfile-based\nfile format. `torch.load` still retains the ability to load files in the old\nformat. If for any reason you want `torch.save` to use the old format, pass\nthe kwarg `_use_new_zipfile_serialization=False`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.scatter()", "path": "generated/torch.scatter#torch.scatter", "type": "torch", "text": "\nOut-of-place version of `torch.Tensor.scatter_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.scatter_add()", "path": "generated/torch.scatter_add#torch.scatter_add", "type": "torch", "text": "\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.searchsorted()", "path": "generated/torch.searchsorted#torch.searchsorted", "type": "torch", "text": "\nFind the indices from the innermost dimension of `sorted_sequence` such that,\nif the corresponding values in `values` were inserted before the indices, the\norder of the corresponding innermost dimension within `sorted_sequence` would\nbe preserved. Return a new tensor with the same size as `values`. If `right`\nis False (default), then the left boundary of `sorted_sequence` is closed.\nMore formally, the returned index satisfies the following rules:\n\n`sorted_sequence`\n\n`right`\n\nreturned index satisfies\n\n1-D\n\nFalse\n\n`sorted_sequence[i-1] < values[m][n]...[l][x] <= sorted_sequence[i]`\n\n1-D\n\nTrue\n\n`sorted_sequence[i-1] <= values[m][n]...[l][x] < sorted_sequence[i]`\n\nN-D\n\nFalse\n\n`sorted_sequence[m][n]...[l][i-1] < values[m][n]...[l][x] <=\nsorted_sequence[m][n]...[l][i]`\n\nN-D\n\nTrue\n\n`sorted_sequence[m][n]...[l][i-1] <= values[m][n]...[l][x] <\nsorted_sequence[m][n]...[l][i]`\n\nNote\n\nIf your use case is always 1-D sorted sequence, `torch.bucketize()` is\npreferred, because it has fewer dimension checks resulting in slightly better\nperformance.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.seed()", "path": "generated/torch.seed#torch.seed", "type": "torch", "text": "\nSets the seed for generating random numbers to a non-deterministic random\nnumber. Returns a 64 bit number used to seed the RNG.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_default_dtype()", "path": "generated/torch.set_default_dtype#torch.set_default_dtype", "type": "torch", "text": "\nSets the default floating point dtype to `d`. This dtype is:\n\nThe default floating point dtype is initially `torch.float32`.\n\nd (`torch.dtype`) \u2013 the floating point dtype to make the default\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_default_tensor_type()", "path": "generated/torch.set_default_tensor_type#torch.set_default_tensor_type", "type": "torch", "text": "\nSets the default `torch.Tensor` type to floating point tensor type `t`. This\ntype will also be used as default floating point type for type inference in\n`torch.tensor()`.\n\nThe default floating point tensor type is initially `torch.FloatTensor`.\n\nt (type or string) \u2013 the floating point tensor type or its name\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_flush_denormal()", "path": "generated/torch.set_flush_denormal#torch.set_flush_denormal", "type": "torch", "text": "\nDisables denormal floating numbers on CPU.\n\nReturns `True` if your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode. `set_flush_denormal()` is only\nsupported on x86 architectures supporting SSE3.\n\nmode (bool) \u2013 Controls whether to enable flush denormal mode or not\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_grad_enabled", "path": "generated/torch.set_grad_enabled#torch.set_grad_enabled", "type": "torch", "text": "\nContext-manager that sets gradient calculation to on or off.\n\n`set_grad_enabled` will enable or disable grads based on its argument `mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nmode (bool) \u2013 Flag whether to enable grad (`True`), or disable (`False`). This\ncan be used to conditionally enable gradients.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_num_interop_threads()", "path": "generated/torch.set_num_interop_threads#torch.set_num_interop_threads", "type": "torch", "text": "\nSets the number of threads used for interop parallelism (e.g. in JIT\ninterpreter) on CPU.\n\nWarning\n\nCan only be called once and before any inter-op parallel work is started (e.g.\nJIT execution).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_num_threads()", "path": "generated/torch.set_num_threads#torch.set_num_threads", "type": "torch", "text": "\nSets the number of threads used for intraop parallelism on CPU.\n\nWarning\n\nTo ensure that the correct number of threads is used, set_num_threads must be\ncalled before running eager, JIT or autograd code.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_printoptions()", "path": "generated/torch.set_printoptions#torch.set_printoptions", "type": "torch", "text": "\nSet options for printing. Items shamelessly taken from NumPy\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.set_rng_state()", "path": "generated/torch.set_rng_state#torch.set_rng_state", "type": "torch", "text": "\nSets the random number generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sgn()", "path": "generated/torch.sgn#torch.sgn", "type": "torch", "text": "\nFor complex tensors, this function returns a new tensor whose elemants have\nthe same angle as that of the elements of `input` and absolute value 1. For a\nnon-complex tensor, this function returns the signs of the elements of `input`\n(see `torch.sign()`).\n\nouti=0\\text{out}_{i} = 0 , if \u2223inputi\u2223==0|{\\text{{input}}_i}| == 0\nouti=inputi\u2223inputi\u2223\\text{out}_{i} =\n\\frac{{\\text{{input}}_i}}{|{\\text{{input}}_i}|} , otherwise\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sigmoid()", "path": "generated/torch.sigmoid#torch.sigmoid", "type": "torch", "text": "\nReturns a new tensor with the sigmoid of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sign()", "path": "generated/torch.sign#torch.sign", "type": "torch", "text": "\nReturns a new tensor with the signs of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.signbit()", "path": "generated/torch.signbit#torch.signbit", "type": "torch", "text": "\nTests if each element of `input` has its sign bit set (is less than zero) or\nnot.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sin()", "path": "generated/torch.sin#torch.sin", "type": "torch", "text": "\nReturns a new tensor with the sine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sinc()", "path": "generated/torch.sinc#torch.sinc", "type": "torch", "text": "\nComputes the normalized sinc of `input.`\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sinh()", "path": "generated/torch.sinh#torch.sinh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic sine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nNote\n\nWhen `input` is on the CPU, the implementation of torch.sinh may use the Sleef\nlibrary, which rounds very large results to infinity or negative infinity. See\nhere for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.slogdet()", "path": "generated/torch.slogdet#torch.slogdet", "type": "torch", "text": "\nCalculates the sign and log absolute value of the determinant(s) of a square\nmatrix or batches of square matrices.\n\nNote\n\n`torch.slogdet()` is deprecated. Please use `torch.linalg.slogdet()` instead.\n\nNote\n\nIf `input` has zero determinant, this returns `(0, -inf)`.\n\nNote\n\nBackward through `slogdet()` internally uses SVD results when `input` is not\ninvertible. In this case, double backward through `slogdet()` will be unstable\nin when `input` doesn\u2019t have distinct singular values. See `svd()` for\ndetails.\n\ninput (Tensor) \u2013 the input tensor of size `(*, n, n)` where `*` is zero or\nmore batch dimensions.\n\nA namedtuple (sign, logabsdet) containing the sign of the determinant, and the\nlog value of the absolute determinant.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.smm()", "path": "sparse#torch.smm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of the sparse matrix `input` with the dense\nmatrix `mat`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.solve()", "path": "generated/torch.solve#torch.solve", "type": "torch", "text": "\nThis function returns the solution to the system of linear equations\nrepresented by AX=BAX = B and the LU factorization of A, in order as a\nnamedtuple `solution, LU`.\n\n`LU` contains `L` and `U` factors for LU factorization of `A`.\n\n`torch.solve(B, A)` can take in 2D inputs `B, A` or inputs that are batches of\n2D matrices. If the inputs are batches, then returns batched outputs\n`solution, LU`.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nIrrespective of the original strides, the returned matrices `solution` and\n`LU` will be transposed, i.e. with strides like `B.contiguous().transpose(-1,\n-2).stride()` and `A.contiguous().transpose(-1, -2).stride()` respectively.\n\nout ((Tensor, Tensor), optional) \u2013 optional output tuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sort()", "path": "generated/torch.sort#torch.sort", "type": "torch", "text": "\nSorts the elements of the `input` tensor along a given dimension in ascending\norder by value.\n\nIf `dim` is not given, the last dimension of the `input` is chosen.\n\nIf `descending` is `True` then the elements are sorted in descending order by\nvalue.\n\nA namedtuple of (values, indices) is returned, where the `values` are the\nsorted values and `indices` are the indices of the elements in the original\n`input` tensor.\n\nout (tuple, optional) \u2013 the output tuple of (`Tensor`, `LongTensor`) that can\nbe optionally given to be used as output buffers\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse", "path": "sparse", "type": "torch.sparse", "text": "\nPyTorch provides `torch.Tensor` to represent a multi-dimensional array\ncontaining elements of a single data type. By default, array elements are\nstored contiguously in memory leading to efficient implementations of various\narray processing algorithms that relay on the fast access to array elements.\nHowever, there exists an important class of multi-dimensional arrays, so-\ncalled sparse arrays, where the contiguous memory storage of array elements\nturns out to be suboptimal. Sparse arrays have a property of having a vast\nportion of elements being equal to zero which means that a lot of memory as\nwell as processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO, CSR/CSC,\nLIL, etc.) have been developed that are optimized for a particular structure\nof non-zero elements in sparse arrays as well as for specific operations on\nthe arrays.\n\nNote\n\nWhen talking about storing only non-zero elements of a sparse array, the usage\nof adjective \u201cnon-zero\u201d is not strict: one is allowed to store also zeros in\nthe sparse array data structure. Hence, in the following, we use \u201cspecified\nelements\u201d for those array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but not only,\nhence we use the term \u201cfill value\u201d to denote such elements.\n\nNote\n\nUsing a sparse storage format for storing sparse arrays can be advantageous\nonly when the size and sparsity levels of arrays are high. Otherwise, for\nsmall-sized or low-sparsity arrays using the contiguous memory storage format\nis likely the most efficient approach.\n\nWarning\n\nThe PyTorch API of sparse tensors is in beta and may change in the near\nfuture.\n\nCurrently, PyTorch implements the so-called Coordinate format, or COO format,\nas the default sparse storage format for storing sparse tensors. In COO\nformat, the specified elements are stored as tuples of element indices and the\ncorresponding values. In particular,\n\nwhere `ndim` is the dimensionality of the tensor and `nse` is the number of\nspecified elements.\n\nNote\n\nThe memory consumption of a sparse COO tensor is at least `(ndim * 8 + <size\nof element type in bytes>) * nse` bytes (plus a constant overhead from storing\nother tensor data).\n\nThe memory consumption of a strided tensor is at least `product(<tensor\nshape>) * <size of element type in bytes>`.\n\nFor example, the memory consumption of a 10 000 x 10 000 tensor with 100 000\nnon-zero 32-bit floating point numbers is at least `(2 * 8 + 4) * 100 000 = 2\n000 000` bytes when using COO tensor layout and `10 000 * 10 000 * 4 = 400 000\n000` bytes when using the default strided tensor layout. Notice the 200 fold\nmemory saving from using the COO storage format.\n\nA sparse COO tensor can be constructed by providing the two tensors of indices\nand values, as well as the size of the sparse tensor (when it cannot be\ninferred from the indices and values tensors) to a function\n`torch.sparse_coo_tensor()`.\n\nSuppose we want to define a sparse tensor with the entry 3 at location (0, 2),\nentry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified\nelements are assumed to have the same value, fill value, which is zero by\ndefault. We would then write:\n\nNote that the input `i` is NOT a list of index tuples. If you want to write\nyour indices this way, you should transpose before passing them to the sparse\nconstructor:\n\nAn empty sparse COO tensor can be constructed by specifying its size only:\n\nPytorch implements an extension of sparse tensors with scalar values to sparse\ntensors with (contiguous) tensor values. Such tensors are called hybrid\ntensors.\n\nPyTorch hybrid COO tensor extends the sparse COO tensor by allowing the\n`values` tensor to be a multi-dimensional tensor so that we have:\n\nNote\n\nWe use (M + K)-dimensional tensor to denote a N-dimensional hybrid sparse\ntensor, where M and K are the numbers of sparse and dense dimensions,\nrespectively, such that M + K == N holds.\n\nSuppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4]\nat location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at\nlocation (1, 2). We would write\n\nIn general, if `s` is a sparse COO tensor and `M = s.sparse_dim()`, `K =\ns.dense_dim()`, then we have the following invariants:\n\nNote\n\nDense dimensions always follow sparse dimensions, that is, mixing of dense and\nsparse dimensions is not supported.\n\nPyTorch sparse COO tensor format permits uncoalesced sparse tensors, where\nthere may be duplicate coordinates in the indices; in this case, the\ninterpretation is that the value at that index is the sum of all duplicate\nvalue entries. For example, one can specify multiple values, `3` and `4`, for\nthe same index `1`, that leads to an 1-D uncoalesced tensor:\n\nwhile the coalescing process will accumulate the multi-valued elements into a\nsingle value using summation:\n\nIn general, the output of `torch.Tensor.coalesce()` method is a sparse tensor\nwith the following properties:\n\nNote\n\nFor the most part, you shouldn\u2019t have to care whether or not a sparse tensor\nis coalesced or not, as most operations will work identically given a\ncoalesced or uncoalesced sparse tensor.\n\nHowever, some operations can be implemented more efficiently on uncoalesced\ntensors, and some on coalesced tensors.\n\nFor instance, addition of sparse COO tensors is implemented by simply\nconcatenating the indices and values tensors:\n\nIf you repeatedly perform an operation that can produce duplicate entries\n(e.g., `torch.Tensor.add()`), you should occasionally coalesce your sparse\ntensors to prevent them from growing too large.\n\nOn the other hand, the lexicographical ordering of indices can be advantageous\nfor implementing algorithms that involve many element selection operations,\nsuch as slicing or matrix products.\n\nLet\u2019s consider the following example:\n\nAs mentioned above, a sparse COO tensor is a `torch.Tensor` instance and to\ndistinguish it from the `Tensor` instances that use some other layout, on can\nuse `torch.Tensor.is_sparse` or `torch.Tensor.layout` properties:\n\nThe number of sparse and dense dimensions can be acquired using methods\n`torch.Tensor.sparse_dim()` and `torch.Tensor.dense_dim()`, respectively. For\ninstance:\n\nIf `s` is a sparse COO tensor then its COO format data can be acquired using\nmethods `torch.Tensor.indices()` and `torch.Tensor.values()`.\n\nNote\n\nCurrently, one can acquire the COO format data only when the tensor instance\nis coalesced:\n\nFor acquiring the COO format data of an uncoalesced tensor, use\n`torch.Tensor._values()` and `torch.Tensor._indices()`:\n\nConstructing a new sparse COO tensor results a tensor that is not coalesced:\n\nbut one can construct a coalesced copy of a sparse COO tensor using the\n`torch.Tensor.coalesce()` method:\n\nWhen working with uncoalesced sparse COO tensors, one must take into an\naccount the additive nature of uncoalesced data: the values of the same\nindices are the terms of a sum that evaluation gives the value of the\ncorresponding tensor element. For example, the scalar multiplication on an\nuncoalesced sparse tensor could be implemented by multiplying all the\nuncoalesced values with the scalar because `c * (a + b) == c * a + c * b`\nholds. However, any nonlinear operation, say, a square root, cannot be\nimplemented by applying the operation to uncoalesced data because `sqrt(a + b)\n== sqrt(a) + sqrt(b)` does not hold in general.\n\nSlicing (with positive step) of a sparse COO tensor is supported only for\ndense dimensions. Indexing is supported for both sparse and dense dimensions:\n\nIn PyTorch, the fill value of a sparse tensor cannot be specified explicitly\nand is assumed to be zero in general. However, there exists operations that\nmay interpret the fill value differently. For instance,\n`torch.sparse.softmax()` computes the softmax with the assumption that the\nfill value is negative infinity.\n\nThe following table summarizes supported Linear Algebra operations on sparse\nmatrices where the operands layouts may vary. Here `T[layout]` denotes a\ntensor with a given layout. Similarly, `M[layout]` denotes a matrix (2-D\nPyTorch tensor), and `V[layout]` denotes a vector (1-D PyTorch tensor). In\naddition, `f` denotes a scalar (float or 0-D PyTorch tensor), `*` is element-\nwise multiplication, and `@` is matrix multiplication.\n\nPyTorch operation\n\nSparse grad?\n\nLayout signature\n\n`torch.mv()`\n\nno\n\n`M[sparse_coo] @ V[strided] -> V[strided]`\n\n`torch.matmul()`\n\nno\n\n`M[sparse_coo] @ M[strided] -> M[strided]`\n\n`torch.mm()`\n\nno\n\n`M[sparse_coo] @ M[strided] -> M[strided]`\n\n`torch.sparse.mm()`\n\nyes\n\n`M[sparse_coo] @ M[strided] -> M[strided]`\n\n`torch.smm()`\n\nno\n\n`M[sparse_coo] @ M[strided] -> M[sparse_coo]`\n\n`torch.hspmm()`\n\nno\n\n`M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]`\n\n`torch.bmm()`\n\nno\n\n`T[sparse_coo] @ T[strided] -> T[strided]`\n\n`torch.addmm()`\n\nno\n\n`f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]`\n\n`torch.sparse.addmm()`\n\nyes\n\n`f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]`\n\n`torch.sspaddmm()`\n\nno\n\n`f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]`\n\n`torch.lobpcg()`\n\nno\n\n`GENEIG(M[sparse_coo]) -> M[strided], M[strided]`\n\n`torch.pca_lowrank()`\n\nyes\n\n`PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]`\n\n`torch.svd_lowrank()`\n\nyes\n\n`SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]`\n\nwhere \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept `torch.smm()`, support backward with respect to strided matrix\narguments.\n\nNote\n\nCurrently, PyTorch does not support matrix multiplication with the layout\nsignature `M[strided] @ M[sparse_coo]`. However, applications can still\ncompute this using the matrix relation `D @ S == (S.t() @ D.t()).t()`.\n\nThe following methods are specific to sparse tensors:\n\nIs `True` if the Tensor uses sparse storage layout, `False` otherwise.\n\nReturn the number of dense dimensions in a sparse tensor `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nSee also `Tensor.sparse_dim()` and hybrid tensors.\n\nReturn the number of sparse dimensions in a sparse tensor `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nSee also `Tensor.dense_dim()` and hybrid tensors.\n\nReturns a new sparse tensor with values from a strided tensor `self` filtered\nby the indices of the sparse tensor `mask`. The values of `mask` sparse tensor\nare ignored. `self` and `mask` tensors must have the same shape.\n\nNote\n\nThe returned sparse tensor has the same indices as the sparse tensor `mask`,\neven when the corresponding values in `self` are zeros.\n\nmask (Tensor) \u2013 a sparse tensor whose indices are used as a filter\n\nExample:\n\nResizes `self` sparse tensor to the desired size and the number of sparse and\ndense dimensions.\n\nNote\n\nIf the number of specified elements in `self` is zero, then `size`,\n`sparse_dim`, and `dense_dim` can be any size and positive integers such that\n`len(size) == sparse_dim + dense_dim`.\n\nIf `self` specifies one or more elements, however, then each dimension in\n`size` must not be smaller than the corresponding dimension of `self`,\n`sparse_dim` must equal the number of sparse dimensions in `self`, and\n`dense_dim` must equal the number of dense dimensions in `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nRemoves all specified elements from a sparse tensor `self` and resizes `self`\nto the desired size and the number of sparse and dense dimensions.\n\nCreates a strided copy of `self`.\n\nWarning\n\nThrows an error if `self` is a strided tensor.\n\nExample:\n\nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in\ncoordinate format.\n\nsparseDims (int, optional) \u2013 the number of sparse dimensions to include in the\nnew sparse tensor\n\nExample:\n\nReturns a coalesced copy of `self` if `self` is an uncoalesced tensor.\n\nReturns `self` if `self` is a coalesced tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nReturns `True` if `self` is a sparse COO tensor that is coalesced, `False`\notherwise.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee `coalesce()` and uncoalesced tensors.\n\nReturn the indices tensor of a sparse COO tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee also `Tensor.values()`.\n\nNote\n\nThis method can only be called on a coalesced sparse tensor. See\n`Tensor.coalesce()` for details.\n\nReturn the values tensor of a sparse COO tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee also `Tensor.indices()`.\n\nNote\n\nThis method can only be called on a coalesced sparse tensor. See\n`Tensor.coalesce()` for details.\n\nThe following `torch.Tensor` methods support sparse COO tensors:\n\n`add()` `add_()` `addmm()` `addmm_()` `any()` `asin()` `asin_()` `arcsin()`\n`arcsin_()` `bmm()` `clone()` `deg2rad()` `deg2rad_()` `detach()` `detach_()`\n`dim()` `div()` `div_()` `floor_divide()` `floor_divide_()` `get_device()`\n`index_select()` `isnan()` `log1p()` `log1p_()` `mm()` `mul()` `mul_()` `mv()`\n`narrow_copy()` `neg()` `neg_()` `negative()` `negative_()` `numel()`\n`rad2deg()` `rad2deg_()` `resize_as_()` `size()` `pow()` `sqrt()` `square()`\n`smm()` `sspaddmm()` `sub()` `sub_()` `t()` `t_()` `transpose()`\n`transpose_()` `zero_()`\n\nConstructs a sparse tensor in COO(rdinate) format with specified values at the\ngiven `indices`.\n\nNote\n\nThis function returns an uncoalesced tensor.\n\nExample:\n\nReturns the sum of each row of the sparse tensor `input` in the given\ndimensions `dim`. If `dim` is a list of dimensions, reduce over all of them.\nWhen sum over all `sparse_dim`, this method returns a dense tensor instead of\na sparse tensor.\n\nAll summed `dim` are squeezed (see `torch.squeeze()`), resulting an output\ntensor having `dim` fewer dimensions than `input`.\n\nDuring backward, only gradients at `nnz` locations of `input` will propagate\nback. Note that the gradients of `input` is coalesced.\n\nExample:\n\nThis function does exact same thing as `torch.addmm()` in the forward, except\nthat it supports backward for sparse matrix `mat1`. `mat1` need to have\n`sparse_dim = 2`. Note that the gradients of `mat1` is a coalesced sparse\ntensor.\n\nPerforms a matrix multiplication of the sparse matrix `mat1` and the (sparse\nor strided) matrix `mat2`. Similar to `torch.mm()`, If `mat1` is a (n\u00d7m)(n\n\\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p) tensor, out will be a (n\u00d7p)(n\n\\times p) tensor. `mat1` need to have `sparse_dim = 2`. This function also\nsupports backward for both matrices. Note that the gradients of `mat1` is a\ncoalesced sparse tensor.\n\nThe format of the output tensor of this function follows: - sparse x sparse ->\nsparse - sparse x dense -> dense\n\nExample:\n\nMatrix multiplies a sparse tensor `mat1` with a dense tensor `mat2`, then adds\nthe sparse tensor `input` to the result.\n\nNote: This function is equivalent to `torch.addmm()`, except `input` and\n`mat1` are sparse.\n\nPerforms a matrix multiplication of a sparse COO matrix `mat1` and a strided\nmatrix `mat2`. The result is a (1 + 1)-dimensional hybrid COO matrix.\n\n{out} \u2013\n\nPerforms a matrix multiplication of the sparse matrix `input` with the dense\nmatrix `mat`.\n\nApplies a softmax function.\n\nSoftmax is defined as:\n\nSoftmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j\nexp(x_j)}\n\nwhere i,ji, j run over sparse tensor indices and unspecified entries are\nignores. This is equivalent to defining unspecified entries as negative\ninfinity so that exp(xk)=0exp(x_k) = 0 when the entry with index kk has not\nspecified.\n\nIt is applied to all slices along `dim`, and will re-scale them so that the\nelements lie in the range `[0, 1]` and sum to 1.\n\nApplies a softmax function followed by logarithm.\n\nSee `softmax` for more details.\n\nThe following `torch` functions support sparse COO tensors:\n\n`cat()` `dstack()` `empty()` `empty_like()` `hstack()` `index_select()`\n`is_complex()` `is_floating_point()` `is_nonzero()` `is_same_size()`\n`is_signed()` `is_tensor()` `lobpcg()` `mm()` `native_norm()` `pca_lowrank()`\n`select()` `stack()` `svd_lowrank()` `unsqueeze()` `vstack()` `zeros()`\n`zeros_like()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse.addmm()", "path": "sparse#torch.sparse.addmm", "type": "torch.sparse", "text": "\nThis function does exact same thing as `torch.addmm()` in the forward, except\nthat it supports backward for sparse matrix `mat1`. `mat1` need to have\n`sparse_dim = 2`. Note that the gradients of `mat1` is a coalesced sparse\ntensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse.log_softmax()", "path": "sparse#torch.sparse.log_softmax", "type": "torch.sparse", "text": "\nApplies a softmax function followed by logarithm.\n\nSee `softmax` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse.mm()", "path": "sparse#torch.sparse.mm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of the sparse matrix `mat1` and the (sparse\nor strided) matrix `mat2`. Similar to `torch.mm()`, If `mat1` is a (n\u00d7m)(n\n\\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p) tensor, out will be a (n\u00d7p)(n\n\\times p) tensor. `mat1` need to have `sparse_dim = 2`. This function also\nsupports backward for both matrices. Note that the gradients of `mat1` is a\ncoalesced sparse tensor.\n\nThe format of the output tensor of this function follows: - sparse x sparse ->\nsparse - sparse x dense -> dense\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse.softmax()", "path": "sparse#torch.sparse.softmax", "type": "torch.sparse", "text": "\nApplies a softmax function.\n\nSoftmax is defined as:\n\nSoftmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j\nexp(x_j)}\n\nwhere i,ji, j run over sparse tensor indices and unspecified entries are\nignores. This is equivalent to defining unspecified entries as negative\ninfinity so that exp(xk)=0exp(x_k) = 0 when the entry with index kk has not\nspecified.\n\nIt is applied to all slices along `dim`, and will re-scale them so that the\nelements lie in the range `[0, 1]` and sum to 1.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse.sum()", "path": "sparse#torch.sparse.sum", "type": "torch.sparse", "text": "\nReturns the sum of each row of the sparse tensor `input` in the given\ndimensions `dim`. If `dim` is a list of dimensions, reduce over all of them.\nWhen sum over all `sparse_dim`, this method returns a dense tensor instead of\na sparse tensor.\n\nAll summed `dim` are squeezed (see `torch.squeeze()`), resulting an output\ntensor having `dim` fewer dimensions than `input`.\n\nDuring backward, only gradients at `nnz` locations of `input` will propagate\nback. Note that the gradients of `input` is coalesced.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sparse_coo_tensor()", "path": "generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor", "type": "torch", "text": "\nConstructs a sparse tensor in COO(rdinate) format with specified values at the\ngiven `indices`.\n\nNote\n\nThis function returns an uncoalesced tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.split()", "path": "generated/torch.split#torch.split", "type": "torch", "text": "\nSplits the tensor into chunks. Each chunk is a view of the original tensor.\n\nIf `split_size_or_sections` is an integer type, then `tensor` will be split\ninto equally sized chunks (if possible). Last chunk will be smaller if the\ntensor size along the given dimension `dim` is not divisible by `split_size`.\n\nIf `split_size_or_sections` is a list, then `tensor` will be split into\n`len(split_size_or_sections)` chunks with sizes in `dim` according to\n`split_size_or_sections`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sqrt()", "path": "generated/torch.sqrt#torch.sqrt", "type": "torch", "text": "\nReturns a new tensor with the square-root of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.square()", "path": "generated/torch.square#torch.square", "type": "torch", "text": "\nReturns a new tensor with the square of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.squeeze()", "path": "generated/torch.squeeze#torch.squeeze", "type": "torch", "text": "\nReturns a tensor with all the dimensions of `input` of size `1` removed.\n\nFor example, if `input` is of shape: (A\u00d71\u00d7B\u00d7C\u00d71\u00d7D)(A \\times 1 \\times B \\times\nC \\times 1 \\times D) then the `out` tensor will be of shape: (A\u00d7B\u00d7C\u00d7D)(A\n\\times B \\times C \\times D) .\n\nWhen `dim` is given, a squeeze operation is done only in the given dimension.\nIf `input` is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B) , `squeeze(input, 0)`\nleaves the tensor unchanged, but `squeeze(input, 1)` will squeeze the tensor\nto the shape (A\u00d7B)(A \\times B) .\n\nNote\n\nThe returned tensor shares the storage with the input tensor, so changing the\ncontents of one will change the contents of the other.\n\nWarning\n\nIf the tensor has a batch dimension of size 1, then `squeeze(input)` will also\nremove the batch dimension, which can lead to unexpected errors.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sspaddmm()", "path": "sparse#torch.sspaddmm", "type": "torch.sparse", "text": "\nMatrix multiplies a sparse tensor `mat1` with a dense tensor `mat2`, then adds\nthe sparse tensor `input` to the result.\n\nNote: This function is equivalent to `torch.addmm()`, except `input` and\n`mat1` are sparse.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.stack()", "path": "generated/torch.stack#torch.stack", "type": "torch", "text": "\nConcatenates a sequence of tensors along a new dimension.\n\nAll tensors need to be of the same size.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.std()", "path": "generated/torch.std#torch.std", "type": "torch", "text": "\nReturns the standard-deviation of all elements in the `input` tensor.\n\nIf `unbiased` is `False`, then the standard-deviation will be calculated via\nthe biased estimator. Otherwise, Bessel\u2019s correction will be used.\n\nExample:\n\nReturns the standard-deviation of each row of the `input` tensor in the\ndimension `dim`. If `dim` is a list of dimensions, reduce over all of them.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nIf `unbiased` is `False`, then the standard-deviation will be calculated via\nthe biased estimator. Otherwise, Bessel\u2019s correction will be used.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.std_mean()", "path": "generated/torch.std_mean#torch.std_mean", "type": "torch", "text": "\nReturns the standard-deviation and mean of all elements in the `input` tensor.\n\nIf `unbiased` is `False`, then the standard-deviation will be calculated via\nthe biased estimator. Otherwise, Bessel\u2019s correction will be used.\n\nExample:\n\nReturns the standard-deviation and mean of each row of the `input` tensor in\nthe dimension `dim`. If `dim` is a list of dimensions, reduce over all of\nthem.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nIf `unbiased` is `False`, then the standard-deviation will be calculated via\nthe biased estimator. Otherwise, Bessel\u2019s correction will be used.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.stft()", "path": "generated/torch.stft#torch.stft", "type": "torch", "text": "\nShort-time Fourier transform (STFT).\n\nWarning\n\nFrom version 1.8.0, `return_complex` must always be given explicitly for real\ninputs and `return_complex=False` has been deprecated. Strongly prefer\n`return_complex=True` as in a future pytorch release, this function will only\nreturn complex tensors.\n\nNote that `torch.view_as_real()` can be used to recover a real tensor with an\nextra last dimension for real and imaginary components.\n\nThe STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft\nfunction.\n\nIgnoring the optional batch dimension, this method computes the following\nexpression:\n\nwhere mm is the index of the sliding window, and \u03c9\\omega is the frequency that\n0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\\\_fft} . When `onesided` is the default value\n`True`,\n\nReturns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T) if\n`return_complex` is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N \\times\nT \\times 2) . Where \u2217* is the optional batch size of `input`, NN is the number\nof frequencies where STFT is applied and TT is the total number of frames\nused.\n\nWarning\n\nThis function changed signature at version 0.4.1. Calling with the previous\nsignature may cause error or return incorrect result.\n\nA tensor containing the STFT result with shape described above\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Storage", "path": "storage", "type": "torch.Storage", "text": "\nA `torch.Storage` is a contiguous, one-dimensional array of a single data\ntype.\n\nEvery `torch.Tensor` has a corresponding storage of the same data type.\n\nCasts this storage to bfloat16 type\n\nCasts this storage to bool type\n\nCasts this storage to byte type\n\nCasts this storage to char type\n\nReturns a copy of this storage\n\nCasts this storage to complex double type\n\nCasts this storage to complex float type\n\nReturns a CPU copy of this storage if it\u2019s not already on the CPU\n\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\nCasts this storage to double type\n\nCasts this storage to float type\n\nIf `shared` is `True`, then memory is shared between all processes. All\nchanges are written to the file. If `shared` is `False`, then the changes on\nthe storage do not affect the file.\n\n`size` is the number of elements in the storage. If `shared` is `False`, then\nthe file must contain at least `size * sizeof(Type)` bytes (`Type` is the type\nof storage). If `shared` is `True` the file will be created if needed.\n\nCasts this storage to half type\n\nCasts this storage to int type\n\nCasts this storage to long type\n\nCopies the storage to pinned memory, if it\u2019s not already pinned.\n\nMoves the storage to shared memory.\n\nThis is a no-op for storages already in shared memory and for CUDA storages,\nwhich do not need to be moved for sharing across processes. Storages in shared\nmemory cannot be resized.\n\nReturns: self\n\nCasts this storage to short type\n\nReturns a list containing the elements of this storage\n\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sub()", "path": "generated/torch.sub#torch.sub", "type": "torch", "text": "\nSubtracts `other`, scaled by `alpha`, from `input`.\n\nSupports broadcasting to a common shape, type promotion, and integer, float,\nand complex inputs.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.subtract()", "path": "generated/torch.subtract#torch.subtract", "type": "torch", "text": "\nAlias for `torch.sub()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.sum()", "path": "generated/torch.sum#torch.sum", "type": "torch", "text": "\nReturns the sum of all elements in the `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\nReturns the sum of each row of the `input` tensor in the given dimension\n`dim`. If `dim` is a list of dimensions, reduce over all of them.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.svd()", "path": "generated/torch.svd#torch.svd", "type": "torch", "text": "\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`. The singular value decomposition is represented as a\nnamedtuple (`U,S,V`), such that `input` = `U` diag(`S`) `V\u1d34`, where `V\u1d34` is\nthe transpose of `V` for the real-valued inputs, or the conjugate transpose of\n`V` for the complex-valued inputs. If `input` is a batch of tensors, then `U`,\n`S`, and `V` are also batched with the same batch dimensions as `input`.\n\nIf `some` is `True` (default), the method returns the reduced singular value\ndecomposition i.e., if the last two dimensions of `input` are `m` and `n`,\nthen the returned `U` and `V` matrices will contain only min(`n, m`)\northonormal columns.\n\nIf `compute_uv` is `False`, the returned `U` and `V` will be zero-filled\nmatrices of shape `(m \u00d7 m)` and `(n \u00d7 n)` respectively, and the same device as\n`input`. The `some` argument has no effect when `compute_uv` is `False`.\n\nSupports input of float, double, cfloat and cdouble data types. The dtypes of\n`U` and `V` are the same as `input`\u2019s. `S` will always be real-valued, even if\n`input` is complex.\n\nWarning\n\n`torch.svd()` is deprecated. Please use `torch.linalg.svd()` instead, which is\nsimilar to NumPy\u2019s `numpy.linalg.svd`.\n\nNote\n\nDifferences with `torch.linalg.svd()`:\n\nNote\n\nThe singular values are returned in descending order. If `input` is a batch of\nmatrices, then the singular values of each matrix in the batch is returned in\ndescending order.\n\nNote\n\nThe implementation of SVD on CPU uses the LAPACK routine `?gesdd` (a divide-\nand-conquer algorithm) instead of `?gesvd` for speed. Analogously, the SVD on\nGPU uses the cuSOLVER routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243\nand later, and uses the MAGMA routine `gesdd` on earlier versions of CUDA.\n\nNote\n\nThe returned matrix `U` will be transposed, i.e. with strides\n`U.contiguous().transpose(-2, -1).stride()`.\n\nNote\n\nGradients computed using `U` and `V` may be unstable if `input` is not full\nrank or has non-unique singular values.\n\nNote\n\nWhen `some` = `False`, the gradients on `U[..., :, min(m, n):]` and `V[..., :,\nmin(m, n):]` will be ignored in backward as those vectors can be arbitrary\nbases of the subspaces.\n\nNote\n\nThe `S` tensor can only be used to compute gradients if `compute_uv` is True.\n\nNote\n\nWith the complex-valued input the backward operation works correctly only for\ngauge invariant loss functions. Please look at Gauge problem in AD for more\ndetails.\n\nNote\n\nSince `U` and `V` of an SVD is not unique, each vector can be multiplied by an\narbitrary phase factor ei\u03d5e^{i \\phi} while the SVD result is still correct.\nDifferent platforms, like Numpy, or inputs on different device types, may\nproduce different `U` and `V` tensors.\n\nout (tuple, optional) \u2013 the output tuple of tensors\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.svd_lowrank()", "path": "generated/torch.svd_lowrank#torch.svd_lowrank", "type": "torch", "text": "\nReturn the singular value decomposition `(U, S, V)` of a matrix, batches of\nmatrices, or a sparse matrix AA such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T\n. In case MM is given, then SVD is computed for the matrix A\u2212MA - M .\n\nNote\n\nThe implementation is based on the Algorithm 5.1 from Halko et al, 2009.\n\nNote\n\nTo obtain repeatable results, reset the seed for the pseudorandom number\ngenerator\n\nNote\n\nThe input is assumed to be a low-rank matrix.\n\nNote\n\nIn general, use the full-rank SVD implementation `torch.svd` for dense\nmatrices due to its 10-fold higher performance characteristics. The low-rank\nSVD will be useful for huge sparse matrices that `torch.svd` cannot handle.\n\nA (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)\n\nq (int, optional): a slightly overestimated rank of A.\n\nconduct; niter must be a nonnegative integer, and defaults to 2\n\n(\u2217,1,n)(*, 1, n) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.swapaxes()", "path": "generated/torch.swapaxes#torch.swapaxes", "type": "torch", "text": "\nAlias for `torch.transpose()`.\n\nThis function is equivalent to NumPy\u2019s swapaxes function.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.swapdims()", "path": "generated/torch.swapdims#torch.swapdims", "type": "torch", "text": "\nAlias for `torch.transpose()`.\n\nThis function is equivalent to NumPy\u2019s swapaxes function.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.symeig()", "path": "generated/torch.symeig#torch.symeig", "type": "torch", "text": "\nThis function returns eigenvalues and eigenvectors of a real symmetric matrix\n`input` or a batch of real symmetric matrices, represented by a namedtuple\n(eigenvalues, eigenvectors).\n\nThis function calculates all eigenvalues (and vectors) of `input` such that\ninput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^T .\n\nThe boolean argument `eigenvectors` defines computation of both eigenvectors\nand eigenvalues or eigenvalues only.\n\nIf it is `False`, only eigenvalues are computed. If it is `True`, both\neigenvalues and eigenvectors are computed.\n\nSince the input matrix `input` is supposed to be symmetric, only the upper\ntriangular portion is used by default.\n\nIf `upper` is `False`, then lower triangular portion is used.\n\nNote\n\nThe eigenvalues are returned in ascending order. If `input` is a batch of\nmatrices, then the eigenvalues of each matrix in the batch is returned in\nascending order.\n\nNote\n\nIrrespective of the original strides, the returned matrix `V` will be\ntransposed, i.e. with strides `V.contiguous().transpose(-1, -2).stride()`.\n\nWarning\n\nExtra care needs to be taken when backward through outputs. Such operation is\nonly stable when all eigenvalues are distinct and becomes less stable the\nsmaller min\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j| is.\n\nout (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\n(Tensor, Tensor)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.t()", "path": "generated/torch.t#torch.t", "type": "torch", "text": "\nExpects `input` to be <= 2-D tensor and transposes dimensions 0 and 1.\n\n0-D and 1-D tensors are returned as is. When input is a 2-D tensor this is\nequivalent to `transpose(input, 0, 1)`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.take()", "path": "generated/torch.take#torch.take", "type": "torch", "text": "\nReturns a new tensor with the elements of `input` at the given indices. The\ninput tensor is treated as if it were viewed as a 1-D tensor. The result takes\nthe same shape as the indices.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tan()", "path": "generated/torch.tan#torch.tan", "type": "torch", "text": "\nReturns a new tensor with the tangent of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tanh()", "path": "generated/torch.tanh#torch.tanh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic tangent of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor", "path": "tensors", "type": "torch.Tensor", "text": "\nA `torch.Tensor` is a multi-dimensional matrix containing elements of a single\ndata type.\n\nTorch defines 10 tensor types with CPU and GPU variants which are as follows:\n\nData type\n\ndtype\n\nCPU tensor\n\nGPU tensor\n\n32-bit floating point\n\n`torch.float32` or `torch.float`\n\n`torch.FloatTensor`\n\n`torch.cuda.FloatTensor`\n\n64-bit floating point\n\n`torch.float64` or `torch.double`\n\n`torch.DoubleTensor`\n\n`torch.cuda.DoubleTensor`\n\n16-bit floating point 1\n\n`torch.float16` or `torch.half`\n\n`torch.HalfTensor`\n\n`torch.cuda.HalfTensor`\n\n16-bit floating point 2\n\n`torch.bfloat16`\n\n`torch.BFloat16Tensor`\n\n`torch.cuda.BFloat16Tensor`\n\n32-bit complex\n\n`torch.complex32`\n\n64-bit complex\n\n`torch.complex64`\n\n128-bit complex\n\n`torch.complex128` or `torch.cdouble`\n\n8-bit integer (unsigned)\n\n`torch.uint8`\n\n`torch.ByteTensor`\n\n`torch.cuda.ByteTensor`\n\n8-bit integer (signed)\n\n`torch.int8`\n\n`torch.CharTensor`\n\n`torch.cuda.CharTensor`\n\n16-bit integer (signed)\n\n`torch.int16` or `torch.short`\n\n`torch.ShortTensor`\n\n`torch.cuda.ShortTensor`\n\n32-bit integer (signed)\n\n`torch.int32` or `torch.int`\n\n`torch.IntTensor`\n\n`torch.cuda.IntTensor`\n\n64-bit integer (signed)\n\n`torch.int64` or `torch.long`\n\n`torch.LongTensor`\n\n`torch.cuda.LongTensor`\n\nBoolean\n\n`torch.bool`\n\n`torch.BoolTensor`\n\n`torch.cuda.BoolTensor`\n\nSometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand\nbits. Useful when precision is important at the expense of range.\n\nSometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same number\nof exponent bits as `float32`\n\n`torch.Tensor` is an alias for the default tensor type (`torch.FloatTensor`).\n\nA tensor can be constructed from a Python `list` or sequence using the\n`torch.tensor()` constructor:\n\nWarning\n\n`torch.tensor()` always copies `data`. If you have a Tensor `data` and just\nwant to change its `requires_grad` flag, use `requires_grad_()` or `detach()`\nto avoid a copy. If you have a numpy array and want to avoid a copy, use\n`torch.as_tensor()`.\n\nA tensor of specific data type can be constructed by passing a `torch.dtype`\nand/or a `torch.device` to a constructor or tensor creation op:\n\nThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:\n\nUse `torch.Tensor.item()` to get a Python number from a tensor containing a\nsingle value:\n\nA tensor can be created with `requires_grad=True` so that `torch.autograd`\nrecords operations on them for automatic differentiation.\n\nEach tensor has an associated `torch.Storage`, which holds its data. The\ntensor class also provides multi-dimensional, strided view of a storage and\ndefines numeric operations on it.\n\nNote\n\nFor more information on tensor views, see Tensor Views.\n\nNote\n\nFor more information on the `torch.dtype`, `torch.device`, and `torch.layout`\nattributes of a `torch.Tensor`, see Tensor Attributes.\n\nNote\n\nMethods which mutate a tensor are marked with an underscore suffix. For\nexample, `torch.FloatTensor.abs_()` computes the absolute value in-place and\nreturns the modified tensor, while `torch.FloatTensor.abs()` computes the\nresult in a new tensor.\n\nNote\n\nTo change an existing tensor\u2019s `torch.device` and/or `torch.dtype`, consider\nusing `to()` method on the tensor.\n\nWarning\n\nCurrent implementation of `torch.Tensor` introduces memory overhead, thus it\nmight lead to unexpectedly high memory usage in the applications with many\ntiny tensors. If this is your case, consider using one large structure.\n\nThere are a few main ways to create a tensor, depending on your use case.\n\nReturns a new Tensor with `data` as the tensor data. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nWarning\n\n`new_tensor()` always copies `data`. If you have a Tensor `data` and want to\navoid a copy, use `torch.Tensor.requires_grad_()` or `torch.Tensor.detach()`.\nIf you have a numpy array and want to avoid a copy, use `torch.from_numpy()`.\n\nWarning\n\nWhen data is a tensor `x`, `new_tensor()` reads out \u2018the data\u2019 from whatever\nit is passed, and constructs a leaf variable. Therefore `tensor.new_tensor(x)`\nis equivalent to `x.clone().detach()` and `tensor.new_tensor(x,\nrequires_grad=True)` is equivalent to\n`x.clone().detach().requires_grad_(True)`. The equivalents using `clone()` and\n`detach()` are recommended.\n\nExample:\n\nReturns a Tensor of size `size` filled with `fill_value`. By default, the\nreturned Tensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nExample:\n\nReturns a Tensor of size `size` filled with uninitialized data. By default,\nthe returned Tensor has the same `torch.dtype` and `torch.device` as this\ntensor.\n\nExample:\n\nReturns a Tensor of size `size` filled with `1`. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nExample:\n\nReturns a Tensor of size `size` filled with `0`. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nExample:\n\nIs `True` if the Tensor is stored on the GPU, `False` otherwise.\n\nIs `True` if the Tensor is quantized, `False` otherwise.\n\nIs `True` if the Tensor is a meta tensor, `False` otherwise. Meta tensors are\nlike normal tensors, but they carry no data.\n\nIs the `torch.device` where this Tensor is.\n\nThis attribute is `None` by default and becomes a Tensor the first time a call\nto `backward()` computes gradients for `self`. The attribute will then contain\nthe gradients computed and future calls to `backward()` will accumulate (add)\ngradients into it.\n\nAlias for `dim()`\n\nIs this Tensor with its dimensions reversed.\n\nIf `n` is the number of dimensions in `x`, `x.T` is equivalent to\n`x.permute(n-1, n-2, ..., 0)`.\n\nReturns a new tensor containing real values of the `self` tensor. The returned\ntensor and `self` share the same underlying storage.\n\nWarning\n\n`real()` is only supported for tensors with complex dtypes.\n\nReturns a new tensor containing imaginary values of the `self` tensor. The\nreturned tensor and `self` share the same underlying storage.\n\nWarning\n\n`imag()` is only supported for tensors with complex dtypes.\n\nSee `torch.abs()`\n\nIn-place version of `abs()`\n\nAlias for `abs()`\n\nIn-place version of `absolute()` Alias for `abs_()`\n\nSee `torch.acos()`\n\nIn-place version of `acos()`\n\nSee `torch.arccos()`\n\nIn-place version of `arccos()`\n\nAdd a scalar or tensor to `self` tensor. If both `alpha` and `other` are\nspecified, each element of `other` is scaled by `alpha` before being used.\n\nWhen `other` is a tensor, the shape of `other` must be broadcastable with the\nshape of the underlying tensor\n\nSee `torch.add()`\n\nIn-place version of `add()`\n\nSee `torch.addbmm()`\n\nIn-place version of `addbmm()`\n\nSee `torch.addcdiv()`\n\nIn-place version of `addcdiv()`\n\nSee `torch.addcmul()`\n\nIn-place version of `addcmul()`\n\nSee `torch.addmm()`\n\nIn-place version of `addmm()`\n\nSee `torch.sspaddmm()`\n\nSee `torch.addmv()`\n\nIn-place version of `addmv()`\n\nSee `torch.addr()`\n\nIn-place version of `addr()`\n\nSee `torch.allclose()`\n\nSee `torch.amax()`\n\nSee `torch.amin()`\n\nSee `torch.angle()`\n\nApplies the function `callable` to each element in the tensor, replacing each\nelement with the value returned by `callable`.\n\nNote\n\nThis function only works with CPU tensors and should not be used in code\nsections that require high performance.\n\nSee `torch.argmax()`\n\nSee `torch.argmin()`\n\nSee `torch.argsort()`\n\nSee `torch.asin()`\n\nIn-place version of `asin()`\n\nSee `torch.arcsin()`\n\nIn-place version of `arcsin()`\n\nSee `torch.as_strided()`\n\nSee `torch.atan()`\n\nIn-place version of `atan()`\n\nSee `torch.arctan()`\n\nIn-place version of `arctan()`\n\nSee `torch.atan2()`\n\nIn-place version of `atan2()`\n\nSee `torch.all()`\n\nSee `torch.any()`\n\nComputes the gradient of current tensor w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If the tensor is non-scalar\n(i.e. its data has more than one element) and requires gradient, the function\nadditionally requires specifying `gradient`. It should be a tensor of matching\ntype and location, that contains the gradient of the differentiated function\nw.r.t. `self`.\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nIf you run any forward ops, create `gradient`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nSee `torch.baddbmm()`\n\nIn-place version of `baddbmm()`\n\nReturns a result tensor where each result[i]\\texttt{result[i]} is\nindependently sampled from\nBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . `self` must have\nfloating point `dtype`, and the result will have the same `dtype`.\n\nSee `torch.bernoulli()`\n\nFills each location of `self` with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p}) . `self` can have integral `dtype`.\n\n`p_tensor` should be a tensor containing probabilities to be used for drawing\nthe binary random number.\n\nThe ith\\text{i}^{th} element of `self` tensor will be set to a value sampled\nfrom Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\\\_tensor[i]}) .\n\n`self` can have integral `dtype`, but `p_tensor` must have floating point\n`dtype`.\n\nSee also `bernoulli()` and `torch.bernoulli()`\n\n`self.bfloat16()` is equivalent to `self.to(torch.bfloat16)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.bincount()`\n\nSee `torch.bitwise_not()`\n\nIn-place version of `bitwise_not()`\n\nSee `torch.bitwise_and()`\n\nIn-place version of `bitwise_and()`\n\nSee `torch.bitwise_or()`\n\nIn-place version of `bitwise_or()`\n\nSee `torch.bitwise_xor()`\n\nIn-place version of `bitwise_xor()`\n\nSee `torch.bmm()`\n\n`self.bool()` is equivalent to `self.to(torch.bool)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n`self.byte()` is equivalent to `self.to(torch.uint8)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.broadcast_to()`.\n\nFills the tensor with numbers drawn from the Cauchy distribution:\n\nSee `torch.ceil()`\n\nIn-place version of `ceil()`\n\n`self.char()` is equivalent to `self.to(torch.int8)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.cholesky()`\n\nSee `torch.cholesky_inverse()`\n\nSee `torch.cholesky_solve()`\n\nSee `torch.chunk()`\n\nSee `torch.clamp()`\n\nIn-place version of `clamp()`\n\nAlias for `clamp()`.\n\nAlias for `clamp_()`.\n\nSee `torch.clone()`\n\nReturns a contiguous in memory tensor containing the same data as `self`\ntensor. If `self` tensor is already in the specified memory format, this\nfunction returns the `self` tensor.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.contiguous_format`.\n\nCopies the elements from `src` into `self` tensor and returns `self`.\n\nThe `src` tensor must be broadcastable with the `self` tensor. It may be of a\ndifferent data type or reside on a different device.\n\nSee `torch.conj()`\n\nSee `torch.copysign()`\n\nIn-place version of `copysign()`\n\nSee `torch.cos()`\n\nIn-place version of `cos()`\n\nSee `torch.cosh()`\n\nIn-place version of `cosh()`\n\nSee `torch.count_nonzero()`\n\nSee `torch.acosh()`\n\nIn-place version of `acosh()`\n\nacosh() -> Tensor\n\nSee `torch.arccosh()`\n\nacosh_() -> Tensor\n\nIn-place version of `arccosh()`\n\nReturns a copy of this object in CPU memory.\n\nIf this object is already in CPU memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.cross()`\n\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\nSee `torch.logcumsumexp()`\n\nSee `torch.cummax()`\n\nSee `torch.cummin()`\n\nSee `torch.cumprod()`\n\nIn-place version of `cumprod()`\n\nSee `torch.cumsum()`\n\nIn-place version of `cumsum()`\n\nReturns the address of the first element of `self` tensor.\n\nSee `torch.deg2rad()`\n\nGiven a quantized Tensor, dequantize it and return the dequantized float\nTensor.\n\nSee `torch.det()`\n\nReturn the number of dense dimensions in a sparse tensor `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nSee also `Tensor.sparse_dim()` and hybrid tensors.\n\nReturns a new Tensor, detached from the current graph.\n\nThe result will never require gradient.\n\nNote\n\nReturned Tensor shares the same storage with the original one. In-place\nmodifications on either of them will be seen, and may trigger errors in\ncorrectness checks. IMPORTANT NOTE: Previously, in-place size / stride /\nstorage changes (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to\nthe returned tensor also update the original tensor. Now, these in-place\nchanges will not update the original tensor anymore, and will instead trigger\nan error. For sparse tensors: In-place indices / values changes (such as\n`zero_` / `copy_` / `add_`) to the returned tensor will not update the\noriginal tensor anymore, and will instead trigger an error.\n\nDetaches the Tensor from the graph that created it, making it a leaf. Views\ncannot be detached in-place.\n\nSee `torch.diag()`\n\nSee `torch.diag_embed()`\n\nSee `torch.diagflat()`\n\nSee `torch.diagonal()`\n\nFill the main diagonal of a tensor that has at least 2-dimensions. When\ndims>2, all dimensions of input must be of equal length. This function\nmodifies the input tensor in-place, and returns the input tensor.\n\nExample:\n\nSee `torch.fmax()`\n\nSee `torch.fmin()`\n\nSee `torch.diff()`\n\nSee `torch.digamma()`\n\nIn-place version of `digamma()`\n\nReturns the number of dimensions of `self` tensor.\n\nSee `torch.dist()`\n\nSee `torch.div()`\n\nIn-place version of `div()`\n\nSee `torch.divide()`\n\nIn-place version of `divide()`\n\nSee `torch.dot()`\n\n`self.double()` is equivalent to `self.to(torch.float64)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.eig()`\n\nReturns the size in bytes of an individual element.\n\nExample:\n\nSee `torch.eq()`\n\nIn-place version of `eq()`\n\nSee `torch.equal()`\n\nSee `torch.erf()`\n\nIn-place version of `erf()`\n\nSee `torch.erfc()`\n\nIn-place version of `erfc()`\n\nSee `torch.erfinv()`\n\nIn-place version of `erfinv()`\n\nSee `torch.exp()`\n\nIn-place version of `exp()`\n\nSee `torch.expm1()`\n\nIn-place version of `expm1()`\n\nReturns a new view of the `self` tensor with singleton dimensions expanded to\na larger size.\n\nPassing -1 as the size for a dimension means not changing the size of that\ndimension.\n\nTensor can be also expanded to a larger number of dimensions, and the new ones\nwill be appended at the front. For the new dimensions, the size cannot be set\nto -1.\n\nExpanding a tensor does not allocate new memory, but only creates a new view\non the existing tensor where a dimension of size one is expanded to a larger\nsize by setting the `stride` to 0. Any dimension of size 1 can be expanded to\nan arbitrary value without allocating new memory.\n\n*sizes (torch.Size or int...) \u2013 the desired expanded size\nWarning\n\nMore than one element of an expanded tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nExample:\n\nExpand this tensor to the same size as `other`. `self.expand_as(other)` is\nequivalent to `self.expand(other.size())`.\n\nPlease see `expand()` for more information about `expand`.\n\nother (`torch.Tensor`) \u2013 The result tensor has the same size as `other`.\n\nFills `self` tensor with elements drawn from the exponential distribution:\n\nSee `torch.fix()`.\n\nIn-place version of `fix()`\n\nFills `self` tensor with the specified value.\n\nsee `torch.flatten()`\n\nSee `torch.flip()`\n\nSee `torch.fliplr()`\n\nSee `torch.flipud()`\n\n`self.float()` is equivalent to `self.to(torch.float32)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.float_power()`\n\nIn-place version of `float_power()`\n\nSee `torch.floor()`\n\nIn-place version of `floor()`\n\nSee `torch.floor_divide()`\n\nIn-place version of `floor_divide()`\n\nSee `torch.fmod()`\n\nIn-place version of `fmod()`\n\nSee `torch.frac()`\n\nIn-place version of `frac()`\n\nSee `torch.gather()`\n\nSee `torch.gcd()`\n\nIn-place version of `gcd()`\n\nSee `torch.ge()`.\n\nIn-place version of `ge()`.\n\nSee `torch.greater_equal()`.\n\nIn-place version of `greater_equal()`.\n\nFills `self` tensor with elements drawn from the geometric distribution:\n\nSee `torch.geqrf()`\n\nSee `torch.ger()`\n\nFor CUDA tensors, this function returns the device ordinal of the GPU on which\nthe tensor resides. For CPU tensors, an error is thrown.\n\nExample:\n\nSee `torch.gt()`.\n\nIn-place version of `gt()`.\n\nSee `torch.greater()`.\n\nIn-place version of `greater()`.\n\n`self.half()` is equivalent to `self.to(torch.float16)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.nn.functional.hardshrink()`\n\nSee `torch.heaviside()`\n\nSee `torch.histc()`\n\nSee `torch.hypot()`\n\nIn-place version of `hypot()`\n\nSee `torch.i0()`\n\nIn-place version of `i0()`\n\nSee `torch.igamma()`\n\nIn-place version of `igamma()`\n\nSee `torch.igammac()`\n\nIn-place version of `igammac()`\n\nAccumulate the elements of `tensor` into the `self` tensor by adding to the\nindices in the order given in `index`. For example, if `dim == 0` and\n`index[i] == j`, then the `i`th row of `tensor` is added to the `j`th row of\n`self`.\n\nThe `dim`th dimension of `tensor` must have the same size as the length of\n`index` (which must be a vector), and all other dimensions must match `self`,\nor an error will be raised.\n\nNote\n\nThis operation may behave nondeterministically when given tensors on a CUDA\ndevice. See Reproducibility for more information.\n\nExample:\n\nOut-of-place version of `torch.Tensor.index_add_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_add_()`.\n\nCopies the elements of `tensor` into the `self` tensor by selecting the\nindices in the order given in `index`. For example, if `dim == 0` and\n`index[i] == j`, then the `i`th row of `tensor` is copied to the `j`th row of\n`self`.\n\nThe `dim`th dimension of `tensor` must have the same size as the length of\n`index` (which must be a vector), and all other dimensions must match `self`,\nor an error will be raised.\n\nNote\n\nIf `index` contains duplicate entries, multiple elements from `tensor` will be\ncopied to the same index of `self`. The result is nondeterministic since it\ndepends on which copy occurs last.\n\nExample:\n\nOut-of-place version of `torch.Tensor.index_copy_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_copy_()`.\n\nFills the elements of the `self` tensor with value `val` by selecting the\nindices in the order given in `index`.\n\nOut-of-place version of `torch.Tensor.index_fill_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_fill_()`.\n\nPuts values from the tensor `values` into the tensor `self` using the indices\nspecified in `indices` (which is a tuple of Tensors). The expression\n`tensor.index_put_(indices, values)` is equivalent to `tensor[indices] =\nvalues`. Returns `self`.\n\nIf `accumulate` is `True`, the elements in `values` are added to `self`. If\naccumulate is `False`, the behavior is undefined if indices contain duplicate\nelements.\n\nOut-place version of `index_put_()`. `tensor1` corresponds to `self` in\n`torch.Tensor.index_put_()`.\n\nSee `torch.index_select()`\n\nReturn the indices tensor of a sparse COO tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee also `Tensor.values()`.\n\nNote\n\nThis method can only be called on a coalesced sparse tensor. See\n`Tensor.coalesce()` for details.\n\nSee `torch.inner()`.\n\n`self.int()` is equivalent to `self.to(torch.int32)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nGiven a quantized Tensor, `self.int_repr()` returns a CPU Tensor with uint8_t\nas data type that stores the underlying uint8_t values of the given Tensor.\n\nSee `torch.inverse()`\n\nSee `torch.isclose()`\n\nSee `torch.isfinite()`\n\nSee `torch.isinf()`\n\nSee `torch.isposinf()`\n\nSee `torch.isneginf()`\n\nSee `torch.isnan()`\n\nReturns True if `self` tensor is contiguous in memory in the order specified\nby memory format.\n\nmemory_format (`torch.memory_format`, optional) \u2013 Specifies memory allocation\norder. Default: `torch.contiguous_format`.\n\nReturns True if the data type of `self` is a complex data type.\n\nReturns True if the data type of `self` is a floating point data type.\n\nAll Tensors that have `requires_grad` which is `False` will be leaf Tensors by\nconvention.\n\nFor Tensors that have `requires_grad` which is `True`, they will be leaf\nTensors if they were created by the user. This means that they are not the\nresult of an operation and so `grad_fn` is None.\n\nOnly leaf Tensors will have their `grad` populated during a call to\n`backward()`. To get `grad` populated for non-leaf Tensors, you can use\n`retain_grad()`.\n\nExample:\n\nReturns true if this tensor resides in pinned memory.\n\nReturns True if both tensors are pointing to the exact same memory (same\nstorage, offset, size and stride).\n\nChecks if tensor is in shared memory.\n\nThis is always `True` for CUDA tensors.\n\nReturns True if the data type of `self` is a signed data type.\n\nIs `True` if the Tensor uses sparse storage layout, `False` otherwise.\n\nSee `torch.istft()`\n\nSee `torch.isreal()`\n\nReturns the value of this tensor as a standard Python number. This only works\nfor tensors with one element. For other cases, see `tolist()`.\n\nThis operation is not differentiable.\n\nExample:\n\nSee `torch.kthvalue()`\n\nSee `torch.lcm()`\n\nIn-place version of `lcm()`\n\nSee `torch.ldexp()`\n\nIn-place version of `ldexp()`\n\nSee `torch.le()`.\n\nIn-place version of `le()`.\n\nSee `torch.less_equal()`.\n\nIn-place version of `less_equal()`.\n\nSee `torch.lerp()`\n\nIn-place version of `lerp()`\n\nSee `torch.lgamma()`\n\nIn-place version of `lgamma()`\n\nSee `torch.log()`\n\nIn-place version of `log()`\n\nSee `torch.logdet()`\n\nSee `torch.log10()`\n\nIn-place version of `log10()`\n\nSee `torch.log1p()`\n\nIn-place version of `log1p()`\n\nSee `torch.log2()`\n\nIn-place version of `log2()`\n\nFills `self` tensor with numbers samples from the log-normal distribution\nparameterized by the given mean \u03bc\\mu and standard deviation \u03c3\\sigma . Note\nthat `mean` and `std` are the mean and standard deviation of the underlying\nnormal distribution, and not of the returned distribution:\n\nSee `torch.logaddexp()`\n\nSee `torch.logaddexp2()`\n\nSee `torch.logsumexp()`\n\nSee `torch.logical_and()`\n\nIn-place version of `logical_and()`\n\nSee `torch.logical_not()`\n\nIn-place version of `logical_not()`\n\nSee `torch.logical_or()`\n\nIn-place version of `logical_or()`\n\nSee `torch.logical_xor()`\n\nIn-place version of `logical_xor()`\n\nSee `torch.logit()`\n\nIn-place version of `logit()`\n\n`self.long()` is equivalent to `self.to(torch.int64)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.lstsq()`\n\nSee `torch.lt()`.\n\nIn-place version of `lt()`.\n\nlt(other) -> Tensor\n\nSee `torch.less()`.\n\nIn-place version of `less()`.\n\nSee `torch.lu()`\n\nSee `torch.lu_solve()`\n\nMakes a `cls` instance with the same data pointer as `self`. Changes in the\noutput mirror changes in `self`, and the output stays attached to the autograd\ngraph. `cls` must be a subclass of `Tensor`.\n\nApplies `callable` for each element in `self` tensor and the given `tensor`\nand stores the results in `self` tensor. `self` tensor and the given `tensor`\nmust be broadcastable.\n\nThe `callable` should have the signature:\n\nCopies elements from `source` into `self` tensor at positions where the `mask`\nis True. The shape of `mask` must be broadcastable with the shape of the\nunderlying tensor. The `source` should have at least as many elements as the\nnumber of ones in `mask`\n\nNote\n\nThe `mask` operates on the `self` tensor, not on the given `source` tensor.\n\nOut-of-place version of `torch.Tensor.masked_scatter_()`\n\nFills elements of `self` tensor with `value` where `mask` is True. The shape\nof `mask` must be broadcastable with the shape of the underlying tensor.\n\nOut-of-place version of `torch.Tensor.masked_fill_()`\n\nSee `torch.masked_select()`\n\nSee `torch.matmul()`\n\nSee `torch.matrix_power()`\n\nSee `torch.matrix_exp()`\n\nSee `torch.max()`\n\nSee `torch.maximum()`\n\nSee `torch.mean()`\n\nSee `torch.median()`\n\nSee `torch.nanmedian()`\n\nSee `torch.min()`\n\nSee `torch.minimum()`\n\nSee `torch.mm()`\n\nSee `torch.smm()`\n\nSee `torch.mode()`\n\nSee `torch.movedim()`\n\nSee `torch.moveaxis()`\n\nSee `torch.msort()`\n\nSee `torch.mul()`.\n\nIn-place version of `mul()`.\n\nSee `torch.multiply()`.\n\nIn-place version of `multiply()`.\n\nSee `torch.multinomial()`\n\nSee `torch.mv()`\n\nSee `torch.mvlgamma()`\n\nIn-place version of `mvlgamma()`\n\nSee `torch.nansum()`\n\nSee `torch.narrow()`\n\nExample:\n\nSame as `Tensor.narrow()` except returning a copy rather than shared storage.\nThis is primarily for sparse tensors, which do not have a shared-storage\nnarrow method. Calling ``narrow_copy` with ``dimemsion > self.sparse_dim()``\nwill return a copy with the relevant dense dimension narrowed, and\n``self.shape`` updated accordingly.\n\nAlias for `dim()`\n\nSee `torch.nan_to_num()`.\n\nIn-place version of `nan_to_num()`.\n\nSee `torch.ne()`.\n\nIn-place version of `ne()`.\n\nSee `torch.not_equal()`.\n\nIn-place version of `not_equal()`.\n\nSee `torch.neg()`\n\nIn-place version of `neg()`\n\nSee `torch.negative()`\n\nIn-place version of `negative()`\n\nAlias for `numel()`\n\nSee `torch.nextafter()`\n\nIn-place version of `nextafter()`\n\nSee `torch.nonzero()`\n\nSee `torch.norm()`\n\nFills `self` tensor with elements samples from the normal distribution\nparameterized by `mean` and `std`.\n\nSee `torch.numel()`\n\nReturns `self` tensor as a NumPy `ndarray`. This tensor and the returned\n`ndarray` share the same underlying storage. Changes to `self` tensor will be\nreflected in the `ndarray` and vice versa.\n\nSee `torch.orgqr()`\n\nSee `torch.ormqr()`\n\nSee `torch.outer()`.\n\nReturns a view of the original tensor with its dimensions permuted.\n\n*dims (int...) \u2013 The desired ordering of dimensions\nCopies the tensor to pinned memory, if it\u2019s not already pinned.\n\nSee `torch.pinverse()`\n\nSee `torch.polygamma()`\n\nIn-place version of `polygamma()`\n\nSee `torch.pow()`\n\nIn-place version of `pow()`\n\nSee `torch.prod()`\n\nCopies the elements from `tensor` into the positions specified by indices. For\nthe purpose of indexing, the `self` tensor is treated as if it were a 1-D\ntensor.\n\nIf `accumulate` is `True`, the elements in `tensor` are added to `self`. If\naccumulate is `False`, the behavior is undefined if indices contain duplicate\nelements.\n\nExample:\n\nSee `torch.qr()`\n\nReturns the quantization scheme of a given QTensor.\n\nSee `torch.quantile()`\n\nSee `torch.nanquantile()`\n\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of\nthe underlying quantizer().\n\nGiven a Tensor quantized by linear(affine) quantization, returns the\nzero_point of the underlying quantizer().\n\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\na Tensor of scales of the underlying quantizer. It has the number of elements\nthat matches the corresponding dimensions (from q_per_channel_axis) of the\ntensor.\n\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\na tensor of zero_points of the underlying quantizer. It has the number of\nelements that matches the corresponding dimensions (from q_per_channel_axis)\nof the tensor.\n\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\nthe index of dimension on which per-channel quantization is applied.\n\nSee `torch.rad2deg()`\n\nFills `self` tensor with numbers sampled from the discrete uniform\ndistribution over `[from, to - 1]`. If not specified, the values are usually\nonly bounded by `self` tensor\u2019s data type. However, for floating point types,\nif unspecified, range will be `[0, 2^mantissa]` to ensure that every value is\nrepresentable. For example, `torch.tensor(1, dtype=torch.double).random_()`\nwill be uniform in `[0, 2^53]`.\n\nsee `torch.ravel()`\n\nSee `torch.reciprocal()`\n\nIn-place version of `reciprocal()`\n\nEnsures that the tensor memory is not reused for another tensor until all\ncurrent work queued on `stream` are complete.\n\nNote\n\nThe caching allocator is aware of only the stream where a tensor was\nallocated. Due to the awareness, it already correctly manages the life cycle\nof tensors on only one stream. But if a tensor is used on a stream different\nfrom the stream of origin, the allocator might reuse the memory unexpectedly.\nCalling this method lets the allocator know which streams have used the\ntensor.\n\nRegisters a backward hook.\n\nThe hook will be called every time a gradient with respect to the Tensor is\ncomputed. The hook should have the following signature:\n\nThe hook should not modify its argument, but it can optionally return a new\ngradient which will be used in place of `grad`.\n\nThis function returns a handle with a method `handle.remove()` that removes\nthe hook from the module.\n\nExample:\n\nSee `torch.remainder()`\n\nIn-place version of `remainder()`\n\nSee `torch.renorm()`\n\nIn-place version of `renorm()`\n\nRepeats this tensor along the specified dimensions.\n\nUnlike `expand()`, this function copies the tensor\u2019s data.\n\nWarning\n\n`repeat()` behaves differently from numpy.repeat, but is more similar to\nnumpy.tile. For the operator similar to `numpy.repeat`, see\n`torch.repeat_interleave()`.\n\nsizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along\neach dimension\n\nExample:\n\nSee `torch.repeat_interleave()`.\n\nIs `True` if gradients need to be computed for this Tensor, `False` otherwise.\n\nNote\n\nThe fact that gradients need to be computed for a Tensor do not mean that the\n`grad` attribute will be populated, see `is_leaf` for more details.\n\nChange if autograd should record operations on this tensor: sets this tensor\u2019s\n`requires_grad` attribute in-place. Returns this tensor.\n\n`requires_grad_()`\u2019s main use case is to tell autograd to begin recording\noperations on a Tensor `tensor`. If `tensor` has `requires_grad=False`\n(because it was obtained through a DataLoader, or required preprocessing or\ninitialization), `tensor.requires_grad_()` makes it so that autograd will\nbegin to record operations on `tensor`.\n\nrequires_grad (bool) \u2013 If autograd should record operations on this tensor.\nDefault: `True`.\n\nExample:\n\nReturns a tensor with the same data and number of elements as `self` but with\nthe specified shape. This method returns a view if `shape` is compatible with\nthe current shape. See `torch.Tensor.view()` on when it is possible to return\na view.\n\nSee `torch.reshape()`\n\nshape (tuple of python:ints or int...) \u2013 the desired shape\n\nReturns this tensor as the same shape as `other`. `self.reshape_as(other)` is\nequivalent to `self.reshape(other.sizes())`. This method returns a view if\n`other.sizes()` is compatible with the current shape. See\n`torch.Tensor.view()` on when it is possible to return a view.\n\nPlease see `reshape()` for more information about `reshape`.\n\nother (`torch.Tensor`) \u2013 The result tensor has the same shape as `other`.\n\nResizes `self` tensor to the specified size. If the number of elements is\nlarger than the current storage size, then the underlying storage is resized\nto fit the new number of elements. If the number of elements is smaller, the\nunderlying storage is not changed. Existing elements are preserved but any new\nmemory is uninitialized.\n\nWarning\n\nThis is a low-level method. The storage is reinterpreted as C-contiguous,\nignoring the current strides (unless the target size equals the current size,\nin which case the tensor is left unchanged). For most purposes, you will\ninstead want to use `view()`, which checks for contiguity, or `reshape()`,\nwhich copies data if needed. To change the size in-place with custom strides,\nsee `set_()`.\n\nExample:\n\nResizes the `self` tensor to be the same size as the specified `tensor`. This\nis equivalent to `self.resize_(tensor.size())`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nTensor. Default: `torch.contiguous_format`. Note that memory format of `self`\nis going to be unaffected if `self.size()` matches `tensor.size()`.\n\nEnables .grad attribute for non-leaf Tensors.\n\nSee `torch.roll()`\n\nSee `torch.rot90()`\n\nSee `torch.round()`\n\nIn-place version of `round()`\n\nSee `torch.rsqrt()`\n\nIn-place version of `rsqrt()`\n\nOut-of-place version of `torch.Tensor.scatter_()`\n\nWrites all values from the tensor `src` into `self` at the indices specified\nin the `index` tensor. For each value in `src`, its output index is specified\nby its index in `src` for `dimension != dim` and by the corresponding value in\n`index` for `dimension = dim`.\n\nFor a 3-D tensor, `self` is updated as:\n\nThis is the reverse operation of the manner described in `gather()`.\n\n`self`, `index` and `src` (if it is a Tensor) should all have the same number\nof dimensions. It is also required that `index.size(d) <= src.size(d)` for all\ndimensions `d`, and that `index.size(d) <= self.size(d)` for all dimensions `d\n!= dim`. Note that `index` and `src` do not broadcast.\n\nMoreover, as for `gather()`, the values of `index` must be between `0` and\n`self.size(dim) - 1` inclusive.\n\nWarning\n\nWhen indices are not unique, the behavior is non-deterministic (one of the\nvalues from `src` will be picked arbitrarily) and the gradient will be\nincorrect (it will be propagated to all locations in the source that\ncorrespond to the same index)!\n\nNote\n\nThe backward pass is implemented only for `src.shape == index.shape`.\n\nAdditionally accepts an optional `reduce` argument that allows specification\nof an optional reduction operation, which is applied to all values in the\ntensor `src` into `self` at the indicies specified in the `index`. For each\nvalue in `src`, the reduction operation is applied to an index in `self` which\nis specified by its index in `src` for `dimension != dim` and by the\ncorresponding value in `index` for `dimension = dim`.\n\nGiven a 3-D tensor and reduction using the multiplication operation, `self` is\nupdated as:\n\nReducing with the addition operation is the same as using `scatter_add_()`.\n\nExample:\n\nAdds all values from the tensor `other` into `self` at the indices specified\nin the `index` tensor in a similar fashion as `scatter_()`. For each value in\n`src`, it is added to an index in `self` which is specified by its index in\n`src` for `dimension != dim` and by the corresponding value in `index` for\n`dimension = dim`.\n\nFor a 3-D tensor, `self` is updated as:\n\n`self`, `index` and `src` should have same number of dimensions. It is also\nrequired that `index.size(d) <= src.size(d)` for all dimensions `d`, and that\n`index.size(d) <= self.size(d)` for all dimensions `d != dim`. Note that\n`index` and `src` do not broadcast.\n\nNote\n\nThis operation may behave nondeterministically when given tensors on a CUDA\ndevice. See Reproducibility for more information.\n\nNote\n\nThe backward pass is implemented only for `src.shape == index.shape`.\n\nExample:\n\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\nSlices the `self` tensor along the selected dimension at the given index. This\nfunction returns a view of the original tensor with the given dimension\nremoved.\n\nNote\n\n`select()` is equivalent to slicing. For example, `tensor.select(0, index)` is\nequivalent to `tensor[index]` and `tensor.select(2, index)` is equivalent to\n`tensor[:,:,index]`.\n\nSets the underlying storage, size, and strides. If `source` is a tensor,\n`self` tensor will share the same storage and have the same size and strides\nas `source`. Changes to elements in one tensor will be reflected in the other.\n\nIf `source` is a `Storage`, the method sets the underlying storage, offset,\nsize, and stride.\n\nMoves the underlying storage to shared memory.\n\nThis is a no-op if the underlying storage is already in shared memory and for\nCUDA tensors. Tensors in shared memory cannot be resized.\n\n`self.short()` is equivalent to `self.to(torch.int16)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nSee `torch.sigmoid()`\n\nIn-place version of `sigmoid()`\n\nSee `torch.sign()`\n\nIn-place version of `sign()`\n\nSee `torch.signbit()`\n\nSee `torch.sgn()`\n\nIn-place version of `sgn()`\n\nSee `torch.sin()`\n\nIn-place version of `sin()`\n\nSee `torch.sinc()`\n\nIn-place version of `sinc()`\n\nSee `torch.sinh()`\n\nIn-place version of `sinh()`\n\nSee `torch.asinh()`\n\nIn-place version of `asinh()`\n\nSee `torch.arcsinh()`\n\nIn-place version of `arcsinh()`\n\nReturns the size of the `self` tensor. The returned value is a subclass of\n`tuple`.\n\nExample:\n\nSee `torch.slogdet()`\n\nSee `torch.solve()`\n\nSee `torch.sort()`\n\nSee `torch.split()`\n\nReturns a new sparse tensor with values from a strided tensor `self` filtered\nby the indices of the sparse tensor `mask`. The values of `mask` sparse tensor\nare ignored. `self` and `mask` tensors must have the same shape.\n\nNote\n\nThe returned sparse tensor has the same indices as the sparse tensor `mask`,\neven when the corresponding values in `self` are zeros.\n\nmask (Tensor) \u2013 a sparse tensor whose indices are used as a filter\n\nExample:\n\nReturn the number of sparse dimensions in a sparse tensor `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nSee also `Tensor.dense_dim()` and hybrid tensors.\n\nSee `torch.sqrt()`\n\nIn-place version of `sqrt()`\n\nSee `torch.square()`\n\nIn-place version of `square()`\n\nSee `torch.squeeze()`\n\nIn-place version of `squeeze()`\n\nSee `torch.std()`\n\nSee `torch.stft()`\n\nWarning\n\nThis function changed signature at version 0.4.1. Calling with the previous\nsignature may cause error or return incorrect result.\n\nReturns the underlying storage.\n\nReturns `self` tensor\u2019s offset in the underlying storage in terms of number of\nstorage elements (not bytes).\n\nExample:\n\nReturns the type of the underlying storage.\n\nReturns the stride of `self` tensor.\n\nStride is the jump necessary to go from one element to the next one in the\nspecified dimension `dim`. A tuple of all strides is returned when no argument\nis passed in. Otherwise, an integer value is returned as the stride in the\nparticular dimension `dim`.\n\ndim (int, optional) \u2013 the desired dimension in which stride is required\n\nExample:\n\nSee `torch.sub()`.\n\nIn-place version of `sub()`\n\nSee `torch.subtract()`.\n\nIn-place version of `subtract()`.\n\nSee `torch.sum()`\n\nSum `this` tensor to `size`. `size` must be broadcastable to `this` tensor\nsize.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor.\n\nSee `torch.svd()`\n\nSee `torch.swapaxes()`\n\nSee `torch.swapdims()`\n\nSee `torch.symeig()`\n\nSee `torch.t()`\n\nIn-place version of `t()`\n\nSee `torch.tensor_split()`\n\nSee `torch.tile()`\n\nPerforms Tensor dtype and/or device conversion. A `torch.dtype` and\n`torch.device` are inferred from the arguments of `self.to(*args, **kwargs)`.\n\nNote\n\nIf the `self` Tensor already has the correct `torch.dtype` and `torch.device`,\nthen `self` is returned. Otherwise, the returned tensor is a copy of `self`\nwith the desired `torch.dtype` and `torch.device`.\n\nHere are the ways to call `to`:\n\nReturns a Tensor with the specified `dtype`\n\nmemory_format (`torch.memory_format`, optional): the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nReturns a Tensor with the specified `device` and (optional) `dtype`. If\n`dtype` is `None` it is inferred to be `self.dtype`. When `non_blocking`,\ntries to convert asynchronously with respect to the host if possible, e.g.,\nconverting a CPU Tensor with pinned memory to a CUDA Tensor. When `copy` is\nset, a new Tensor is created even when the Tensor already matches the desired\nconversion.\n\nmemory_format (`torch.memory_format`, optional): the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nReturns a Tensor with same `torch.dtype` and `torch.device` as the Tensor\n`other`. When `non_blocking`, tries to convert asynchronously with respect to\nthe host if possible, e.g., converting a CPU Tensor with pinned memory to a\nCUDA Tensor. When `copy` is set, a new Tensor is created even when the Tensor\nalready matches the desired conversion.\n\nExample:\n\nReturns a copy of the tensor in `torch.mkldnn` layout.\n\nSee `torch.take()`\n\nSee `torch.tan()`\n\nIn-place version of `tan()`\n\nSee `torch.tanh()`\n\nIn-place version of `tanh()`\n\nSee `torch.atanh()`\n\nIn-place version of `atanh()`\n\nSee `torch.arctanh()`\n\nIn-place version of `arctanh()`\n\nReturns the tensor as a (nested) list. For scalars, a standard Python number\nis returned, just like with `item()`. Tensors are automatically moved to the\nCPU first if necessary.\n\nThis operation is not differentiable.\n\nExamples:\n\nSee `torch.topk()`\n\nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in\ncoordinate format.\n\nsparseDims (int, optional) \u2013 the number of sparse dimensions to include in the\nnew sparse tensor\n\nExample:\n\nSee `torch.trace()`\n\nSee `torch.transpose()`\n\nIn-place version of `transpose()`\n\nSee `torch.triangular_solve()`\n\nSee `torch.tril()`\n\nIn-place version of `tril()`\n\nSee `torch.triu()`\n\nIn-place version of `triu()`\n\nSee `torch.true_divide()`\n\nIn-place version of `true_divide_()`\n\nSee `torch.trunc()`\n\nIn-place version of `trunc()`\n\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\nReturns this tensor cast to the type of the given tensor.\n\nThis is a no-op if the tensor is already of the correct type. This is\nequivalent to `self.type(tensor.type())`\n\ntensor (Tensor) \u2013 the tensor which has the desired type\n\nSee `torch.unbind()`\n\nReturns a view of the original tensor which contains all slices of size `size`\nfrom `self` tensor in the dimension `dimension`.\n\nStep between two slices is given by `step`.\n\nIf `sizedim` is the size of dimension `dimension` for `self`, the size of\ndimension `dimension` in the returned tensor will be `(sizedim - size) / step\n+ 1`.\n\nAn additional dimension of size `size` is appended in the returned tensor.\n\nExample:\n\nFills `self` tensor with numbers sampled from the continuous uniform\ndistribution:\n\nReturns the unique elements of the input tensor.\n\nSee `torch.unique()`\n\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\nSee `torch.unique_consecutive()`\n\nSee `torch.unsqueeze()`\n\nIn-place version of `unsqueeze()`\n\nReturn the values tensor of a sparse COO tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee also `Tensor.indices()`.\n\nNote\n\nThis method can only be called on a coalesced sparse tensor. See\n`Tensor.coalesce()` for details.\n\nSee `torch.var()`\n\nSee `torch.vdot()`\n\nReturns a new tensor with the same data as the `self` tensor but of a\ndifferent `shape`.\n\nThe returned tensor shares the same data and must have the same number of\nelements, but may have a different size. For a tensor to be viewed, the new\nview size must be compatible with its original size and stride, i.e., each new\nview dimension must either be a subspace of an original dimension, or only\nspan across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k that satisfy the\nfollowing contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots,\nd+k-1 ,\n\nOtherwise, it will not be possible to view `self` tensor as `shape` without\ncopying it (e.g., via `contiguous()`). When it is unclear whether a `view()`\ncan be performed, it is advisable to use `reshape()`, which returns a view if\nthe shapes are compatible, and copies (equivalent to calling `contiguous()`)\notherwise.\n\nshape (torch.Size or int...) \u2013 the desired size\n\nExample:\n\nReturns a new tensor with the same data as the `self` tensor but of a\ndifferent `dtype`. `dtype` must have the same number of bytes per element as\n`self`\u2019s dtype.\n\nWarning\n\nThis overload is not supported by TorchScript, and using it in a Torchscript\nprogram will cause undefined behavior.\n\ndtype (`torch.dtype`) \u2013 the desired dtype\n\nExample:\n\nView this tensor as the same size as `other`. `self.view_as(other)` is\nequivalent to `self.view(other.size())`.\n\nPlease see `view()` for more information about `view`.\n\nother (`torch.Tensor`) \u2013 The result tensor has the same size as `other`.\n\n`self.where(condition, y)` is equivalent to `torch.where(condition, self, y)`.\nSee `torch.where()`\n\nSee `torch.xlogy()`\n\nIn-place version of `xlogy()`\n\nFills `self` tensor with zeros.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tensor()", "path": "generated/torch.tensor#torch.tensor", "type": "torch", "text": "\nConstructs a tensor with `data`.\n\nWarning\n\n`torch.tensor()` always copies `data`. If you have a Tensor `data` and want to\navoid a copy, use `torch.Tensor.requires_grad_()` or `torch.Tensor.detach()`.\nIf you have a NumPy `ndarray` and want to avoid a copy, use\n`torch.as_tensor()`.\n\nWarning\n\nWhen data is a tensor `x`, `torch.tensor()` reads out \u2018the data\u2019 from whatever\nit is passed, and constructs a leaf variable. Therefore `torch.tensor(x)` is\nequivalent to `x.clone().detach()` and `torch.tensor(x, requires_grad=True)`\nis equivalent to `x.clone().detach().requires_grad_(True)`. The equivalents\nusing `clone()` and `detach()` are recommended.\n\ndata (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy\n`ndarray`, scalar, and other types.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.abs()", "path": "tensors#torch.Tensor.abs", "type": "torch.Tensor", "text": "\nSee `torch.abs()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.absolute()", "path": "tensors#torch.Tensor.absolute", "type": "torch.Tensor", "text": "\nAlias for `abs()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.absolute_()", "path": "tensors#torch.Tensor.absolute_", "type": "torch.Tensor", "text": "\nIn-place version of `absolute()` Alias for `abs_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.abs_()", "path": "tensors#torch.Tensor.abs_", "type": "torch.Tensor", "text": "\nIn-place version of `abs()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.acos()", "path": "tensors#torch.Tensor.acos", "type": "torch.Tensor", "text": "\nSee `torch.acos()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.acosh()", "path": "tensors#torch.Tensor.acosh", "type": "torch.Tensor", "text": "\nSee `torch.acosh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.acosh_()", "path": "tensors#torch.Tensor.acosh_", "type": "torch.Tensor", "text": "\nIn-place version of `acosh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.acos_()", "path": "tensors#torch.Tensor.acos_", "type": "torch.Tensor", "text": "\nIn-place version of `acos()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.add()", "path": "tensors#torch.Tensor.add", "type": "torch.Tensor", "text": "\nAdd a scalar or tensor to `self` tensor. If both `alpha` and `other` are\nspecified, each element of `other` is scaled by `alpha` before being used.\n\nWhen `other` is a tensor, the shape of `other` must be broadcastable with the\nshape of the underlying tensor\n\nSee `torch.add()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addbmm()", "path": "tensors#torch.Tensor.addbmm", "type": "torch.Tensor", "text": "\nSee `torch.addbmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addbmm_()", "path": "tensors#torch.Tensor.addbmm_", "type": "torch.Tensor", "text": "\nIn-place version of `addbmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addcdiv()", "path": "tensors#torch.Tensor.addcdiv", "type": "torch.Tensor", "text": "\nSee `torch.addcdiv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addcdiv_()", "path": "tensors#torch.Tensor.addcdiv_", "type": "torch.Tensor", "text": "\nIn-place version of `addcdiv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addcmul()", "path": "tensors#torch.Tensor.addcmul", "type": "torch.Tensor", "text": "\nSee `torch.addcmul()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addcmul_()", "path": "tensors#torch.Tensor.addcmul_", "type": "torch.Tensor", "text": "\nIn-place version of `addcmul()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addmm()", "path": "tensors#torch.Tensor.addmm", "type": "torch.Tensor", "text": "\nSee `torch.addmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addmm_()", "path": "tensors#torch.Tensor.addmm_", "type": "torch.Tensor", "text": "\nIn-place version of `addmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addmv()", "path": "tensors#torch.Tensor.addmv", "type": "torch.Tensor", "text": "\nSee `torch.addmv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addmv_()", "path": "tensors#torch.Tensor.addmv_", "type": "torch.Tensor", "text": "\nIn-place version of `addmv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addr()", "path": "tensors#torch.Tensor.addr", "type": "torch.Tensor", "text": "\nSee `torch.addr()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.addr_()", "path": "tensors#torch.Tensor.addr_", "type": "torch.Tensor", "text": "\nIn-place version of `addr()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.add_()", "path": "tensors#torch.Tensor.add_", "type": "torch.Tensor", "text": "\nIn-place version of `add()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.align_as()", "path": "named_tensor#torch.Tensor.align_as", "type": "Named Tensors", "text": "\nPermutes the dimensions of the `self` tensor to match the dimension order in\nthe `other` tensor, adding size-one dims for any new names.\n\nThis operation is useful for explicit broadcasting by names (see examples).\n\nAll of the dims of `self` must be named in order to use this method. The\nresulting tensor is a view on the original tensor.\n\nAll dimension names of `self` must be present in `other.names`. `other` may\ncontain named dimensions that are not in `self.names`; the output tensor has a\nsize-one dimension for each of those new names.\n\nTo align a tensor to a specific order, use `align_to()`.\n\nExamples:\n\nWarning\n\nThe named tensor API is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.align_to()", "path": "named_tensor#torch.Tensor.align_to", "type": "Named Tensors", "text": "\nPermutes the dimensions of the `self` tensor to match the order specified in\n`names`, adding size-one dims for any new names.\n\nAll of the dims of `self` must be named in order to use this method. The\nresulting tensor is a view on the original tensor.\n\nAll dimension names of `self` must be present in `names`. `names` may contain\nadditional names that are not in `self.names`; the output tensor has a size-\none dimension for each of those new names.\n\n`names` may contain up to one Ellipsis (`...`). The Ellipsis is expanded to be\nequal to all dimension names of `self` that are not mentioned in `names`, in\nthe order that they appear in `self`.\n\nPython 2 does not support Ellipsis but one may use a string literal instead\n(`'...'`).\n\nnames (iterable of str) \u2013 The desired dimension ordering of the output tensor.\nMay contain up to one Ellipsis that is expanded to all unmentioned dim names\nof `self`.\n\nExamples:\n\nWarning\n\nThe named tensor API is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.all()", "path": "tensors#torch.Tensor.all", "type": "torch.Tensor", "text": "\nSee `torch.all()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.allclose()", "path": "tensors#torch.Tensor.allclose", "type": "torch.Tensor", "text": "\nSee `torch.allclose()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.amax()", "path": "tensors#torch.Tensor.amax", "type": "torch.Tensor", "text": "\nSee `torch.amax()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.amin()", "path": "tensors#torch.Tensor.amin", "type": "torch.Tensor", "text": "\nSee `torch.amin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.angle()", "path": "tensors#torch.Tensor.angle", "type": "torch.Tensor", "text": "\nSee `torch.angle()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.any()", "path": "tensors#torch.Tensor.any", "type": "torch.Tensor", "text": "\nSee `torch.any()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.apply_()", "path": "tensors#torch.Tensor.apply_", "type": "torch.Tensor", "text": "\nApplies the function `callable` to each element in the tensor, replacing each\nelement with the value returned by `callable`.\n\nNote\n\nThis function only works with CPU tensors and should not be used in code\nsections that require high performance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arccos()", "path": "tensors#torch.Tensor.arccos", "type": "torch.Tensor", "text": "\nSee `torch.arccos()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arccosh()", "path": "tensors#torch.Tensor.arccosh", "type": "torch.Tensor", "text": "\nacosh() -> Tensor\n\nSee `torch.arccosh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arccosh_()", "path": "tensors#torch.Tensor.arccosh_", "type": "torch.Tensor", "text": "\nacosh_() -> Tensor\n\nIn-place version of `arccosh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arccos_()", "path": "tensors#torch.Tensor.arccos_", "type": "torch.Tensor", "text": "\nIn-place version of `arccos()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arcsin()", "path": "tensors#torch.Tensor.arcsin", "type": "torch.Tensor", "text": "\nSee `torch.arcsin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arcsinh()", "path": "tensors#torch.Tensor.arcsinh", "type": "torch.Tensor", "text": "\nSee `torch.arcsinh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arcsinh_()", "path": "tensors#torch.Tensor.arcsinh_", "type": "torch.Tensor", "text": "\nIn-place version of `arcsinh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arcsin_()", "path": "tensors#torch.Tensor.arcsin_", "type": "torch.Tensor", "text": "\nIn-place version of `arcsin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arctan()", "path": "tensors#torch.Tensor.arctan", "type": "torch.Tensor", "text": "\nSee `torch.arctan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arctanh()", "path": "tensors#torch.Tensor.arctanh", "type": "torch.Tensor", "text": "\nSee `torch.arctanh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arctanh_()", "path": "tensors#torch.Tensor.arctanh_", "type": "torch.Tensor", "text": "\nIn-place version of `arctanh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.arctan_()", "path": "tensors#torch.Tensor.arctan_", "type": "torch.Tensor", "text": "\nIn-place version of `arctan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.argmax()", "path": "tensors#torch.Tensor.argmax", "type": "torch.Tensor", "text": "\nSee `torch.argmax()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.argmin()", "path": "tensors#torch.Tensor.argmin", "type": "torch.Tensor", "text": "\nSee `torch.argmin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.argsort()", "path": "tensors#torch.Tensor.argsort", "type": "torch.Tensor", "text": "\nSee `torch.argsort()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.asin()", "path": "tensors#torch.Tensor.asin", "type": "torch.Tensor", "text": "\nSee `torch.asin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.asinh()", "path": "tensors#torch.Tensor.asinh", "type": "torch.Tensor", "text": "\nSee `torch.asinh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.asinh_()", "path": "tensors#torch.Tensor.asinh_", "type": "torch.Tensor", "text": "\nIn-place version of `asinh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.asin_()", "path": "tensors#torch.Tensor.asin_", "type": "torch.Tensor", "text": "\nIn-place version of `asin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.as_strided()", "path": "tensors#torch.Tensor.as_strided", "type": "torch.Tensor", "text": "\nSee `torch.as_strided()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.as_subclass()", "path": "tensors#torch.Tensor.as_subclass", "type": "torch.Tensor", "text": "\nMakes a `cls` instance with the same data pointer as `self`. Changes in the\noutput mirror changes in `self`, and the output stays attached to the autograd\ngraph. `cls` must be a subclass of `Tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.atan()", "path": "tensors#torch.Tensor.atan", "type": "torch.Tensor", "text": "\nSee `torch.atan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.atan2()", "path": "tensors#torch.Tensor.atan2", "type": "torch.Tensor", "text": "\nSee `torch.atan2()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.atan2_()", "path": "tensors#torch.Tensor.atan2_", "type": "torch.Tensor", "text": "\nIn-place version of `atan2()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.atanh()", "path": "tensors#torch.Tensor.atanh", "type": "torch.Tensor", "text": "\nSee `torch.atanh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.atanh_()", "path": "tensors#torch.Tensor.atanh_", "type": "torch.Tensor", "text": "\nIn-place version of `atanh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.atan_()", "path": "tensors#torch.Tensor.atan_", "type": "torch.Tensor", "text": "\nIn-place version of `atan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.backward()", "path": "autograd#torch.Tensor.backward", "type": "torch.autograd", "text": "\nComputes the gradient of current tensor w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If the tensor is non-scalar\n(i.e. its data has more than one element) and requires gradient, the function\nadditionally requires specifying `gradient`. It should be a tensor of matching\ntype and location, that contains the gradient of the differentiated function\nw.r.t. `self`.\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nIf you run any forward ops, create `gradient`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.baddbmm()", "path": "tensors#torch.Tensor.baddbmm", "type": "torch.Tensor", "text": "\nSee `torch.baddbmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.baddbmm_()", "path": "tensors#torch.Tensor.baddbmm_", "type": "torch.Tensor", "text": "\nIn-place version of `baddbmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bernoulli()", "path": "tensors#torch.Tensor.bernoulli", "type": "torch.Tensor", "text": "\nReturns a result tensor where each result[i]\\texttt{result[i]} is\nindependently sampled from\nBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . `self` must have\nfloating point `dtype`, and the result will have the same `dtype`.\n\nSee `torch.bernoulli()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bernoulli_()", "path": "tensors#torch.Tensor.bernoulli_", "type": "torch.Tensor", "text": "\nFills each location of `self` with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p}) . `self` can have integral `dtype`.\n\n`p_tensor` should be a tensor containing probabilities to be used for drawing\nthe binary random number.\n\nThe ith\\text{i}^{th} element of `self` tensor will be set to a value sampled\nfrom Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\\\_tensor[i]}) .\n\n`self` can have integral `dtype`, but `p_tensor` must have floating point\n`dtype`.\n\nSee also `bernoulli()` and `torch.bernoulli()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bfloat16()", "path": "tensors#torch.Tensor.bfloat16", "type": "torch.Tensor", "text": "\n`self.bfloat16()` is equivalent to `self.to(torch.bfloat16)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bincount()", "path": "tensors#torch.Tensor.bincount", "type": "torch.Tensor", "text": "\nSee `torch.bincount()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_and()", "path": "tensors#torch.Tensor.bitwise_and", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_and()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_and_()", "path": "tensors#torch.Tensor.bitwise_and_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_and()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_not()", "path": "tensors#torch.Tensor.bitwise_not", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_not()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_not_()", "path": "tensors#torch.Tensor.bitwise_not_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_not()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_or()", "path": "tensors#torch.Tensor.bitwise_or", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_or()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_or_()", "path": "tensors#torch.Tensor.bitwise_or_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_or()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_xor()", "path": "tensors#torch.Tensor.bitwise_xor", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_xor()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_xor_()", "path": "tensors#torch.Tensor.bitwise_xor_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_xor()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bmm()", "path": "tensors#torch.Tensor.bmm", "type": "torch.Tensor", "text": "\nSee `torch.bmm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.bool()", "path": "tensors#torch.Tensor.bool", "type": "torch.Tensor", "text": "\n`self.bool()` is equivalent to `self.to(torch.bool)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.broadcast_to()", "path": "tensors#torch.Tensor.broadcast_to", "type": "torch.Tensor", "text": "\nSee `torch.broadcast_to()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.byte()", "path": "tensors#torch.Tensor.byte", "type": "torch.Tensor", "text": "\n`self.byte()` is equivalent to `self.to(torch.uint8)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cauchy_()", "path": "tensors#torch.Tensor.cauchy_", "type": "torch.Tensor", "text": "\nFills the tensor with numbers drawn from the Cauchy distribution:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ceil()", "path": "tensors#torch.Tensor.ceil", "type": "torch.Tensor", "text": "\nSee `torch.ceil()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ceil_()", "path": "tensors#torch.Tensor.ceil_", "type": "torch.Tensor", "text": "\nIn-place version of `ceil()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.char()", "path": "tensors#torch.Tensor.char", "type": "torch.Tensor", "text": "\n`self.char()` is equivalent to `self.to(torch.int8)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cholesky()", "path": "tensors#torch.Tensor.cholesky", "type": "torch.Tensor", "text": "\nSee `torch.cholesky()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cholesky_inverse()", "path": "tensors#torch.Tensor.cholesky_inverse", "type": "torch.Tensor", "text": "\nSee `torch.cholesky_inverse()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cholesky_solve()", "path": "tensors#torch.Tensor.cholesky_solve", "type": "torch.Tensor", "text": "\nSee `torch.cholesky_solve()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.chunk()", "path": "tensors#torch.Tensor.chunk", "type": "torch.Tensor", "text": "\nSee `torch.chunk()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.clamp()", "path": "tensors#torch.Tensor.clamp", "type": "torch.Tensor", "text": "\nSee `torch.clamp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.clamp_()", "path": "tensors#torch.Tensor.clamp_", "type": "torch.Tensor", "text": "\nIn-place version of `clamp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.clip()", "path": "tensors#torch.Tensor.clip", "type": "torch.Tensor", "text": "\nAlias for `clamp()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.clip_()", "path": "tensors#torch.Tensor.clip_", "type": "torch.Tensor", "text": "\nAlias for `clamp_()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.clone()", "path": "tensors#torch.Tensor.clone", "type": "torch.Tensor", "text": "\nSee `torch.clone()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.coalesce()", "path": "sparse#torch.Tensor.coalesce", "type": "torch.sparse", "text": "\nReturns a coalesced copy of `self` if `self` is an uncoalesced tensor.\n\nReturns `self` if `self` is a coalesced tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.conj()", "path": "tensors#torch.Tensor.conj", "type": "torch.Tensor", "text": "\nSee `torch.conj()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.contiguous()", "path": "tensors#torch.Tensor.contiguous", "type": "torch.Tensor", "text": "\nReturns a contiguous in memory tensor containing the same data as `self`\ntensor. If `self` tensor is already in the specified memory format, this\nfunction returns the `self` tensor.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.contiguous_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.copysign()", "path": "tensors#torch.Tensor.copysign", "type": "torch.Tensor", "text": "\nSee `torch.copysign()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.copysign_()", "path": "tensors#torch.Tensor.copysign_", "type": "torch.Tensor", "text": "\nIn-place version of `copysign()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.copy_()", "path": "tensors#torch.Tensor.copy_", "type": "torch.Tensor", "text": "\nCopies the elements from `src` into `self` tensor and returns `self`.\n\nThe `src` tensor must be broadcastable with the `self` tensor. It may be of a\ndifferent data type or reside on a different device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cos()", "path": "tensors#torch.Tensor.cos", "type": "torch.Tensor", "text": "\nSee `torch.cos()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cosh()", "path": "tensors#torch.Tensor.cosh", "type": "torch.Tensor", "text": "\nSee `torch.cosh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cosh_()", "path": "tensors#torch.Tensor.cosh_", "type": "torch.Tensor", "text": "\nIn-place version of `cosh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cos_()", "path": "tensors#torch.Tensor.cos_", "type": "torch.Tensor", "text": "\nIn-place version of `cos()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.count_nonzero()", "path": "tensors#torch.Tensor.count_nonzero", "type": "torch.Tensor", "text": "\nSee `torch.count_nonzero()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cpu()", "path": "tensors#torch.Tensor.cpu", "type": "torch.Tensor", "text": "\nReturns a copy of this object in CPU memory.\n\nIf this object is already in CPU memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cross()", "path": "tensors#torch.Tensor.cross", "type": "torch.Tensor", "text": "\nSee `torch.cross()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cuda()", "path": "tensors#torch.Tensor.cuda", "type": "torch.Tensor", "text": "\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cummax()", "path": "tensors#torch.Tensor.cummax", "type": "torch.Tensor", "text": "\nSee `torch.cummax()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cummin()", "path": "tensors#torch.Tensor.cummin", "type": "torch.Tensor", "text": "\nSee `torch.cummin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cumprod()", "path": "tensors#torch.Tensor.cumprod", "type": "torch.Tensor", "text": "\nSee `torch.cumprod()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cumprod_()", "path": "tensors#torch.Tensor.cumprod_", "type": "torch.Tensor", "text": "\nIn-place version of `cumprod()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cumsum()", "path": "tensors#torch.Tensor.cumsum", "type": "torch.Tensor", "text": "\nSee `torch.cumsum()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.cumsum_()", "path": "tensors#torch.Tensor.cumsum_", "type": "torch.Tensor", "text": "\nIn-place version of `cumsum()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.data_ptr()", "path": "tensors#torch.Tensor.data_ptr", "type": "torch.Tensor", "text": "\nReturns the address of the first element of `self` tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.deg2rad()", "path": "tensors#torch.Tensor.deg2rad", "type": "torch.Tensor", "text": "\nSee `torch.deg2rad()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.dense_dim()", "path": "sparse#torch.Tensor.dense_dim", "type": "torch.sparse", "text": "\nReturn the number of dense dimensions in a sparse tensor `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nSee also `Tensor.sparse_dim()` and hybrid tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.dequantize()", "path": "tensors#torch.Tensor.dequantize", "type": "torch.Tensor", "text": "\nGiven a quantized Tensor, dequantize it and return the dequantized float\nTensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.det()", "path": "tensors#torch.Tensor.det", "type": "torch.Tensor", "text": "\nSee `torch.det()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.detach()", "path": "autograd#torch.Tensor.detach", "type": "torch.autograd", "text": "\nReturns a new Tensor, detached from the current graph.\n\nThe result will never require gradient.\n\nNote\n\nReturned Tensor shares the same storage with the original one. In-place\nmodifications on either of them will be seen, and may trigger errors in\ncorrectness checks. IMPORTANT NOTE: Previously, in-place size / stride /\nstorage changes (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to\nthe returned tensor also update the original tensor. Now, these in-place\nchanges will not update the original tensor anymore, and will instead trigger\nan error. For sparse tensors: In-place indices / values changes (such as\n`zero_` / `copy_` / `add_`) to the returned tensor will not update the\noriginal tensor anymore, and will instead trigger an error.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.detach_()", "path": "autograd#torch.Tensor.detach_", "type": "torch.autograd", "text": "\nDetaches the Tensor from the graph that created it, making it a leaf. Views\ncannot be detached in-place.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.device", "path": "tensors#torch.Tensor.device", "type": "torch.Tensor", "text": "\nIs the `torch.device` where this Tensor is.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.diag()", "path": "tensors#torch.Tensor.diag", "type": "torch.Tensor", "text": "\nSee `torch.diag()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.diagflat()", "path": "tensors#torch.Tensor.diagflat", "type": "torch.Tensor", "text": "\nSee `torch.diagflat()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.diagonal()", "path": "tensors#torch.Tensor.diagonal", "type": "torch.Tensor", "text": "\nSee `torch.diagonal()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.diag_embed()", "path": "tensors#torch.Tensor.diag_embed", "type": "torch.Tensor", "text": "\nSee `torch.diag_embed()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.diff()", "path": "tensors#torch.Tensor.diff", "type": "torch.Tensor", "text": "\nSee `torch.diff()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.digamma()", "path": "tensors#torch.Tensor.digamma", "type": "torch.Tensor", "text": "\nSee `torch.digamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.digamma_()", "path": "tensors#torch.Tensor.digamma_", "type": "torch.Tensor", "text": "\nIn-place version of `digamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.dim()", "path": "tensors#torch.Tensor.dim", "type": "torch.Tensor", "text": "\nReturns the number of dimensions of `self` tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.dist()", "path": "tensors#torch.Tensor.dist", "type": "torch.Tensor", "text": "\nSee `torch.dist()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.div()", "path": "tensors#torch.Tensor.div", "type": "torch.Tensor", "text": "\nSee `torch.div()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.divide()", "path": "tensors#torch.Tensor.divide", "type": "torch.Tensor", "text": "\nSee `torch.divide()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.divide_()", "path": "tensors#torch.Tensor.divide_", "type": "torch.Tensor", "text": "\nIn-place version of `divide()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.div_()", "path": "tensors#torch.Tensor.div_", "type": "torch.Tensor", "text": "\nIn-place version of `div()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.dot()", "path": "tensors#torch.Tensor.dot", "type": "torch.Tensor", "text": "\nSee `torch.dot()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.double()", "path": "tensors#torch.Tensor.double", "type": "torch.Tensor", "text": "\n`self.double()` is equivalent to `self.to(torch.float64)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.eig()", "path": "tensors#torch.Tensor.eig", "type": "torch.Tensor", "text": "\nSee `torch.eig()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.element_size()", "path": "tensors#torch.Tensor.element_size", "type": "torch.Tensor", "text": "\nReturns the size in bytes of an individual element.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.eq()", "path": "tensors#torch.Tensor.eq", "type": "torch.Tensor", "text": "\nSee `torch.eq()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.equal()", "path": "tensors#torch.Tensor.equal", "type": "torch.Tensor", "text": "\nSee `torch.equal()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.eq_()", "path": "tensors#torch.Tensor.eq_", "type": "torch.Tensor", "text": "\nIn-place version of `eq()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.erf()", "path": "tensors#torch.Tensor.erf", "type": "torch.Tensor", "text": "\nSee `torch.erf()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.erfc()", "path": "tensors#torch.Tensor.erfc", "type": "torch.Tensor", "text": "\nSee `torch.erfc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.erfc_()", "path": "tensors#torch.Tensor.erfc_", "type": "torch.Tensor", "text": "\nIn-place version of `erfc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.erfinv()", "path": "tensors#torch.Tensor.erfinv", "type": "torch.Tensor", "text": "\nSee `torch.erfinv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.erfinv_()", "path": "tensors#torch.Tensor.erfinv_", "type": "torch.Tensor", "text": "\nIn-place version of `erfinv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.erf_()", "path": "tensors#torch.Tensor.erf_", "type": "torch.Tensor", "text": "\nIn-place version of `erf()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.exp()", "path": "tensors#torch.Tensor.exp", "type": "torch.Tensor", "text": "\nSee `torch.exp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.expand()", "path": "tensors#torch.Tensor.expand", "type": "torch.Tensor", "text": "\nReturns a new view of the `self` tensor with singleton dimensions expanded to\na larger size.\n\nPassing -1 as the size for a dimension means not changing the size of that\ndimension.\n\nTensor can be also expanded to a larger number of dimensions, and the new ones\nwill be appended at the front. For the new dimensions, the size cannot be set\nto -1.\n\nExpanding a tensor does not allocate new memory, but only creates a new view\non the existing tensor where a dimension of size one is expanded to a larger\nsize by setting the `stride` to 0. Any dimension of size 1 can be expanded to\nan arbitrary value without allocating new memory.\n\n*sizes (torch.Size or int...) \u2013 the desired expanded size\nWarning\n\nMore than one element of an expanded tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.expand_as()", "path": "tensors#torch.Tensor.expand_as", "type": "torch.Tensor", "text": "\nExpand this tensor to the same size as `other`. `self.expand_as(other)` is\nequivalent to `self.expand(other.size())`.\n\nPlease see `expand()` for more information about `expand`.\n\nother (`torch.Tensor`) \u2013 The result tensor has the same size as `other`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.expm1()", "path": "tensors#torch.Tensor.expm1", "type": "torch.Tensor", "text": "\nSee `torch.expm1()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.expm1_()", "path": "tensors#torch.Tensor.expm1_", "type": "torch.Tensor", "text": "\nIn-place version of `expm1()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.exponential_()", "path": "tensors#torch.Tensor.exponential_", "type": "torch.Tensor", "text": "\nFills `self` tensor with elements drawn from the exponential distribution:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.exp_()", "path": "tensors#torch.Tensor.exp_", "type": "torch.Tensor", "text": "\nIn-place version of `exp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fill_()", "path": "tensors#torch.Tensor.fill_", "type": "torch.Tensor", "text": "\nFills `self` tensor with the specified value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fill_diagonal_()", "path": "tensors#torch.Tensor.fill_diagonal_", "type": "torch.Tensor", "text": "\nFill the main diagonal of a tensor that has at least 2-dimensions. When\ndims>2, all dimensions of input must be of equal length. This function\nmodifies the input tensor in-place, and returns the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fix()", "path": "tensors#torch.Tensor.fix", "type": "torch.Tensor", "text": "\nSee `torch.fix()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fix_()", "path": "tensors#torch.Tensor.fix_", "type": "torch.Tensor", "text": "\nIn-place version of `fix()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.flatten()", "path": "tensors#torch.Tensor.flatten", "type": "torch.Tensor", "text": "\nsee `torch.flatten()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.flip()", "path": "tensors#torch.Tensor.flip", "type": "torch.Tensor", "text": "\nSee `torch.flip()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fliplr()", "path": "tensors#torch.Tensor.fliplr", "type": "torch.Tensor", "text": "\nSee `torch.fliplr()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.flipud()", "path": "tensors#torch.Tensor.flipud", "type": "torch.Tensor", "text": "\nSee `torch.flipud()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.float()", "path": "tensors#torch.Tensor.float", "type": "torch.Tensor", "text": "\n`self.float()` is equivalent to `self.to(torch.float32)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.float_power()", "path": "tensors#torch.Tensor.float_power", "type": "torch.Tensor", "text": "\nSee `torch.float_power()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.float_power_()", "path": "tensors#torch.Tensor.float_power_", "type": "torch.Tensor", "text": "\nIn-place version of `float_power()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.floor()", "path": "tensors#torch.Tensor.floor", "type": "torch.Tensor", "text": "\nSee `torch.floor()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.floor_()", "path": "tensors#torch.Tensor.floor_", "type": "torch.Tensor", "text": "\nIn-place version of `floor()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.floor_divide()", "path": "tensors#torch.Tensor.floor_divide", "type": "torch.Tensor", "text": "\nSee `torch.floor_divide()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.floor_divide_()", "path": "tensors#torch.Tensor.floor_divide_", "type": "torch.Tensor", "text": "\nIn-place version of `floor_divide()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fmax()", "path": "tensors#torch.Tensor.fmax", "type": "torch.Tensor", "text": "\nSee `torch.fmax()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fmin()", "path": "tensors#torch.Tensor.fmin", "type": "torch.Tensor", "text": "\nSee `torch.fmin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fmod()", "path": "tensors#torch.Tensor.fmod", "type": "torch.Tensor", "text": "\nSee `torch.fmod()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.fmod_()", "path": "tensors#torch.Tensor.fmod_", "type": "torch.Tensor", "text": "\nIn-place version of `fmod()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.frac()", "path": "tensors#torch.Tensor.frac", "type": "torch.Tensor", "text": "\nSee `torch.frac()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.frac_()", "path": "tensors#torch.Tensor.frac_", "type": "torch.Tensor", "text": "\nIn-place version of `frac()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.gather()", "path": "tensors#torch.Tensor.gather", "type": "torch.Tensor", "text": "\nSee `torch.gather()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.gcd()", "path": "tensors#torch.Tensor.gcd", "type": "torch.Tensor", "text": "\nSee `torch.gcd()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.gcd_()", "path": "tensors#torch.Tensor.gcd_", "type": "torch.Tensor", "text": "\nIn-place version of `gcd()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ge()", "path": "tensors#torch.Tensor.ge", "type": "torch.Tensor", "text": "\nSee `torch.ge()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.geometric_()", "path": "tensors#torch.Tensor.geometric_", "type": "torch.Tensor", "text": "\nFills `self` tensor with elements drawn from the geometric distribution:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.geqrf()", "path": "tensors#torch.Tensor.geqrf", "type": "torch.Tensor", "text": "\nSee `torch.geqrf()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ger()", "path": "tensors#torch.Tensor.ger", "type": "torch.Tensor", "text": "\nSee `torch.ger()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.get_device()", "path": "tensors#torch.Tensor.get_device", "type": "torch.Tensor", "text": "\nFor CUDA tensors, this function returns the device ordinal of the GPU on which\nthe tensor resides. For CPU tensors, an error is thrown.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ge_()", "path": "tensors#torch.Tensor.ge_", "type": "torch.Tensor", "text": "\nIn-place version of `ge()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.grad", "path": "autograd#torch.Tensor.grad", "type": "torch.autograd", "text": "\nThis attribute is `None` by default and becomes a Tensor the first time a call\nto `backward()` computes gradients for `self`. The attribute will then contain\nthe gradients computed and future calls to `backward()` will accumulate (add)\ngradients into it.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.greater()", "path": "tensors#torch.Tensor.greater", "type": "torch.Tensor", "text": "\nSee `torch.greater()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.greater_()", "path": "tensors#torch.Tensor.greater_", "type": "torch.Tensor", "text": "\nIn-place version of `greater()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.greater_equal()", "path": "tensors#torch.Tensor.greater_equal", "type": "torch.Tensor", "text": "\nSee `torch.greater_equal()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.greater_equal_()", "path": "tensors#torch.Tensor.greater_equal_", "type": "torch.Tensor", "text": "\nIn-place version of `greater_equal()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.gt()", "path": "tensors#torch.Tensor.gt", "type": "torch.Tensor", "text": "\nSee `torch.gt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.gt_()", "path": "tensors#torch.Tensor.gt_", "type": "torch.Tensor", "text": "\nIn-place version of `gt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.half()", "path": "tensors#torch.Tensor.half", "type": "torch.Tensor", "text": "\n`self.half()` is equivalent to `self.to(torch.float16)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.hardshrink()", "path": "tensors#torch.Tensor.hardshrink", "type": "torch.Tensor", "text": "\nSee `torch.nn.functional.hardshrink()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.heaviside()", "path": "tensors#torch.Tensor.heaviside", "type": "torch.Tensor", "text": "\nSee `torch.heaviside()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.histc()", "path": "tensors#torch.Tensor.histc", "type": "torch.Tensor", "text": "\nSee `torch.histc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.hypot()", "path": "tensors#torch.Tensor.hypot", "type": "torch.Tensor", "text": "\nSee `torch.hypot()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.hypot_()", "path": "tensors#torch.Tensor.hypot_", "type": "torch.Tensor", "text": "\nIn-place version of `hypot()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.i0()", "path": "tensors#torch.Tensor.i0", "type": "torch.Tensor", "text": "\nSee `torch.i0()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.i0_()", "path": "tensors#torch.Tensor.i0_", "type": "torch.Tensor", "text": "\nIn-place version of `i0()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.igamma()", "path": "tensors#torch.Tensor.igamma", "type": "torch.Tensor", "text": "\nSee `torch.igamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.igammac()", "path": "tensors#torch.Tensor.igammac", "type": "torch.Tensor", "text": "\nSee `torch.igammac()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.igammac_()", "path": "tensors#torch.Tensor.igammac_", "type": "torch.Tensor", "text": "\nIn-place version of `igammac()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.igamma_()", "path": "tensors#torch.Tensor.igamma_", "type": "torch.Tensor", "text": "\nIn-place version of `igamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.imag", "path": "tensors#torch.Tensor.imag", "type": "torch.Tensor", "text": "\nReturns a new tensor containing imaginary values of the `self` tensor. The\nreturned tensor and `self` share the same underlying storage.\n\nWarning\n\n`imag()` is only supported for tensors with complex dtypes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_add()", "path": "tensors#torch.Tensor.index_add", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.index_add_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_add_()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_add_()", "path": "tensors#torch.Tensor.index_add_", "type": "torch.Tensor", "text": "\nAccumulate the elements of `tensor` into the `self` tensor by adding to the\nindices in the order given in `index`. For example, if `dim == 0` and\n`index[i] == j`, then the `i`th row of `tensor` is added to the `j`th row of\n`self`.\n\nThe `dim`th dimension of `tensor` must have the same size as the length of\n`index` (which must be a vector), and all other dimensions must match `self`,\nor an error will be raised.\n\nNote\n\nThis operation may behave nondeterministically when given tensors on a CUDA\ndevice. See Reproducibility for more information.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_copy()", "path": "tensors#torch.Tensor.index_copy", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.index_copy_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_copy_()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_copy_()", "path": "tensors#torch.Tensor.index_copy_", "type": "torch.Tensor", "text": "\nCopies the elements of `tensor` into the `self` tensor by selecting the\nindices in the order given in `index`. For example, if `dim == 0` and\n`index[i] == j`, then the `i`th row of `tensor` is copied to the `j`th row of\n`self`.\n\nThe `dim`th dimension of `tensor` must have the same size as the length of\n`index` (which must be a vector), and all other dimensions must match `self`,\nor an error will be raised.\n\nNote\n\nIf `index` contains duplicate entries, multiple elements from `tensor` will be\ncopied to the same index of `self`. The result is nondeterministic since it\ndepends on which copy occurs last.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_fill()", "path": "tensors#torch.Tensor.index_fill", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.index_fill_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_fill_()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_fill_()", "path": "tensors#torch.Tensor.index_fill_", "type": "torch.Tensor", "text": "\nFills the elements of the `self` tensor with value `val` by selecting the\nindices in the order given in `index`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_put()", "path": "tensors#torch.Tensor.index_put", "type": "torch.Tensor", "text": "\nOut-place version of `index_put_()`. `tensor1` corresponds to `self` in\n`torch.Tensor.index_put_()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_put_()", "path": "tensors#torch.Tensor.index_put_", "type": "torch.Tensor", "text": "\nPuts values from the tensor `values` into the tensor `self` using the indices\nspecified in `indices` (which is a tuple of Tensors). The expression\n`tensor.index_put_(indices, values)` is equivalent to `tensor[indices] =\nvalues`. Returns `self`.\n\nIf `accumulate` is `True`, the elements in `values` are added to `self`. If\naccumulate is `False`, the behavior is undefined if indices contain duplicate\nelements.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.index_select()", "path": "tensors#torch.Tensor.index_select", "type": "torch.Tensor", "text": "\nSee `torch.index_select()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.indices()", "path": "sparse#torch.Tensor.indices", "type": "torch.sparse", "text": "\nReturn the indices tensor of a sparse COO tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee also `Tensor.values()`.\n\nNote\n\nThis method can only be called on a coalesced sparse tensor. See\n`Tensor.coalesce()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.inner()", "path": "tensors#torch.Tensor.inner", "type": "torch.Tensor", "text": "\nSee `torch.inner()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.int()", "path": "tensors#torch.Tensor.int", "type": "torch.Tensor", "text": "\n`self.int()` is equivalent to `self.to(torch.int32)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.int_repr()", "path": "tensors#torch.Tensor.int_repr", "type": "torch.Tensor", "text": "\nGiven a quantized Tensor, `self.int_repr()` returns a CPU Tensor with uint8_t\nas data type that stores the underlying uint8_t values of the given Tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.inverse()", "path": "tensors#torch.Tensor.inverse", "type": "torch.Tensor", "text": "\nSee `torch.inverse()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isclose()", "path": "tensors#torch.Tensor.isclose", "type": "torch.Tensor", "text": "\nSee `torch.isclose()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isfinite()", "path": "tensors#torch.Tensor.isfinite", "type": "torch.Tensor", "text": "\nSee `torch.isfinite()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isinf()", "path": "tensors#torch.Tensor.isinf", "type": "torch.Tensor", "text": "\nSee `torch.isinf()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isnan()", "path": "tensors#torch.Tensor.isnan", "type": "torch.Tensor", "text": "\nSee `torch.isnan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isneginf()", "path": "tensors#torch.Tensor.isneginf", "type": "torch.Tensor", "text": "\nSee `torch.isneginf()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isposinf()", "path": "tensors#torch.Tensor.isposinf", "type": "torch.Tensor", "text": "\nSee `torch.isposinf()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.isreal()", "path": "tensors#torch.Tensor.isreal", "type": "torch.Tensor", "text": "\nSee `torch.isreal()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.istft()", "path": "tensors#torch.Tensor.istft", "type": "torch.Tensor", "text": "\nSee `torch.istft()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_coalesced()", "path": "sparse#torch.Tensor.is_coalesced", "type": "torch.sparse", "text": "\nReturns `True` if `self` is a sparse COO tensor that is coalesced, `False`\notherwise.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee `coalesce()` and uncoalesced tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_complex()", "path": "tensors#torch.Tensor.is_complex", "type": "torch.Tensor", "text": "\nReturns True if the data type of `self` is a complex data type.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_contiguous()", "path": "tensors#torch.Tensor.is_contiguous", "type": "torch.Tensor", "text": "\nReturns True if `self` tensor is contiguous in memory in the order specified\nby memory format.\n\nmemory_format (`torch.memory_format`, optional) \u2013 Specifies memory allocation\norder. Default: `torch.contiguous_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_cuda", "path": "tensors#torch.Tensor.is_cuda", "type": "torch.Tensor", "text": "\nIs `True` if the Tensor is stored on the GPU, `False` otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_floating_point()", "path": "tensors#torch.Tensor.is_floating_point", "type": "torch.Tensor", "text": "\nReturns True if the data type of `self` is a floating point data type.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_leaf", "path": "autograd#torch.Tensor.is_leaf", "type": "torch.autograd", "text": "\nAll Tensors that have `requires_grad` which is `False` will be leaf Tensors by\nconvention.\n\nFor Tensors that have `requires_grad` which is `True`, they will be leaf\nTensors if they were created by the user. This means that they are not the\nresult of an operation and so `grad_fn` is None.\n\nOnly leaf Tensors will have their `grad` populated during a call to\n`backward()`. To get `grad` populated for non-leaf Tensors, you can use\n`retain_grad()`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_meta", "path": "tensors#torch.Tensor.is_meta", "type": "torch.Tensor", "text": "\nIs `True` if the Tensor is a meta tensor, `False` otherwise. Meta tensors are\nlike normal tensors, but they carry no data.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_pinned()", "path": "tensors#torch.Tensor.is_pinned", "type": "torch.Tensor", "text": "\nReturns true if this tensor resides in pinned memory.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_quantized", "path": "tensors#torch.Tensor.is_quantized", "type": "torch.Tensor", "text": "\nIs `True` if the Tensor is quantized, `False` otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_set_to()", "path": "tensors#torch.Tensor.is_set_to", "type": "torch.Tensor", "text": "\nReturns True if both tensors are pointing to the exact same memory (same\nstorage, offset, size and stride).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_shared()", "path": "tensors#torch.Tensor.is_shared", "type": "torch.Tensor", "text": "\nChecks if tensor is in shared memory.\n\nThis is always `True` for CUDA tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_signed()", "path": "tensors#torch.Tensor.is_signed", "type": "torch.Tensor", "text": "\nReturns True if the data type of `self` is a signed data type.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.is_sparse", "path": "sparse#torch.Tensor.is_sparse", "type": "torch.sparse", "text": "\nIs `True` if the Tensor uses sparse storage layout, `False` otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.item()", "path": "tensors#torch.Tensor.item", "type": "torch.Tensor", "text": "\nReturns the value of this tensor as a standard Python number. This only works\nfor tensors with one element. For other cases, see `tolist()`.\n\nThis operation is not differentiable.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.kthvalue()", "path": "tensors#torch.Tensor.kthvalue", "type": "torch.Tensor", "text": "\nSee `torch.kthvalue()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lcm()", "path": "tensors#torch.Tensor.lcm", "type": "torch.Tensor", "text": "\nSee `torch.lcm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lcm_()", "path": "tensors#torch.Tensor.lcm_", "type": "torch.Tensor", "text": "\nIn-place version of `lcm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ldexp()", "path": "tensors#torch.Tensor.ldexp", "type": "torch.Tensor", "text": "\nSee `torch.ldexp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ldexp_()", "path": "tensors#torch.Tensor.ldexp_", "type": "torch.Tensor", "text": "\nIn-place version of `ldexp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.le()", "path": "tensors#torch.Tensor.le", "type": "torch.Tensor", "text": "\nSee `torch.le()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lerp()", "path": "tensors#torch.Tensor.lerp", "type": "torch.Tensor", "text": "\nSee `torch.lerp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lerp_()", "path": "tensors#torch.Tensor.lerp_", "type": "torch.Tensor", "text": "\nIn-place version of `lerp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.less()", "path": "tensors#torch.Tensor.less", "type": "torch.Tensor", "text": "\nlt(other) -> Tensor\n\nSee `torch.less()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.less_()", "path": "tensors#torch.Tensor.less_", "type": "torch.Tensor", "text": "\nIn-place version of `less()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.less_equal()", "path": "tensors#torch.Tensor.less_equal", "type": "torch.Tensor", "text": "\nSee `torch.less_equal()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.less_equal_()", "path": "tensors#torch.Tensor.less_equal_", "type": "torch.Tensor", "text": "\nIn-place version of `less_equal()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.le_()", "path": "tensors#torch.Tensor.le_", "type": "torch.Tensor", "text": "\nIn-place version of `le()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lgamma()", "path": "tensors#torch.Tensor.lgamma", "type": "torch.Tensor", "text": "\nSee `torch.lgamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lgamma_()", "path": "tensors#torch.Tensor.lgamma_", "type": "torch.Tensor", "text": "\nIn-place version of `lgamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log()", "path": "tensors#torch.Tensor.log", "type": "torch.Tensor", "text": "\nSee `torch.log()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log10()", "path": "tensors#torch.Tensor.log10", "type": "torch.Tensor", "text": "\nSee `torch.log10()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log10_()", "path": "tensors#torch.Tensor.log10_", "type": "torch.Tensor", "text": "\nIn-place version of `log10()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log1p()", "path": "tensors#torch.Tensor.log1p", "type": "torch.Tensor", "text": "\nSee `torch.log1p()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log1p_()", "path": "tensors#torch.Tensor.log1p_", "type": "torch.Tensor", "text": "\nIn-place version of `log1p()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log2()", "path": "tensors#torch.Tensor.log2", "type": "torch.Tensor", "text": "\nSee `torch.log2()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log2_()", "path": "tensors#torch.Tensor.log2_", "type": "torch.Tensor", "text": "\nIn-place version of `log2()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logaddexp()", "path": "tensors#torch.Tensor.logaddexp", "type": "torch.Tensor", "text": "\nSee `torch.logaddexp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logaddexp2()", "path": "tensors#torch.Tensor.logaddexp2", "type": "torch.Tensor", "text": "\nSee `torch.logaddexp2()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logcumsumexp()", "path": "tensors#torch.Tensor.logcumsumexp", "type": "torch.Tensor", "text": "\nSee `torch.logcumsumexp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logdet()", "path": "tensors#torch.Tensor.logdet", "type": "torch.Tensor", "text": "\nSee `torch.logdet()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_and()", "path": "tensors#torch.Tensor.logical_and", "type": "torch.Tensor", "text": "\nSee `torch.logical_and()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_and_()", "path": "tensors#torch.Tensor.logical_and_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_and()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_not()", "path": "tensors#torch.Tensor.logical_not", "type": "torch.Tensor", "text": "\nSee `torch.logical_not()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_not_()", "path": "tensors#torch.Tensor.logical_not_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_not()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_or()", "path": "tensors#torch.Tensor.logical_or", "type": "torch.Tensor", "text": "\nSee `torch.logical_or()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_or_()", "path": "tensors#torch.Tensor.logical_or_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_or()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_xor()", "path": "tensors#torch.Tensor.logical_xor", "type": "torch.Tensor", "text": "\nSee `torch.logical_xor()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logical_xor_()", "path": "tensors#torch.Tensor.logical_xor_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_xor()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logit()", "path": "tensors#torch.Tensor.logit", "type": "torch.Tensor", "text": "\nSee `torch.logit()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logit_()", "path": "tensors#torch.Tensor.logit_", "type": "torch.Tensor", "text": "\nIn-place version of `logit()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.logsumexp()", "path": "tensors#torch.Tensor.logsumexp", "type": "torch.Tensor", "text": "\nSee `torch.logsumexp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log_()", "path": "tensors#torch.Tensor.log_", "type": "torch.Tensor", "text": "\nIn-place version of `log()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.log_normal_()", "path": "tensors#torch.Tensor.log_normal_", "type": "torch.Tensor", "text": "\nFills `self` tensor with numbers samples from the log-normal distribution\nparameterized by the given mean \u03bc\\mu and standard deviation \u03c3\\sigma . Note\nthat `mean` and `std` are the mean and standard deviation of the underlying\nnormal distribution, and not of the returned distribution:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.long()", "path": "tensors#torch.Tensor.long", "type": "torch.Tensor", "text": "\n`self.long()` is equivalent to `self.to(torch.int64)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lstsq()", "path": "tensors#torch.Tensor.lstsq", "type": "torch.Tensor", "text": "\nSee `torch.lstsq()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lt()", "path": "tensors#torch.Tensor.lt", "type": "torch.Tensor", "text": "\nSee `torch.lt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lt_()", "path": "tensors#torch.Tensor.lt_", "type": "torch.Tensor", "text": "\nIn-place version of `lt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lu()", "path": "tensors#torch.Tensor.lu", "type": "torch.Tensor", "text": "\nSee `torch.lu()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.lu_solve()", "path": "tensors#torch.Tensor.lu_solve", "type": "torch.Tensor", "text": "\nSee `torch.lu_solve()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.map_()", "path": "tensors#torch.Tensor.map_", "type": "torch.Tensor", "text": "\nApplies `callable` for each element in `self` tensor and the given `tensor`\nand stores the results in `self` tensor. `self` tensor and the given `tensor`\nmust be broadcastable.\n\nThe `callable` should have the signature:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.masked_fill()", "path": "tensors#torch.Tensor.masked_fill", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.masked_fill_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.masked_fill_()", "path": "tensors#torch.Tensor.masked_fill_", "type": "torch.Tensor", "text": "\nFills elements of `self` tensor with `value` where `mask` is True. The shape\nof `mask` must be broadcastable with the shape of the underlying tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.masked_scatter()", "path": "tensors#torch.Tensor.masked_scatter", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.masked_scatter_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.masked_scatter_()", "path": "tensors#torch.Tensor.masked_scatter_", "type": "torch.Tensor", "text": "\nCopies elements from `source` into `self` tensor at positions where the `mask`\nis True. The shape of `mask` must be broadcastable with the shape of the\nunderlying tensor. The `source` should have at least as many elements as the\nnumber of ones in `mask`\n\nNote\n\nThe `mask` operates on the `self` tensor, not on the given `source` tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.masked_select()", "path": "tensors#torch.Tensor.masked_select", "type": "torch.Tensor", "text": "\nSee `torch.masked_select()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.matmul()", "path": "tensors#torch.Tensor.matmul", "type": "torch.Tensor", "text": "\nSee `torch.matmul()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.matrix_exp()", "path": "tensors#torch.Tensor.matrix_exp", "type": "torch.Tensor", "text": "\nSee `torch.matrix_exp()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.matrix_power()", "path": "tensors#torch.Tensor.matrix_power", "type": "torch.Tensor", "text": "\nSee `torch.matrix_power()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.max()", "path": "tensors#torch.Tensor.max", "type": "torch.Tensor", "text": "\nSee `torch.max()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.maximum()", "path": "tensors#torch.Tensor.maximum", "type": "torch.Tensor", "text": "\nSee `torch.maximum()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mean()", "path": "tensors#torch.Tensor.mean", "type": "torch.Tensor", "text": "\nSee `torch.mean()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.median()", "path": "tensors#torch.Tensor.median", "type": "torch.Tensor", "text": "\nSee `torch.median()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.min()", "path": "tensors#torch.Tensor.min", "type": "torch.Tensor", "text": "\nSee `torch.min()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.minimum()", "path": "tensors#torch.Tensor.minimum", "type": "torch.Tensor", "text": "\nSee `torch.minimum()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mm()", "path": "tensors#torch.Tensor.mm", "type": "torch.Tensor", "text": "\nSee `torch.mm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mode()", "path": "tensors#torch.Tensor.mode", "type": "torch.Tensor", "text": "\nSee `torch.mode()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.moveaxis()", "path": "tensors#torch.Tensor.moveaxis", "type": "torch.Tensor", "text": "\nSee `torch.moveaxis()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.movedim()", "path": "tensors#torch.Tensor.movedim", "type": "torch.Tensor", "text": "\nSee `torch.movedim()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.msort()", "path": "tensors#torch.Tensor.msort", "type": "torch.Tensor", "text": "\nSee `torch.msort()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mul()", "path": "tensors#torch.Tensor.mul", "type": "torch.Tensor", "text": "\nSee `torch.mul()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.multinomial()", "path": "tensors#torch.Tensor.multinomial", "type": "torch.Tensor", "text": "\nSee `torch.multinomial()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.multiply()", "path": "tensors#torch.Tensor.multiply", "type": "torch.Tensor", "text": "\nSee `torch.multiply()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.multiply_()", "path": "tensors#torch.Tensor.multiply_", "type": "torch.Tensor", "text": "\nIn-place version of `multiply()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mul_()", "path": "tensors#torch.Tensor.mul_", "type": "torch.Tensor", "text": "\nIn-place version of `mul()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mv()", "path": "tensors#torch.Tensor.mv", "type": "torch.Tensor", "text": "\nSee `torch.mv()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mvlgamma()", "path": "tensors#torch.Tensor.mvlgamma", "type": "torch.Tensor", "text": "\nSee `torch.mvlgamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.mvlgamma_()", "path": "tensors#torch.Tensor.mvlgamma_", "type": "torch.Tensor", "text": "\nIn-place version of `mvlgamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.names", "path": "named_tensor#torch.Tensor.names", "type": "Named Tensors", "text": "\nStores names for each of this tensor\u2019s dimensions.\n\n`names[idx]` corresponds to the name of tensor dimension `idx`. Names are\neither a string if the dimension is named or `None` if the dimension is\nunnamed.\n\nDimension names may contain characters or underscore. Furthermore, a dimension\nname must be a valid Python variable name (i.e., does not start with\nunderscore).\n\nTensors may not have two named dimensions with the same name.\n\nWarning\n\nThe named tensor API is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nanmedian()", "path": "tensors#torch.Tensor.nanmedian", "type": "torch.Tensor", "text": "\nSee `torch.nanmedian()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nanquantile()", "path": "tensors#torch.Tensor.nanquantile", "type": "torch.Tensor", "text": "\nSee `torch.nanquantile()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nansum()", "path": "tensors#torch.Tensor.nansum", "type": "torch.Tensor", "text": "\nSee `torch.nansum()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nan_to_num()", "path": "tensors#torch.Tensor.nan_to_num", "type": "torch.Tensor", "text": "\nSee `torch.nan_to_num()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nan_to_num_()", "path": "tensors#torch.Tensor.nan_to_num_", "type": "torch.Tensor", "text": "\nIn-place version of `nan_to_num()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.narrow()", "path": "tensors#torch.Tensor.narrow", "type": "torch.Tensor", "text": "\nSee `torch.narrow()`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.narrow_copy()", "path": "tensors#torch.Tensor.narrow_copy", "type": "torch.Tensor", "text": "\nSame as `Tensor.narrow()` except returning a copy rather than shared storage.\nThis is primarily for sparse tensors, which do not have a shared-storage\nnarrow method. Calling ``narrow_copy` with ``dimemsion > self.sparse_dim()``\nwill return a copy with the relevant dense dimension narrowed, and\n``self.shape`` updated accordingly.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ndim", "path": "tensors#torch.Tensor.ndim", "type": "torch.Tensor", "text": "\nAlias for `dim()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ndimension()", "path": "tensors#torch.Tensor.ndimension", "type": "torch.Tensor", "text": "\nAlias for `dim()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ne()", "path": "tensors#torch.Tensor.ne", "type": "torch.Tensor", "text": "\nSee `torch.ne()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.neg()", "path": "tensors#torch.Tensor.neg", "type": "torch.Tensor", "text": "\nSee `torch.neg()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.negative()", "path": "tensors#torch.Tensor.negative", "type": "torch.Tensor", "text": "\nSee `torch.negative()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.negative_()", "path": "tensors#torch.Tensor.negative_", "type": "torch.Tensor", "text": "\nIn-place version of `negative()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.neg_()", "path": "tensors#torch.Tensor.neg_", "type": "torch.Tensor", "text": "\nIn-place version of `neg()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nelement()", "path": "tensors#torch.Tensor.nelement", "type": "torch.Tensor", "text": "\nAlias for `numel()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.new_empty()", "path": "tensors#torch.Tensor.new_empty", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with uninitialized data. By default,\nthe returned Tensor has the same `torch.dtype` and `torch.device` as this\ntensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.new_full()", "path": "tensors#torch.Tensor.new_full", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with `fill_value`. By default, the\nreturned Tensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.new_ones()", "path": "tensors#torch.Tensor.new_ones", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with `1`. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.new_tensor()", "path": "tensors#torch.Tensor.new_tensor", "type": "torch.Tensor", "text": "\nReturns a new Tensor with `data` as the tensor data. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nWarning\n\n`new_tensor()` always copies `data`. If you have a Tensor `data` and want to\navoid a copy, use `torch.Tensor.requires_grad_()` or `torch.Tensor.detach()`.\nIf you have a numpy array and want to avoid a copy, use `torch.from_numpy()`.\n\nWarning\n\nWhen data is a tensor `x`, `new_tensor()` reads out \u2018the data\u2019 from whatever\nit is passed, and constructs a leaf variable. Therefore `tensor.new_tensor(x)`\nis equivalent to `x.clone().detach()` and `tensor.new_tensor(x,\nrequires_grad=True)` is equivalent to\n`x.clone().detach().requires_grad_(True)`. The equivalents using `clone()` and\n`detach()` are recommended.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.new_zeros()", "path": "tensors#torch.Tensor.new_zeros", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with `0`. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nextafter()", "path": "tensors#torch.Tensor.nextafter", "type": "torch.Tensor", "text": "\nSee `torch.nextafter()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nextafter_()", "path": "tensors#torch.Tensor.nextafter_", "type": "torch.Tensor", "text": "\nIn-place version of `nextafter()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ne_()", "path": "tensors#torch.Tensor.ne_", "type": "torch.Tensor", "text": "\nIn-place version of `ne()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.nonzero()", "path": "tensors#torch.Tensor.nonzero", "type": "torch.Tensor", "text": "\nSee `torch.nonzero()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.norm()", "path": "tensors#torch.Tensor.norm", "type": "torch.Tensor", "text": "\nSee `torch.norm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.normal_()", "path": "tensors#torch.Tensor.normal_", "type": "torch.Tensor", "text": "\nFills `self` tensor with elements samples from the normal distribution\nparameterized by `mean` and `std`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.not_equal()", "path": "tensors#torch.Tensor.not_equal", "type": "torch.Tensor", "text": "\nSee `torch.not_equal()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.not_equal_()", "path": "tensors#torch.Tensor.not_equal_", "type": "torch.Tensor", "text": "\nIn-place version of `not_equal()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.numel()", "path": "tensors#torch.Tensor.numel", "type": "torch.Tensor", "text": "\nSee `torch.numel()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.numpy()", "path": "tensors#torch.Tensor.numpy", "type": "torch.Tensor", "text": "\nReturns `self` tensor as a NumPy `ndarray`. This tensor and the returned\n`ndarray` share the same underlying storage. Changes to `self` tensor will be\nreflected in the `ndarray` and vice versa.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.orgqr()", "path": "tensors#torch.Tensor.orgqr", "type": "torch.Tensor", "text": "\nSee `torch.orgqr()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ormqr()", "path": "tensors#torch.Tensor.ormqr", "type": "torch.Tensor", "text": "\nSee `torch.ormqr()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.outer()", "path": "tensors#torch.Tensor.outer", "type": "torch.Tensor", "text": "\nSee `torch.outer()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.permute()", "path": "tensors#torch.Tensor.permute", "type": "torch.Tensor", "text": "\nReturns a view of the original tensor with its dimensions permuted.\n\n*dims (int...) \u2013 The desired ordering of dimensions\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.pinverse()", "path": "tensors#torch.Tensor.pinverse", "type": "torch.Tensor", "text": "\nSee `torch.pinverse()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.pin_memory()", "path": "tensors#torch.Tensor.pin_memory", "type": "torch.Tensor", "text": "\nCopies the tensor to pinned memory, if it\u2019s not already pinned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.polygamma()", "path": "tensors#torch.Tensor.polygamma", "type": "torch.Tensor", "text": "\nSee `torch.polygamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.polygamma_()", "path": "tensors#torch.Tensor.polygamma_", "type": "torch.Tensor", "text": "\nIn-place version of `polygamma()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.pow()", "path": "tensors#torch.Tensor.pow", "type": "torch.Tensor", "text": "\nSee `torch.pow()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.pow_()", "path": "tensors#torch.Tensor.pow_", "type": "torch.Tensor", "text": "\nIn-place version of `pow()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.prod()", "path": "tensors#torch.Tensor.prod", "type": "torch.Tensor", "text": "\nSee `torch.prod()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.put_()", "path": "tensors#torch.Tensor.put_", "type": "torch.Tensor", "text": "\nCopies the elements from `tensor` into the positions specified by indices. For\nthe purpose of indexing, the `self` tensor is treated as if it were a 1-D\ntensor.\n\nIf `accumulate` is `True`, the elements in `tensor` are added to `self`. If\naccumulate is `False`, the behavior is undefined if indices contain duplicate\nelements.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.qr()", "path": "tensors#torch.Tensor.qr", "type": "torch.Tensor", "text": "\nSee `torch.qr()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.qscheme()", "path": "tensors#torch.Tensor.qscheme", "type": "torch.Tensor", "text": "\nReturns the quantization scheme of a given QTensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.quantile()", "path": "tensors#torch.Tensor.quantile", "type": "torch.Tensor", "text": "\nSee `torch.quantile()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.q_per_channel_axis()", "path": "tensors#torch.Tensor.q_per_channel_axis", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\nthe index of dimension on which per-channel quantization is applied.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.q_per_channel_scales()", "path": "tensors#torch.Tensor.q_per_channel_scales", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\na Tensor of scales of the underlying quantizer. It has the number of elements\nthat matches the corresponding dimensions (from q_per_channel_axis) of the\ntensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.q_per_channel_zero_points()", "path": "tensors#torch.Tensor.q_per_channel_zero_points", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\na tensor of zero_points of the underlying quantizer. It has the number of\nelements that matches the corresponding dimensions (from q_per_channel_axis)\nof the tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.q_scale()", "path": "tensors#torch.Tensor.q_scale", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of\nthe underlying quantizer().\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.q_zero_point()", "path": "tensors#torch.Tensor.q_zero_point", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear(affine) quantization, returns the\nzero_point of the underlying quantizer().\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.rad2deg()", "path": "tensors#torch.Tensor.rad2deg", "type": "torch.Tensor", "text": "\nSee `torch.rad2deg()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.random_()", "path": "tensors#torch.Tensor.random_", "type": "torch.Tensor", "text": "\nFills `self` tensor with numbers sampled from the discrete uniform\ndistribution over `[from, to - 1]`. If not specified, the values are usually\nonly bounded by `self` tensor\u2019s data type. However, for floating point types,\nif unspecified, range will be `[0, 2^mantissa]` to ensure that every value is\nrepresentable. For example, `torch.tensor(1, dtype=torch.double).random_()`\nwill be uniform in `[0, 2^53]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.ravel()", "path": "tensors#torch.Tensor.ravel", "type": "torch.Tensor", "text": "\nsee `torch.ravel()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.real", "path": "tensors#torch.Tensor.real", "type": "torch.Tensor", "text": "\nReturns a new tensor containing real values of the `self` tensor. The returned\ntensor and `self` share the same underlying storage.\n\nWarning\n\n`real()` is only supported for tensors with complex dtypes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.reciprocal()", "path": "tensors#torch.Tensor.reciprocal", "type": "torch.Tensor", "text": "\nSee `torch.reciprocal()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.reciprocal_()", "path": "tensors#torch.Tensor.reciprocal_", "type": "torch.Tensor", "text": "\nIn-place version of `reciprocal()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.record_stream()", "path": "tensors#torch.Tensor.record_stream", "type": "torch.Tensor", "text": "\nEnsures that the tensor memory is not reused for another tensor until all\ncurrent work queued on `stream` are complete.\n\nNote\n\nThe caching allocator is aware of only the stream where a tensor was\nallocated. Due to the awareness, it already correctly manages the life cycle\nof tensors on only one stream. But if a tensor is used on a stream different\nfrom the stream of origin, the allocator might reuse the memory unexpectedly.\nCalling this method lets the allocator know which streams have used the\ntensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.refine_names()", "path": "named_tensor#torch.Tensor.refine_names", "type": "Named Tensors", "text": "\nRefines the dimension names of `self` according to `names`.\n\nRefining is a special case of renaming that \u201clifts\u201d unnamed dimensions. A\n`None` dim can be refined to have any name; a named dim can only be refined to\nhave the same name.\n\nBecause named tensors can coexist with unnamed tensors, refining names gives a\nnice way to write named-tensor-aware code that works with both named and\nunnamed tensors.\n\n`names` may contain up to one Ellipsis (`...`). The Ellipsis is expanded\ngreedily; it is expanded in-place to fill `names` to the same length as\n`self.dim()` using names from the corresponding indices of `self.names`.\n\nPython 2 does not support Ellipsis but one may use a string literal instead\n(`'...'`).\n\nnames (iterable of str) \u2013 The desired names of the output tensor. May contain\nup to one Ellipsis.\n\nExamples:\n\nWarning\n\nThe named tensor API is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.register_hook()", "path": "autograd#torch.Tensor.register_hook", "type": "torch.autograd", "text": "\nRegisters a backward hook.\n\nThe hook will be called every time a gradient with respect to the Tensor is\ncomputed. The hook should have the following signature:\n\nThe hook should not modify its argument, but it can optionally return a new\ngradient which will be used in place of `grad`.\n\nThis function returns a handle with a method `handle.remove()` that removes\nthe hook from the module.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.remainder()", "path": "tensors#torch.Tensor.remainder", "type": "torch.Tensor", "text": "\nSee `torch.remainder()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.remainder_()", "path": "tensors#torch.Tensor.remainder_", "type": "torch.Tensor", "text": "\nIn-place version of `remainder()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.rename()", "path": "named_tensor#torch.Tensor.rename", "type": "Named Tensors", "text": "\nRenames dimension names of `self`.\n\nThere are two main usages:\n\n`self.rename(**rename_map)` returns a view on tensor that has dims renamed as\nspecified in the mapping `rename_map`.\n\n`self.rename(*names)` returns a view on tensor, renaming all dimensions\npositionally using `names`. Use `self.rename(None)` to drop names on a tensor.\n\nOne cannot specify both positional args `names` and keyword args `rename_map`.\n\nExamples:\n\nWarning\n\nThe named tensor API is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.rename_()", "path": "named_tensor#torch.Tensor.rename_", "type": "Named Tensors", "text": "\nIn-place version of `rename()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.renorm()", "path": "tensors#torch.Tensor.renorm", "type": "torch.Tensor", "text": "\nSee `torch.renorm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.renorm_()", "path": "tensors#torch.Tensor.renorm_", "type": "torch.Tensor", "text": "\nIn-place version of `renorm()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.repeat()", "path": "tensors#torch.Tensor.repeat", "type": "torch.Tensor", "text": "\nRepeats this tensor along the specified dimensions.\n\nUnlike `expand()`, this function copies the tensor\u2019s data.\n\nWarning\n\n`repeat()` behaves differently from numpy.repeat, but is more similar to\nnumpy.tile. For the operator similar to `numpy.repeat`, see\n`torch.repeat_interleave()`.\n\nsizes (torch.Size or int...) \u2013 The number of times to repeat this tensor along\neach dimension\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.repeat_interleave()", "path": "tensors#torch.Tensor.repeat_interleave", "type": "torch.Tensor", "text": "\nSee `torch.repeat_interleave()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.requires_grad", "path": "autograd#torch.Tensor.requires_grad", "type": "torch.autograd", "text": "\nIs `True` if gradients need to be computed for this Tensor, `False` otherwise.\n\nNote\n\nThe fact that gradients need to be computed for a Tensor do not mean that the\n`grad` attribute will be populated, see `is_leaf` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.requires_grad_()", "path": "tensors#torch.Tensor.requires_grad_", "type": "torch.Tensor", "text": "\nChange if autograd should record operations on this tensor: sets this tensor\u2019s\n`requires_grad` attribute in-place. Returns this tensor.\n\n`requires_grad_()`\u2019s main use case is to tell autograd to begin recording\noperations on a Tensor `tensor`. If `tensor` has `requires_grad=False`\n(because it was obtained through a DataLoader, or required preprocessing or\ninitialization), `tensor.requires_grad_()` makes it so that autograd will\nbegin to record operations on `tensor`.\n\nrequires_grad (bool) \u2013 If autograd should record operations on this tensor.\nDefault: `True`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.reshape()", "path": "tensors#torch.Tensor.reshape", "type": "torch.Tensor", "text": "\nReturns a tensor with the same data and number of elements as `self` but with\nthe specified shape. This method returns a view if `shape` is compatible with\nthe current shape. See `torch.Tensor.view()` on when it is possible to return\na view.\n\nSee `torch.reshape()`\n\nshape (tuple of python:ints or int...) \u2013 the desired shape\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.reshape_as()", "path": "tensors#torch.Tensor.reshape_as", "type": "torch.Tensor", "text": "\nReturns this tensor as the same shape as `other`. `self.reshape_as(other)` is\nequivalent to `self.reshape(other.sizes())`. This method returns a view if\n`other.sizes()` is compatible with the current shape. See\n`torch.Tensor.view()` on when it is possible to return a view.\n\nPlease see `reshape()` for more information about `reshape`.\n\nother (`torch.Tensor`) \u2013 The result tensor has the same shape as `other`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.resize_()", "path": "tensors#torch.Tensor.resize_", "type": "torch.Tensor", "text": "\nResizes `self` tensor to the specified size. If the number of elements is\nlarger than the current storage size, then the underlying storage is resized\nto fit the new number of elements. If the number of elements is smaller, the\nunderlying storage is not changed. Existing elements are preserved but any new\nmemory is uninitialized.\n\nWarning\n\nThis is a low-level method. The storage is reinterpreted as C-contiguous,\nignoring the current strides (unless the target size equals the current size,\nin which case the tensor is left unchanged). For most purposes, you will\ninstead want to use `view()`, which checks for contiguity, or `reshape()`,\nwhich copies data if needed. To change the size in-place with custom strides,\nsee `set_()`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.resize_as_()", "path": "tensors#torch.Tensor.resize_as_", "type": "torch.Tensor", "text": "\nResizes the `self` tensor to be the same size as the specified `tensor`. This\nis equivalent to `self.resize_(tensor.size())`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nTensor. Default: `torch.contiguous_format`. Note that memory format of `self`\nis going to be unaffected if `self.size()` matches `tensor.size()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.retain_grad()", "path": "autograd#torch.Tensor.retain_grad", "type": "torch.autograd", "text": "\nEnables .grad attribute for non-leaf Tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.roll()", "path": "tensors#torch.Tensor.roll", "type": "torch.Tensor", "text": "\nSee `torch.roll()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.rot90()", "path": "tensors#torch.Tensor.rot90", "type": "torch.Tensor", "text": "\nSee `torch.rot90()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.round()", "path": "tensors#torch.Tensor.round", "type": "torch.Tensor", "text": "\nSee `torch.round()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.round_()", "path": "tensors#torch.Tensor.round_", "type": "torch.Tensor", "text": "\nIn-place version of `round()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.rsqrt()", "path": "tensors#torch.Tensor.rsqrt", "type": "torch.Tensor", "text": "\nSee `torch.rsqrt()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.rsqrt_()", "path": "tensors#torch.Tensor.rsqrt_", "type": "torch.Tensor", "text": "\nIn-place version of `rsqrt()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.scatter()", "path": "tensors#torch.Tensor.scatter", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.scatter_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.scatter_()", "path": "tensors#torch.Tensor.scatter_", "type": "torch.Tensor", "text": "\nWrites all values from the tensor `src` into `self` at the indices specified\nin the `index` tensor. For each value in `src`, its output index is specified\nby its index in `src` for `dimension != dim` and by the corresponding value in\n`index` for `dimension = dim`.\n\nFor a 3-D tensor, `self` is updated as:\n\nThis is the reverse operation of the manner described in `gather()`.\n\n`self`, `index` and `src` (if it is a Tensor) should all have the same number\nof dimensions. It is also required that `index.size(d) <= src.size(d)` for all\ndimensions `d`, and that `index.size(d) <= self.size(d)` for all dimensions `d\n!= dim`. Note that `index` and `src` do not broadcast.\n\nMoreover, as for `gather()`, the values of `index` must be between `0` and\n`self.size(dim) - 1` inclusive.\n\nWarning\n\nWhen indices are not unique, the behavior is non-deterministic (one of the\nvalues from `src` will be picked arbitrarily) and the gradient will be\nincorrect (it will be propagated to all locations in the source that\ncorrespond to the same index)!\n\nNote\n\nThe backward pass is implemented only for `src.shape == index.shape`.\n\nAdditionally accepts an optional `reduce` argument that allows specification\nof an optional reduction operation, which is applied to all values in the\ntensor `src` into `self` at the indicies specified in the `index`. For each\nvalue in `src`, the reduction operation is applied to an index in `self` which\nis specified by its index in `src` for `dimension != dim` and by the\ncorresponding value in `index` for `dimension = dim`.\n\nGiven a 3-D tensor and reduction using the multiplication operation, `self` is\nupdated as:\n\nReducing with the addition operation is the same as using `scatter_add_()`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.scatter_add()", "path": "tensors#torch.Tensor.scatter_add", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.scatter_add_()", "path": "tensors#torch.Tensor.scatter_add_", "type": "torch.Tensor", "text": "\nAdds all values from the tensor `other` into `self` at the indices specified\nin the `index` tensor in a similar fashion as `scatter_()`. For each value in\n`src`, it is added to an index in `self` which is specified by its index in\n`src` for `dimension != dim` and by the corresponding value in `index` for\n`dimension = dim`.\n\nFor a 3-D tensor, `self` is updated as:\n\n`self`, `index` and `src` should have same number of dimensions. It is also\nrequired that `index.size(d) <= src.size(d)` for all dimensions `d`, and that\n`index.size(d) <= self.size(d)` for all dimensions `d != dim`. Note that\n`index` and `src` do not broadcast.\n\nNote\n\nThis operation may behave nondeterministically when given tensors on a CUDA\ndevice. See Reproducibility for more information.\n\nNote\n\nThe backward pass is implemented only for `src.shape == index.shape`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.select()", "path": "tensors#torch.Tensor.select", "type": "torch.Tensor", "text": "\nSlices the `self` tensor along the selected dimension at the given index. This\nfunction returns a view of the original tensor with the given dimension\nremoved.\n\nNote\n\n`select()` is equivalent to slicing. For example, `tensor.select(0, index)` is\nequivalent to `tensor[index]` and `tensor.select(2, index)` is equivalent to\n`tensor[:,:,index]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.set_()", "path": "tensors#torch.Tensor.set_", "type": "torch.Tensor", "text": "\nSets the underlying storage, size, and strides. If `source` is a tensor,\n`self` tensor will share the same storage and have the same size and strides\nas `source`. Changes to elements in one tensor will be reflected in the other.\n\nIf `source` is a `Storage`, the method sets the underlying storage, offset,\nsize, and stride.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sgn()", "path": "tensors#torch.Tensor.sgn", "type": "torch.Tensor", "text": "\nSee `torch.sgn()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sgn_()", "path": "tensors#torch.Tensor.sgn_", "type": "torch.Tensor", "text": "\nIn-place version of `sgn()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.share_memory_()", "path": "tensors#torch.Tensor.share_memory_", "type": "torch.Tensor", "text": "\nMoves the underlying storage to shared memory.\n\nThis is a no-op if the underlying storage is already in shared memory and for\nCUDA tensors. Tensors in shared memory cannot be resized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.short()", "path": "tensors#torch.Tensor.short", "type": "torch.Tensor", "text": "\n`self.short()` is equivalent to `self.to(torch.int16)`. See `to()`.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sigmoid()", "path": "tensors#torch.Tensor.sigmoid", "type": "torch.Tensor", "text": "\nSee `torch.sigmoid()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sigmoid_()", "path": "tensors#torch.Tensor.sigmoid_", "type": "torch.Tensor", "text": "\nIn-place version of `sigmoid()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sign()", "path": "tensors#torch.Tensor.sign", "type": "torch.Tensor", "text": "\nSee `torch.sign()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.signbit()", "path": "tensors#torch.Tensor.signbit", "type": "torch.Tensor", "text": "\nSee `torch.signbit()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sign_()", "path": "tensors#torch.Tensor.sign_", "type": "torch.Tensor", "text": "\nIn-place version of `sign()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sin()", "path": "tensors#torch.Tensor.sin", "type": "torch.Tensor", "text": "\nSee `torch.sin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sinc()", "path": "tensors#torch.Tensor.sinc", "type": "torch.Tensor", "text": "\nSee `torch.sinc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sinc_()", "path": "tensors#torch.Tensor.sinc_", "type": "torch.Tensor", "text": "\nIn-place version of `sinc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sinh()", "path": "tensors#torch.Tensor.sinh", "type": "torch.Tensor", "text": "\nSee `torch.sinh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sinh_()", "path": "tensors#torch.Tensor.sinh_", "type": "torch.Tensor", "text": "\nIn-place version of `sinh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sin_()", "path": "tensors#torch.Tensor.sin_", "type": "torch.Tensor", "text": "\nIn-place version of `sin()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.size()", "path": "tensors#torch.Tensor.size", "type": "torch.Tensor", "text": "\nReturns the size of the `self` tensor. The returned value is a subclass of\n`tuple`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.slogdet()", "path": "tensors#torch.Tensor.slogdet", "type": "torch.Tensor", "text": "\nSee `torch.slogdet()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.solve()", "path": "tensors#torch.Tensor.solve", "type": "torch.Tensor", "text": "\nSee `torch.solve()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sort()", "path": "tensors#torch.Tensor.sort", "type": "torch.Tensor", "text": "\nSee `torch.sort()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_dim()", "path": "sparse#torch.Tensor.sparse_dim", "type": "torch.sparse", "text": "\nReturn the number of sparse dimensions in a sparse tensor `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\nSee also `Tensor.dense_dim()` and hybrid tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_mask()", "path": "sparse#torch.Tensor.sparse_mask", "type": "torch.sparse", "text": "\nReturns a new sparse tensor with values from a strided tensor `self` filtered\nby the indices of the sparse tensor `mask`. The values of `mask` sparse tensor\nare ignored. `self` and `mask` tensors must have the same shape.\n\nNote\n\nThe returned sparse tensor has the same indices as the sparse tensor `mask`,\neven when the corresponding values in `self` are zeros.\n\nmask (Tensor) \u2013 a sparse tensor whose indices are used as a filter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_resize_()", "path": "sparse#torch.Tensor.sparse_resize_", "type": "torch.sparse", "text": "\nResizes `self` sparse tensor to the desired size and the number of sparse and\ndense dimensions.\n\nNote\n\nIf the number of specified elements in `self` is zero, then `size`,\n`sparse_dim`, and `dense_dim` can be any size and positive integers such that\n`len(size) == sparse_dim + dense_dim`.\n\nIf `self` specifies one or more elements, however, then each dimension in\n`size` must not be smaller than the corresponding dimension of `self`,\n`sparse_dim` must equal the number of sparse dimensions in `self`, and\n`dense_dim` must equal the number of dense dimensions in `self`.\n\nWarning\n\nThrows an error if `self` is not a sparse tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_resize_and_clear_()", "path": "sparse#torch.Tensor.sparse_resize_and_clear_", "type": "torch.sparse", "text": "\nRemoves all specified elements from a sparse tensor `self` and resizes `self`\nto the desired size and the number of sparse and dense dimensions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.split()", "path": "tensors#torch.Tensor.split", "type": "torch.Tensor", "text": "\nSee `torch.split()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sqrt()", "path": "tensors#torch.Tensor.sqrt", "type": "torch.Tensor", "text": "\nSee `torch.sqrt()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sqrt_()", "path": "tensors#torch.Tensor.sqrt_", "type": "torch.Tensor", "text": "\nIn-place version of `sqrt()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.square()", "path": "tensors#torch.Tensor.square", "type": "torch.Tensor", "text": "\nSee `torch.square()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.square_()", "path": "tensors#torch.Tensor.square_", "type": "torch.Tensor", "text": "\nIn-place version of `square()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.squeeze()", "path": "tensors#torch.Tensor.squeeze", "type": "torch.Tensor", "text": "\nSee `torch.squeeze()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.squeeze_()", "path": "tensors#torch.Tensor.squeeze_", "type": "torch.Tensor", "text": "\nIn-place version of `squeeze()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.std()", "path": "tensors#torch.Tensor.std", "type": "torch.Tensor", "text": "\nSee `torch.std()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.stft()", "path": "tensors#torch.Tensor.stft", "type": "torch.Tensor", "text": "\nSee `torch.stft()`\n\nWarning\n\nThis function changed signature at version 0.4.1. Calling with the previous\nsignature may cause error or return incorrect result.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.storage()", "path": "tensors#torch.Tensor.storage", "type": "torch.Tensor", "text": "\nReturns the underlying storage.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.storage_offset()", "path": "tensors#torch.Tensor.storage_offset", "type": "torch.Tensor", "text": "\nReturns `self` tensor\u2019s offset in the underlying storage in terms of number of\nstorage elements (not bytes).\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.storage_type()", "path": "tensors#torch.Tensor.storage_type", "type": "torch.Tensor", "text": "\nReturns the type of the underlying storage.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.stride()", "path": "tensors#torch.Tensor.stride", "type": "torch.Tensor", "text": "\nReturns the stride of `self` tensor.\n\nStride is the jump necessary to go from one element to the next one in the\nspecified dimension `dim`. A tuple of all strides is returned when no argument\nis passed in. Otherwise, an integer value is returned as the stride in the\nparticular dimension `dim`.\n\ndim (int, optional) \u2013 the desired dimension in which stride is required\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sub()", "path": "tensors#torch.Tensor.sub", "type": "torch.Tensor", "text": "\nSee `torch.sub()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.subtract()", "path": "tensors#torch.Tensor.subtract", "type": "torch.Tensor", "text": "\nSee `torch.subtract()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.subtract_()", "path": "tensors#torch.Tensor.subtract_", "type": "torch.Tensor", "text": "\nIn-place version of `subtract()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sub_()", "path": "tensors#torch.Tensor.sub_", "type": "torch.Tensor", "text": "\nIn-place version of `sub()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sum()", "path": "tensors#torch.Tensor.sum", "type": "torch.Tensor", "text": "\nSee `torch.sum()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.sum_to_size()", "path": "tensors#torch.Tensor.sum_to_size", "type": "torch.Tensor", "text": "\nSum `this` tensor to `size`. `size` must be broadcastable to `this` tensor\nsize.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.svd()", "path": "tensors#torch.Tensor.svd", "type": "torch.Tensor", "text": "\nSee `torch.svd()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.swapaxes()", "path": "tensors#torch.Tensor.swapaxes", "type": "torch.Tensor", "text": "\nSee `torch.swapaxes()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.swapdims()", "path": "tensors#torch.Tensor.swapdims", "type": "torch.Tensor", "text": "\nSee `torch.swapdims()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.symeig()", "path": "tensors#torch.Tensor.symeig", "type": "torch.Tensor", "text": "\nSee `torch.symeig()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.T", "path": "tensors#torch.Tensor.T", "type": "torch.Tensor", "text": "\nIs this Tensor with its dimensions reversed.\n\nIf `n` is the number of dimensions in `x`, `x.T` is equivalent to\n`x.permute(n-1, n-2, ..., 0)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.t()", "path": "tensors#torch.Tensor.t", "type": "torch.Tensor", "text": "\nSee `torch.t()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.take()", "path": "tensors#torch.Tensor.take", "type": "torch.Tensor", "text": "\nSee `torch.take()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tan()", "path": "tensors#torch.Tensor.tan", "type": "torch.Tensor", "text": "\nSee `torch.tan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tanh()", "path": "tensors#torch.Tensor.tanh", "type": "torch.Tensor", "text": "\nSee `torch.tanh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tanh_()", "path": "tensors#torch.Tensor.tanh_", "type": "torch.Tensor", "text": "\nIn-place version of `tanh()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tan_()", "path": "tensors#torch.Tensor.tan_", "type": "torch.Tensor", "text": "\nIn-place version of `tan()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tensor_split()", "path": "tensors#torch.Tensor.tensor_split", "type": "torch.Tensor", "text": "\nSee `torch.tensor_split()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tile()", "path": "tensors#torch.Tensor.tile", "type": "torch.Tensor", "text": "\nSee `torch.tile()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.to()", "path": "tensors#torch.Tensor.to", "type": "torch.Tensor", "text": "\nPerforms Tensor dtype and/or device conversion. A `torch.dtype` and\n`torch.device` are inferred from the arguments of `self.to(*args, **kwargs)`.\n\nNote\n\nIf the `self` Tensor already has the correct `torch.dtype` and `torch.device`,\nthen `self` is returned. Otherwise, the returned tensor is a copy of `self`\nwith the desired `torch.dtype` and `torch.device`.\n\nHere are the ways to call `to`:\n\nReturns a Tensor with the specified `dtype`\n\nmemory_format (`torch.memory_format`, optional): the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nReturns a Tensor with the specified `device` and (optional) `dtype`. If\n`dtype` is `None` it is inferred to be `self.dtype`. When `non_blocking`,\ntries to convert asynchronously with respect to the host if possible, e.g.,\nconverting a CPU Tensor with pinned memory to a CUDA Tensor. When `copy` is\nset, a new Tensor is created even when the Tensor already matches the desired\nconversion.\n\nmemory_format (`torch.memory_format`, optional): the desired memory format of\nreturned Tensor. Default: `torch.preserve_format`.\n\nReturns a Tensor with same `torch.dtype` and `torch.device` as the Tensor\n`other`. When `non_blocking`, tries to convert asynchronously with respect to\nthe host if possible, e.g., converting a CPU Tensor with pinned memory to a\nCUDA Tensor. When `copy` is set, a new Tensor is created even when the Tensor\nalready matches the desired conversion.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tolist()", "path": "tensors#torch.Tensor.tolist", "type": "torch.Tensor", "text": "\nReturns the tensor as a (nested) list. For scalars, a standard Python number\nis returned, just like with `item()`. Tensors are automatically moved to the\nCPU first if necessary.\n\nThis operation is not differentiable.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.topk()", "path": "tensors#torch.Tensor.topk", "type": "torch.Tensor", "text": "\nSee `torch.topk()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.to_dense()", "path": "sparse#torch.Tensor.to_dense", "type": "torch.sparse", "text": "\nCreates a strided copy of `self`.\n\nWarning\n\nThrows an error if `self` is a strided tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.to_mkldnn()", "path": "tensors#torch.Tensor.to_mkldnn", "type": "torch.Tensor", "text": "\nReturns a copy of the tensor in `torch.mkldnn` layout.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.to_sparse()", "path": "sparse#torch.Tensor.to_sparse", "type": "torch.sparse", "text": "\nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in\ncoordinate format.\n\nsparseDims (int, optional) \u2013 the number of sparse dimensions to include in the\nnew sparse tensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.trace()", "path": "tensors#torch.Tensor.trace", "type": "torch.Tensor", "text": "\nSee `torch.trace()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.transpose()", "path": "tensors#torch.Tensor.transpose", "type": "torch.Tensor", "text": "\nSee `torch.transpose()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.transpose_()", "path": "tensors#torch.Tensor.transpose_", "type": "torch.Tensor", "text": "\nIn-place version of `transpose()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.triangular_solve()", "path": "tensors#torch.Tensor.triangular_solve", "type": "torch.Tensor", "text": "\nSee `torch.triangular_solve()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tril()", "path": "tensors#torch.Tensor.tril", "type": "torch.Tensor", "text": "\nSee `torch.tril()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.tril_()", "path": "tensors#torch.Tensor.tril_", "type": "torch.Tensor", "text": "\nIn-place version of `tril()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.triu()", "path": "tensors#torch.Tensor.triu", "type": "torch.Tensor", "text": "\nSee `torch.triu()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.triu_()", "path": "tensors#torch.Tensor.triu_", "type": "torch.Tensor", "text": "\nIn-place version of `triu()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.true_divide()", "path": "tensors#torch.Tensor.true_divide", "type": "torch.Tensor", "text": "\nSee `torch.true_divide()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.true_divide_()", "path": "tensors#torch.Tensor.true_divide_", "type": "torch.Tensor", "text": "\nIn-place version of `true_divide_()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.trunc()", "path": "tensors#torch.Tensor.trunc", "type": "torch.Tensor", "text": "\nSee `torch.trunc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.trunc_()", "path": "tensors#torch.Tensor.trunc_", "type": "torch.Tensor", "text": "\nIn-place version of `trunc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.type()", "path": "tensors#torch.Tensor.type", "type": "torch.Tensor", "text": "\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.type_as()", "path": "tensors#torch.Tensor.type_as", "type": "torch.Tensor", "text": "\nReturns this tensor cast to the type of the given tensor.\n\nThis is a no-op if the tensor is already of the correct type. This is\nequivalent to `self.type(tensor.type())`\n\ntensor (Tensor) \u2013 the tensor which has the desired type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.t_()", "path": "tensors#torch.Tensor.t_", "type": "torch.Tensor", "text": "\nIn-place version of `t()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unbind()", "path": "tensors#torch.Tensor.unbind", "type": "torch.Tensor", "text": "\nSee `torch.unbind()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unflatten()", "path": "named_tensor#torch.Tensor.unflatten", "type": "Named Tensors", "text": "\nExpands the dimension `dim` of the `self` tensor over multiple dimensions of\nsizes given by `sizes`.\n\n[[-1.1819, -0.8899], [ 1.5813, 0.2274]]], names=(\u2018A\u2019, \u2018B1\u2019, \u2018B2\u2019))\n\nWarning\n\nThe named tensor API is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unfold()", "path": "tensors#torch.Tensor.unfold", "type": "torch.Tensor", "text": "\nReturns a view of the original tensor which contains all slices of size `size`\nfrom `self` tensor in the dimension `dimension`.\n\nStep between two slices is given by `step`.\n\nIf `sizedim` is the size of dimension `dimension` for `self`, the size of\ndimension `dimension` in the returned tensor will be `(sizedim - size) / step\n+ 1`.\n\nAn additional dimension of size `size` is appended in the returned tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.uniform_()", "path": "tensors#torch.Tensor.uniform_", "type": "torch.Tensor", "text": "\nFills `self` tensor with numbers sampled from the continuous uniform\ndistribution:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unique()", "path": "tensors#torch.Tensor.unique", "type": "torch.Tensor", "text": "\nReturns the unique elements of the input tensor.\n\nSee `torch.unique()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unique_consecutive()", "path": "tensors#torch.Tensor.unique_consecutive", "type": "torch.Tensor", "text": "\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\nSee `torch.unique_consecutive()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unsqueeze()", "path": "tensors#torch.Tensor.unsqueeze", "type": "torch.Tensor", "text": "\nSee `torch.unsqueeze()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.unsqueeze_()", "path": "tensors#torch.Tensor.unsqueeze_", "type": "torch.Tensor", "text": "\nIn-place version of `unsqueeze()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.values()", "path": "sparse#torch.Tensor.values", "type": "torch.sparse", "text": "\nReturn the values tensor of a sparse COO tensor.\n\nWarning\n\nThrows an error if `self` is not a sparse COO tensor.\n\nSee also `Tensor.indices()`.\n\nNote\n\nThis method can only be called on a coalesced sparse tensor. See\n`Tensor.coalesce()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.var()", "path": "tensors#torch.Tensor.var", "type": "torch.Tensor", "text": "\nSee `torch.var()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.vdot()", "path": "tensors#torch.Tensor.vdot", "type": "torch.Tensor", "text": "\nSee `torch.vdot()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.view()", "path": "tensors#torch.Tensor.view", "type": "torch.Tensor", "text": "\nReturns a new tensor with the same data as the `self` tensor but of a\ndifferent `shape`.\n\nThe returned tensor shares the same data and must have the same number of\nelements, but may have a different size. For a tensor to be viewed, the new\nview size must be compatible with its original size and stride, i.e., each new\nview dimension must either be a subspace of an original dimension, or only\nspan across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+k that satisfy the\nfollowing contiguity-like condition that \u2200i=d,\u2026,d+k\u22121\\forall i = d, \\dots,\nd+k-1 ,\n\nOtherwise, it will not be possible to view `self` tensor as `shape` without\ncopying it (e.g., via `contiguous()`). When it is unclear whether a `view()`\ncan be performed, it is advisable to use `reshape()`, which returns a view if\nthe shapes are compatible, and copies (equivalent to calling `contiguous()`)\notherwise.\n\nshape (torch.Size or int...) \u2013 the desired size\n\nExample:\n\nReturns a new tensor with the same data as the `self` tensor but of a\ndifferent `dtype`. `dtype` must have the same number of bytes per element as\n`self`\u2019s dtype.\n\nWarning\n\nThis overload is not supported by TorchScript, and using it in a Torchscript\nprogram will cause undefined behavior.\n\ndtype (`torch.dtype`) \u2013 the desired dtype\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.view_as()", "path": "tensors#torch.Tensor.view_as", "type": "torch.Tensor", "text": "\nView this tensor as the same size as `other`. `self.view_as(other)` is\nequivalent to `self.view(other.size())`.\n\nPlease see `view()` for more information about `view`.\n\nother (`torch.Tensor`) \u2013 The result tensor has the same size as `other`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.where()", "path": "tensors#torch.Tensor.where", "type": "torch.Tensor", "text": "\n`self.where(condition, y)` is equivalent to `torch.where(condition, self, y)`.\nSee `torch.where()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.xlogy()", "path": "tensors#torch.Tensor.xlogy", "type": "torch.Tensor", "text": "\nSee `torch.xlogy()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.xlogy_()", "path": "tensors#torch.Tensor.xlogy_", "type": "torch.Tensor", "text": "\nIn-place version of `xlogy()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Tensor.zero_()", "path": "tensors#torch.Tensor.zero_", "type": "torch.Tensor", "text": "\nFills `self` tensor with zeros.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tensordot()", "path": "generated/torch.tensordot#torch.tensordot", "type": "torch", "text": "\nReturns a contraction of a and b over multiple dimensions.\n\n`tensordot` implements a generalized matrix product.\n\nWhen called with a non-negative integer argument `dims` = dd , and the number\nof dimensions of `a` and `b` is mm and nn , respectively, `tensordot()`\ncomputes\n\nWhen called with `dims` of the list form, the given dimensions will be\ncontracted in place of the last dd of `a` and the first dd of bb . The sizes\nin these dimensions must match, but `tensordot()` will deal with broadcasted\ndimensions.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tensor_split()", "path": "generated/torch.tensor_split#torch.tensor_split", "type": "torch", "text": "\nSplits a tensor into multiple sub-tensors, all of which are views of `input`,\nalong dimension `dim` according to the indices or number of sections specified\nby `indices_or_sections`. This function is based on NumPy\u2019s\n`numpy.array_split()`.\n\nindices_or_sections (Tensor, int or list or tuple of python:ints) \u2013\n\nIf `indices_or_sections` is an integer `n` or a zero dimensional long tensor\nwith value `n`, `input` is split into `n` sections along dimension `dim`. If\n`input` is divisible by `n` along dimension `dim`, each section will be of\nequal size, `input.size(dim) / n`. If `input` is not divisible by `n`, the\nsizes of the first `int(input.size(dim) % n)` sections will have size\n`int(input.size(dim) / n) + 1`, and the rest will have size\n`int(input.size(dim) / n)`.\n\nIf `indices_or_sections` is a list or tuple of ints, or a one-dimensional long\ntensor, then `input` is split along dimension `dim` at each of the indices in\nthe list, tuple or tensor. For instance, `indices_or_sections=[2, 3]` and\n`dim=0` would result in the tensors `input[:2]`, `input[2:3]`, and\n`input[3:]`.\n\nIf indices_or_sections is a tensor, it must be a zero-dimensional or one-\ndimensional long tensor on the CPU.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tile()", "path": "generated/torch.tile#torch.tile", "type": "torch", "text": "\nConstructs a tensor by repeating the elements of `input`. The `reps` argument\nspecifies the number of repetitions in each dimension.\n\nIf `reps` specifies fewer dimensions than `input` has, then ones are prepended\nto `reps` until all dimensions are specified. For example, if `input` has\nshape (8, 6, 4, 2) and `reps` is (2, 2), then `reps` is treated as (1, 1, 2,\n2).\n\nAnalogously, if `input` has fewer dimensions than `reps` specifies, then\n`input` is treated as if it were unsqueezed at dimension zero until it has as\nmany dimensions as `reps` specifies. For example, if `input` has shape (4, 2)\nand `reps` is (3, 3, 2, 2), then `input` is treated as if it had the shape (1,\n1, 4, 2).\n\nNote\n\nThis function is similar to NumPy\u2019s tile function.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.topk()", "path": "generated/torch.topk#torch.topk", "type": "torch", "text": "\nReturns the `k` largest elements of the given `input` tensor along a given\ndimension.\n\nIf `dim` is not given, the last dimension of the `input` is chosen.\n\nIf `largest` is `False` then the `k` smallest elements are returned.\n\nA namedtuple of `(values, indices)` is returned, where the `indices` are the\nindices of the elements in the original `input` tensor.\n\nThe boolean option `sorted` if `True`, will make sure that the returned `k`\nelements are themselves sorted\n\nout (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.default_generator", "path": "torch#torch.torch.default_generator", "type": "torch", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.device", "path": "tensor_attributes#torch.torch.device", "type": "Tensor Attributes", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.dtype", "path": "tensor_attributes#torch.torch.dtype", "type": "Tensor Attributes", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.finfo", "path": "type_info#torch.torch.finfo", "type": "Type Info", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.iinfo", "path": "type_info#torch.torch.iinfo", "type": "Type Info", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.layout", "path": "tensor_attributes#torch.torch.layout", "type": "Tensor Attributes", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.torch.memory_format", "path": "tensor_attributes#torch.torch.memory_format", "type": "Tensor Attributes", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.trace()", "path": "generated/torch.trace#torch.trace", "type": "torch", "text": "\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.transpose()", "path": "generated/torch.transpose#torch.transpose", "type": "torch", "text": "\nReturns a tensor that is a transposed version of `input`. The given dimensions\n`dim0` and `dim1` are swapped.\n\nThe resulting `out` tensor shares its underlying storage with the `input`\ntensor, so changing the content of one would change the content of the other.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.trapz()", "path": "generated/torch.trapz#torch.trapz", "type": "torch", "text": "\nEstimate \u222bydx\\int y\\,dx along `dim`, using the trapezoid rule.\n\nA Tensor with the same shape as the input, except with `dim` removed. Each\nelement of the returned tensor represents the estimated integral \u222bydx\\int\ny\\,dx along `dim`.\n\nExample:\n\nAs above, but the sample points are spaced uniformly at a distance of `dx`.\n\ny (Tensor) \u2013 The values of the function to integrate\n\nA Tensor with the same shape as the input, except with `dim` removed. Each\nelement of the returned tensor represents the estimated integral \u222bydx\\int\ny\\,dx along `dim`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.triangular_solve()", "path": "generated/torch.triangular_solve#torch.triangular_solve", "type": "torch", "text": "\nSolves a system of equations with a triangular coefficient matrix AA and\nmultiple right-hand sides bb .\n\nIn particular, solves AX=bAX = b and assumes AA is upper-triangular with the\ndefault keyword arguments.\n\n`torch.triangular_solve(b, A)` can take in 2D inputs `b, A` or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns batched\noutputs `X`\n\nSupports real-valued and complex-valued inputs.\n\nA namedtuple `(solution, cloned_coefficient)` where `cloned_coefficient` is a\nclone of AA and `solution` is the solution XX to AX=bAX = b (or whatever\nvariant of the system of equations, depending on the keyword arguments.)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tril()", "path": "generated/torch.tril#torch.tril", "type": "torch", "text": "\nReturns the lower triangular part of the matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\nThe lower triangular part of the matrix is defined as the elements on and\nbelow the diagonal.\n\nThe argument `diagonal` controls which diagonal to consider. If `diagonal` =\n0, all elements on and below the main diagonal are retained. A positive value\nincludes just as many diagonals above the main diagonal, and similarly a\nnegative value excludes just as many diagonals below the main diagonal. The\nmain diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for\ni\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\\\{d_{1}, d_{2}\\\\} - 1] where d1,d2d_{1},\nd_{2} are the dimensions of the matrix.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.tril_indices()", "path": "generated/torch.tril_indices#torch.tril_indices", "type": "torch", "text": "\nReturns the indices of the lower triangular part of a `row`-by- `col` matrix\nin a 2-by-N Tensor, where the first row contains row coordinates of all\nindices and the second row contains column coordinates. Indices are ordered\nbased on rows and then columns.\n\nThe lower triangular part of the matrix is defined as the elements on and\nbelow the diagonal.\n\nThe argument `offset` controls which diagonal to consider. If `offset` = 0,\nall elements on and below the main diagonal are retained. A positive value\nincludes just as many diagonals above the main diagonal, and similarly a\nnegative value excludes just as many diagonals below the main diagonal. The\nmain diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for\ni\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\\\{d_{1}, d_{2}\\\\} - 1] where d1,d2d_{1},\nd_{2} are the dimensions of the matrix.\n\nNote\n\nWhen running on CUDA, `row * col` must be less than 2592^{59} to prevent\noverflow during calculation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.triu()", "path": "generated/torch.triu#torch.triu", "type": "torch", "text": "\nReturns the upper triangular part of a matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\nThe upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.\n\nThe argument `diagonal` controls which diagonal to consider. If `diagonal` =\n0, all elements on and above the main diagonal are retained. A positive value\nexcludes just as many diagonals above the main diagonal, and similarly a\nnegative value includes just as many diagonals below the main diagonal. The\nmain diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for\ni\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\\\{d_{1}, d_{2}\\\\} - 1] where d1,d2d_{1},\nd_{2} are the dimensions of the matrix.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.triu_indices()", "path": "generated/torch.triu_indices#torch.triu_indices", "type": "torch", "text": "\nReturns the indices of the upper triangular part of a `row` by `col` matrix in\na 2-by-N Tensor, where the first row contains row coordinates of all indices\nand the second row contains column coordinates. Indices are ordered based on\nrows and then columns.\n\nThe upper triangular part of the matrix is defined as the elements on and\nabove the diagonal.\n\nThe argument `offset` controls which diagonal to consider. If `offset` = 0,\nall elements on and above the main diagonal are retained. A positive value\nexcludes just as many diagonals above the main diagonal, and similarly a\nnegative value includes just as many diagonals below the main diagonal. The\nmain diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace for\ni\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\\\{d_{1}, d_{2}\\\\} - 1] where d1,d2d_{1},\nd_{2} are the dimensions of the matrix.\n\nNote\n\nWhen running on CUDA, `row * col` must be less than 2592^{59} to prevent\noverflow during calculation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.true_divide()", "path": "generated/torch.true_divide#torch.true_divide", "type": "torch", "text": "\nAlias for `torch.div()` with `rounding_mode=None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.trunc()", "path": "generated/torch.trunc#torch.trunc", "type": "torch", "text": "\nReturns a new tensor with the truncated integer values of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.unbind()", "path": "generated/torch.unbind#torch.unbind", "type": "torch", "text": "\nRemoves a tensor dimension.\n\nReturns a tuple of all slices along a given dimension, already without it.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.unique()", "path": "generated/torch.unique#torch.unique", "type": "torch", "text": "\nReturns the unique elements of the input tensor.\n\nNote\n\nThis function is different from `torch.unique_consecutive()` in the sense that\nthis function also eliminates non-consecutive duplicate values.\n\nNote\n\nCurrently in the CUDA implementation and the CPU implementation when dim is\nspecified, `torch.unique` always sort the tensor at the beginning regardless\nof the `sort` argument. Sorting could be slow, so if your input tensor is\nalready sorted, it is recommended to use `torch.unique_consecutive()` which\navoids the sorting.\n\nA tensor or a tuple of tensors containing\n\n(Tensor, Tensor (optional), Tensor (optional))\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.unique_consecutive()", "path": "generated/torch.unique_consecutive#torch.unique_consecutive", "type": "torch", "text": "\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\nNote\n\nThis function is different from `torch.unique()` in the sense that this\nfunction only eliminates consecutive duplicate values. This semantics is\nsimilar to `std::unique` in C++.\n\nA tensor or a tuple of tensors containing\n\n(Tensor, Tensor (optional), Tensor (optional))\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.unsqueeze()", "path": "generated/torch.unsqueeze#torch.unsqueeze", "type": "torch", "text": "\nReturns a new tensor with a dimension of size one inserted at the specified\nposition.\n\nThe returned tensor shares the same underlying data with this tensor.\n\nA `dim` value within the range `[-input.dim() - 1, input.dim() + 1)` can be\nused. Negative `dim` will correspond to `unsqueeze()` applied at `dim` = `dim\n+ input.dim() + 1`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.use_deterministic_algorithms()", "path": "generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms", "type": "torch", "text": "\nSets whether PyTorch operations must use \u201cdeterministic\u201d algorithms. That is,\nalgorithms which, given the same input, and when run on the same software and\nhardware, always produce the same output. When True, operations will use\ndeterministic algorithms when available, and if only nondeterministic\nalgorithms are available they will throw a :class:RuntimeError when called.\n\nWarning\n\nThis feature is in beta, and its design and implementation may change in the\nfuture.\n\nThe following normally-nondeterministic operations will act deterministically\nwhen `d=True`:\n\nThe following normally-nondeterministic operations will throw a `RuntimeError`\nwhen `d=True`:\n\n`torch.nn.functional.interpolate()` when called on a CUDA tensor that requires\ngrad and one of the following modes is used:\n\nA handful of CUDA operations are nondeterministic if the CUDA version is 10.2\nor greater, unless the environment variable `CUBLAS_WORKSPACE_CONFIG=:4096:8`\nor `CUBLAS_WORKSPACE_CONFIG=:16:8` is set. See the CUDA documentation for more\ndetails:\nhttps://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility If\none of these environment variable configurations is not set, a `RuntimeError`\nwill be raised from these operations when called with CUDA tensors:\n\nNote that deterministic operations tend to have worse performance than non-\ndeterministic operations.\n\nd (`bool`) \u2013 If True, force operations to be deterministic. If False, allow\nnon-deterministic operations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark", "path": "benchmark_utils", "type": "torch.utils.benchmark", "text": "\nHelper class for measuring execution time of PyTorch statements.\n\nFor a full tutorial on how to use this class, see:\nhttps://pytorch.org/tutorials/recipes/recipes/benchmark.html\n\nThe PyTorch Timer is based on `timeit.Timer` (and in fact uses `timeit.Timer`\ninternally), but with several key differences:\n\nTimer will perform warmups (important as some elements of PyTorch are lazily\ninitialized), set threadpool size so that comparisons are apples-to-apples,\nand synchronize asynchronous CUDA functions when necessary.\n\nWhen measuring code, and particularly complex kernels / models, run-to-run\nvariation is a significant confounding factor. It is expected that all\nmeasurements should include replicates to quantify noise and allow median\ncomputation, which is more robust than mean. To that effect, this class\ndeviates from the `timeit` API by conceptually merging `timeit.Timer.repeat`\nand `timeit.Timer.autorange`. (Exact algorithms are discussed in method\ndocstrings.) The `timeit` method is replicated for cases where an adaptive\nstrategy is not desired.\n\nWhen defining a Timer, one can optionally specify `label`, `sub_label`,\n`description`, and `env`. (Defined later) These fields are included in the\nrepresentation of result object and by the `Compare` class to group and\ndisplay results for comparison.\n\nIn addition to wall times, Timer can run a statement under Callgrind and\nreport instructions executed.\n\nDirectly analogous to `timeit.Timer` constructor arguments:\n\n`stmt`, `setup`, `timer`, `globals`\n\nPyTorch Timer specific constructor arguments:\n\n`label`, `sub_label`, `description`, `env`, `num_threads`\n\nsub_label \u2013\n\nProvide supplemental information to disambiguate measurements with identical\nstmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d\nor \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d\n\n\u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using\n`Compare`.\n\ndescription \u2013\n\nString to distinguish measurements with identical label and sub_label. The\nprincipal use of `description` is to signal to `Compare` the columns of data.\nFor instance one might set it based on the input size to create a table of the\nform:\n\nusing `Compare`. It is also included when printing a Measurement.\n\nMeasure many replicates while keeping timer overhead to a minimum.\n\nAt a high level, blocked_autorange executes the following pseudo-code:\n\nNote the variable `block_size` in the inner loop. The choice of block size is\nimportant to measurement quality, and must balance two competing objectives:\n\nblocked_autorange sets block_size by running a warmup period, increasing block\nsize until timer overhead is less than 0.1% of the overall computation. This\nvalue is then used for the main measurement loop.\n\nA `Measurement` object that contains measured runtimes and repetition counts,\nand can be used to compute statistics. (mean, median, etc.)\n\nCollect instruction counts using Callgrind.\n\nUnlike wall times, instruction counts are deterministic (modulo non-\ndeterminism in the program itself and small amounts of jitter from the Python\ninterpreter.) This makes them ideal for detailed performance analysis. This\nmethod runs `stmt` in a separate process so that Valgrind can instrument the\nprogram. Performance is severely degraded due to the instrumentation,\nhowevever this is ameliorated by the fact that a small number of iterations is\ngenerally sufficient to obtain good measurements.\n\nIn order to to use this method `valgrind`, `callgrind_control`, and\n`callgrind_annotate` must be installed.\n\nBecause there is a process boundary between the caller (this process) and the\n`stmt` execution, `globals` cannot contain arbitrary in-memory data\nstructures. (Unlike timing methods) Instead, globals are restricted to\nbuiltins, `nn.Modules`\u2019s, and TorchScripted functions/modules to reduce the\nsurprise factor from serialization and subsequent deserialization. The\n`GlobalsBridge` class provides more detail on this subject. Take particular\ncare with nn.Modules: they rely on pickle and you may need to add an import to\n`setup` for them to transfer properly.\n\nBy default, a profile for an empty statement will be collected and cached to\nindicate how many instructions are from the Python loop which drives `stmt`.\n\nA `CallgrindStats` object which provides instruction counts and some basic\nfacilities for analyzing and manipulating results.\n\nMirrors the semantics of timeit.Timer.timeit().\n\nExecute the main statement (`stmt`) `number` times.\nhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\n\nThe result of a Timer measurement.\n\nThis class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods (including a detailed\n__repr__) for downstream consumers.\n\nConvenience method for merging replicates.\n\nMerge will extrapolate times to `number_per_run=1` and will not transfer any\nmetadata. (Since it might differ between replicates)\n\nApproximate significant figure estimate.\n\nThis property is intended to give a convenient way to estimate the precision\nof a measurement. It only uses the interquartile region to estimate statistics\nto try to mitigate skew from the tails, and uses a static z value of 1.645\nsince it is not expected to be used for small values of `n`, so z can\napproximate `t`.\n\nThe significant figure estimation used in conjunction with the `trim_sigfig`\nmethod to provide a more human interpretable data summary. __repr__ does not\nuse this method; it simply displays raw values. Significant figure estimation\nis intended for `Compare`.\n\nTop level container for Callgrind results collected by Timer.\n\nManipulation is generally done using the FunctionCounts class, which is\nobtained by calling `CallgrindStats.stats(\u2026)`. Several convenience methods are\nprovided as well; the most significant is `CallgrindStats.as_standardized()`.\n\nStrip library names and some prefixes from function strings.\n\nWhen comparing two different sets of instruction counts, on stumbling block\ncan be path prefixes. Callgrind includes the full filepath when reporting a\nfunction (as it should). However, this can cause issues when diffing profiles.\nIf a key component such as Python or PyTorch was built in separate locations\nin the two profiles, which can result in something resembling:\n\nStripping prefixes can ameliorate this issue by regularizing the strings and\ncausing better cancellation of equivilent call sites when diffing.\n\nReturns the total number of instructions executed.\n\nSee `FunctionCounts.denoise()` for an explation of the `denoise` arg.\n\nDiff two sets of counts.\n\nOne common reason to collect instruction counts is to determine the the effect\nthat a particular change will have on the number of instructions needed to\nperform some unit of work. If a change increases that number, the next logical\nquestion is \u201cwhy\u201d. This generally involves looking at what part if the code\nincreased in instruction count. This function automates that process so that\none can easily diff counts on both an inclusive and exclusive basis. The\n`subtract_baselines` argument allows one to disable baseline correction,\nthough in most cases it shouldn\u2019t matter as the baselines are expected to more\nor less cancel out.\n\nReturns detailed function counts.\n\nConceptually, the FunctionCounts returned can be thought of as a tuple of\n(count, path_and_function_name) tuples.\n\n`inclusive` matches the semantics of callgrind. If True, the counts include\ninstructions executed by children. `inclusive=True` is useful for identifying\nhot spots in code; `inclusive=False` is useful for reducing noise when diffing\ncounts from two different runs. (See CallgrindStats.delta(\u2026) for more details)\n\nContainer for manipulating Callgrind results.\n\nRemove known noisy instructions.\n\nSeveral instructions in the CPython interpreter are rather noisy. These\ninstructions involve unicode to dictionary lookups which Python uses to map\nvariable names. FunctionCounts is generally a content agnostic container,\nhowever this is sufficiently important for obtaining reliable results to\nwarrant an exception.\n\nKeep only the elements where `filter_fn` applied to function name returns\nTrue.\n\nApply `map_fn` to all of the function names.\n\nThis can be used to regularize function names (e.g. stripping irrelevant parts\nof the file path), coalesce entries by mapping multiple functions to the same\nname (in which case the counts are added together), etc.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats", "type": "torch.utils.benchmark", "text": "\nTop level container for Callgrind results collected by Timer.\n\nManipulation is generally done using the FunctionCounts class, which is\nobtained by calling `CallgrindStats.stats(\u2026)`. Several convenience methods are\nprovided as well; the most significant is `CallgrindStats.as_standardized()`.\n\nStrip library names and some prefixes from function strings.\n\nWhen comparing two different sets of instruction counts, on stumbling block\ncan be path prefixes. Callgrind includes the full filepath when reporting a\nfunction (as it should). However, this can cause issues when diffing profiles.\nIf a key component such as Python or PyTorch was built in separate locations\nin the two profiles, which can result in something resembling:\n\nStripping prefixes can ameliorate this issue by regularizing the strings and\ncausing better cancellation of equivilent call sites when diffing.\n\nReturns the total number of instructions executed.\n\nSee `FunctionCounts.denoise()` for an explation of the `denoise` arg.\n\nDiff two sets of counts.\n\nOne common reason to collect instruction counts is to determine the the effect\nthat a particular change will have on the number of instructions needed to\nperform some unit of work. If a change increases that number, the next logical\nquestion is \u201cwhy\u201d. This generally involves looking at what part if the code\nincreased in instruction count. This function automates that process so that\none can easily diff counts on both an inclusive and exclusive basis. The\n`subtract_baselines` argument allows one to disable baseline correction,\nthough in most cases it shouldn\u2019t matter as the baselines are expected to more\nor less cancel out.\n\nReturns detailed function counts.\n\nConceptually, the FunctionCounts returned can be thought of as a tuple of\n(count, path_and_function_name) tuples.\n\n`inclusive` matches the semantics of callgrind. If True, the counts include\ninstructions executed by children. `inclusive=True` is useful for identifying\nhot spots in code; `inclusive=False` is useful for reducing noise when diffing\ncounts from two different runs. (See CallgrindStats.delta(\u2026) for more details)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.as_standardized()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.as_standardized", "type": "torch.utils.benchmark", "text": "\nStrip library names and some prefixes from function strings.\n\nWhen comparing two different sets of instruction counts, on stumbling block\ncan be path prefixes. Callgrind includes the full filepath when reporting a\nfunction (as it should). However, this can cause issues when diffing profiles.\nIf a key component such as Python or PyTorch was built in separate locations\nin the two profiles, which can result in something resembling:\n\nStripping prefixes can ameliorate this issue by regularizing the strings and\ncausing better cancellation of equivilent call sites when diffing.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.counts()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.counts", "type": "torch.utils.benchmark", "text": "\nReturns the total number of instructions executed.\n\nSee `FunctionCounts.denoise()` for an explation of the `denoise` arg.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.delta()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.delta", "type": "torch.utils.benchmark", "text": "\nDiff two sets of counts.\n\nOne common reason to collect instruction counts is to determine the the effect\nthat a particular change will have on the number of instructions needed to\nperform some unit of work. If a change increases that number, the next logical\nquestion is \u201cwhy\u201d. This generally involves looking at what part if the code\nincreased in instruction count. This function automates that process so that\none can easily diff counts on both an inclusive and exclusive basis. The\n`subtract_baselines` argument allows one to disable baseline correction,\nthough in most cases it shouldn\u2019t matter as the baselines are expected to more\nor less cancel out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.stats()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.stats", "type": "torch.utils.benchmark", "text": "\nReturns detailed function counts.\n\nConceptually, the FunctionCounts returned can be thought of as a tuple of\n(count, path_and_function_name) tuples.\n\n`inclusive` matches the semantics of callgrind. If True, the counts include\ninstructions executed by children. `inclusive=True` is useful for identifying\nhot spots in code; `inclusive=False` is useful for reducing noise when diffing\ncounts from two different runs. (See CallgrindStats.delta(\u2026) for more details)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts", "type": "torch.utils.benchmark", "text": "\nContainer for manipulating Callgrind results.\n\nRemove known noisy instructions.\n\nSeveral instructions in the CPython interpreter are rather noisy. These\ninstructions involve unicode to dictionary lookups which Python uses to map\nvariable names. FunctionCounts is generally a content agnostic container,\nhowever this is sufficiently important for obtaining reliable results to\nwarrant an exception.\n\nKeep only the elements where `filter_fn` applied to function name returns\nTrue.\n\nApply `map_fn` to all of the function names.\n\nThis can be used to regularize function names (e.g. stripping irrelevant parts\nof the file path), coalesce entries by mapping multiple functions to the same\nname (in which case the counts are added together), etc.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts.denoise()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.denoise", "type": "torch.utils.benchmark", "text": "\nRemove known noisy instructions.\n\nSeveral instructions in the CPython interpreter are rather noisy. These\ninstructions involve unicode to dictionary lookups which Python uses to map\nvariable names. FunctionCounts is generally a content agnostic container,\nhowever this is sufficiently important for obtaining reliable results to\nwarrant an exception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts.filter()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.filter", "type": "torch.utils.benchmark", "text": "\nKeep only the elements where `filter_fn` applied to function name returns\nTrue.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts.transform()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.transform", "type": "torch.utils.benchmark", "text": "\nApply `map_fn` to all of the function names.\n\nThis can be used to regularize function names (e.g. stripping irrelevant parts\nof the file path), coalesce entries by mapping multiple functions to the same\nname (in which case the counts are added together), etc.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Measurement", "path": "benchmark_utils#torch.utils.benchmark.Measurement", "type": "torch.utils.benchmark", "text": "\nThe result of a Timer measurement.\n\nThis class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods (including a detailed\n__repr__) for downstream consumers.\n\nConvenience method for merging replicates.\n\nMerge will extrapolate times to `number_per_run=1` and will not transfer any\nmetadata. (Since it might differ between replicates)\n\nApproximate significant figure estimate.\n\nThis property is intended to give a convenient way to estimate the precision\nof a measurement. It only uses the interquartile region to estimate statistics\nto try to mitigate skew from the tails, and uses a static z value of 1.645\nsince it is not expected to be used for small values of `n`, so z can\napproximate `t`.\n\nThe significant figure estimation used in conjunction with the `trim_sigfig`\nmethod to provide a more human interpretable data summary. __repr__ does not\nuse this method; it simply displays raw values. Significant figure estimation\nis intended for `Compare`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Measurement.merge()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.merge", "type": "torch.utils.benchmark", "text": "\nConvenience method for merging replicates.\n\nMerge will extrapolate times to `number_per_run=1` and will not transfer any\nmetadata. (Since it might differ between replicates)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Measurement.significant_figures()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.significant_figures", "type": "torch.utils.benchmark", "text": "\nApproximate significant figure estimate.\n\nThis property is intended to give a convenient way to estimate the precision\nof a measurement. It only uses the interquartile region to estimate statistics\nto try to mitigate skew from the tails, and uses a static z value of 1.645\nsince it is not expected to be used for small values of `n`, so z can\napproximate `t`.\n\nThe significant figure estimation used in conjunction with the `trim_sigfig`\nmethod to provide a more human interpretable data summary. __repr__ does not\nuse this method; it simply displays raw values. Significant figure estimation\nis intended for `Compare`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer", "path": "benchmark_utils#torch.utils.benchmark.Timer", "type": "torch.utils.benchmark", "text": "\nHelper class for measuring execution time of PyTorch statements.\n\nFor a full tutorial on how to use this class, see:\nhttps://pytorch.org/tutorials/recipes/recipes/benchmark.html\n\nThe PyTorch Timer is based on `timeit.Timer` (and in fact uses `timeit.Timer`\ninternally), but with several key differences:\n\nTimer will perform warmups (important as some elements of PyTorch are lazily\ninitialized), set threadpool size so that comparisons are apples-to-apples,\nand synchronize asynchronous CUDA functions when necessary.\n\nWhen measuring code, and particularly complex kernels / models, run-to-run\nvariation is a significant confounding factor. It is expected that all\nmeasurements should include replicates to quantify noise and allow median\ncomputation, which is more robust than mean. To that effect, this class\ndeviates from the `timeit` API by conceptually merging `timeit.Timer.repeat`\nand `timeit.Timer.autorange`. (Exact algorithms are discussed in method\ndocstrings.) The `timeit` method is replicated for cases where an adaptive\nstrategy is not desired.\n\nWhen defining a Timer, one can optionally specify `label`, `sub_label`,\n`description`, and `env`. (Defined later) These fields are included in the\nrepresentation of result object and by the `Compare` class to group and\ndisplay results for comparison.\n\nIn addition to wall times, Timer can run a statement under Callgrind and\nreport instructions executed.\n\nDirectly analogous to `timeit.Timer` constructor arguments:\n\n`stmt`, `setup`, `timer`, `globals`\n\nPyTorch Timer specific constructor arguments:\n\n`label`, `sub_label`, `description`, `env`, `num_threads`\n\nsub_label \u2013\n\nProvide supplemental information to disambiguate measurements with identical\nstmt or label. For instance, in our example above sub_label might be \u201cfloat\u201d\nor \u201cint\u201d, so that it is easy to differentiate: \u201cReLU(x + 1): (float)\u201d\n\n\u201dReLU(x + 1): (int)\u201d when printing Measurements or summarizing using\n`Compare`.\n\ndescription \u2013\n\nString to distinguish measurements with identical label and sub_label. The\nprincipal use of `description` is to signal to `Compare` the columns of data.\nFor instance one might set it based on the input size to create a table of the\nform:\n\nusing `Compare`. It is also included when printing a Measurement.\n\nMeasure many replicates while keeping timer overhead to a minimum.\n\nAt a high level, blocked_autorange executes the following pseudo-code:\n\nNote the variable `block_size` in the inner loop. The choice of block size is\nimportant to measurement quality, and must balance two competing objectives:\n\nblocked_autorange sets block_size by running a warmup period, increasing block\nsize until timer overhead is less than 0.1% of the overall computation. This\nvalue is then used for the main measurement loop.\n\nA `Measurement` object that contains measured runtimes and repetition counts,\nand can be used to compute statistics. (mean, median, etc.)\n\nCollect instruction counts using Callgrind.\n\nUnlike wall times, instruction counts are deterministic (modulo non-\ndeterminism in the program itself and small amounts of jitter from the Python\ninterpreter.) This makes them ideal for detailed performance analysis. This\nmethod runs `stmt` in a separate process so that Valgrind can instrument the\nprogram. Performance is severely degraded due to the instrumentation,\nhowevever this is ameliorated by the fact that a small number of iterations is\ngenerally sufficient to obtain good measurements.\n\nIn order to to use this method `valgrind`, `callgrind_control`, and\n`callgrind_annotate` must be installed.\n\nBecause there is a process boundary between the caller (this process) and the\n`stmt` execution, `globals` cannot contain arbitrary in-memory data\nstructures. (Unlike timing methods) Instead, globals are restricted to\nbuiltins, `nn.Modules`\u2019s, and TorchScripted functions/modules to reduce the\nsurprise factor from serialization and subsequent deserialization. The\n`GlobalsBridge` class provides more detail on this subject. Take particular\ncare with nn.Modules: they rely on pickle and you may need to add an import to\n`setup` for them to transfer properly.\n\nBy default, a profile for an empty statement will be collected and cached to\nindicate how many instructions are from the Python loop which drives `stmt`.\n\nA `CallgrindStats` object which provides instruction counts and some basic\nfacilities for analyzing and manipulating results.\n\nMirrors the semantics of timeit.Timer.timeit().\n\nExecute the main statement (`stmt`) `number` times.\nhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer.blocked_autorange()", "path": "benchmark_utils#torch.utils.benchmark.Timer.blocked_autorange", "type": "torch.utils.benchmark", "text": "\nMeasure many replicates while keeping timer overhead to a minimum.\n\nAt a high level, blocked_autorange executes the following pseudo-code:\n\nNote the variable `block_size` in the inner loop. The choice of block size is\nimportant to measurement quality, and must balance two competing objectives:\n\nblocked_autorange sets block_size by running a warmup period, increasing block\nsize until timer overhead is less than 0.1% of the overall computation. This\nvalue is then used for the main measurement loop.\n\nA `Measurement` object that contains measured runtimes and repetition counts,\nand can be used to compute statistics. (mean, median, etc.)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer.collect_callgrind()", "path": "benchmark_utils#torch.utils.benchmark.Timer.collect_callgrind", "type": "torch.utils.benchmark", "text": "\nCollect instruction counts using Callgrind.\n\nUnlike wall times, instruction counts are deterministic (modulo non-\ndeterminism in the program itself and small amounts of jitter from the Python\ninterpreter.) This makes them ideal for detailed performance analysis. This\nmethod runs `stmt` in a separate process so that Valgrind can instrument the\nprogram. Performance is severely degraded due to the instrumentation,\nhowevever this is ameliorated by the fact that a small number of iterations is\ngenerally sufficient to obtain good measurements.\n\nIn order to to use this method `valgrind`, `callgrind_control`, and\n`callgrind_annotate` must be installed.\n\nBecause there is a process boundary between the caller (this process) and the\n`stmt` execution, `globals` cannot contain arbitrary in-memory data\nstructures. (Unlike timing methods) Instead, globals are restricted to\nbuiltins, `nn.Modules`\u2019s, and TorchScripted functions/modules to reduce the\nsurprise factor from serialization and subsequent deserialization. The\n`GlobalsBridge` class provides more detail on this subject. Take particular\ncare with nn.Modules: they rely on pickle and you may need to add an import to\n`setup` for them to transfer properly.\n\nBy default, a profile for an empty statement will be collected and cached to\nindicate how many instructions are from the Python loop which drives `stmt`.\n\nA `CallgrindStats` object which provides instruction counts and some basic\nfacilities for analyzing and manipulating results.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer.timeit()", "path": "benchmark_utils#torch.utils.benchmark.Timer.timeit", "type": "torch.utils.benchmark", "text": "\nMirrors the semantics of timeit.Timer.timeit().\n\nExecute the main statement (`stmt`) `number` times.\nhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.bottleneck", "path": "bottleneck", "type": "torch.utils.bottleneck", "text": "\n`torch.utils.bottleneck` is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler.\n\nRun it on the command line with\n\nwhere [args] are any number of arguments to `script.py`, or run `python -m\ntorch.utils.bottleneck -h` for more usage instructions.\n\nWarning\n\nBecause your script will be profiled, please ensure that it exits in a finite\namount of time.\n\nWarning\n\nDue to the asynchronous nature of CUDA kernels, when running against CUDA\ncode, the cProfile output and CPU-mode autograd profilers may not show correct\ntimings: the reported CPU time reports the amount of time used to launch the\nkernels but does not include the time the kernel spent executing on a GPU\nunless the operation does a synchronize. Ops that do synchronize appear to be\nextremely expensive under regular CPU-mode profilers. In these case where\ntimings are incorrect, the CUDA-mode autograd profiler may be helpful.\n\nNote\n\nTo decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look\nat, you should first check if your script is CPU-bound (\u201cCPU total time is\nmuch greater than CUDA total time\u201d). If it is CPU-bound, looking at the\nresults of the CPU-mode autograd profiler will help. If on the other hand your\nscript spends most of its time executing on the GPU, then it makes sense to\nstart looking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.\n\nOf course the reality is much more complicated and your script might not be in\none of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at the\nresult of `torch.autograd.profiler.emit_nvtx()` with `nvprof`. However, please\ntake into account that the NVTX overhead is very high and often gives a\nheavily skewed timeline.\n\nWarning\n\nIf you are profiling CUDA code, the first profiler that `bottleneck` runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in\nits time reporting. This should not matter if your bottlenecks result in code\nmuch slower than the CUDA startup time.\n\nFor more complicated uses of the profilers (like in a multi-GPU case), please\nsee https://docs.python.org/3/library/profile.html or\n`torch.autograd.profiler.profile()` for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.checkpoint", "path": "checkpoint", "type": "torch.utils.checkpoint", "text": "\nNote\n\nCheckpointing is implemented by rerunning a forward-pass segment for each\ncheckpointed segment during backward. This can cause persistent states like\nthe RNG state to be advanced than they would without checkpointing. By\ndefault, checkpointing includes logic to juggle the RNG state such that\ncheckpointed passes making use of RNG (through dropout for example) have\ndeterministic output as compared to non-checkpointed passes. The logic to\nstash and restore RNG states can incur a moderate performance hit depending on\nthe runtime of checkpointed operations. If deterministic output compared to\nnon-checkpointed passes is not required, supply `preserve_rng_state=False` to\n`checkpoint` or `checkpoint_sequential` to omit stashing and restoring the RNG\nstate during each checkpoint.\n\nThe stashing logic saves and restores the RNG state for the current device and\nthe device of all cuda Tensor arguments to the `run_fn`. However, the logic\nhas no way to anticipate if the user will move Tensors to a new device within\nthe `run_fn` itself. Therefore, if you move Tensors to a new device (\u201cnew\u201d\nmeaning not belonging to the set of [current device + devices of Tensor\narguments]) within `run_fn`, deterministic output compared to non-checkpointed\npasses is never guaranteed.\n\nCheckpoint a model or part of the model\n\nCheckpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations, and\ninstead recomputes them in backward pass. It can be applied on any part of a\nmodel.\n\nSpecifically, in the forward pass, `function` will run in `torch.no_grad()`\nmanner, i.e., not storing the intermediate activations. Instead, the forward\npass saves the inputs tuple and the `function` parameter. In the backwards\npass, the saved inputs and `function` is retrieved, and the forward pass is\ncomputed on `function` again, now tracking the intermediate activations, and\nthen the gradients are calculated using these activation values.\n\nWarning\n\nCheckpointing doesn\u2019t work with `torch.autograd.grad()`, but only with\n`torch.autograd.backward()`.\n\nWarning\n\nIf `function` invocation during backward does anything different than the one\nduring forward, e.g., due to some global variable, the checkpointed version\nwon\u2019t be equivalent, and unfortunately it can\u2019t be detected.\n\nWarning\n\nIf checkpointed segment contains tensors detached from the computational graph\nby `detach()` or `torch.no_grad()`, the backward pass will raise an error.\nThis is because `checkpoint` makes all the outputs require gradients which\ncauses issues when a tensor is defined to have no gradient in the model. To\ncircumvent this, detach the tensors outside of the `checkpoint` function.\n\nOutput of running `function` on `*args`\n\nA helper function for checkpointing sequential models.\n\nSequential models execute a list of modules/functions in order (sequentially).\nTherefore, we can divide such a model in various segments and checkpoint each\nsegment. All segments except the last will run in `torch.no_grad()` manner,\ni.e., not storing the intermediate activations. The inputs of each\ncheckpointed segment will be saved for re-running the segment in the backward\npass.\n\nSee `checkpoint()` on how checkpointing works.\n\nWarning\n\nCheckpointing doesn\u2019t work with `torch.autograd.grad()`, but only with\n`torch.autograd.backward()`.\n\nOutput of running `functions` sequentially on `*inputs`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.checkpoint.checkpoint()", "path": "checkpoint#torch.utils.checkpoint.checkpoint", "type": "torch.utils.checkpoint", "text": "\nCheckpoint a model or part of the model\n\nCheckpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations, and\ninstead recomputes them in backward pass. It can be applied on any part of a\nmodel.\n\nSpecifically, in the forward pass, `function` will run in `torch.no_grad()`\nmanner, i.e., not storing the intermediate activations. Instead, the forward\npass saves the inputs tuple and the `function` parameter. In the backwards\npass, the saved inputs and `function` is retrieved, and the forward pass is\ncomputed on `function` again, now tracking the intermediate activations, and\nthen the gradients are calculated using these activation values.\n\nWarning\n\nCheckpointing doesn\u2019t work with `torch.autograd.grad()`, but only with\n`torch.autograd.backward()`.\n\nWarning\n\nIf `function` invocation during backward does anything different than the one\nduring forward, e.g., due to some global variable, the checkpointed version\nwon\u2019t be equivalent, and unfortunately it can\u2019t be detected.\n\nWarning\n\nIf checkpointed segment contains tensors detached from the computational graph\nby `detach()` or `torch.no_grad()`, the backward pass will raise an error.\nThis is because `checkpoint` makes all the outputs require gradients which\ncauses issues when a tensor is defined to have no gradient in the model. To\ncircumvent this, detach the tensors outside of the `checkpoint` function.\n\nOutput of running `function` on `*args`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.checkpoint.checkpoint_sequential()", "path": "checkpoint#torch.utils.checkpoint.checkpoint_sequential", "type": "torch.utils.checkpoint", "text": "\nA helper function for checkpointing sequential models.\n\nSequential models execute a list of modules/functions in order (sequentially).\nTherefore, we can divide such a model in various segments and checkpoint each\nsegment. All segments except the last will run in `torch.no_grad()` manner,\ni.e., not storing the intermediate activations. The inputs of each\ncheckpointed segment will be saved for re-running the segment in the backward\npass.\n\nSee `checkpoint()` on how checkpointing works.\n\nWarning\n\nCheckpointing doesn\u2019t work with `torch.autograd.grad()`, but only with\n`torch.autograd.backward()`.\n\nOutput of running `functions` sequentially on `*inputs`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension", "path": "cpp_extension", "type": "torch.utils.cpp_extension", "text": "\nCreates a `setuptools.Extension` for C++.\n\nConvenience method that creates a `setuptools.Extension` with the bare minimum\n(but often sufficient) arguments to build a C++ extension.\n\nAll arguments are forwarded to the `setuptools.Extension` constructor.\n\nCreates a `setuptools.Extension` for CUDA/C++.\n\nConvenience method that creates a `setuptools.Extension` with the bare minimum\n(but often sufficient) arguments to build a CUDA/C++ extension. This includes\nthe CUDA include path, library path and runtime library.\n\nAll arguments are forwarded to the `setuptools.Extension` constructor.\n\nCompute capabilities:\n\nBy default the extension will be compiled to run on all archs of the cards\nvisible during the building process of the extension, plus PTX. If down the\nroad a new card is installed the extension may need to be recompiled. If a\nvisible card has a compute capability (CC) that\u2019s newer than the newest\nversion for which your nvcc can build fully-compiled binaries, Pytorch will\nmake nvcc fall back to building kernels with the newest version of PTX your\nnvcc does support (see below for details on PTX).\n\nYou can override the default behavior using `TORCH_CUDA_ARCH_LIST` to\nexplicitly specify which CCs you want the extension to support:\n\nTORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python\nbuild_my_extension.py\n\nThe +PTX option causes extension kernel binaries to include PTX instructions\nfor the specified CC. PTX is an intermediate representation that allows\nkernels to runtime-compile for any CC >= the specified CC (for example,\n8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6).\nThis improves your binary\u2019s forward compatibility. However, relying on older\nPTX to provide forward compat by runtime-compiling for newer CCs can modestly\nreduce performance on those newer CCs. If you know exact CC(s) of the GPUs you\nwant to target, you\u2019re always better off specifying them individually. For\nexample, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would\nwork functionally because it includes PTX that can runtime-compile for 8.6,\nbut \u201c8.0 8.6\u201d would be better.\n\nNote that while it\u2019s possible to include all supported archs, the more archs\nget included the slower the building process will be, as it will build a\nseparate kernel image for each arch.\n\nA custom `setuptools` build extension .\n\nThis `setuptools.build_ext` subclass takes care of passing the minimum\nrequired compiler flags (e.g. `-std=c++14`) as well as mixed C++/CUDA\ncompilation (and support for CUDA files in general).\n\nWhen using `BuildExtension`, it is allowed to supply a dictionary for\n`extra_compile_args` (rather than the usual list) that maps from languages\n(`cxx` or `nvcc`) to a list of additional compiler flags to supply to the\ncompiler. This makes it possible to supply different flags to the C++ and CUDA\ncompiler during mixed compilation.\n\n`use_ninja` (bool): If `use_ninja` is `True` (default), then we attempt to\nbuild using the Ninja backend. Ninja greatly speeds up compilation compared to\nthe standard `setuptools.build_ext`. Fallbacks to the standard distutils\nbackend if Ninja is not available.\n\nNote\n\nBy default, the Ninja backend uses #CPUS + 2 workers to build the extension.\nThis may use up too many resources on some systems. One can control the number\nof workers by setting the `MAX_JOBS` environment variable to a non-negative\nnumber.\n\nLoads a PyTorch C++ extension just-in-time (JIT).\n\nTo load an extension, a Ninja build file is emitted, which is used to compile\nthe given sources into a dynamic library. This library is subsequently loaded\ninto the current Python process as a module and returned from this function,\nready for use.\n\nBy default, the directory to which the build file is emitted and the resulting\nlibrary compiled to is `<tmp>/torch_extensions/<name>`, where `<tmp>` is the\ntemporary folder on the current platform and `<name>` the name of the\nextension. This location can be overridden in two ways. First, if the\n`TORCH_EXTENSIONS_DIR` environment variable is set, it replaces\n`<tmp>/torch_extensions` and all extensions will be compiled into subfolders\nof this directory. Second, if the `build_directory` argument to this function\nis supplied, it overrides the entire path, i.e. the library will be compiled\ninto that folder directly.\n\nTo compile the sources, the default system compiler (`c++`) is used, which can\nbe overridden by setting the `CXX` environment variable. To pass additional\narguments to the compilation process, `extra_cflags` or `extra_ldflags` can be\nprovided. For example, to compile your extension with optimizations, pass\n`extra_cflags=['-O3']`. You can also use `extra_cflags` to pass further\ninclude directories.\n\nCUDA support with mixed compilation is provided. Simply pass CUDA source files\n(`.cu` or `.cuh`) along with other sources. Such files will be detected and\ncompiled with nvcc rather than the C++ compiler. This includes passing the\nCUDA lib64 directory as a library directory, and linking `cudart`. You can\npass additional flags to nvcc via `extra_cuda_cflags`, just like with\n`extra_cflags` for C++. Various heuristics for finding the CUDA install\ndirectory are used, which usually work fine. If not, setting the `CUDA_HOME`\nenvironment variable is the safest option.\n\nReturns the loaded PyTorch extension as a Python module.\n\nReturns nothing. (The shared library is loaded into the process as a side\neffect.)\n\nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is added to the\nPATH environment variable as a side effect.)\n\nIf `is_python_module` is `True`\n\nLoads a PyTorch C++ extension just-in-time (JIT) from string sources.\n\nThis function behaves exactly like `load()`, but takes its sources as strings\nrather than filenames. These strings are stored to files in the build\ndirectory, after which the behavior of `load_inline()` is identical to\n`load()`.\n\nSee the tests for good examples of using this function.\n\nSources may omit two required parts of a typical non-inline C++ extension: the\nnecessary header includes, as well as the (pybind11) binding code. More\nprecisely, strings passed to `cpp_sources` are first concatenated into a\nsingle `.cpp` file. This file is then prepended with `#include\n<torch/extension.h>`.\n\nFurthermore, if the `functions` argument is supplied, bindings will be\nautomatically generated for each function specified. `functions` can either be\na list of function names, or a dictionary mapping from function names to\ndocstrings. If a list is given, the name of each function is used as its\ndocstring.\n\nThe sources in `cuda_sources` are concatenated into a separate `.cu` file and\nprepended with `torch/types.h`, `cuda.h` and `cuda_runtime.h` includes. The\n`.cpp` and `.cu` files are compiled separately, but ultimately linked into a\nsingle library. Note that no bindings are generated for functions in\n`cuda_sources` per se. To bind to a CUDA kernel, you must create a C++\nfunction that calls it, and either declare or define this C++ function in one\nof the `cpp_sources` (and include its name in `functions`).\n\nSee `load()` for a description of arguments omitted below.\n\nNote\n\nBy default, the Ninja backend uses #CPUS + 2 workers to build the extension.\nThis may use up too many resources on some systems. One can control the number\nof workers by setting the `MAX_JOBS` environment variable to a non-negative\nnumber.\n\nGet the include paths required to build a C++ or CUDA extension.\n\ncuda \u2013 If `True`, includes CUDA-specific include paths.\n\nA list of include path strings.\n\nVerifies that the given compiler is ABI-compatible with PyTorch.\n\ncompiler (str) \u2013 The compiler executable name to check (e.g. `g++`). Must be\nexecutable in a shell process.\n\nFalse if the compiler is (likely) ABI-incompatible with PyTorch, else True.\n\nRaises `RuntimeError` if ninja build system is not available on the system,\ndoes nothing otherwise.\n\nReturns `True` if the ninja build system is available on the system, `False`\notherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.BuildExtension()", "path": "cpp_extension#torch.utils.cpp_extension.BuildExtension", "type": "torch.utils.cpp_extension", "text": "\nA custom `setuptools` build extension .\n\nThis `setuptools.build_ext` subclass takes care of passing the minimum\nrequired compiler flags (e.g. `-std=c++14`) as well as mixed C++/CUDA\ncompilation (and support for CUDA files in general).\n\nWhen using `BuildExtension`, it is allowed to supply a dictionary for\n`extra_compile_args` (rather than the usual list) that maps from languages\n(`cxx` or `nvcc`) to a list of additional compiler flags to supply to the\ncompiler. This makes it possible to supply different flags to the C++ and CUDA\ncompiler during mixed compilation.\n\n`use_ninja` (bool): If `use_ninja` is `True` (default), then we attempt to\nbuild using the Ninja backend. Ninja greatly speeds up compilation compared to\nthe standard `setuptools.build_ext`. Fallbacks to the standard distutils\nbackend if Ninja is not available.\n\nNote\n\nBy default, the Ninja backend uses #CPUS + 2 workers to build the extension.\nThis may use up too many resources on some systems. One can control the number\nof workers by setting the `MAX_JOBS` environment variable to a non-negative\nnumber.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.check_compiler_abi_compatibility()", "path": "cpp_extension#torch.utils.cpp_extension.check_compiler_abi_compatibility", "type": "torch.utils.cpp_extension", "text": "\nVerifies that the given compiler is ABI-compatible with PyTorch.\n\ncompiler (str) \u2013 The compiler executable name to check (e.g. `g++`). Must be\nexecutable in a shell process.\n\nFalse if the compiler is (likely) ABI-incompatible with PyTorch, else True.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.CppExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CppExtension", "type": "torch.utils.cpp_extension", "text": "\nCreates a `setuptools.Extension` for C++.\n\nConvenience method that creates a `setuptools.Extension` with the bare minimum\n(but often sufficient) arguments to build a C++ extension.\n\nAll arguments are forwarded to the `setuptools.Extension` constructor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.CUDAExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CUDAExtension", "type": "torch.utils.cpp_extension", "text": "\nCreates a `setuptools.Extension` for CUDA/C++.\n\nConvenience method that creates a `setuptools.Extension` with the bare minimum\n(but often sufficient) arguments to build a CUDA/C++ extension. This includes\nthe CUDA include path, library path and runtime library.\n\nAll arguments are forwarded to the `setuptools.Extension` constructor.\n\nCompute capabilities:\n\nBy default the extension will be compiled to run on all archs of the cards\nvisible during the building process of the extension, plus PTX. If down the\nroad a new card is installed the extension may need to be recompiled. If a\nvisible card has a compute capability (CC) that\u2019s newer than the newest\nversion for which your nvcc can build fully-compiled binaries, Pytorch will\nmake nvcc fall back to building kernels with the newest version of PTX your\nnvcc does support (see below for details on PTX).\n\nYou can override the default behavior using `TORCH_CUDA_ARCH_LIST` to\nexplicitly specify which CCs you want the extension to support:\n\nTORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python\nbuild_my_extension.py\n\nThe +PTX option causes extension kernel binaries to include PTX instructions\nfor the specified CC. PTX is an intermediate representation that allows\nkernels to runtime-compile for any CC >= the specified CC (for example,\n8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6).\nThis improves your binary\u2019s forward compatibility. However, relying on older\nPTX to provide forward compat by runtime-compiling for newer CCs can modestly\nreduce performance on those newer CCs. If you know exact CC(s) of the GPUs you\nwant to target, you\u2019re always better off specifying them individually. For\nexample, if you want your extension to run on 8.0 and 8.6, \u201c8.0+PTX\u201d would\nwork functionally because it includes PTX that can runtime-compile for 8.6,\nbut \u201c8.0 8.6\u201d would be better.\n\nNote that while it\u2019s possible to include all supported archs, the more archs\nget included the slower the building process will be, as it will build a\nseparate kernel image for each arch.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.include_paths()", "path": "cpp_extension#torch.utils.cpp_extension.include_paths", "type": "torch.utils.cpp_extension", "text": "\nGet the include paths required to build a C++ or CUDA extension.\n\ncuda \u2013 If `True`, includes CUDA-specific include paths.\n\nA list of include path strings.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.is_ninja_available()", "path": "cpp_extension#torch.utils.cpp_extension.is_ninja_available", "type": "torch.utils.cpp_extension", "text": "\nReturns `True` if the ninja build system is available on the system, `False`\notherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.load()", "path": "cpp_extension#torch.utils.cpp_extension.load", "type": "torch.utils.cpp_extension", "text": "\nLoads a PyTorch C++ extension just-in-time (JIT).\n\nTo load an extension, a Ninja build file is emitted, which is used to compile\nthe given sources into a dynamic library. This library is subsequently loaded\ninto the current Python process as a module and returned from this function,\nready for use.\n\nBy default, the directory to which the build file is emitted and the resulting\nlibrary compiled to is `<tmp>/torch_extensions/<name>`, where `<tmp>` is the\ntemporary folder on the current platform and `<name>` the name of the\nextension. This location can be overridden in two ways. First, if the\n`TORCH_EXTENSIONS_DIR` environment variable is set, it replaces\n`<tmp>/torch_extensions` and all extensions will be compiled into subfolders\nof this directory. Second, if the `build_directory` argument to this function\nis supplied, it overrides the entire path, i.e. the library will be compiled\ninto that folder directly.\n\nTo compile the sources, the default system compiler (`c++`) is used, which can\nbe overridden by setting the `CXX` environment variable. To pass additional\narguments to the compilation process, `extra_cflags` or `extra_ldflags` can be\nprovided. For example, to compile your extension with optimizations, pass\n`extra_cflags=['-O3']`. You can also use `extra_cflags` to pass further\ninclude directories.\n\nCUDA support with mixed compilation is provided. Simply pass CUDA source files\n(`.cu` or `.cuh`) along with other sources. Such files will be detected and\ncompiled with nvcc rather than the C++ compiler. This includes passing the\nCUDA lib64 directory as a library directory, and linking `cudart`. You can\npass additional flags to nvcc via `extra_cuda_cflags`, just like with\n`extra_cflags` for C++. Various heuristics for finding the CUDA install\ndirectory are used, which usually work fine. If not, setting the `CUDA_HOME`\nenvironment variable is the safest option.\n\nReturns the loaded PyTorch extension as a Python module.\n\nReturns nothing. (The shared library is loaded into the process as a side\neffect.)\n\nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is added to the\nPATH environment variable as a side effect.)\n\nIf `is_python_module` is `True`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.load_inline()", "path": "cpp_extension#torch.utils.cpp_extension.load_inline", "type": "torch.utils.cpp_extension", "text": "\nLoads a PyTorch C++ extension just-in-time (JIT) from string sources.\n\nThis function behaves exactly like `load()`, but takes its sources as strings\nrather than filenames. These strings are stored to files in the build\ndirectory, after which the behavior of `load_inline()` is identical to\n`load()`.\n\nSee the tests for good examples of using this function.\n\nSources may omit two required parts of a typical non-inline C++ extension: the\nnecessary header includes, as well as the (pybind11) binding code. More\nprecisely, strings passed to `cpp_sources` are first concatenated into a\nsingle `.cpp` file. This file is then prepended with `#include\n<torch/extension.h>`.\n\nFurthermore, if the `functions` argument is supplied, bindings will be\nautomatically generated for each function specified. `functions` can either be\na list of function names, or a dictionary mapping from function names to\ndocstrings. If a list is given, the name of each function is used as its\ndocstring.\n\nThe sources in `cuda_sources` are concatenated into a separate `.cu` file and\nprepended with `torch/types.h`, `cuda.h` and `cuda_runtime.h` includes. The\n`.cpp` and `.cu` files are compiled separately, but ultimately linked into a\nsingle library. Note that no bindings are generated for functions in\n`cuda_sources` per se. To bind to a CUDA kernel, you must create a C++\nfunction that calls it, and either declare or define this C++ function in one\nof the `cpp_sources` (and include its name in `functions`).\n\nSee `load()` for a description of arguments omitted below.\n\nNote\n\nBy default, the Ninja backend uses #CPUS + 2 workers to build the extension.\nThis may use up too many resources on some systems. One can control the number\nof workers by setting the `MAX_JOBS` environment variable to a non-negative\nnumber.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.verify_ninja_availability()", "path": "cpp_extension#torch.utils.cpp_extension.verify_ninja_availability", "type": "torch.utils.cpp_extension", "text": "\nRaises `RuntimeError` if ninja build system is not available on the system,\ndoes nothing otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data", "path": "data", "type": "torch.utils.data", "text": "\nAt the heart of PyTorch data loading utility is the\n`torch.utils.data.DataLoader` class. It represents a Python iterable over a\ndataset, with support for\n\nThese options are configured by the constructor arguments of a `DataLoader`,\nwhich has signature:\n\nThe sections below describe in details the effects and usages of these\noptions.\n\nThe most important argument of `DataLoader` constructor is `dataset`, which\nindicates a dataset object to load data from. PyTorch supports two different\ntypes of datasets:\n\nA map-style dataset is one that implements the `__getitem__()` and `__len__()`\nprotocols, and represents a map from (possibly non-integral) indices/keys to\ndata samples.\n\nFor example, such a dataset, when accessed with `dataset[idx]`, could read the\n`idx`-th image and its corresponding label from a folder on the disk.\n\nSee `Dataset` for more details.\n\nAn iterable-style dataset is an instance of a subclass of `IterableDataset`\nthat implements the `__iter__()` protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size\ndepends on the fetched data.\n\nFor example, such a dataset, when called `iter(dataset)`, could return a\nstream of data reading from a database, a remote server, or even logs\ngenerated in real time.\n\nSee `IterableDataset` for more details.\n\nNote\n\nWhen using an `IterableDataset` with multi-process data loading. The same\ndataset object is replicated on each worker process, and thus the replicas\nmust be configured differently to avoid duplicated data. See `IterableDataset`\ndocumentations for how to achieve this.\n\nFor iterable-style datasets, data loading order is entirely controlled by the\nuser-defined iterable. This allows easier implementations of chunk-reading and\ndynamic batch size (e.g., by yielding a batched sample at each time).\n\nThe rest of this section concerns the case with map-style datasets.\n`torch.utils.data.Sampler` classes are used to specify the sequence of\nindices/keys used in data loading. They represent iterable objects over the\nindices to datasets. E.g., in the common case with stochastic gradient decent\n(SGD), a `Sampler` could randomly permute a list of indices and yield each one\nat a time, or yield a small number of them for mini-batch SGD.\n\nA sequential or shuffled sampler will be automatically constructed based on\nthe `shuffle` argument to a `DataLoader`. Alternatively, users may use the\n`sampler` argument to specify a custom `Sampler` object that at each time\nyields the next index/key to fetch.\n\nA custom `Sampler` that yields a list of batch indices at a time can be passed\nas the `batch_sampler` argument. Automatic batching can also be enabled via\n`batch_size` and `drop_last` arguments. See the next section for more details\non this.\n\nNote\n\nNeither `sampler` nor `batch_sampler` is compatible with iterable-style\ndatasets, since such datasets have no notion of a key or an index.\n\n`DataLoader` supports automatically collating individual fetched data samples\ninto batches via arguments `batch_size`, `drop_last`, and `batch_sampler`.\n\nThis is the most common case, and corresponds to fetching a minibatch of data\nand collating them into batched samples, i.e., containing Tensors with one\ndimension being the batch dimension (usually the first).\n\nWhen `batch_size` (default `1`) is not `None`, the data loader yields batched\nsamples instead of individual samples. `batch_size` and `drop_last` arguments\nare used to specify how the data loader obtains batches of dataset keys. For\nmap-style datasets, users can alternatively specify `batch_sampler`, which\nyields a list of keys at a time.\n\nNote\n\nThe `batch_size` and `drop_last` arguments essentially are used to construct a\n`batch_sampler` from `sampler`. For map-style datasets, the `sampler` is\neither provided by user or constructed based on the `shuffle` argument. For\niterable-style datasets, the `sampler` is a dummy infinite one. See this\nsection on more details on samplers.\n\nNote\n\nWhen fetching from iterable-style datasets with multi-processing, the\n`drop_last` argument drops the last non-full batch of each worker\u2019s dataset\nreplica.\n\nAfter fetching a list of samples using the indices from sampler, the function\npassed as the `collate_fn` argument is used to collate lists of samples into\nbatches.\n\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\nA custom `collate_fn` can be used to customize collation, e.g., padding\nsequential data to max length of a batch. See this section on more about\n`collate_fn`.\n\nIn certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to\ndirectly load batched data (e.g., bulk reads from a database or reading\ncontinuous chunks of memory), or the batch size is data dependent, or the\nprogram is designed to work on individual samples. Under these scenarios, it\u2019s\nlikely better to not use automatic batching (where `collate_fn` is used to\ncollate the samples), but let the data loader directly return each member of\nthe `dataset` object.\n\nWhen both `batch_size` and `batch_sampler` are `None` (default value for\n`batch_sampler` is already `None`), automatic batching is disabled. Each\nsample obtained from the `dataset` is processed with the function passed as\nthe `collate_fn` argument.\n\nWhen automatic batching is disabled, the default `collate_fn` simply converts\nNumPy arrays into PyTorch Tensors, and keeps everything else untouched.\n\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\nSee this section on more about `collate_fn`.\n\nThe use of `collate_fn` is slightly different when automatic batching is\nenabled or disabled.\n\nWhen automatic batching is disabled, `collate_fn` is called with each\nindividual data sample, and the output is yielded from the data loader\niterator. In this case, the default `collate_fn` simply converts NumPy arrays\nin PyTorch tensors.\n\nWhen automatic batching is enabled, `collate_fn` is called with a list of data\nsamples at each time. It is expected to collate the input samples into a batch\nfor yielding from the data loader iterator. The rest of this section describes\nbehavior of the default `collate_fn` in this case.\n\nFor instance, if each data sample consists of a 3-channel image and an\nintegral class label, i.e., each element of the dataset returns a tuple\n`(image, class_index)`, the default `collate_fn` collates a list of such\ntuples into a single tuple of a batched image tensor and a batched class label\nTensor. In particular, the default `collate_fn` has the following properties:\n\nUsers may use customized `collate_fn` to achieve custom batching, e.g.,\ncollating along a dimension other than the first, padding sequences of various\nlengths, or adding support for custom data types.\n\nA `DataLoader` uses single-process data loading by default.\n\nWithin a Python process, the Global Interpreter Lock (GIL) prevents true fully\nparallelizing Python code across threads. To avoid blocking computation code\nwith data loading, PyTorch provides an easy switch to perform multi-process\ndata loading by simply setting the argument `num_workers` to a positive\ninteger.\n\nIn this mode, data fetching is done in the same process a `DataLoader` is\ninitialized. Therefore, data loading may block computing. However, this mode\nmay be preferred when resource(s) used for sharing data among processes (e.g.,\nshared memory, file descriptors) is limited, or when the entire dataset is\nsmall and can be loaded entirely in memory. Additionally, single-process\nloading often shows more readable error traces and thus is useful for\ndebugging.\n\nSetting the argument `num_workers` as a positive integer will turn on multi-\nprocess data loading with the specified number of loader worker processes.\n\nIn this mode, each time an iterator of a `DataLoader` is created (e.g., when\nyou call `enumerate(dataloader)`), `num_workers` worker processes are created.\nAt this point, the `dataset`, `collate_fn`, and `worker_init_fn` are passed to\neach worker, where they are used to initialize, and fetch data. This means\nthat dataset access together with its internal IO, transforms (including\n`collate_fn`) runs in the worker process.\n\n`torch.utils.data.get_worker_info()` returns various useful information in a\nworker process (including the worker id, dataset replica, initial seed, etc.),\nand returns `None` in main process. Users may use this function in dataset\ncode and/or `worker_init_fn` to individually configure each dataset replica,\nand to determine whether the code is running in a worker process. For example,\nthis can be particularly helpful in sharding the dataset.\n\nFor map-style datasets, the main process generates the indices using `sampler`\nand sends them to the workers. So any shuffle randomization is done in the\nmain process which guides loading by assigning indices to load.\n\nFor iterable-style datasets, since each worker process gets a replica of the\n`dataset` object, naive multi-process loading will often result in duplicated\ndata. Using `torch.utils.data.get_worker_info()` and/or `worker_init_fn`,\nusers may configure each replica independently. (See `IterableDataset`\ndocumentations for how to achieve this. ) For similar reasons, in multi-\nprocess loading, the `drop_last` argument drops the last non-full batch of\neach worker\u2019s iterable-style dataset replica.\n\nWorkers are shut down once the end of the iteration is reached, or when the\niterator becomes garbage collected.\n\nWarning\n\nIt is generally not recommended to return CUDA tensors in multi-process\nloading because of many subtleties in using CUDA and sharing CUDA tensors in\nmultiprocessing (see CUDA in multiprocessing). Instead, we recommend using\nautomatic memory pinning (i.e., setting `pin_memory=True`), which enables fast\ndata transfer to CUDA-enabled GPUs.\n\nSince workers rely on Python `multiprocessing`, worker launch behavior is\ndifferent on Windows compared to Unix.\n\nThis separate serialization means that you should take two steps to ensure you\nare compatible with Windows while using multi-process data loading:\n\nBy default, each worker will have its PyTorch seed set to `base_seed +\nworker_id`, where `base_seed` is a long generated by main process using its\nRNG (thereby, consuming a RNG state mandatorily). However, seeds for other\nlibraries may be duplicated upon initializing workers (e.g., NumPy), causing\neach worker to return identical random numbers. (See this section in FAQ.).\n\nIn `worker_init_fn`, you may access the PyTorch seed set for each worker with\neither `torch.utils.data.get_worker_info().seed` or `torch.initial_seed()`,\nand use it to seed other libraries before data loading.\n\nHost to GPU copies are much faster when they originate from pinned (page-\nlocked) memory. See Use pinned memory buffers for more details on when and how\nto use pinned memory generally.\n\nFor data loading, passing `pin_memory=True` to a `DataLoader` will\nautomatically put the fetched data Tensors in pinned memory, and thus enables\nfaster data transfer to CUDA-enabled GPUs.\n\nThe default memory pinning logic only recognizes Tensors and maps and\niterables containing Tensors. By default, if the pinning logic sees a batch\nthat is a custom type (which will occur if you have a `collate_fn` that\nreturns a custom batch type), or if each element of your batch is a custom\ntype, the pinning logic will not recognize them, and it will return that batch\n(or those elements) without pinning the memory. To enable memory pinning for\ncustom batch or data type(s), define a `pin_memory()` method on your custom\ntype(s).\n\nSee the example below.\n\nExample:\n\nData loader. Combines a dataset and a sampler, and provides an iterable over\nthe given dataset.\n\nThe `DataLoader` supports both map-style and iterable-style datasets with\nsingle- or multi-process loading, customizing loading order and optional\nautomatic batching (collation) and memory pinning.\n\nSee `torch.utils.data` documentation page for more details.\n\nWarning\n\nIf the `spawn` start method is used, `worker_init_fn` cannot be an unpicklable\nobject, e.g., a lambda function. See Multiprocessing best practices on more\ndetails related to multiprocessing in PyTorch.\n\nWarning\n\n`len(dataloader)` heuristic is based on the length of the sampler used. When\n`dataset` is an `IterableDataset`, it instead returns an estimate based on\n`len(dataset) / batch_size`, with proper rounding depending on `drop_last`,\nregardless of multi-process loading configurations. This represents the best\nguess PyTorch can make because PyTorch trusts user `dataset` code in correctly\nhandling multi-process loading to avoid duplicate data.\n\nHowever, if sharding results in multiple workers having incomplete last\nbatches, this estimate can still be inaccurate, because (1) an otherwise\ncomplete batch can be broken into multiple ones and (2) more than one batch\nworth of samples can be dropped when `drop_last` is set. Unfortunately,\nPyTorch can not detect such cases in general.\n\nSee Dataset Types for more details on these two types of datasets and how\n`IterableDataset` interacts with Multi-process data loading.\n\nWarning\n\nSee Reproducibility, and My data loader workers return identical random\nnumbers, and Randomness in multi-process data loading notes for random seed\nrelated questions.\n\nAn abstract class representing a `Dataset`.\n\nAll datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite `__getitem__()`, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n`__len__()`, which is expected to return the size of the dataset by many\n`Sampler` implementations and the default options of `DataLoader`.\n\nNote\n\n`DataLoader` by default constructs a index sampler that yields integral\nindices. To make it work with a map-style dataset with non-integral\nindices/keys, a custom sampler must be provided.\n\nAn iterable Dataset.\n\nAll datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.\n\nAll subclasses should overwrite `__iter__()`, which would return an iterator\nof samples in this dataset.\n\nWhen a subclass is used with `DataLoader`, each item in the dataset will be\nyielded from the `DataLoader` iterator. When `num_workers > 0`, each worker\nprocess will have a different copy of the dataset object, so it is often\ndesired to configure each copy independently to avoid having duplicate data\nreturned from the workers. `get_worker_info()`, when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s `__iter__()` method or the `DataLoader` \u2018s `worker_init_fn` option\nto modify each copy\u2019s behavior.\n\nExample 1: splitting workload across all workers in `__iter__()`:\n\nExample 2: splitting workload across all workers using `worker_init_fn`:\n\nDataset wrapping tensors.\n\nEach sample will be retrieved by indexing tensors along the first dimension.\n\n*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.\nDataset as a concatenation of multiple datasets.\n\nThis class is useful to assemble different existing datasets.\n\ndatasets (sequence) \u2013 List of datasets to be concatenated\n\nDataset for chainning multiple `IterableDataset` s.\n\nThis class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale datasets\nwith this class will be efficient.\n\ndatasets (iterable of IterableDataset) \u2013 datasets to be chained together\n\nDataset shuffled from the original dataset.\n\nThis class is useful to shuffle an existing instance of an IterableDataset.\nThe buffer with `buffer_size` is filled with the items from the dataset first.\nThen, each item will be yielded from the buffer by reservoir sampling via\niterator.\n\n`buffer_size` is required to be larger than 0. For `buffer_size == 1`, the\ndataset is not shuffled. In order to fully shuffle the whole dataset,\n`buffer_size` is required to be greater than or equal to the size of dataset.\n\nWhen it is used with `DataLoader`, each item in the dataset will be yielded\nfrom the `DataLoader` iterator. And, the method to set up a random seed is\ndifferent based on `num_workers`.\n\nFor single-process mode (`num_workers == 0`), the random seed is required to\nbe set before the `DataLoader` in the main process.\n\nFor multi-process mode (`num_workers > 0`), the random seed is set by a\ncallable function in each worker.\n\nSubset of a dataset at specified indices.\n\nReturns the information about the current `DataLoader` iterator worker\nprocess.\n\nWhen called in a worker, this returns an object guaranteed to have the\nfollowing attributes:\n\nWhen called in the main process, this returns `None`.\n\nNote\n\nWhen used in a `worker_init_fn` passed over to `DataLoader`, this method can\nbe useful to set up each worker process differently, for instance, using\n`worker_id` to configure the `dataset` object to only read a specific fraction\nof a sharded dataset, or use `seed` to seed other libraries used in dataset\ncode (e.g., NumPy).\n\nRandomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.:\n\nBase class for all Samplers.\n\nEvery Sampler subclass has to provide an `__iter__()` method, providing a way\nto iterate over indices of dataset elements, and a `__len__()` method that\nreturns the length of the returned iterators.\n\nNote\n\nThe `__len__()` method isn\u2019t strictly required by `DataLoader`, but is\nexpected in any calculation involving the length of a `DataLoader`.\n\nSamples elements sequentially, always in the same order.\n\ndata_source (Dataset) \u2013 dataset to sample from\n\nSamples elements randomly. If without replacement, then sample from a shuffled\ndataset. If with replacement, then user can specify `num_samples` to draw.\n\nSamples elements randomly from a given list of indices, without replacement.\n\nSamples elements from `[0,..,len(weights)-1]` with given probabilities\n(weights).\n\nWraps another sampler to yield a mini-batch of indices.\n\nSampler that restricts data loading to a subset of the dataset.\n\nIt is especially useful in conjunction with\n`torch.nn.parallel.DistributedDataParallel`. In such a case, each process can\npass a `DistributedSampler` instance as a `DataLoader` sampler, and load a\nsubset of the original dataset that is exclusive to it.\n\nNote\n\nDataset is assumed to be of constant size.\n\nWarning\n\nIn distributed mode, calling the `set_epoch()` method at the beginning of each\nepoch before creating the `DataLoader` iterator is necessary to make shuffling\nwork properly across multiple epochs. Otherwise, the same ordering will be\nalways used.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.BatchSampler", "path": "data#torch.utils.data.BatchSampler", "type": "torch.utils.data", "text": "\nWraps another sampler to yield a mini-batch of indices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.BufferedShuffleDataset", "path": "data#torch.utils.data.BufferedShuffleDataset", "type": "torch.utils.data", "text": "\nDataset shuffled from the original dataset.\n\nThis class is useful to shuffle an existing instance of an IterableDataset.\nThe buffer with `buffer_size` is filled with the items from the dataset first.\nThen, each item will be yielded from the buffer by reservoir sampling via\niterator.\n\n`buffer_size` is required to be larger than 0. For `buffer_size == 1`, the\ndataset is not shuffled. In order to fully shuffle the whole dataset,\n`buffer_size` is required to be greater than or equal to the size of dataset.\n\nWhen it is used with `DataLoader`, each item in the dataset will be yielded\nfrom the `DataLoader` iterator. And, the method to set up a random seed is\ndifferent based on `num_workers`.\n\nFor single-process mode (`num_workers == 0`), the random seed is required to\nbe set before the `DataLoader` in the main process.\n\nFor multi-process mode (`num_workers > 0`), the random seed is set by a\ncallable function in each worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.ChainDataset", "path": "data#torch.utils.data.ChainDataset", "type": "torch.utils.data", "text": "\nDataset for chainning multiple `IterableDataset` s.\n\nThis class is useful to assemble different existing dataset streams. The\nchainning operation is done on-the-fly, so concatenating large-scale datasets\nwith this class will be efficient.\n\ndatasets (iterable of IterableDataset) \u2013 datasets to be chained together\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.ConcatDataset", "path": "data#torch.utils.data.ConcatDataset", "type": "torch.utils.data", "text": "\nDataset as a concatenation of multiple datasets.\n\nThis class is useful to assemble different existing datasets.\n\ndatasets (sequence) \u2013 List of datasets to be concatenated\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.DataLoader", "path": "data#torch.utils.data.DataLoader", "type": "torch.utils.data", "text": "\nData loader. Combines a dataset and a sampler, and provides an iterable over\nthe given dataset.\n\nThe `DataLoader` supports both map-style and iterable-style datasets with\nsingle- or multi-process loading, customizing loading order and optional\nautomatic batching (collation) and memory pinning.\n\nSee `torch.utils.data` documentation page for more details.\n\nWarning\n\nIf the `spawn` start method is used, `worker_init_fn` cannot be an unpicklable\nobject, e.g., a lambda function. See Multiprocessing best practices on more\ndetails related to multiprocessing in PyTorch.\n\nWarning\n\n`len(dataloader)` heuristic is based on the length of the sampler used. When\n`dataset` is an `IterableDataset`, it instead returns an estimate based on\n`len(dataset) / batch_size`, with proper rounding depending on `drop_last`,\nregardless of multi-process loading configurations. This represents the best\nguess PyTorch can make because PyTorch trusts user `dataset` code in correctly\nhandling multi-process loading to avoid duplicate data.\n\nHowever, if sharding results in multiple workers having incomplete last\nbatches, this estimate can still be inaccurate, because (1) an otherwise\ncomplete batch can be broken into multiple ones and (2) more than one batch\nworth of samples can be dropped when `drop_last` is set. Unfortunately,\nPyTorch can not detect such cases in general.\n\nSee Dataset Types for more details on these two types of datasets and how\n`IterableDataset` interacts with Multi-process data loading.\n\nWarning\n\nSee Reproducibility, and My data loader workers return identical random\nnumbers, and Randomness in multi-process data loading notes for random seed\nrelated questions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.Dataset", "path": "data#torch.utils.data.Dataset", "type": "torch.utils.data", "text": "\nAn abstract class representing a `Dataset`.\n\nAll datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite `__getitem__()`, supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite\n`__len__()`, which is expected to return the size of the dataset by many\n`Sampler` implementations and the default options of `DataLoader`.\n\nNote\n\n`DataLoader` by default constructs a index sampler that yields integral\nindices. To make it work with a map-style dataset with non-integral\nindices/keys, a custom sampler must be provided.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.distributed.DistributedSampler", "path": "data#torch.utils.data.distributed.DistributedSampler", "type": "torch.utils.data", "text": "\nSampler that restricts data loading to a subset of the dataset.\n\nIt is especially useful in conjunction with\n`torch.nn.parallel.DistributedDataParallel`. In such a case, each process can\npass a `DistributedSampler` instance as a `DataLoader` sampler, and load a\nsubset of the original dataset that is exclusive to it.\n\nNote\n\nDataset is assumed to be of constant size.\n\nWarning\n\nIn distributed mode, calling the `set_epoch()` method at the beginning of each\nepoch before creating the `DataLoader` iterator is necessary to make shuffling\nwork properly across multiple epochs. Otherwise, the same ordering will be\nalways used.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.get_worker_info()", "path": "data#torch.utils.data.get_worker_info", "type": "torch.utils.data", "text": "\nReturns the information about the current `DataLoader` iterator worker\nprocess.\n\nWhen called in a worker, this returns an object guaranteed to have the\nfollowing attributes:\n\nWhen called in the main process, this returns `None`.\n\nNote\n\nWhen used in a `worker_init_fn` passed over to `DataLoader`, this method can\nbe useful to set up each worker process differently, for instance, using\n`worker_id` to configure the `dataset` object to only read a specific fraction\nof a sharded dataset, or use `seed` to seed other libraries used in dataset\ncode (e.g., NumPy).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.IterableDataset", "path": "data#torch.utils.data.IterableDataset", "type": "torch.utils.data", "text": "\nAn iterable Dataset.\n\nAll datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.\n\nAll subclasses should overwrite `__iter__()`, which would return an iterator\nof samples in this dataset.\n\nWhen a subclass is used with `DataLoader`, each item in the dataset will be\nyielded from the `DataLoader` iterator. When `num_workers > 0`, each worker\nprocess will have a different copy of the dataset object, so it is often\ndesired to configure each copy independently to avoid having duplicate data\nreturned from the workers. `get_worker_info()`, when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s `__iter__()` method or the `DataLoader` \u2018s `worker_init_fn` option\nto modify each copy\u2019s behavior.\n\nExample 1: splitting workload across all workers in `__iter__()`:\n\nExample 2: splitting workload across all workers using `worker_init_fn`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.RandomSampler", "path": "data#torch.utils.data.RandomSampler", "type": "torch.utils.data", "text": "\nSamples elements randomly. If without replacement, then sample from a shuffled\ndataset. If with replacement, then user can specify `num_samples` to draw.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.random_split()", "path": "data#torch.utils.data.random_split", "type": "torch.utils.data", "text": "\nRandomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.Sampler", "path": "data#torch.utils.data.Sampler", "type": "torch.utils.data", "text": "\nBase class for all Samplers.\n\nEvery Sampler subclass has to provide an `__iter__()` method, providing a way\nto iterate over indices of dataset elements, and a `__len__()` method that\nreturns the length of the returned iterators.\n\nNote\n\nThe `__len__()` method isn\u2019t strictly required by `DataLoader`, but is\nexpected in any calculation involving the length of a `DataLoader`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.SequentialSampler", "path": "data#torch.utils.data.SequentialSampler", "type": "torch.utils.data", "text": "\nSamples elements sequentially, always in the same order.\n\ndata_source (Dataset) \u2013 dataset to sample from\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.Subset", "path": "data#torch.utils.data.Subset", "type": "torch.utils.data", "text": "\nSubset of a dataset at specified indices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.SubsetRandomSampler", "path": "data#torch.utils.data.SubsetRandomSampler", "type": "torch.utils.data", "text": "\nSamples elements randomly from a given list of indices, without replacement.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.TensorDataset", "path": "data#torch.utils.data.TensorDataset", "type": "torch.utils.data", "text": "\nDataset wrapping tensors.\n\nEach sample will be retrieved by indexing tensors along the first dimension.\n\n*tensors (Tensor) \u2013 tensors that have the same size of the first dimension.\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.data.WeightedRandomSampler", "path": "data#torch.utils.data.WeightedRandomSampler", "type": "torch.utils.data", "text": "\nSamples elements from `[0,..,len(weights)-1]` with given probabilities\n(weights).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.dlpack", "path": "dlpack", "type": "torch.utils.dlpack", "text": "\nDecodes a DLPack to a tensor.\n\ndlpack \u2013 a PyCapsule object with the dltensor\n\nThe tensor will share the memory with the object represented in the dlpack.\nNote that each dlpack can only be consumed once.\n\nReturns a DLPack representing the tensor.\n\ntensor \u2013 a tensor to be exported\n\nThe dlpack shares the tensors memory. Note that each dlpack can only be\nconsumed once.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.dlpack.from_dlpack()", "path": "dlpack#torch.utils.dlpack.from_dlpack", "type": "torch.utils.dlpack", "text": "\nDecodes a DLPack to a tensor.\n\ndlpack \u2013 a PyCapsule object with the dltensor\n\nThe tensor will share the memory with the object represented in the dlpack.\nNote that each dlpack can only be consumed once.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.dlpack.to_dlpack()", "path": "dlpack#torch.utils.dlpack.to_dlpack", "type": "torch.utils.dlpack", "text": "\nReturns a DLPack representing the tensor.\n\ntensor \u2013 a tensor to be exported\n\nThe dlpack shares the tensors memory. Note that each dlpack can only be\nconsumed once.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.mobile_optimizer", "path": "mobile_optimizer", "type": "torch.utils.mobile_optimizer", "text": "\nWarning\n\nThis API is in beta and may change in the near future.\n\nTorch mobile supports `torch.mobile_optimizer.optimize_for_mobile` utility to\nrun a list of optimization pass with modules in eval mode. The method takes\nthe following parameters: a torch.jit.ScriptModule object, a blocklisting\noptimization set and a preserved method list\n\n`optimize_for_mobile` will also invoke freeze_module pass which only preserves\n`forward` method. If you have other method to that needed to be preserved, add\nthem into the preserved method list and pass into the method.\n\nA new optimized torch script module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.mobile_optimizer.optimize_for_mobile()", "path": "mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile", "type": "torch.utils.mobile_optimizer", "text": "\nA new optimized torch script module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.model_zoo", "path": "model_zoo", "type": "torch.utils.model_zoo", "text": "\nMoved to `torch.hub`.\n\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it\u2019s deserialized and\nreturned. The default value of `model_dir` is `<hub_dir>/checkpoints` where\n`hub_dir` is the directory returned by `get_dir()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.model_zoo.load_url()", "path": "model_zoo#torch.utils.model_zoo.load_url", "type": "torch.utils.model_zoo", "text": "\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it\u2019s deserialized and\nreturned. The default value of `model_dir` is `<hub_dir>/checkpoints` where\n`hub_dir` is the directory returned by `get_dir()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard", "path": "tensorboard", "type": "torch.utils.tensorboard", "text": "\nBefore going further, more details on TensorBoard can be found at\nhttps://www.tensorflow.org/tensorboard/\n\nOnce you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.\n\nThe SummaryWriter class is your main entry to log data for consumption and\nvisualization by TensorBoard. For example:\n\nThis can then be visualized with TensorBoard, which should be installable and\nrunnable with:\n\nLots of information can be logged for one experiment. To avoid cluttering the\nUI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped\nseparately in the TensorBoard interface.\n\nExpected result:\n\nWrites entries directly to event files in the log_dir to be consumed by\nTensorBoard.\n\nThe `SummaryWriter` class provides a high-level API to create an event file in\na given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.\n\nCreates a `SummaryWriter` that will write out events and summaries to the\nevent file.\n\nExamples:\n\nAdd scalar data to summary.\n\nExamples:\n\nExpected result:\n\nAdds many scalar data to summary.\n\nExamples:\n\nExpected result:\n\nAdd histogram to summary.\n\nExamples:\n\nExpected result:\n\nAdd image data to summary.\n\nNote that this requires the `pillow` package.\n\nimg_tensor: Default is (3,H,W)(3, H, W) . You can use\n`torchvision.utils.make_grid()` to convert a batch of tensor into 3xHxW format\nor call `add_images` and let us do the job. Tensor with (1,H,W)(1, H, W) ,\n(H,W)(H, W) , (H,W,3)(H, W, 3) is also suitable as long as corresponding\n`dataformats` argument is passed, e.g. `CHW`, `HWC`, `HW`.\n\nExamples:\n\nExpected result:\n\nAdd batched image data to summary.\n\nNote that this requires the `pillow` package.\n\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W) . If `dataformats` is specified,\nother shape will be accepted. e.g. NCHW or NHWC.\n\nExamples:\n\nExpected result:\n\nRender matplotlib figure into an image and add it to summary.\n\nNote that this requires the `matplotlib` package.\n\nAdd video data to summary.\n\nNote that this requires the `moviepy` package.\n\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for\ntype `uint8` or [0, 1] for type `float`.\n\nAdd audio data to summary.\n\nsnd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].\n\nAdd text data to summary.\n\nExamples:\n\nAdd graph data to summary.\n\nAdd embedding projector data to summary.\n\nmat: (N,D)(N, D) , where N is number of data and D is feature dimension\n\nlabel_img: (N,C,H,W)(N, C, H, W)\n\nExamples:\n\nAdds precision recall curve. Plotting a precision-recall curve lets you\nunderstand your model\u2019s performance under different threshold settings. With\nthis function, you provide the ground truth labeling (T/F) and prediction\nconfidence (usually the output of your model) for each target. The TensorBoard\nUI will let you choose the threshold interactively.\n\nExamples:\n\nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this\nfunction can only be called once for each SummaryWriter() object. Because it\nonly provides metadata to tensorboard, the function can be called before or\nafter the training loop.\n\nlayout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary\n{chartName: ListOfProperties}. The first element in ListOfProperties is the\nchart\u2019s type (one of Multiline or Margin) and the second element should be a\nlist containing the tags you have used in add_scalar function, which will be\ncollected into the new chart.\n\nExamples:\n\nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on\nThree.js, so it allows users to interact with the rendered object. Besides the\nbasic definitions such as vertices, faces, users can further provide camera\nparameter, lighting condition, etc. Please check\nhttps://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene\nfor advanced usage.\n\nvertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels)\n\ncolors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type `uint8`\nor [0, 1] for type `float`.\n\nfaces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for\ntype `uint8`.\n\nExamples:\n\nAdd a set of hyperparameters to be compared in TensorBoard.\n\nExamples:\n\nExpected result:\n\nFlushes the event file to disk. Call this method to make sure that all pending\nevents have been written to disk.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter", "type": "torch.utils.tensorboard", "text": "\nWrites entries directly to event files in the log_dir to be consumed by\nTensorBoard.\n\nThe `SummaryWriter` class provides a high-level API to create an event file in\na given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.\n\nCreates a `SummaryWriter` that will write out events and summaries to the\nevent file.\n\nExamples:\n\nAdd scalar data to summary.\n\nExamples:\n\nExpected result:\n\nAdds many scalar data to summary.\n\nExamples:\n\nExpected result:\n\nAdd histogram to summary.\n\nExamples:\n\nExpected result:\n\nAdd image data to summary.\n\nNote that this requires the `pillow` package.\n\nimg_tensor: Default is (3,H,W)(3, H, W) . You can use\n`torchvision.utils.make_grid()` to convert a batch of tensor into 3xHxW format\nor call `add_images` and let us do the job. Tensor with (1,H,W)(1, H, W) ,\n(H,W)(H, W) , (H,W,3)(H, W, 3) is also suitable as long as corresponding\n`dataformats` argument is passed, e.g. `CHW`, `HWC`, `HW`.\n\nExamples:\n\nExpected result:\n\nAdd batched image data to summary.\n\nNote that this requires the `pillow` package.\n\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W) . If `dataformats` is specified,\nother shape will be accepted. e.g. NCHW or NHWC.\n\nExamples:\n\nExpected result:\n\nRender matplotlib figure into an image and add it to summary.\n\nNote that this requires the `matplotlib` package.\n\nAdd video data to summary.\n\nNote that this requires the `moviepy` package.\n\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for\ntype `uint8` or [0, 1] for type `float`.\n\nAdd audio data to summary.\n\nsnd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].\n\nAdd text data to summary.\n\nExamples:\n\nAdd graph data to summary.\n\nAdd embedding projector data to summary.\n\nmat: (N,D)(N, D) , where N is number of data and D is feature dimension\n\nlabel_img: (N,C,H,W)(N, C, H, W)\n\nExamples:\n\nAdds precision recall curve. Plotting a precision-recall curve lets you\nunderstand your model\u2019s performance under different threshold settings. With\nthis function, you provide the ground truth labeling (T/F) and prediction\nconfidence (usually the output of your model) for each target. The TensorBoard\nUI will let you choose the threshold interactively.\n\nExamples:\n\nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this\nfunction can only be called once for each SummaryWriter() object. Because it\nonly provides metadata to tensorboard, the function can be called before or\nafter the training loop.\n\nlayout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary\n{chartName: ListOfProperties}. The first element in ListOfProperties is the\nchart\u2019s type (one of Multiline or Margin) and the second element should be a\nlist containing the tags you have used in add_scalar function, which will be\ncollected into the new chart.\n\nExamples:\n\nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on\nThree.js, so it allows users to interact with the rendered object. Besides the\nbasic definitions such as vertices, faces, users can further provide camera\nparameter, lighting condition, etc. Please check\nhttps://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene\nfor advanced usage.\n\nvertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels)\n\ncolors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type `uint8`\nor [0, 1] for type `float`.\n\nfaces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for\ntype `uint8`.\n\nExamples:\n\nAdd a set of hyperparameters to be compared in TensorBoard.\n\nExamples:\n\nExpected result:\n\nFlushes the event file to disk. Call this method to make sure that all pending\nevents have been written to disk.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_audio()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_audio", "type": "torch.utils.tensorboard", "text": "\nAdd audio data to summary.\n\nsnd_tensor: (1,L)(1, L) . The values should lie between [-1, 1].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars", "type": "torch.utils.tensorboard", "text": "\nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this\nfunction can only be called once for each SummaryWriter() object. Because it\nonly provides metadata to tensorboard, the function can be called before or\nafter the training loop.\n\nlayout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary\n{chartName: ListOfProperties}. The first element in ListOfProperties is the\nchart\u2019s type (one of Multiline or Margin) and the second element should be a\nlist containing the tags you have used in add_scalar function, which will be\ncollected into the new chart.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_embedding", "type": "torch.utils.tensorboard", "text": "\nAdd embedding projector data to summary.\n\nmat: (N,D)(N, D) , where N is number of data and D is feature dimension\n\nlabel_img: (N,C,H,W)(N, C, H, W)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_figure()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_figure", "type": "torch.utils.tensorboard", "text": "\nRender matplotlib figure into an image and add it to summary.\n\nNote that this requires the `matplotlib` package.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_graph()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_graph", "type": "torch.utils.tensorboard", "text": "\nAdd graph data to summary.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_histogram", "type": "torch.utils.tensorboard", "text": "\nAdd histogram to summary.\n\nExamples:\n\nExpected result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_hparams", "type": "torch.utils.tensorboard", "text": "\nAdd a set of hyperparameters to be compared in TensorBoard.\n\nExamples:\n\nExpected result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_image()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_image", "type": "torch.utils.tensorboard", "text": "\nAdd image data to summary.\n\nNote that this requires the `pillow` package.\n\nimg_tensor: Default is (3,H,W)(3, H, W) . You can use\n`torchvision.utils.make_grid()` to convert a batch of tensor into 3xHxW format\nor call `add_images` and let us do the job. Tensor with (1,H,W)(1, H, W) ,\n(H,W)(H, W) , (H,W,3)(H, W, 3) is also suitable as long as corresponding\n`dataformats` argument is passed, e.g. `CHW`, `HWC`, `HW`.\n\nExamples:\n\nExpected result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_images()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_images", "type": "torch.utils.tensorboard", "text": "\nAdd batched image data to summary.\n\nNote that this requires the `pillow` package.\n\nimg_tensor: Default is (N,3,H,W)(N, 3, H, W) . If `dataformats` is specified,\nother shape will be accepted. e.g. NCHW or NHWC.\n\nExamples:\n\nExpected result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_mesh", "type": "torch.utils.tensorboard", "text": "\nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on\nThree.js, so it allows users to interact with the rendered object. Besides the\nbasic definitions such as vertices, faces, users can further provide camera\nparameter, lighting condition, etc. Please check\nhttps://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene\nfor advanced usage.\n\nvertices: (B,N,3)(B, N, 3) . (batch, number_of_vertices, channels)\n\ncolors: (B,N,3)(B, N, 3) . The values should lie in [0, 255] for type `uint8`\nor [0, 1] for type `float`.\n\nfaces: (B,N,3)(B, N, 3) . The values should lie in [0, number_of_vertices] for\ntype `uint8`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve", "type": "torch.utils.tensorboard", "text": "\nAdds precision recall curve. Plotting a precision-recall curve lets you\nunderstand your model\u2019s performance under different threshold settings. With\nthis function, you provide the ground truth labeling (T/F) and prediction\nconfidence (usually the output of your model) for each target. The TensorBoard\nUI will let you choose the threshold interactively.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalar", "type": "torch.utils.tensorboard", "text": "\nAdd scalar data to summary.\n\nExamples:\n\nExpected result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalars", "type": "torch.utils.tensorboard", "text": "\nAdds many scalar data to summary.\n\nExamples:\n\nExpected result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_text()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_text", "type": "torch.utils.tensorboard", "text": "\nAdd text data to summary.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_video()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_video", "type": "torch.utils.tensorboard", "text": "\nAdd video data to summary.\n\nNote that this requires the `moviepy` package.\n\nvid_tensor: (N,T,C,H,W)(N, T, C, H, W) . The values should lie in [0, 255] for\ntype `uint8` or [0, 1] for type `float`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.close()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.close", "type": "torch.utils.tensorboard", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.flush()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.flush", "type": "torch.utils.tensorboard", "text": "\nFlushes the event file to disk. Call this method to make sure that all pending\nevents have been written to disk.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.__init__()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.__init__", "type": "torch.utils.tensorboard", "text": "\nCreates a `SummaryWriter` that will write out events and summaries to the\nevent file.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.vander()", "path": "generated/torch.vander#torch.vander", "type": "torch", "text": "\nGenerates a Vandermonde matrix.\n\nThe columns of the output matrix are elementwise powers of the input vector\nx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0 . If increasing is True,\nthe order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}\n. Such a matrix with a geometric progression in each row is named for\nAlexandre-Theophile Vandermonde.\n\nVandermonde matrix. If increasing is False, the first column is\nx(N\u22121)x^{(N-1)} , the second x(N\u22122)x^{(N-2)} and so forth. If increasing is\nTrue, the columns are x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)} .\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.var()", "path": "generated/torch.var#torch.var", "type": "torch", "text": "\nReturns the variance of all elements in the `input` tensor.\n\nIf `unbiased` is `False`, then the variance will be calculated via the biased\nestimator. Otherwise, Bessel\u2019s correction will be used.\n\nExample:\n\nReturns the variance of each row of the `input` tensor in the given dimension\n`dim`.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nIf `unbiased` is `False`, then the variance will be calculated via the biased\nestimator. Otherwise, Bessel\u2019s correction will be used.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.var_mean()", "path": "generated/torch.var_mean#torch.var_mean", "type": "torch", "text": "\nReturns the variance and mean of all elements in the `input` tensor.\n\nIf `unbiased` is `False`, then the variance will be calculated via the biased\nestimator. Otherwise, Bessel\u2019s correction will be used.\n\nExample:\n\nReturns the variance and mean of each row of the `input` tensor in the given\ndimension `dim`.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nIf `unbiased` is `False`, then the variance will be calculated via the biased\nestimator. Otherwise, Bessel\u2019s correction will be used.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.vdot()", "path": "generated/torch.vdot#torch.vdot", "type": "torch", "text": "\nComputes the dot product of two 1D tensors. The vdot(a, b) function handles\ncomplex numbers differently than dot(a, b). If the first argument is complex,\nthe complex conjugate of the first argument is used for the calculation of the\ndot product.\n\nNote\n\nUnlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot\nproduct of two 1D tensors with the same number of elements.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.view_as_complex()", "path": "generated/torch.view_as_complex#torch.view_as_complex", "type": "torch", "text": "\nReturns a view of `input` as a complex tensor. For an input complex tensor of\n`size` m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , this function returns a new complex\ntensor of `size` m1,m2,\u2026,mim1, m2, \\dots, mi where the last dimension of the\ninput tensor is expected to represent the real and imaginary components of\ncomplex numbers.\n\nWarning\n\n`view_as_complex()` is only supported for tensors with `torch.dtype`\n`torch.float64` and `torch.float32`. The input is expected to have the last\ndimension of `size` 2\\. In addition, the tensor must have a `stride` of 1 for\nits last dimension. The strides of all other dimensions must be even numbers.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.view_as_real()", "path": "generated/torch.view_as_real#torch.view_as_real", "type": "torch", "text": "\nReturns a view of `input` as a real tensor. For an input complex tensor of\n`size` m1,m2,\u2026,mim1, m2, \\dots, mi , this function returns a new real tensor\nof size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , where the last dimension of size 2\nrepresents the real and imaginary components of complex numbers.\n\nWarning\n\n`view_as_real()` is only supported for tensors with `complex dtypes`.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.vstack()", "path": "generated/torch.vstack#torch.vstack", "type": "torch", "text": "\nStack tensors in sequence vertically (row wise).\n\nThis is equivalent to concatenation along the first axis after all 1-D tensors\nhave been reshaped by `torch.atleast_2d()`.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.where()", "path": "generated/torch.where#torch.where", "type": "torch", "text": "\nReturn a tensor of elements selected from either `x` or `y`, depending on\n`condition`.\n\nThe operation is defined as:\n\nNote\n\nThe tensors `condition`, `x`, `y` must be broadcastable.\n\nNote\n\nCurrently valid scalar and tensor combination are 1. Scalar of floating dtype\nand torch.double 2. Scalar of integral dtype and torch.long 3. Scalar of\ncomplex dtype and torch.complex128\n\nA tensor of shape equal to the broadcasted shape of `condition`, `x`, `y`\n\nTensor\n\nExample:\n\n`torch.where(condition)` is identical to `torch.nonzero(condition,\nas_tuple=True)`.\n\nNote\n\nSee also `torch.nonzero()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.xlogy()", "path": "generated/torch.xlogy#torch.xlogy", "type": "torch", "text": "\nComputes `input * log(other)` with the following cases.\n\nSimilar to SciPy\u2019s `scipy.special.xlogy`.\n\nNote\n\nAt least one of `input` or `other` must be a tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.zeros()", "path": "generated/torch.zeros#torch.zeros", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `0`, with the shape defined by\nthe variable argument `size`.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor. Can be a variable number of arguments or a collection like a list or\ntuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.zeros_like()", "path": "generated/torch.zeros_like#torch.zeros_like", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `0`, with the same size as\n`input`. `torch.zeros_like(input)` is equivalent to `torch.zeros(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\nWarning\n\nAs of 0.4, this function does not support an `out` keyword. As an alternative,\nthe old `torch.zeros_like(input, out=output)` is equivalent to\n`torch.zeros(input.size(), out=output)`.\n\ninput (Tensor) \u2013 the size of `input` will determine size of the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch._assert()", "path": "generated/torch._assert#torch._assert", "type": "torch", "text": "\nA wrapper around Python\u2019s assert which is symbolically traceable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}]