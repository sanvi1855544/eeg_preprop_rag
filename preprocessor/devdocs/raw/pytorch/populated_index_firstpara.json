[{"name": "clear()", "path": "backends#clear", "type": "torch.backends", "text": "\nClears the cuFFT plan cache.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "max_size", "path": "backends#max_size", "type": "torch.backends", "text": "\nA `int` that controls cache capacity of cuFFT plan.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch", "path": "torch", "type": "torch", "text": "\nThe torch package contains data structures for multi-dimensional tensors and\ndefines mathematical operations over these tensors. Additionally, it provides\nmany utilities for efficient serializing of Tensors and arbitrary types, and\nother useful utilities.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.abs()", "path": "generated/torch.abs#torch.abs", "type": "torch", "text": "\nComputes the absolute value of each element in `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.absolute()", "path": "generated/torch.absolute#torch.absolute", "type": "torch", "text": "\nAlias for `torch.abs()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.acos()", "path": "generated/torch.acos#torch.acos", "type": "torch", "text": "\nComputes the inverse cosine of each element in `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.acosh()", "path": "generated/torch.acosh#torch.acosh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic cosine of the elements of\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.add()", "path": "generated/torch.add#torch.add", "type": "torch", "text": "\nAdds the scalar `other` to each element of the input `input` and returns a new\nresulting tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.addbmm()", "path": "generated/torch.addbmm#torch.addbmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices stored in `batch1` and\n`batch2`, with a reduced add step (all matrix multiplications get accumulated\nalong the first dimension). `input` is added to the final result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.addcdiv()", "path": "generated/torch.addcdiv#torch.addcdiv", "type": "torch", "text": "\nPerforms the element-wise division of `tensor1` by `tensor2`, multiply the\nresult by the scalar `value` and add it to `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.addcmul()", "path": "generated/torch.addcmul#torch.addcmul", "type": "torch", "text": "\nPerforms the element-wise multiplication of `tensor1` by `tensor2`, multiply\nthe result by the scalar `value` and add it to `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.addmm()", "path": "generated/torch.addmm#torch.addmm", "type": "torch", "text": "\nPerforms a matrix multiplication of the matrices `mat1` and `mat2`. The matrix\n`input` is added to the final result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.addmv()", "path": "generated/torch.addmv#torch.addmv", "type": "torch", "text": "\nPerforms a matrix-vector product of the matrix `mat` and the vector `vec`. The\nvector `input` is added to the final result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.addr()", "path": "generated/torch.addr#torch.addr", "type": "torch", "text": "\nPerforms the outer-product of vectors `vec1` and `vec2` and adds it to the\nmatrix `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.all()", "path": "generated/torch.all#torch.all", "type": "torch", "text": "\nTests if all elements in `input` evaluate to `True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.allclose()", "path": "generated/torch.allclose#torch.allclose", "type": "torch", "text": "\nThis function checks if all `input` and `other` satisfy the condition:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.amax()", "path": "generated/torch.amax#torch.amax", "type": "torch", "text": "\nReturns the maximum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.amin()", "path": "generated/torch.amin#torch.amin", "type": "torch", "text": "\nReturns the minimum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.angle()", "path": "generated/torch.angle#torch.angle", "type": "torch", "text": "\nComputes the element-wise angle (in radians) of the given `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.any()", "path": "generated/torch.any#torch.any", "type": "torch", "text": "\ninput (Tensor) \u2013 the input tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arange()", "path": "generated/torch.arange#torch.arange", "type": "torch", "text": "\nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rceil with values from the interval `[start,\nend)` taken with common difference `step` beginning from `start`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arccos()", "path": "generated/torch.arccos#torch.arccos", "type": "torch", "text": "\nAlias for `torch.acos()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arccosh()", "path": "generated/torch.arccosh#torch.arccosh", "type": "torch", "text": "\nAlias for `torch.acosh()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arcsin()", "path": "generated/torch.arcsin#torch.arcsin", "type": "torch", "text": "\nAlias for `torch.asin()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arcsinh()", "path": "generated/torch.arcsinh#torch.arcsinh", "type": "torch", "text": "\nAlias for `torch.asinh()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arctan()", "path": "generated/torch.arctan#torch.arctan", "type": "torch", "text": "\nAlias for `torch.atan()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.arctanh()", "path": "generated/torch.arctanh#torch.arctanh", "type": "torch", "text": "\nAlias for `torch.atanh()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.are_deterministic_algorithms_enabled()", "path": "generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled", "type": "torch", "text": "\nReturns True if the global deterministic flag is turned on. Refer to\n`torch.use_deterministic_algorithms()` documentation for more details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.argmax()", "path": "generated/torch.argmax#torch.argmax", "type": "torch", "text": "\nReturns the indices of the maximum value of all elements in the `input`\ntensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.argmin()", "path": "generated/torch.argmin#torch.argmin", "type": "torch", "text": "\nReturns the indices of the minimum value(s) of the flattened tensor or along a\ndimension\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.argsort()", "path": "generated/torch.argsort#torch.argsort", "type": "torch", "text": "\nReturns the indices that sort a tensor along a given dimension in ascending\norder by value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.asin()", "path": "generated/torch.asin#torch.asin", "type": "torch", "text": "\nReturns a new tensor with the arcsine of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.asinh()", "path": "generated/torch.asinh#torch.asinh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic sine of the elements of\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.as_strided()", "path": "generated/torch.as_strided#torch.as_strided", "type": "torch", "text": "\nCreate a view of an existing `torch.Tensor` `input` with specified `size`,\n`stride` and `storage_offset`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.as_tensor()", "path": "generated/torch.as_tensor#torch.as_tensor", "type": "torch", "text": "\nConvert the data into a `torch.Tensor`. If the data is already a `Tensor` with\nthe same `dtype` and `device`, no copy will be performed, otherwise a new\n`Tensor` will be returned with computational graph retained if data `Tensor`\nhas `requires_grad=True`. Similarly, if the data is an `ndarray` of the\ncorresponding `dtype` and the `device` is the cpu, no copy will be performed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.atan()", "path": "generated/torch.atan#torch.atan", "type": "torch", "text": "\nReturns a new tensor with the arctangent of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.atan2()", "path": "generated/torch.atan2#torch.atan2", "type": "torch", "text": "\nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}\nwith consideration of the quadrant. Returns a new tensor with the signed\nangles in radians between vector (otheri,inputi)(\\text{other}_{i},\n\\text{input}_{i}) and vector (1,0)(1, 0) . (Note that otheri\\text{other}_{i} ,\nthe second parameter, is the x-coordinate, while inputi\\text{input}_{i} , the\nfirst parameter, is the y-coordinate.)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.atanh()", "path": "generated/torch.atanh#torch.atanh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic tangent of the elements of\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.atleast_1d()", "path": "generated/torch.atleast_1d#torch.atleast_1d", "type": "torch", "text": "\nReturns a 1-dimensional view of each input tensor with zero dimensions. Input\ntensors with one or more dimensions are returned as-is.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.atleast_2d()", "path": "generated/torch.atleast_2d#torch.atleast_2d", "type": "torch", "text": "\nReturns a 2-dimensional view of each input tensor with zero dimensions. Input\ntensors with two or more dimensions are returned as-is. :param input: :type\ninput: Tensor or list of Tensors\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.atleast_3d()", "path": "generated/torch.atleast_3d#torch.atleast_3d", "type": "torch", "text": "\nReturns a 3-dimensional view of each input tensor with zero dimensions. Input\ntensors with three or more dimensions are returned as-is. :param input: :type\ninput: Tensor or list of Tensors\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd", "path": "autograd", "type": "torch.autograd", "text": "\n`torch.autograd` provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare `Tensor` s for which\ngradients should be computed with the `requires_grad=True` keyword. As of now,\nwe only support autograd for floating point `Tensor` types ( half, float,\ndouble and bfloat16) and complex `Tensor` types (cfloat, cdouble).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.backward()", "path": "autograd#torch.autograd.backward", "type": "torch.autograd", "text": "\nComputes the sum of gradients of given tensors w.r.t. graph leaves.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "torch.autograd", "text": "\nContext-manager that enable anomaly detection for the autograd engine.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.enable_grad", "path": "autograd#torch.autograd.enable_grad", "type": "torch.autograd", "text": "\nContext-manager that enables gradient calculation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "torch.autograd", "text": "\nRecords operation history and defines formulas for differentiating ops.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.Function.backward()", "path": "autograd#torch.autograd.Function.backward", "type": "torch.autograd", "text": "\nDefines a formula for differentiating the operation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.Function.forward()", "path": "autograd#torch.autograd.Function.forward", "type": "torch.autograd", "text": "\nPerforms the operation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin", "path": "autograd#torch.autograd.function._ContextMethodMixin", "type": "torch.autograd", "text": "\nMarks given tensors as modified in an in-place operation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_dirty()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_dirty", "type": "torch.autograd", "text": "\nMarks given tensors as modified in an in-place operation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "type": "torch.autograd", "text": "\nMarks outputs as non-differentiable.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.save_for_backward()", "path": "autograd#torch.autograd.function._ContextMethodMixin.save_for_backward", "type": "torch.autograd", "text": "\nSaves given tensors for a future call to `backward()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.set_materialize_grads()", "path": "autograd#torch.autograd.function._ContextMethodMixin.set_materialize_grads", "type": "torch.autograd", "text": "\nSets whether to materialize output grad tensors. Default is true.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.functional.hessian()", "path": "autograd#torch.autograd.functional.hessian", "type": "torch.autograd", "text": "\nFunction that computes the Hessian of a given scalar function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.functional.hvp()", "path": "autograd#torch.autograd.functional.hvp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between the Hessian of a given scalar\nfunction and a vector `v` at the point given by the inputs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.functional.jacobian()", "path": "autograd#torch.autograd.functional.jacobian", "type": "torch.autograd", "text": "\nFunction that computes the Jacobian of a given function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.functional.jvp()", "path": "autograd#torch.autograd.functional.jvp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between the Jacobian of the given\nfunction at the point given by the inputs and a vector `v`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.functional.vhp()", "path": "autograd#torch.autograd.functional.vhp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between a vector `v` and the Hessian of\na given scalar function at the point given by the inputs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.functional.vjp()", "path": "autograd#torch.autograd.functional.vjp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between a vector `v` and the Jacobian\nof the given function at the point given by the inputs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.grad()", "path": "autograd#torch.autograd.grad", "type": "torch.autograd", "text": "\nComputes and returns the sum of gradients of outputs w.r.t. the inputs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.gradcheck()", "path": "autograd#torch.autograd.gradcheck", "type": "torch.autograd", "text": "\nCheck gradients computed via small finite differences against analytical\ngradients w.r.t. tensors in `inputs` that are of floating point or complex\ntype and with `requires_grad=True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.gradgradcheck()", "path": "autograd#torch.autograd.gradgradcheck", "type": "torch.autograd", "text": "\nCheck gradients of gradients computed via small finite differences against\nanalytical gradients w.r.t. tensors in `inputs` and `grad_outputs` that are of\nfloating point or complex type and with `requires_grad=True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.no_grad", "path": "autograd#torch.autograd.no_grad", "type": "torch.autograd", "text": "\nContext-manager that disabled gradient calculation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "torch.autograd", "text": "\nContext manager that makes every autograd operation emit an NVTX range.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "autograd#torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "text": "\nOpens an nvprof trace file and parses autograd annotations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "torch.autograd", "text": "\nContext manager that manages autograd profiler state and holds a summary of\nresults. Under the hood it just records events of functions being executed in\nC++ and exposes those events to Python. You can wrap any code into it and it\nwill only report runtime of PyTorch functions. Note: profiler is thread local\nand is automatically propagated into the async tasks\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "autograd#torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "text": "\nExports an EventList as a Chrome tracing tools file.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "autograd#torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "text": "\nAverages all function events over their keys.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total()", "path": "autograd#torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "text": "\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.table()", "path": "autograd#torch.autograd.profiler.profile.table", "type": "torch.autograd", "text": "\nPrints an EventList as a nicely formatted table.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "autograd#torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "text": "\nAverages all events.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "torch.autograd", "text": "\nContext-manager that sets the anomaly detection for the autograd engine on or\noff.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.autograd.set_grad_enabled", "path": "autograd#torch.autograd.set_grad_enabled", "type": "torch.autograd", "text": "\nContext-manager that sets gradient calculation to on or off.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends", "path": "backends", "type": "torch.backends", "text": "\n`torch.backends` controls the behavior of various backends that PyTorch\nsupports.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "torch.backends", "text": "\n`cufft_plan_cache` caches the cuFFT plans\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t\nnecessarily mean CUDA is available; just that if this PyTorch binary were run\na machine with working CUDA drivers and devices, we would be able to use it.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "torch.backends", "text": "\nA `bool` that controls whether TensorFloat-32 tensor cores may be used in\nmatrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on\nAmpere devices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cuda.size", "path": "backends#torch.backends.cuda.size", "type": "torch.backends", "text": "\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "torch.backends", "text": "\nA `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere\ndevices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "torch.backends", "text": "\nA `bool` that, if True, causes cuDNN to benchmark multiple convolution\nalgorithms and select the fastest.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "torch.backends", "text": "\nA `bool` that, if True, causes cuDNN to only use deterministic convolution\nalgorithms. See also `torch.are_deterministic_algorithms_enabled()` and\n`torch.use_deterministic_algorithms()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "torch.backends", "text": "\nA `bool` that controls whether cuDNN is enabled.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "torch.backends", "text": "\nReturns a bool indicating if CUDNN is currently available.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "torch.backends", "text": "\nReturns the version of cuDNN\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with MKL support.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with MKL-DNN support.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with OpenMP support.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.baddbmm()", "path": "generated/torch.baddbmm#torch.baddbmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices in `batch1` and `batch2`.\n`input` is added to the final result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bartlett_window()", "path": "generated/torch.bartlett_window#torch.bartlett_window", "type": "torch", "text": "\nBartlett window function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bernoulli()", "path": "generated/torch.bernoulli#torch.bernoulli", "type": "torch", "text": "\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bincount()", "path": "generated/torch.bincount#torch.bincount", "type": "torch", "text": "\nCount the frequency of each value in an array of non-negative ints.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bitwise_and()", "path": "generated/torch.bitwise_and#torch.bitwise_and", "type": "torch", "text": "\nComputes the bitwise AND of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical AND.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bitwise_not()", "path": "generated/torch.bitwise_not#torch.bitwise_not", "type": "torch", "text": "\nComputes the bitwise NOT of the given input tensor. The input tensor must be\nof integral or Boolean types. For bool tensors, it computes the logical NOT.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bitwise_or()", "path": "generated/torch.bitwise_or#torch.bitwise_or", "type": "torch", "text": "\nComputes the bitwise OR of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical OR.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bitwise_xor()", "path": "generated/torch.bitwise_xor#torch.bitwise_xor", "type": "torch", "text": "\nComputes the bitwise XOR of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.blackman_window()", "path": "generated/torch.blackman_window#torch.blackman_window", "type": "torch", "text": "\nBlackman window function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.block_diag()", "path": "generated/torch.block_diag#torch.block_diag", "type": "torch", "text": "\nCreate a block diagonal matrix from provided tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bmm()", "path": "generated/torch.bmm#torch.bmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices stored in `input` and\n`mat2`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.broadcast_shapes()", "path": "generated/torch.broadcast_shapes#torch.broadcast_shapes", "type": "torch", "text": "\nSimilar to `broadcast_tensors()` but for shapes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.broadcast_tensors()", "path": "generated/torch.broadcast_tensors#torch.broadcast_tensors", "type": "torch", "text": "\nBroadcasts the given tensors according to Broadcasting semantics.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.broadcast_to()", "path": "generated/torch.broadcast_to#torch.broadcast_to", "type": "torch", "text": "\nBroadcasts `input` to the shape `shape`. Equivalent to calling\n`input.expand(shape)`. See `expand()` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.bucketize()", "path": "generated/torch.bucketize#torch.bucketize", "type": "torch", "text": "\nReturns the indices of the buckets to which each value in the `input` belongs,\nwhere the boundaries of the buckets are set by `boundaries`. Return a new\ntensor with the same size as `input`. If `right` is False (default), then the\nleft boundary is closed. More formally, the returned index satisfies the\nfollowing rules:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.can_cast()", "path": "generated/torch.can_cast#torch.can_cast", "type": "torch", "text": "\nDetermines if a type conversion is allowed under PyTorch casting rules\ndescribed in the type promotion documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cartesian_prod()", "path": "generated/torch.cartesian_prod#torch.cartesian_prod", "type": "torch", "text": "\nDo cartesian product of the given sequence of tensors. The behavior is similar\nto python\u2019s `itertools.product`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cat()", "path": "generated/torch.cat#torch.cat", "type": "torch", "text": "\nConcatenates the given sequence of `seq` tensors in the given dimension. All\ntensors must either have the same shape (except in the concatenating\ndimension) or be empty.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cdist()", "path": "generated/torch.cdist#torch.cdist", "type": "torch", "text": "\nComputes batched the p-norm distance between each pair of the two collections\nof row vectors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ceil()", "path": "generated/torch.ceil#torch.ceil", "type": "torch", "text": "\nReturns a new tensor with the ceil of the elements of `input`, the smallest\ninteger greater than or equal to each element.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.chain_matmul()", "path": "generated/torch.chain_matmul#torch.chain_matmul", "type": "torch", "text": "\nReturns the matrix product of the NN 2-D tensors. This product is efficiently\ncomputed using the matrix chain order algorithm which selects the order in\nwhich incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note\nthat since this is a function to compute the product, NN needs to be greater\nthan or equal to 2; if equal to 2 then a trivial matrix-matrix product is\nreturned. If NN is 1, then this is a no-op - the original matrix is returned\nas is.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cholesky()", "path": "generated/torch.cholesky#torch.cholesky", "type": "torch", "text": "\nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA\nor for batches of symmetric positive-definite matrices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cholesky_inverse()", "path": "generated/torch.cholesky_inverse#torch.cholesky_inverse", "type": "torch", "text": "\nComputes the inverse of a symmetric positive-definite matrix AA using its\nCholesky factor uu : returns matrix `inv`. The inverse is computed using\nLAPACK routines `dpotri` and `spotri` (and the corresponding MAGMA routines).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cholesky_solve()", "path": "generated/torch.cholesky_solve#torch.cholesky_solve", "type": "torch", "text": "\nSolves a linear system of equations with a positive semidefinite matrix to be\ninverted given its Cholesky factor matrix uu .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.chunk()", "path": "generated/torch.chunk#torch.chunk", "type": "torch", "text": "\nSplits a tensor into a specific number of chunks. Each chunk is a view of the\ninput tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.clamp()", "path": "generated/torch.clamp#torch.clamp", "type": "torch", "text": "\nClamp all elements in `input` into the range `[` `min`, `max` `]`. Let\nmin_value and max_value be `min` and `max`, respectively, this returns:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.clip()", "path": "generated/torch.clip#torch.clip", "type": "torch", "text": "\nAlias for `torch.clamp()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.clone()", "path": "generated/torch.clone#torch.clone", "type": "torch", "text": "\nReturns a copy of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.column_stack()", "path": "generated/torch.column_stack#torch.column_stack", "type": "torch", "text": "\nCreates a new tensor by horizontally stacking the tensors in `tensors`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.combinations()", "path": "generated/torch.combinations#torch.combinations", "type": "torch", "text": "\nCompute combinations of length rr of the given tensor. The behavior is similar\nto python\u2019s `itertools.combinations` when `with_replacement` is set to\n`False`, and `itertools.combinations_with_replacement` when `with_replacement`\nis set to `True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.compiled_with_cxx11_abi()", "path": "generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi", "type": "torch", "text": "\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.complex()", "path": "generated/torch.complex#torch.complex", "type": "torch", "text": "\nConstructs a complex tensor with its real part equal to `real` and its\nimaginary part equal to `imag`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.conj()", "path": "generated/torch.conj#torch.conj", "type": "torch", "text": "\nComputes the element-wise conjugate of the given `input` tensor. If\n:attr`input` has a non-complex dtype, this function just returns `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.copysign()", "path": "generated/torch.copysign#torch.copysign", "type": "torch", "text": "\nCreate a new floating-point tensor with the magnitude of `input` and the sign\nof `other`, elementwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cos()", "path": "generated/torch.cos#torch.cos", "type": "torch", "text": "\nReturns a new tensor with the cosine of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cosh()", "path": "generated/torch.cosh#torch.cosh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic cosine of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.count_nonzero()", "path": "generated/torch.count_nonzero#torch.count_nonzero", "type": "torch", "text": "\nCounts the number of non-zero values in the tensor `input` along the given\n`dim`. If no dim is specified then all non-zeros in the tensor are counted.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cross()", "path": "generated/torch.cross#torch.cross", "type": "torch", "text": "\nReturns the cross product of vectors in dimension `dim` of `input` and\n`other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda", "path": "cuda", "type": "torch.cuda", "text": "\nThis package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp", "path": "amp", "type": "torch.cuda.amp", "text": "\n`torch.cuda.amp` provides convenience methods for mixed precision, where some\noperations use the `torch.float32` (`float`) datatype and other operations use\n`torch.float16` (`half`). Some ops, like linear layers and convolutions, are\nmuch faster in `float16`. Other ops, like reductions, often require the\ndynamic range of `float32`. Mixed precision tries to match each op to its\nappropriate datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "torch.cuda.amp", "text": "\nInstances of `autocast` serve as context managers or decorators that allow\nregions of your script to run in mixed precision.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "torch.cuda.amp", "text": "\nHelper decorator for backward methods of custom autograd functions (subclasses\nof `torch.autograd.Function`). Ensures that `backward` executes with the same\nautocast state as `forward`. See the example page for more detail.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "torch.cuda.amp", "text": "\nHelper decorator for `forward` methods of custom autograd functions\n(subclasses of `torch.autograd.Function`). See the example page for more\ndetail.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale backoff factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale backoff factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale growth factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "torch.cuda.amp", "text": "\nReturns a Python int containing the growth interval.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "torch.cuda.amp", "text": "\nReturns a bool indicating whether this instance is enabled.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "torch.cuda.amp", "text": "\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "torch.cuda.amp", "text": "\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "torch.cuda.amp", "text": "\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "torch.cuda.amp", "text": "\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "torch.cuda.amp", "text": "\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "torch.cuda.amp", "text": "\nReturns the state of the scaler as a `dict`. It contains five entries:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "torch.cuda.amp", "text": "\n`step()` carries out the following two operations:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "torch.cuda.amp", "text": "\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "torch.cuda.amp", "text": "\nUpdates the scale factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.can_device_access_peer()", "path": "cuda#torch.cuda.can_device_access_peer", "type": "torch.cuda", "text": "\nChecks if peer access between two devices is possible.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.comm.broadcast()", "path": "cuda#torch.cuda.comm.broadcast", "type": "torch.cuda", "text": "\nBroadcasts a tensor to specified GPU devices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "cuda#torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "text": "\nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first\ncoalesced into a buffer to reduce the number of synchronizations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.comm.gather()", "path": "cuda#torch.cuda.comm.gather", "type": "torch.cuda", "text": "\nGathers tensors from multiple GPU devices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.comm.reduce_add()", "path": "cuda#torch.cuda.comm.reduce_add", "type": "torch.cuda", "text": "\nSums tensors from multiple GPUs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.comm.scatter()", "path": "cuda#torch.cuda.comm.scatter", "type": "torch.cuda", "text": "\nScatters tensor across multiple GPUs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.current_blas_handle()", "path": "cuda#torch.cuda.current_blas_handle", "type": "torch.cuda", "text": "\nReturns cublasHandle_t pointer to current cuBLAS handle\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.current_device()", "path": "cuda#torch.cuda.current_device", "type": "torch.cuda", "text": "\nReturns the index of a currently selected device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.current_stream()", "path": "cuda#torch.cuda.current_stream", "type": "torch.cuda", "text": "\nReturns the currently selected `Stream` for a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.default_stream()", "path": "cuda#torch.cuda.default_stream", "type": "torch.cuda", "text": "\nReturns the default `Stream` for a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.device", "path": "cuda#torch.cuda.device", "type": "torch.cuda", "text": "\nContext-manager that changes the selected device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.device_count()", "path": "cuda#torch.cuda.device_count", "type": "torch.cuda", "text": "\nReturns the number of GPUs available.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.device_of", "path": "cuda#torch.cuda.device_of", "type": "torch.cuda", "text": "\nContext-manager that changes the current device to that of given object.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.empty_cache()", "path": "cuda#torch.cuda.empty_cache", "type": "torch.cuda", "text": "\nReleases all unoccupied cached memory currently held by the caching allocator\nso that those can be used in other GPU application and visible in `nvidia-\nsmi`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event", "path": "cuda#torch.cuda.Event", "type": "torch.cuda", "text": "\nWrapper around a CUDA event.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.elapsed_time()", "path": "cuda#torch.cuda.Event.elapsed_time", "type": "torch.cuda", "text": "\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "cuda#torch.cuda.Event.from_ipc_handle", "type": "torch.cuda", "text": "\nReconstruct an event from an IPC handle on the given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.ipc_handle()", "path": "cuda#torch.cuda.Event.ipc_handle", "type": "torch.cuda", "text": "\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.query()", "path": "cuda#torch.cuda.Event.query", "type": "torch.cuda", "text": "\nChecks if all work currently captured by event has completed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.record()", "path": "cuda#torch.cuda.Event.record", "type": "torch.cuda", "text": "\nRecords the event in a given stream.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.synchronize()", "path": "cuda#torch.cuda.Event.synchronize", "type": "torch.cuda", "text": "\nWaits for the event to complete.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Event.wait()", "path": "cuda#torch.cuda.Event.wait", "type": "torch.cuda", "text": "\nMakes all future work submitted to the given stream wait for this event.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_arch_list()", "path": "cuda#torch.cuda.get_arch_list", "type": "torch.cuda", "text": "\nReturns list CUDA architectures this library was compiled for.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_device_capability()", "path": "cuda#torch.cuda.get_device_capability", "type": "torch.cuda", "text": "\nGets the cuda capability of a device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_device_name()", "path": "cuda#torch.cuda.get_device_name", "type": "torch.cuda", "text": "\nGets the name of a device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_device_properties()", "path": "cuda#torch.cuda.get_device_properties", "type": "torch.cuda", "text": "\nGets the properties of a device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_gencode_flags()", "path": "cuda#torch.cuda.get_gencode_flags", "type": "torch.cuda", "text": "\nReturns NVCC gencode flags this library were compiled with.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_rng_state()", "path": "cuda#torch.cuda.get_rng_state", "type": "torch.cuda", "text": "\nReturns the random number generator state of the specified GPU as a\nByteTensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.get_rng_state_all()", "path": "cuda#torch.cuda.get_rng_state_all", "type": "torch.cuda", "text": "\nReturns a list of ByteTensor representing the random number states of all\ndevices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.init()", "path": "cuda#torch.cuda.init", "type": "torch.cuda", "text": "\nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you\nare interacting with PyTorch via its C API, as Python bindings for CUDA\nfunctionality will not be available until this initialization takes place.\nOrdinary users should not need this, as all of PyTorch\u2019s CUDA methods\nautomatically initialize CUDA state on-demand.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.initial_seed()", "path": "cuda#torch.cuda.initial_seed", "type": "torch.cuda", "text": "\nReturns the current random seed of the current GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.ipc_collect()", "path": "cuda#torch.cuda.ipc_collect", "type": "torch.cuda", "text": "\nForce collects GPU memory after it has been released by CUDA IPC.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.is_available()", "path": "cuda#torch.cuda.is_available", "type": "torch.cuda", "text": "\nReturns a bool indicating if CUDA is currently available.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.is_initialized()", "path": "cuda#torch.cuda.is_initialized", "type": "torch.cuda", "text": "\nReturns whether PyTorch\u2019s CUDA state has been initialized.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.list_gpu_processes()", "path": "cuda#torch.cuda.list_gpu_processes", "type": "torch.cuda", "text": "\nReturns a human-readable printout of the running processes and their GPU\nmemory use for a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.manual_seed()", "path": "cuda#torch.cuda.manual_seed", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers for the current GPU. It\u2019s safe to\ncall this function if CUDA is not available; in that case, it is silently\nignored.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.manual_seed_all()", "path": "cuda#torch.cuda.manual_seed_all", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call\nthis function if CUDA is not available; in that case, it is silently ignored.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_allocated()", "path": "cuda#torch.cuda.max_memory_allocated", "type": "torch.cuda", "text": "\nReturns the maximum GPU memory occupied by tensors in bytes for a given\ndevice.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_cached()", "path": "cuda#torch.cuda.max_memory_cached", "type": "torch.cuda", "text": "\nDeprecated; see `max_memory_reserved()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_reserved()", "path": "cuda#torch.cuda.max_memory_reserved", "type": "torch.cuda", "text": "\nReturns the maximum GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.memory_allocated()", "path": "cuda#torch.cuda.memory_allocated", "type": "torch.cuda", "text": "\nReturns the current GPU memory occupied by tensors in bytes for a given\ndevice.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.memory_cached()", "path": "cuda#torch.cuda.memory_cached", "type": "torch.cuda", "text": "\nDeprecated; see `memory_reserved()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.memory_reserved()", "path": "cuda#torch.cuda.memory_reserved", "type": "torch.cuda", "text": "\nReturns the current GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.memory_snapshot()", "path": "cuda#torch.cuda.memory_snapshot", "type": "torch.cuda", "text": "\nReturns a snapshot of the CUDA memory allocator state across all devices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.memory_stats()", "path": "cuda#torch.cuda.memory_stats", "type": "torch.cuda", "text": "\nReturns a dictionary of CUDA memory allocator statistics for a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.memory_summary()", "path": "cuda#torch.cuda.memory_summary", "type": "torch.cuda", "text": "\nReturns a human-readable printout of the current memory allocator statistics\nfor a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.mark()", "path": "cuda#torch.cuda.nvtx.mark", "type": "torch.cuda", "text": "\nDescribe an instantaneous event that occurred at some point.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.range_pop()", "path": "cuda#torch.cuda.nvtx.range_pop", "type": "torch.cuda", "text": "\nPops a range off of a stack of nested range spans. Returns the zero-based\ndepth of the range that is ended.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.range_push()", "path": "cuda#torch.cuda.nvtx.range_push", "type": "torch.cuda", "text": "\nPushes a range onto a stack of nested range span. Returns zero-based depth of\nthe range that is started.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "cuda#torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "text": "\nResets the starting point in tracking maximum GPU memory occupied by tensors\nfor a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "cuda#torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "text": "\nResets the starting point in tracking maximum GPU memory managed by the\ncaching allocator for a given device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.seed()", "path": "cuda#torch.cuda.seed", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers to a random number for the current\nGPU. It\u2019s safe to call this function if CUDA is not available; in that case,\nit is silently ignored.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.seed_all()", "path": "cuda#torch.cuda.seed_all", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers to a random number on all GPUs.\nIt\u2019s safe to call this function if CUDA is not available; in that case, it is\nsilently ignored.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.set_device()", "path": "cuda#torch.cuda.set_device", "type": "torch.cuda", "text": "\nSets the current device.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "cuda#torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "text": "\nSet memory fraction for a process. The fraction is used to limit an caching\nallocator to allocated memory on a CUDA device. The allowed value equals the\ntotal visible memory multiplied fraction. If trying to allocate more than the\nallowed value in a process, will raise an out of memory error in allocator.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.set_rng_state()", "path": "cuda#torch.cuda.set_rng_state", "type": "torch.cuda", "text": "\nSets the random number generator state of the specified GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.set_rng_state_all()", "path": "cuda#torch.cuda.set_rng_state_all", "type": "torch.cuda", "text": "\nSets the random number generator state of all devices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Stream", "path": "cuda#torch.cuda.Stream", "type": "torch.cuda", "text": "\nWrapper around a CUDA stream.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.stream()", "path": "cuda#torch.cuda.stream", "type": "torch.cuda", "text": "\nContext-manager that selects a given stream.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Stream.query()", "path": "cuda#torch.cuda.Stream.query", "type": "torch.cuda", "text": "\nChecks if all the work submitted has been completed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Stream.record_event()", "path": "cuda#torch.cuda.Stream.record_event", "type": "torch.cuda", "text": "\nRecords an event.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Stream.synchronize()", "path": "cuda#torch.cuda.Stream.synchronize", "type": "torch.cuda", "text": "\nWait for all the kernels in this stream to complete.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Stream.wait_event()", "path": "cuda#torch.cuda.Stream.wait_event", "type": "torch.cuda", "text": "\nMakes all future work submitted to the stream wait for an event.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.Stream.wait_stream()", "path": "cuda#torch.cuda.Stream.wait_stream", "type": "torch.cuda", "text": "\nSynchronizes with another stream.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cuda.synchronize()", "path": "cuda#torch.cuda.synchronize", "type": "torch.cuda", "text": "\nWaits for all kernels in all streams on a CUDA device to complete.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cummax()", "path": "generated/torch.cummax#torch.cummax", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nmaximum of elements of `input` in the dimension `dim`. And `indices` is the\nindex location of each maximum value found in the dimension `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cummin()", "path": "generated/torch.cummin#torch.cummin", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nminimum of elements of `input` in the dimension `dim`. And `indices` is the\nindex location of each maximum value found in the dimension `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cumprod()", "path": "generated/torch.cumprod#torch.cumprod", "type": "torch", "text": "\nReturns the cumulative product of elements of `input` in the dimension `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.cumsum()", "path": "generated/torch.cumsum#torch.cumsum", "type": "torch", "text": "\nReturns the cumulative sum of elements of `input` in the dimension `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.deg2rad()", "path": "generated/torch.deg2rad#torch.deg2rad", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` converted from\nangles in degrees to radians.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.dequantize()", "path": "generated/torch.dequantize#torch.dequantize", "type": "torch", "text": "\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.det()", "path": "generated/torch.det#torch.det", "type": "torch", "text": "\nCalculates determinant of a square matrix or batches of square matrices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.diag()", "path": "generated/torch.diag#torch.diag", "type": "torch", "text": "\nThe argument `diagonal` controls which diagonal to consider:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.diagflat()", "path": "generated/torch.diagflat#torch.diagflat", "type": "torch", "text": "\nThe argument `offset` controls which diagonal to consider:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.diagonal()", "path": "generated/torch.diagonal#torch.diagonal", "type": "torch", "text": "\nReturns a partial view of `input` with the its diagonal elements with respect\nto `dim1` and `dim2` appended as a dimension at the end of the shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.diag_embed()", "path": "generated/torch.diag_embed#torch.diag_embed", "type": "torch", "text": "\nCreates a tensor whose diagonals of certain 2D planes (specified by `dim1` and\n`dim2`) are filled by `input`. To facilitate creating batched diagonal\nmatrices, the 2D planes formed by the last two dimensions of the returned\ntensor are chosen by default.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.diff()", "path": "generated/torch.diff#torch.diff", "type": "torch", "text": "\nComputes the n-th forward difference along the given dimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.digamma()", "path": "generated/torch.digamma#torch.digamma", "type": "torch", "text": "\nComputes the logarithmic derivative of the gamma function on `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.dist()", "path": "generated/torch.dist#torch.dist", "type": "torch", "text": "\nReturns the p-norm of (`input` \\- `other`)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed", "path": "distributed", "type": "torch.distributed", "text": "\nNote\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook just calls `allreduce` using `GradBucket` tensors.\nOnce gradient tensors are aggregated across all workers, its `then` callback\ntakes the mean and returns the result. If user registers this hook, DDP\nresults is expected to be same as the case where no hook was registered.\nHence, this won\u2019t change behavior of DDP and user can use this as a reference\nor modify this hook to log useful information or any other purposes while\nunaffecting DDP behavior.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements a simple gradient compression approach\nthat converts `GradBucket` tensors whose type is assumed to be `torch.float32`\nto half-precision floating point format (`torch.float16`). It allreduces those\n`float16` gradient tensors. Once compressed gradient tensors are allreduced,\nits then callback called `decompress` converts the aggregated result back to\n`float32` and takes the mean.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements a simplified PowerSGD gradient\ncompression algorithm described in the paper. This variant does not compress\nthe gradients layer by layer, but instead compresses the flattened input\ntensor that batches all the gradients. Therefore, it is faster than\n`powerSGD_hook()`, but usually results in a much lower accuracy, unless\n`matrix_approximation_rank` is 1.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": "\nStores both the algorithm\u2019s hyperparameters and the internal state for all the\ngradients during the training. Particularly, `matrix_approximation_rank` and\n`start_powerSGD_iter` are the main hyperparameters that should be tuned by the\nuser. For performance, we suggest to keep binary hyperparameters\n`use_error_feedback` and `warm_start` on.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements PowerSGD gradient compression algorithm\ndescribed in the paper. Once gradient tensors are aggregated across all\nworkers, this hook applies compression as follows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "torch.distributed", "text": "\nGathers tensors from the whole group in a list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "torch.distributed", "text": "\nGathers tensors from the whole group in a list. Each tensor in `tensor_list`\nshould reside on a separate GPU\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "torch.distributed", "text": "\nGathers picklable objects from the whole group into a list. Similar to\n`all_gather()`, but Python objects can be passed in. Note that the object must\nbe picklable in order to be gathered.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines in such a way that all get the\nfinal result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines in such a way that all get the\nfinal result. This function reduces a number of tensors on every node, while\neach tensor resides on different GPUs. Therefore, the input tensor in the\ntensor list needs to be GPU tensors. Also, each tensor in the tensor list\nneeds to reside on a different GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "torch.distributed", "text": "\nEach process scatters list of input tensors to all processes in a group and\nreturn gathered list of tensors in output list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC Framework", "text": "\nKicks off the distributed backward pass using the provided roots. This\ncurrently implements the FAST mode algorithm which assumes all RPC messages\nsent in the same distributed autograd context across workers would be part of\nthe autograd graph during the backward pass.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC Framework", "text": "\nContext object to wrap forward and backward passes when using distributed\nautograd. The `context_id` generated in the `with` statement is required to\nuniquely identify a distributed backward pass on all workers. Each worker\nstores metadata associated with this `context_id`, which is required to\ncorrectly execute a distributed autograd pass.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC Framework", "text": "\nRetrieves a map from Tensor to the appropriate gradient for that Tensor\naccumulated in the provided context corresponding to the given `context_id` as\npart of the distributed autograd backward pass.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "torch.distributed", "text": "\nAn enum-like class of available backends: GLOO, NCCL, MPI, and other\nregistered backends.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "torch.distributed", "text": "\nSynchronizes all processes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "torch.distributed", "text": "\nBroadcasts the tensor to the whole group.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "torch.distributed", "text": "\nBroadcasts the tensor to the whole group with multiple GPU tensors per node.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "torch.distributed", "text": "\nBroadcasts picklable objects in `object_list` to the whole group. Similar to\n`broadcast()`, but Python objects can be passed in. Note that all objects in\n`object_list` must be picklable in order to be broadcasted.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "torch.distributed", "text": "\nA store implementation that uses a file to store the underlying key-value\npairs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "torch.distributed", "text": "\nGathers a list of tensors in a single process.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "torch.distributed", "text": "\nGathers picklable objects from the whole group in a single process. Similar to\n`gather()`, but Python objects can be passed in. Note that the object must be\npicklable in order to be gathered.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "torch.distributed", "text": "\nReturns the backend of the given process group.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "torch.distributed", "text": "\nReturns the rank of current process group\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "torch.distributed", "text": "\nReturns the number of processes in the current process group\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "torch.distributed", "text": "\nA thread-safe store implementation based on an underlying hashmap. This store\ncan be used within the same process (for example, by other threads), but\ncannot be used across processes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "torch.distributed", "text": "\nInitializes the default distributed process group, and this will also\ninitialize the distributed package.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "torch.distributed", "text": "\nReceives a tensor asynchronously.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "torch.distributed", "text": "\nSends a tensor asynchronously.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "torch.distributed", "text": "\nReturns `True` if the distributed package is available. Otherwise,\n`torch.distributed` does not expose any other APIs. Currently,\n`torch.distributed` is available on Linux, MacOS and Windows. Set\n`USE_DISTRIBUTED=1` to enable it when building PyTorch from source. Currently,\nthe default value is `USE_DISTRIBUTED=1` for Linux and Windows,\n`USE_DISTRIBUTED=0` for MacOS.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "torch.distributed", "text": "\nChecking if the default process group has been initialized\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "torch.distributed", "text": "\nChecks if the MPI backend is available.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "torch.distributed", "text": "\nChecks if the NCCL backend is available.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "torch.distributed", "text": "\nCreates a new distributed group.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "rpc#torch.distributed.optim.DistributedOptimizer", "type": "Distributed RPC Framework", "text": "\nDistributedOptimizer takes remote references to parameters scattered across\nworkers and applies the given optimizer locally for each parameter.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "rpc#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed RPC Framework", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Pipeline Parallelism", "text": "\nWraps an arbitrary `nn.Sequential` module to train on using synchronous\npipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on\na single GPU, pipeline parallelism is a useful technique to employ for\ntraining.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Pipeline Parallelism", "text": "\nProcesses a single input mini-batch through the pipe and returns an `RRef`\npointing to the output. `Pipe` is a fairly transparent module wrapper. It\ndoesn\u2019t modify the input and output signature of the underlying module. But\nthere\u2019s type restriction. Input and output have to be a `Tensor` or a sequence\nof tensors. This restriction is applied at partition boundaries too.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Pipeline Parallelism", "text": "\nThe command to pop a skip tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Pipeline Parallelism", "text": "\nThe decorator to define a `nn.Module` with skip connections. Decorated modules\nare called \u201cskippable\u201d. This functionality works perfectly fine even when the\nmodule is not wrapped by `Pipe`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Pipeline Parallelism", "text": "\nThe command to stash a skip tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Pipeline Parallelism", "text": "\nVerifies if the underlying skippable modules satisfy integrity.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "torch.distributed", "text": "\nA wrapper around any of the 3 key-value stores (`TCPStore`, `FileStore`, and\n`HashStore`) that adds a prefix to each key inserted to the store.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "torch.distributed", "text": "\nReceives a tensor synchronously.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "torch.distributed", "text": "\nAn enum-like class for available reduction operations: `SUM`, `PRODUCT`,\n`MIN`, `MAX`, `BAND`, `BOR`, and `BXOR`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "torch.distributed", "text": "\nReduces the tensor data on multiple GPUs across all machines. Each tensor in\n`tensor_list` should reside on a separate GPU\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "torch.distributed", "text": "\nDeprecated enum-like class for reduction operations: `SUM`, `PRODUCT`, `MIN`,\nand `MAX`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "torch.distributed", "text": "\nReduces, then scatters a list of tensors to all processes in a group.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "torch.distributed", "text": "\nReduce and scatter a list of tensors to the whole group. Only nccl backend is\ncurrently supported.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC Framework", "text": "\nAn enum class of available backends.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC Framework", "text": "\nA decorator for a function indicating that the return value of the function is\nguaranteed to be a `Future` object and this function can run asynchronously on\nthe RPC callee. More specifically, the callee extracts the `Future` returned\nby the wrapped function and installs subsequent processing steps as a callback\nto that `Future`. The installed callback will read the value from the `Future`\nwhen completed and send the value back as the RPC response. That also means\nthe returned `Future` only exists on the callee side and is never sent through\nRPC. This decorator is useful when the wrapped function\u2019s (`fn`) execution\nneeds to pause and resume due to, e.g., containing `rpc_async()` or waiting\nfor other signals.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC Framework", "text": "\nGet `WorkerInfo` of a given worker name. Use this `WorkerInfo` to avoid\npassing an expensive string on every invocation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC Framework", "text": "\nInitializes RPC primitives such as the local RPC agent and distributed\nautograd, which immediately makes the current process ready to send and\nreceive RPCs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nThe backend options class for `ProcessGroupAgent`, which is derived from\n`RpcBackendOptions`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads", "type": "Distributed RPC Framework", "text": "\nThe number of threads in the thread-pool used by ProcessGroupAgent.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC Framework", "text": "\nMake a remote call to run `func` on worker `to` and return an `RRef` to the\nresult value immediately. Worker `to` will be the owner of the returned\n`RRef`, and the worker calling `remote` is a user. The owner manages the\nglobal reference count of its `RRef`, and the owner `RRef` is only destructed\nwhen globally there are no living references to it.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nAn abstract structure encapsulating the options passed into the RPC backend.\nAn instance of this class can be passed in to `init_rpc()` in order to\ninitialize RPC with specific configurations, such as the RPC timeout and\n`init_method` to be used.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC Framework", "text": "\nMake a non-blocking RPC call to run function `func` on worker `to`. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe. This method will immediately return a `Future` that can\nbe awaited on.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC Framework", "text": "\nMake a blocking RPC call to run function `func` on worker `to`. RPC messages\nare sent and received in parallel to execution of Python code. This method is\nthread-safe.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef", "path": "rpc#torch.distributed.rpc.RRef", "type": "Distributed RPC Framework", "text": "\nRuns the backward pass using the RRef as the root of the backward pass. If\n`dist_autograd_ctx_id` is provided, we perform a distributed backward pass\nusing the provided ctx_id starting from the owner of the RRef. In this case,\n`get_gradients()` should be used to retrieve the gradients. If\n`dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd\ngraph and we only perform a local backward pass. In the local case, the node\ncalling this API has to be the owner of the RRef. The value of the RRef is\nexpected to be a scalar Tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.backward()", "path": "rpc#torch.distributed.rpc.RRef.backward", "type": "Distributed RPC Framework", "text": "\nRuns the backward pass using the RRef as the root of the backward pass. If\n`dist_autograd_ctx_id` is provided, we perform a distributed backward pass\nusing the provided ctx_id starting from the owner of the RRef. In this case,\n`get_gradients()` should be used to retrieve the gradients. If\n`dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd\ngraph and we only perform a local backward pass. In the local case, the node\ncalling this API has to be the owner of the RRef. The value of the RRef is\nexpected to be a scalar Tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.RRef.confirmed_by_owner", "type": "Distributed RPC Framework", "text": "\nReturns whether this `RRef` has been confirmed by the owner. `OwnerRRef`\nalways returns true, while `UserRRef` only returns true when the owner knowns\nabout this `UserRRef`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.is_owner()", "path": "rpc#torch.distributed.rpc.RRef.is_owner", "type": "Distributed RPC Framework", "text": "\nReturns whether or not the current node is the owner of this `RRef`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.local_value()", "path": "rpc#torch.distributed.rpc.RRef.local_value", "type": "Distributed RPC Framework", "text": "\nIf the current node is the owner, returns a reference to the local value.\nOtherwise, throws an exception.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.owner()", "path": "rpc#torch.distributed.rpc.RRef.owner", "type": "Distributed RPC Framework", "text": "\nReturns worker information of the node that owns this `RRef`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.owner_name()", "path": "rpc#torch.distributed.rpc.RRef.owner_name", "type": "Distributed RPC Framework", "text": "\nReturns worker name of the node that owns this `RRef`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.remote()", "path": "rpc#torch.distributed.rpc.RRef.remote", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch a `remote` using the owner of the RRef\nas the destination to run functions on the object referenced by this RRef.\nMore specifically, `rref.remote().func_name(*args, **kwargs)` is the same as\nthe following:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.rpc_async()", "path": "rpc#torch.distributed.rpc.RRef.rpc_async", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch an `rpc_async` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the\nsame as the following:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.RRef.rpc_sync", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch an `rpc_sync` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the\nsame as the following:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.to_here()", "path": "rpc#torch.distributed.rpc.RRef.to_here", "type": "Distributed RPC Framework", "text": "\nBlocking call that copies the value of the RRef from the owner to the local\nnode and returns it. If the current node is the owner, returns a reference to\nthe local value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC Framework", "text": "\nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This\nstops the local agent from accepting outstanding requests, and shuts down the\nRPC framework by terminating all RPC threads. If `graceful=True`, this will\nblock until all local and remote RPC processes reach this method and wait for\nall outstanding work to complete. Otherwise, if `graceful=False`, this is a\nlocal shutdown, and it does not wait for other RPC processes to reach this\nmethod.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nThe backend options for `TensorPipeAgent`, derived from `RpcBackendOptions`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC Framework", "text": "\nThe device map locations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC Framework", "text": "\nThe number of threads in the thread-pool used by `TensorPipeAgent` to execute\nrequests.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC Framework", "text": "\nSet device mapping between each RPC caller and callee pair. This function can\nbe called multiple times to incrementally add device placement configurations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC Framework", "text": "\nA structure that encapsulates information of a worker in the system. Contains\nthe name and ID of the worker. This class is not meant to be constructed\ndirectly, rather, an instance can be retrieved through `get_worker_info()` and\nthe result can be passed in to functions such as `rpc_sync()`, `rpc_async()`,\n`remote()` to avoid copying a string on every invocation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo.id()", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC Framework", "text": "\nGlobally unique id to identify the worker.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo.name()", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC Framework", "text": "\nThe name of the worker.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "torch.distributed", "text": "\nScatters a list of tensors to all processes in a group.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "torch.distributed", "text": "\nScatters picklable objects in `scatter_object_input_list` to the whole group.\nSimilar to `scatter()`, but Python objects can be passed in. On each rank, the\nscattered object will be stored as the first element of\n`scatter_object_output_list`. Note that all objects in\n`scatter_object_input_list` must be picklable in order to be scattered.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "torch.distributed", "text": "\nSends a tensor synchronously.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "torch.distributed", "text": "\nBase class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (`TCPStore`, `FileStore`, and `HashStore`).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "torch.distributed", "text": "\nThe first call to add for a given `key` creates a counter associated with\n`key` in the store, initialized to `amount`. Subsequent calls to add with the\nsame `key` increment the counter by the specified `amount`. Calling `add()`\nwith a key that has already been set in the store by `set()` will result in an\nexception.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "torch.distributed", "text": "\nDeletes the key-value pair associated with `key` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "torch.distributed", "text": "\nRetrieves the value associated with the given `key` in the store. If `key` is\nnot present in the store, the function will wait for `timeout`, which is\ndefined when initializing the store, before throwing an exception.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "torch.distributed", "text": "\nReturns the number of keys set in the store. Note that this number will\ntypically be one greater than the number of keys added by `set()` and `add()`\nsince one key is used to coordinate all the workers using the store.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "torch.distributed", "text": "\nInserts the key-value pair into the store based on the supplied `key` and\n`value`. If `key` already exists in the store, it will overwrite the old value\nwith the new supplied `value`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "torch.distributed", "text": "\nSets the store\u2019s default timeout. This timeout is used during initialization\nand in `wait()` and `get()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "torch.distributed", "text": "\nOverloaded function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "torch.distributed", "text": "\nA TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as `set()` to insert a key-value pair, `get()` to\nretrieve a key-value pair, etc.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions", "path": "distributions", "type": "torch.distributions", "text": "\nThe `distributions` package contains parameterizable probability distributions\nand sampling functions. This allows the construction of stochastic computation\ngraphs and stochastic gradient estimators for optimization. This package\ngenerally follows the design of the TensorFlow Distributions package.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.mean()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.variance()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.concentration0()", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.concentration1()", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.mean()", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.variance()", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.mean()", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.param_shape()", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.support()", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.variance()", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.mean()", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.param_shape()", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.support()", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.variance()", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.mean()", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.variance()", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "torch.distributions", "text": "\nBases: `torch.distributions.gamma.Gamma`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.df()", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Cat`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "torch.distributions", "text": "\nAbstract base class for constraints.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "torch.distributions", "text": "\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._DependentProperty`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._GreaterThan`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._GreaterThanEq`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._HalfOpenInterval`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._IndependentConstraint`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._IntegerInterval`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Interval`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._LessThan`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Multinomial`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Stack`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "torch.distributions", "text": "\nRegistry to link constraints to transforms.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "torch.distributions", "text": "\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.mean()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.variance()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "torch.distributions", "text": "\nBases: `object`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.arg_constraints()", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "torch.distributions", "text": "\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.batch_shape()", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "torch.distributions", "text": "\nReturns the shape over which parameters are batched.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "torch.distributions", "text": "\nReturns the cumulative density/mass function evaluated at `value`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "torch.distributions", "text": "\nReturns entropy of distribution, batched over batch_shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "torch.distributions", "text": "\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.event_shape()", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "torch.distributions", "text": "\nReturns the shape of a single sample (without batching).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "torch.distributions", "text": "\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "torch.distributions", "text": "\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "torch.distributions", "text": "\nReturns the log of the probability density/mass function evaluated at `value`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.mean()", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "torch.distributions", "text": "\nReturns the mean of the distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "torch.distributions", "text": "\nReturns perplexity of distribution, batched over batch_shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "torch.distributions", "text": "\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "torch.distributions", "text": "\nSets whether validation is enabled or disabled.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.stddev()", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "torch.distributions", "text": "\nReturns the standard deviation of the distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.support()", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "torch.distributions", "text": "\nReturns a `Constraint` object representing this distribution\u2019s support.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.variance()", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "torch.distributions", "text": "\nReturns the variance of the distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.mean()", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.stddev()", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.variance()", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "torch.distributions", "text": "\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.mean()", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.variance()", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.mean()", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.variance()", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.mean()", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.stddev()", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.variance()", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.mean()", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.scale()", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.variance()", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.has_enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.has_rsample()", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.mean()", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.support()", "path": "distributions#torch.distributions.independent.Independent.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.variance()", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "torch.distributions", "text": "\nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two\ndistributions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "torch.distributions", "text": "\nDecorator to register a pairwise function with `kl_divergence()`. Usage:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.mean()", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.stddev()", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.variance()", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.loc()", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.mean()", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.scale()", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.variance()", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.logits()", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.mean()", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.param_shape()", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.probs()", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.support()", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.variance()", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.mean()", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.stddev()", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.variance()", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.mean()", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.support()", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.variance()", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.mean()", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.variance()", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.mean()", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.variance()", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "torch.distributions", "text": "\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "torch.distributions", "text": "\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "torch.distributions", "text": "\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=\u2223x\u2223y = |x| .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "torch.distributions", "text": "\nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} +\n\\text{scale} \\times x .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "torch.distributions", "text": "\nComposes multiple transforms in a chain. The transforms being composed are\nresponsible for caching.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "torch.distributions", "text": "\nTransforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into\nthe Cholesky factor of a D-dimension correlation matrix. This Cholesky factor\nis a lower triangular matrix with positive diagonals and unit Euclidean norm\nfor each row. The transform is processed as follows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=exp\u2061(x)y = \\exp(x) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "torch.distributions", "text": "\nWrapper around another transform to treat `reinterpreted_batch_ndims`-many\nextra of the right most dimensions as dependent. This has no effect on the\nforward or backward transforms, but does sum out\n`reinterpreted_batch_ndims`-many of the rightmost dimensions in\n`log_abs_det_jacobian()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=xexponenty = x^{\\text{exponent}} .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "torch.distributions", "text": "\nUnit Jacobian transform to reshape the rightmost part of a tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and\nx=logit(y)x = \\text{logit}(y) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)\nthen normalizing.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "torch.distributions", "text": "\nTransform functor that applies a sequence of transforms `tseq` component-wise\nto each submatrix at `dim` in a way compatible with `torch.stack()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained space to the simplex of one additional dimension\nvia a stick-breaking process.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "torch.distributions", "text": "\nAbstract class for invertable transformations with computable log det\njacobians. They are primarily used in\n`torch.distributions.TransformedDistribution`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "torch.distributions", "text": "\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.inv()", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "torch.distributions", "text": "\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "torch.distributions", "text": "\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "torch.distributions", "text": "\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.sign()", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "torch.distributions", "text": "\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.mean()", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.stddev()", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.support()", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.variance()", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.mean()", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "torch.distributions", "text": "\nThe provided mean is the circular one.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "torch.distributions", "text": "\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "torch.distributions", "text": "\nThe provided variance is the circular one.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.mean()", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.variance()", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "torch.distributions", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.div()", "path": "generated/torch.div#torch.div", "type": "torch", "text": "\nDivides each element of the input `input` by the corresponding element of\n`other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.divide()", "path": "generated/torch.divide#torch.divide", "type": "torch", "text": "\nAlias for `torch.div()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.dot()", "path": "generated/torch.dot#torch.dot", "type": "torch", "text": "\nComputes the dot product of two 1D tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.dstack()", "path": "generated/torch.dstack#torch.dstack", "type": "torch", "text": "\nStack tensors in sequence depthwise (along third axis).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.eig()", "path": "generated/torch.eig#torch.eig", "type": "torch", "text": "\nComputes the eigenvalues and eigenvectors of a real square matrix.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.einsum()", "path": "generated/torch.einsum#torch.einsum", "type": "torch", "text": "\nSums the product of the elements of the input `operands` along dimensions\nspecified using a notation based on the Einstein summation convention.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.empty()", "path": "generated/torch.empty#torch.empty", "type": "torch", "text": "\nReturns a tensor filled with uninitialized data. The shape of the tensor is\ndefined by the variable argument `size`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.empty_like()", "path": "generated/torch.empty_like#torch.empty_like", "type": "torch", "text": "\nReturns an uninitialized tensor with the same size as `input`.\n`torch.empty_like(input)` is equivalent to `torch.empty(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.empty_strided()", "path": "generated/torch.empty_strided#torch.empty_strided", "type": "torch", "text": "\nReturns a tensor filled with uninitialized data. The shape and strides of the\ntensor is defined by the variable argument `size` and `stride` respectively.\n`torch.empty_strided(size, stride)` is equivalent to\n`torch.empty(size).as_strided(size, stride)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "torch", "text": "\nContext-manager that enables gradient calculation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.eq()", "path": "generated/torch.eq#torch.eq", "type": "torch", "text": "\nComputes element-wise equality\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.equal()", "path": "generated/torch.equal#torch.equal", "type": "torch", "text": "\n`True` if two tensors have the same size and elements, `False` otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.erf()", "path": "generated/torch.erf#torch.erf", "type": "torch", "text": "\nComputes the error function of each element. The error function is defined as\nfollows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.erfc()", "path": "generated/torch.erfc#torch.erfc", "type": "torch", "text": "\nComputes the complementary error function of each element of `input`. The\ncomplementary error function is defined as follows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.erfinv()", "path": "generated/torch.erfinv#torch.erfinv", "type": "torch", "text": "\nComputes the inverse error function of each element of `input`. The inverse\nerror function is defined in the range (\u22121,1)(-1, 1) as:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.exp()", "path": "generated/torch.exp#torch.exp", "type": "torch", "text": "\nReturns a new tensor with the exponential of the elements of the input tensor\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.exp2()", "path": "generated/torch.exp2#torch.exp2", "type": "torch", "text": "\nComputes the base two exponential function of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.expm1()", "path": "generated/torch.expm1#torch.expm1", "type": "torch", "text": "\nReturns a new tensor with the exponential of the elements minus 1 of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.eye()", "path": "generated/torch.eye#torch.eye", "type": "torch", "text": "\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fake_quantize_per_channel_affine()", "path": "generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine", "type": "torch", "text": "\nReturns a new tensor with the data in `input` fake quantized per channel using\n`scale`, `zero_point`, `quant_min` and `quant_max`, across the channel\nspecified by `axis`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fake_quantize_per_tensor_affine()", "path": "generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine", "type": "torch", "text": "\nReturns a new tensor with the data in `input` fake quantized using `scale`,\n`zero_point`, `quant_min` and `quant_max`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft", "path": "fft", "type": "torch.fft", "text": "\nDiscrete Fourier transforms and related functions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.fft()", "path": "fft#torch.fft.fft", "type": "torch.fft", "text": "\nComputes the one dimensional discrete Fourier transform of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.fft2()", "path": "fft#torch.fft.fft2", "type": "torch.fft", "text": "\nComputes the 2 dimensional discrete Fourier transform of `input`. Equivalent\nto `fftn()` but FFTs only the last two dimensions by default.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.fftfreq()", "path": "fft#torch.fft.fftfreq", "type": "torch.fft", "text": "\nComputes the discrete Fourier Transform sample frequencies for a signal of\nsize `n`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.fftn()", "path": "fft#torch.fft.fftn", "type": "torch.fft", "text": "\nComputes the N dimensional discrete Fourier transform of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.fftshift()", "path": "fft#torch.fft.fftshift", "type": "torch.fft", "text": "\nReorders n-dimensional FFT data, as provided by `fftn()`, to have negative\nfrequency terms first.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.hfft()", "path": "fft#torch.fft.hfft", "type": "torch.fft", "text": "\nComputes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric `input` signal.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.ifft()", "path": "fft#torch.fft.ifft", "type": "torch.fft", "text": "\nComputes the one dimensional inverse discrete Fourier transform of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.ifft2()", "path": "fft#torch.fft.ifft2", "type": "torch.fft", "text": "\nComputes the 2 dimensional inverse discrete Fourier transform of `input`.\nEquivalent to `ifftn()` but IFFTs only the last two dimensions by default.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.ifftn()", "path": "fft#torch.fft.ifftn", "type": "torch.fft", "text": "\nComputes the N dimensional inverse discrete Fourier transform of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.ifftshift()", "path": "fft#torch.fft.ifftshift", "type": "torch.fft", "text": "\nInverse of `fftshift()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.ihfft()", "path": "fft#torch.fft.ihfft", "type": "torch.fft", "text": "\nComputes the inverse of `hfft()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.irfft()", "path": "fft#torch.fft.irfft", "type": "torch.fft", "text": "\nComputes the inverse of `rfft()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.irfft2()", "path": "fft#torch.fft.irfft2", "type": "torch.fft", "text": "\nComputes the inverse of `rfft2()`. Equivalent to `irfftn()` but IFFTs only the\nlast two dimensions by default.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.irfftn()", "path": "fft#torch.fft.irfftn", "type": "torch.fft", "text": "\nComputes the inverse of `rfftn()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.rfft()", "path": "fft#torch.fft.rfft", "type": "torch.fft", "text": "\nComputes the one dimensional Fourier transform of real-valued `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.rfft2()", "path": "fft#torch.fft.rfft2", "type": "torch.fft", "text": "\nComputes the 2-dimensional discrete Fourier transform of real `input`.\nEquivalent to `rfftn()` but FFTs only the last two dimensions by default.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.rfftfreq()", "path": "fft#torch.fft.rfftfreq", "type": "torch.fft", "text": "\nComputes the sample frequencies for `rfft()` with a signal of size `n`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fft.rfftn()", "path": "fft#torch.fft.rfftn", "type": "torch.fft", "text": "\nComputes the N-dimensional discrete Fourier transform of real `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fix()", "path": "generated/torch.fix#torch.fix", "type": "torch", "text": "\nAlias for `torch.trunc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.flatten()", "path": "generated/torch.flatten#torch.flatten", "type": "torch", "text": "\nFlattens `input` by reshaping it into a one-dimensional tensor. If `start_dim`\nor `end_dim` are passed, only dimensions starting with `start_dim` and ending\nwith `end_dim` are flattened. The order of elements in `input` is unchanged.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.flip()", "path": "generated/torch.flip#torch.flip", "type": "torch", "text": "\nReverse the order of a n-D tensor along given axis in dims.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fliplr()", "path": "generated/torch.fliplr#torch.fliplr", "type": "torch", "text": "\nFlip tensor in the left/right direction, returning a new tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.flipud()", "path": "generated/torch.flipud#torch.flipud", "type": "torch", "text": "\nFlip tensor in the up/down direction, returning a new tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "torch.Storage", "text": "\nCasts this storage to bfloat16 type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.bfloat16()", "path": "storage#torch.FloatStorage.bfloat16", "type": "torch.Storage", "text": "\nCasts this storage to bfloat16 type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.bool()", "path": "storage#torch.FloatStorage.bool", "type": "torch.Storage", "text": "\nCasts this storage to bool type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.byte()", "path": "storage#torch.FloatStorage.byte", "type": "torch.Storage", "text": "\nCasts this storage to byte type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.char()", "path": "storage#torch.FloatStorage.char", "type": "torch.Storage", "text": "\nCasts this storage to char type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.clone()", "path": "storage#torch.FloatStorage.clone", "type": "torch.Storage", "text": "\nReturns a copy of this storage\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.complex_double()", "path": "storage#torch.FloatStorage.complex_double", "type": "torch.Storage", "text": "\nCasts this storage to complex double type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.complex_float()", "path": "storage#torch.FloatStorage.complex_float", "type": "torch.Storage", "text": "\nCasts this storage to complex float type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.copy_()", "path": "storage#torch.FloatStorage.copy_", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.cpu()", "path": "storage#torch.FloatStorage.cpu", "type": "torch.Storage", "text": "\nReturns a CPU copy of this storage if it\u2019s not already on the CPU\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.cuda()", "path": "storage#torch.FloatStorage.cuda", "type": "torch.Storage", "text": "\nReturns a copy of this object in CUDA memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.data_ptr()", "path": "storage#torch.FloatStorage.data_ptr", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.device", "path": "storage#torch.FloatStorage.device", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.double()", "path": "storage#torch.FloatStorage.double", "type": "torch.Storage", "text": "\nCasts this storage to double type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.element_size()", "path": "storage#torch.FloatStorage.element_size", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.fill_()", "path": "storage#torch.FloatStorage.fill_", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.float()", "path": "storage#torch.FloatStorage.float", "type": "torch.Storage", "text": "\nCasts this storage to float type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.from_buffer()", "path": "storage#torch.FloatStorage.from_buffer", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.from_file()", "path": "storage#torch.FloatStorage.from_file", "type": "torch.Storage", "text": "\nIf `shared` is `True`, then memory is shared between all processes. All\nchanges are written to the file. If `shared` is `False`, then the changes on\nthe storage do not affect the file.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.get_device()", "path": "storage#torch.FloatStorage.get_device", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.half()", "path": "storage#torch.FloatStorage.half", "type": "torch.Storage", "text": "\nCasts this storage to half type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.int()", "path": "storage#torch.FloatStorage.int", "type": "torch.Storage", "text": "\nCasts this storage to int type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_cuda", "path": "storage#torch.FloatStorage.is_cuda", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_pinned()", "path": "storage#torch.FloatStorage.is_pinned", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_shared()", "path": "storage#torch.FloatStorage.is_shared", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_sparse", "path": "storage#torch.FloatStorage.is_sparse", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.long()", "path": "storage#torch.FloatStorage.long", "type": "torch.Storage", "text": "\nCasts this storage to long type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.new()", "path": "storage#torch.FloatStorage.new", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.pin_memory()", "path": "storage#torch.FloatStorage.pin_memory", "type": "torch.Storage", "text": "\nCopies the storage to pinned memory, if it\u2019s not already pinned.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.resize_()", "path": "storage#torch.FloatStorage.resize_", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.share_memory_()", "path": "storage#torch.FloatStorage.share_memory_", "type": "torch.Storage", "text": "\nMoves the storage to shared memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.short()", "path": "storage#torch.FloatStorage.short", "type": "torch.Storage", "text": "\nCasts this storage to short type\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.size()", "path": "storage#torch.FloatStorage.size", "type": "torch.Storage", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.tolist()", "path": "storage#torch.FloatStorage.tolist", "type": "torch.Storage", "text": "\nReturns a list containing the elements of this storage\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.FloatStorage.type()", "path": "storage#torch.FloatStorage.type", "type": "torch.Storage", "text": "\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.float_power()", "path": "generated/torch.float_power#torch.float_power", "type": "torch", "text": "\nRaises `input` to the power of `exponent`, elementwise, in double precision.\nIf neither input is complex returns a `torch.float64` tensor, and if one or\nmore inputs is complex returns a `torch.complex128` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.floor()", "path": "generated/torch.floor#torch.floor", "type": "torch", "text": "\nReturns a new tensor with the floor of the elements of `input`, the largest\ninteger less than or equal to each element.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.floor_divide()", "path": "generated/torch.floor_divide#torch.floor_divide", "type": "torch", "text": "\nWarning\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fmax()", "path": "generated/torch.fmax#torch.fmax", "type": "torch", "text": "\nComputes the element-wise maximum of `input` and `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fmin()", "path": "generated/torch.fmin#torch.fmin", "type": "torch", "text": "\nComputes the element-wise minimum of `input` and `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fmod()", "path": "generated/torch.fmod#torch.fmod", "type": "torch", "text": "\nComputes the element-wise remainder of division.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.frac()", "path": "generated/torch.frac#torch.frac", "type": "torch", "text": "\nComputes the fractional portion of each element in `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.from_numpy()", "path": "generated/torch.from_numpy#torch.from_numpy", "type": "torch", "text": "\nCreates a `Tensor` from a `numpy.ndarray`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.full()", "path": "generated/torch.full#torch.full", "type": "torch", "text": "\nCreates a tensor of size `size` filled with `fill_value`. The tensor\u2019s dtype\nis inferred from `fill_value`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.full_like()", "path": "generated/torch.full_like#torch.full_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` filled with `fill_value`.\n`torch.full_like(input, fill_value)` is equivalent to\n`torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout,\ndevice=input.device)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures", "path": "futures", "type": "torch.futures", "text": "\nWarning\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "torch.futures", "text": "\nCollects the provided `Future` objects into a single combined `Future` that is\ncompleted when all of the sub-futures are completed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "torch.futures", "text": "\nWrapper around a `torch._C.Future` which encapsulates an asynchronous\nexecution of a callable, e.g. `rpc_async()`. It also exposes a set of APIs to\nadd callback functions and set results.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "torch.futures", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "torch.futures", "text": "\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "torch.futures", "text": "\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "torch.futures", "text": "\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "torch.futures", "text": "\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "torch.futures", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "torch.futures", "text": "\nBlock until the value of this `Future` is ready.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "torch.futures", "text": "\nWaits for all provided futures to be complete, and returns the list of\ncompleted values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx", "path": "fx", "type": "torch.fx", "text": "\nThis feature is under a Beta release and its API may change.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "torch.fx", "text": "\n`Graph` is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of `Node` s, each representing callsites (or other\nsyntactic constructs). The list of `Node` s, taken together, constitute a\nvalid Python function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "torch.fx", "text": "\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "torch.fx", "text": "\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "torch.fx", "text": "\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "torch.fx", "text": "\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "torch.fx", "text": "\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "torch.fx", "text": "\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "torch.fx", "text": "\nCopy all nodes from a given graph into `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "torch.fx", "text": "\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "torch.fx", "text": "\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "torch.fx", "text": "\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.nodes()", "path": "fx#torch.fx.Graph.nodes", "type": "torch.fx", "text": "\nGet the list of Nodes that constitute this Graph.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "torch.fx", "text": "\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "torch.fx", "text": "\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "torch.fx", "text": "\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "torch.fx", "text": "\nPrints the intermediate representation of the graph in tabular format.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "torch.fx", "text": "\nTurn this `Graph` into valid Python code.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "torch.fx", "text": "\nConstruct an empty Graph.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "torch.fx", "text": "\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\n`graph` attribute, as well as `code` and `forward` attributes generated from\nthat `graph`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.code()", "path": "fx#torch.fx.GraphModule.code", "type": "torch.fx", "text": "\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.graph()", "path": "fx#torch.fx.GraphModule.graph", "type": "torch.fx", "text": "\nReturn the `Graph` underlying this `GraphModule`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "torch.fx", "text": "\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "torch.fx", "text": "\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "torch.fx", "text": "\nConstruct a GraphModule.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "torch.fx", "text": "\nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful\nfor many things, including writing code transformations as well as analysis\npasses.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "torch.fx", "text": "\nExecute a `call_function` node and return the result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "torch.fx", "text": "\nExecute a `call_method` node and return the result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "torch.fx", "text": "\nExecute a `call_module` node and return the result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "torch.fx", "text": "\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "torch.fx", "text": "\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "torch.fx", "text": "\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "torch.fx", "text": "\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "torch.fx", "text": "\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "torch.fx", "text": "\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "torch.fx", "text": "\nRun `module` via interpretation and return the result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "torch.fx", "text": "\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "torch.fx", "text": "\n`Node` is the data structure that represents individual operations within a\n`Graph`. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each `Node` has a function specified by\nits `op` property. The `Node` semantics for each value of `op` are as follows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.all_input_nodes()", "path": "fx#torch.fx.Node.all_input_nodes", "type": "torch.fx", "text": "\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "torch.fx", "text": "\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.args()", "path": "fx#torch.fx.Node.args", "type": "torch.fx", "text": "\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.kwargs()", "path": "fx#torch.fx.Node.kwargs", "type": "torch.fx", "text": "\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.next()", "path": "fx#torch.fx.Node.next", "type": "torch.fx", "text": "\nReturns the next `Node` in the linked list of Nodes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "torch.fx", "text": "\nInsert x before this node in the list of nodes in the graph. Example:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.prev()", "path": "fx#torch.fx.Node.prev", "type": "torch.fx", "text": "\nReturns the previous `Node` in the linked list of Nodes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "torch.fx", "text": "\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "torch.fx", "text": "\n`Proxy` objects are `Node` wrappers that flow through the program during\nsymbolic tracing and record all the operations (`torch` function calls, method\ncalls, operators) that they touch into the growing FX Graph.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "torch.fx", "text": "\nMatches all possible non-overlapping sets of operators and their data\ndependencies (`pattern`) in the Graph of a GraphModule (`gm`), then replaces\neach of these matched subgraphs with another subgraph (`replacement`).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "torch.fx", "text": "\nSymbolic tracing API\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "torch.fx", "text": "\n`Tracer` is the class that implements the symbolic tracing functionality of\n`torch.fx.symbolic_trace`. A call to `symbolic_trace(m)` is equivalent to\n`Tracer().trace(m)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "torch.fx", "text": "\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "torch.fx", "text": "\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "torch.fx", "text": "\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "torch.fx", "text": "\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "torch.fx", "text": "\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "torch.fx", "text": "\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "torch.fx", "text": "\n`Transformer` is a special type of interpreter that produces a new `Module`.\nIt exposes a `transform()` method that returns the transformed `Module`.\n`Transformer` does not require arguments to run, as `Interpreter` does.\n`Transformer` works entirely symbolically.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "torch.fx", "text": "\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "torch.fx", "text": "\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "torch.fx", "text": "\nTransform `self.module` and return the transformed `GraphModule`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "torch.fx", "text": "\nThis function can be called at module-level scope to register fn_or_name as a\n\u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in\nthe FX trace instead of being traced through:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.gather()", "path": "generated/torch.gather#torch.gather", "type": "torch", "text": "\nGathers values along an axis specified by `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.gcd()", "path": "generated/torch.gcd#torch.gcd", "type": "torch", "text": "\nComputes the element-wise greatest common divisor (GCD) of `input` and\n`other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ge()", "path": "generated/torch.ge#torch.ge", "type": "torch", "text": "\nComputes input\u2265other\\text{input} \\geq \\text{other} element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator", "path": "generated/torch.generator#torch.Generator", "type": "torch", "text": "\nCreates and returns a generator object that manages the state of the algorithm\nwhich produces pseudo random numbers. Used as a keyword argument in many In-\nplace random sampling functions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator.device", "path": "generated/torch.generator#torch.Generator.device", "type": "torch", "text": "\nGenerator.device -> device\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator.get_state()", "path": "generated/torch.generator#torch.Generator.get_state", "type": "torch", "text": "\nReturns the Generator state as a `torch.ByteTensor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator.initial_seed()", "path": "generated/torch.generator#torch.Generator.initial_seed", "type": "torch", "text": "\nReturns the initial seed for generating random numbers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator.manual_seed()", "path": "generated/torch.generator#torch.Generator.manual_seed", "type": "torch", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject. It is recommended to set a large seed, i.e. a number that has a good\nbalance of 0 and 1 bits. Avoid having many 0 bits in the seed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator.seed()", "path": "generated/torch.generator#torch.Generator.seed", "type": "torch", "text": "\nGets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Generator.set_state()", "path": "generated/torch.generator#torch.Generator.set_state", "type": "torch", "text": "\nSets the Generator state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.geqrf()", "path": "generated/torch.geqrf#torch.geqrf", "type": "torch", "text": "\nThis is a low-level function for calling LAPACK directly. This function\nreturns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ger()", "path": "generated/torch.ger#torch.ger", "type": "torch", "text": "\nAlias of `torch.outer()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.get_default_dtype()", "path": "generated/torch.get_default_dtype#torch.get_default_dtype", "type": "torch", "text": "\nGet the current default floating point `torch.dtype`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.get_num_interop_threads()", "path": "generated/torch.get_num_interop_threads#torch.get_num_interop_threads", "type": "torch", "text": "\nReturns the number of threads used for inter-op parallelism on CPU (e.g. in\nJIT interpreter)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.get_num_threads()", "path": "generated/torch.get_num_threads#torch.get_num_threads", "type": "torch", "text": "\nReturns the number of threads used for parallelizing CPU operations\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.get_rng_state()", "path": "generated/torch.get_rng_state#torch.get_rng_state", "type": "torch", "text": "\nReturns the random number generator state as a `torch.ByteTensor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.greater()", "path": "generated/torch.greater#torch.greater", "type": "torch", "text": "\nAlias for `torch.gt()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.greater_equal()", "path": "generated/torch.greater_equal#torch.greater_equal", "type": "torch", "text": "\nAlias for `torch.ge()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.gt()", "path": "generated/torch.gt#torch.gt", "type": "torch", "text": "\nComputes input>other\\text{input} > \\text{other} element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hamming_window()", "path": "generated/torch.hamming_window#torch.hamming_window", "type": "torch", "text": "\nHamming window function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hann_window()", "path": "generated/torch.hann_window#torch.hann_window", "type": "torch", "text": "\nHann window function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.heaviside()", "path": "generated/torch.heaviside#torch.heaviside", "type": "torch", "text": "\nComputes the Heaviside step function for each element in `input`. The\nHeaviside step function is defined as:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.histc()", "path": "generated/torch.histc#torch.histc", "type": "torch", "text": "\nComputes the histogram of a tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hspmm()", "path": "sparse#torch.hspmm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of a sparse COO matrix `mat1` and a strided\nmatrix `mat2`. The result is a (1 + 1)-dimensional hybrid COO matrix.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hstack()", "path": "generated/torch.hstack#torch.hstack", "type": "torch", "text": "\nStack tensors in sequence horizontally (column wise).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub", "path": "hub", "type": "torch.hub", "text": "\nPytorch Hub is a pre-trained model repository designed to facilitate research\nreproducibility.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.download_url_to_file()", "path": "hub#torch.hub.download_url_to_file", "type": "torch.hub", "text": "\nDownload object at the given URL to a local path.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.get_dir()", "path": "hub#torch.hub.get_dir", "type": "torch.hub", "text": "\nGet the Torch Hub cache directory used for storing downloaded models &\nweights.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.help()", "path": "hub#torch.hub.help", "type": "torch.hub", "text": "\nShow the docstring of entrypoint `model`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.list()", "path": "hub#torch.hub.list", "type": "torch.hub", "text": "\nList all entrypoints available in `github` hubconf.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.load()", "path": "hub#torch.hub.load", "type": "torch.hub", "text": "\nLoad a model from a github repo or a local directory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.load_state_dict_from_url()", "path": "hub#torch.hub.load_state_dict_from_url", "type": "torch.hub", "text": "\nLoads the Torch serialized object at the given URL.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hub.set_dir()", "path": "hub#torch.hub.set_dir", "type": "torch.hub", "text": "\nOptionally set the Torch Hub directory used to save downloaded models &\nweights.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.hypot()", "path": "generated/torch.hypot#torch.hypot", "type": "torch", "text": "\nGiven the legs of a right triangle, return its hypotenuse.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.i0()", "path": "generated/torch.i0#torch.i0", "type": "torch", "text": "\nComputes the zeroth order modified Bessel function of the first kind for each\nelement of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.igamma()", "path": "generated/torch.igamma#torch.igamma", "type": "torch", "text": "\nComputes the regularized lower incomplete gamma function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.igammac()", "path": "generated/torch.igammac#torch.igammac", "type": "torch", "text": "\nComputes the regularized upper incomplete gamma function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.imag()", "path": "generated/torch.imag#torch.imag", "type": "torch", "text": "\nReturns a new tensor containing imaginary values of the `self` tensor. The\nreturned tensor and `self` share the same underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.index_select()", "path": "generated/torch.index_select#torch.index_select", "type": "torch", "text": "\nReturns a new tensor which indexes the `input` tensor along dimension `dim`\nusing the entries in `index` which is a `LongTensor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.initial_seed()", "path": "generated/torch.initial_seed#torch.initial_seed", "type": "torch", "text": "\nReturns the initial seed for generating random numbers as a Python `long`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.inner()", "path": "generated/torch.inner#torch.inner", "type": "torch", "text": "\nComputes the dot product for 1D tensors. For higher dimensions, sums the\nproduct of elements from `input` and `other` along their last dimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.inverse()", "path": "generated/torch.inverse#torch.inverse", "type": "torch", "text": "\nTakes the inverse of the square matrix `input`. `input` can be batches of 2D\nsquare tensors, in which case this function would return a tensor composed of\nindividual inverses.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isclose()", "path": "generated/torch.isclose#torch.isclose", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is \u201cclose\u201d to the corresponding element of `other`. Closeness is\ndefined as:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isfinite()", "path": "generated/torch.isfinite#torch.isfinite", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element is\n`finite` or not.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isinf()", "path": "generated/torch.isinf#torch.isinf", "type": "torch", "text": "\nTests if each element of `input` is infinite (positive or negative infinity)\nor not.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isnan()", "path": "generated/torch.isnan#torch.isnan", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is NaN or not. Complex values are considered NaN when either their\nreal and/or imaginary part is NaN.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isneginf()", "path": "generated/torch.isneginf#torch.isneginf", "type": "torch", "text": "\nTests if each element of `input` is negative infinity or not.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isposinf()", "path": "generated/torch.isposinf#torch.isposinf", "type": "torch", "text": "\nTests if each element of `input` is positive infinity or not.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.isreal()", "path": "generated/torch.isreal#torch.isreal", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is real-valued or not. All real-valued types are considered real.\nComplex values are considered real when their imaginary part is 0.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.istft()", "path": "generated/torch.istft#torch.istft", "type": "torch", "text": "\nInverse short time Fourier Transform. This is expected to be the inverse of\n`stft()`. It has the same parameters (+ additional optional parameter of\n`length`) and it should return the least squares estimation of the original\nsignal. The algorithm will check using the NOLA condition ( nonzero overlap).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.is_complex()", "path": "generated/torch.is_complex#torch.is_complex", "type": "torch", "text": "\nReturns True if the data type of `input` is a complex data type i.e., one of\n`torch.complex64`, and `torch.complex128`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.is_floating_point()", "path": "generated/torch.is_floating_point#torch.is_floating_point", "type": "torch", "text": "\nReturns True if the data type of `input` is a floating point data type i.e.,\none of `torch.float64`, `torch.float32`, `torch.float16`, and\n`torch.bfloat16`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.is_nonzero()", "path": "generated/torch.is_nonzero#torch.is_nonzero", "type": "torch", "text": "\nReturns True if the `input` is a single element tensor which is not equal to\nzero after type conversions. i.e. not equal to `torch.tensor([0.])` or\n`torch.tensor([0])` or `torch.tensor([False])`. Throws a `RuntimeError` if\n`torch.numel() != 1` (even in case of sparse tensors).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.is_storage()", "path": "generated/torch.is_storage#torch.is_storage", "type": "torch", "text": "\nReturns True if `obj` is a PyTorch storage object.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.is_tensor()", "path": "generated/torch.is_tensor#torch.is_tensor", "type": "torch", "text": "\nReturns True if `obj` is a PyTorch tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.export()", "path": "jit#torch.jit.export", "type": "TorchScript", "text": "\nThis decorator indicates that a method on an `nn.Module` is used as an entry\npoint into a `ScriptModule` and should be compiled.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.fork()", "path": "generated/torch.jit.fork#torch.jit.fork", "type": "TorchScript", "text": "\nCreates an asynchronous task executing `func` and a reference to the value of\nthe result of this execution. `fork` will return immediately, so the return\nvalue of `func` may not have been computed yet. To force completion of the\ntask and access the return value invoke `torch.jit.wait` on the Future. `fork`\ninvoked with a `func` which returns `T` is typed as `torch.jit.Future[T]`.\n`fork` calls can be arbitrarily nested, and may be invoked with positional and\nkeyword arguments. Asynchronous execution will only occur when run in\nTorchScript. If run in pure python, `fork` will not execute in parallel.\n`fork` will also not execute in parallel when invoked while tracing, however\nthe `fork` and `wait` calls will be captured in the exported IR Graph. ..\nwarning:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.freeze()", "path": "generated/torch.jit.freeze#torch.jit.freeze", "type": "TorchScript", "text": "\nFreezing a `ScriptModule` will clone it and attempt to inline the cloned\nmodule\u2019s submodules, parameters, and attributes as constants in the\nTorchScript IR Graph. By default, `forward` will be preserved, as well as\nattributes & methods specified in `preserved_attrs`. Additionally, any\nattribute that is modified within a preserved method will be preserved.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ignore()", "path": "generated/torch.jit.ignore#torch.jit.ignore", "type": "TorchScript", "text": "\nThis decorator indicates to the compiler that a function or method should be\nignored and left as a Python function. This allows you to leave code in your\nmodel that is not yet TorchScript compatible. If called from TorchScript,\nignored functions will dispatch the call to the Python interpreter. Models\nwith ignored functions cannot be exported; use `@torch.jit.unused` instead.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.isinstance()", "path": "generated/torch.jit.isinstance#torch.jit.isinstance", "type": "TorchScript", "text": "\nThis function provides for conatiner type refinement in TorchScript. It can\nrefine parameterized containers of the List, Dict, Tuple, and Optional types.\nE.g. `List[str]`, `Dict[str, List[torch.Tensor]]`,\n`Optional[Tuple[int,str,int]]`. It can also refine basic types such as bools\nand ints that are available in TorchScript.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.is_scripting()", "path": "jit_language_reference#torch.jit.is_scripting", "type": "TorchScript", "text": "\nFunction that returns True when in compilation and False otherwise. This is\nuseful especially with the @unused decorator to leave code in your model that\nis not yet TorchScript compatible. .. testcode:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.load()", "path": "generated/torch.jit.load#torch.jit.load", "type": "TorchScript", "text": "\nLoad a `ScriptModule` or `ScriptFunction` previously saved with\n`torch.jit.save`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.save()", "path": "generated/torch.jit.save#torch.jit.save", "type": "TorchScript", "text": "\nSave an offline version of this module for use in a separate process. The\nsaved module serializes all of the methods, submodules, parameters, and\nattributes of this module. It can be loaded into the C++ API using\n`torch::jit::load(filename)` or into the Python API with `torch.jit.load`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.script()", "path": "generated/torch.jit.script#torch.jit.script", "type": "TorchScript", "text": "\nScripting a function or `nn.Module` will inspect the source code, compile it\nas TorchScript code using the TorchScript compiler, and return a\n`ScriptModule` or `ScriptFunction`. TorchScript itself is a subset of the\nPython language, so not all features in Python work, but we provide enough\nfunctionality to compute on tensors and do control-dependent operations. For a\ncomplete guide, see the TorchScript Language Reference.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction", "type": "TorchScript", "text": "\nFunctionally equivalent to a `ScriptModule`, but represents a single function\nand does not have any attributes or Parameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.get_debug_state()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.get_debug_state", "type": "TorchScript", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.save()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save", "type": "TorchScript", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.save_to_buffer()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer", "type": "TorchScript", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule", "type": "TorchScript", "text": "\nA wrapper around C++ `torch::jit::Module`. `ScriptModule`s contain methods,\nattributes, parameters, and constants. These can be accessed the same as on a\nnormal `nn.Module`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.add_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.add_module", "type": "TorchScript", "text": "\nAdds a child module to the current module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.apply()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.apply", "type": "TorchScript", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.bfloat16()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.bfloat16", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.buffers", "type": "TorchScript", "text": "\nReturns an iterator over module buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.children", "type": "TorchScript", "text": "\nReturns an iterator over immediate children modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.code()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code", "type": "TorchScript", "text": "\nReturns a pretty-printed representation (as valid Python syntax) of the\ninternal graph for the `forward` method. See Inspecting Code for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.code_with_constants()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code_with_constants", "type": "TorchScript", "text": "\nReturns a tuple of:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.cpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cpu", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.cuda()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cuda", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.double()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.double", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.eval()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.eval", "type": "TorchScript", "text": "\nSets the module in evaluation mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.extra_repr()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.extra_repr", "type": "TorchScript", "text": "\nSet the extra representation of the module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.float()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.float", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.graph", "type": "TorchScript", "text": "\nReturns a string representation of the internal graph for the `forward`\nmethod. See Interpreting Graphs for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.half()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.half", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.inlined_graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.inlined_graph", "type": "TorchScript", "text": "\nReturns a string representation of the internal graph for the `forward`\nmethod. This graph will be preprocessed to inline all function and method\ncalls. See Interpreting Graphs for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.load_state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.load_state_dict", "type": "TorchScript", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.modules", "type": "TorchScript", "text": "\nReturns an iterator over all modules in the network.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_buffers", "type": "TorchScript", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_children", "type": "TorchScript", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_modules", "type": "TorchScript", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_parameters", "type": "TorchScript", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.parameters", "type": "TorchScript", "text": "\nReturns an iterator over module parameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_backward_hook", "type": "TorchScript", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_buffer", "type": "TorchScript", "text": "\nAdds a buffer to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_forward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_hook", "type": "TorchScript", "text": "\nRegisters a forward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_forward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_pre_hook", "type": "TorchScript", "text": "\nRegisters a forward pre-hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_full_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_hook", "type": "TorchScript", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_parameter", "type": "TorchScript", "text": "\nAdds a parameter to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.requires_grad_()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.requires_grad_", "type": "TorchScript", "text": "\nChange if autograd should record operations on parameters in this module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.save()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.save", "type": "TorchScript", "text": "\nSee `torch.jit.save` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.state_dict", "type": "TorchScript", "text": "\nReturns a dictionary containing a whole state of the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.to()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to", "type": "TorchScript", "text": "\nMoves and/or casts the parameters and buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.train()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.train", "type": "TorchScript", "text": "\nSets the module in training mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.type()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.type", "type": "TorchScript", "text": "\nCasts all parameters and buffers to `dst_type`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.xpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.xpu", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the XPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.zero_grad()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.zero_grad", "type": "TorchScript", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.script_if_tracing()", "path": "generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing", "type": "TorchScript", "text": "\nCompiles `fn` when it is first called during tracing. `torch.jit.script` has a\nnon-negligible start up time when it is first called due to lazy-\ninitializations of many compiler builtins. Therefore you should not use it in\nlibrary code. However, you may want to have parts of your library work in\ntracing even if they use control flow. In these cases, you should use\n`@torch.jit.script_if_tracing` to substitute for `torch.jit.script`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.trace()", "path": "generated/torch.jit.trace#torch.jit.trace", "type": "TorchScript", "text": "\nTrace a function and return an executable or `ScriptFunction` that will be\noptimized using just-in-time compilation. Tracing is ideal for code that\noperates only on `Tensor`s and lists, dictionaries, and tuples of `Tensor`s.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.trace_module()", "path": "generated/torch.jit.trace_module#torch.jit.trace_module", "type": "TorchScript", "text": "\nTrace a module and return an executable `ScriptModule` that will be optimized\nusing just-in-time compilation. When a module is passed to `torch.jit.trace`,\nonly the `forward` method is run and traced. With `trace_module`, you can\nspecify a dictionary of method names to example inputs to trace (see the\n`inputs`) argument below.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.unused()", "path": "generated/torch.jit.unused#torch.jit.unused", "type": "TorchScript", "text": "\nThis decorator indicates to the compiler that a function or method should be\nignored and replaced with the raising of an exception. This allows you to\nleave code in your model that is not yet TorchScript compatible and still\nexport your model.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.jit.wait()", "path": "generated/torch.jit.wait#torch.jit.wait", "type": "TorchScript", "text": "\nForces completion of a `torch.jit.Future[T]` asynchronous task, returning the\nresult of the task. See `fork()` for docs and examples. :param func: an\nasynchronous task reference, created through `torch.jit.fork` :type func:\ntorch.jit.Future[T]\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.kaiser_window()", "path": "generated/torch.kaiser_window#torch.kaiser_window", "type": "torch", "text": "\nComputes the Kaiser window with window length `window_length` and shape\nparameter `beta`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.kron()", "path": "generated/torch.kron#torch.kron", "type": "torch", "text": "\nComputes the Kronecker product, denoted by \u2297\\otimes , of `input` and `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.kthvalue()", "path": "generated/torch.kthvalue#torch.kthvalue", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the `k` th smallest\nelement of each row of the `input` tensor in the given dimension `dim`. And\n`indices` is the index location of each element found.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lcm()", "path": "generated/torch.lcm#torch.lcm", "type": "torch", "text": "\nComputes the element-wise least common multiple (LCM) of `input` and `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ldexp()", "path": "generated/torch.ldexp#torch.ldexp", "type": "torch", "text": "\nMultiplies `input` by 2**:attr:`other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.le()", "path": "generated/torch.le#torch.le", "type": "torch", "text": "\nComputes input\u2264other\\text{input} \\leq \\text{other} element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lerp()", "path": "generated/torch.lerp#torch.lerp", "type": "torch", "text": "\nDoes a linear interpolation of two tensors `start` (given by `input`) and\n`end` based on a scalar or tensor `weight` and returns the resulting `out`\ntensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.less()", "path": "generated/torch.less#torch.less", "type": "torch", "text": "\nAlias for `torch.lt()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.less_equal()", "path": "generated/torch.less_equal#torch.less_equal", "type": "torch", "text": "\nAlias for `torch.le()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lgamma()", "path": "generated/torch.lgamma#torch.lgamma", "type": "torch", "text": "\nComputes the logarithm of the gamma function on `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg", "path": "linalg", "type": "torch.linalg", "text": "\nCommon linear algebra operations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.cholesky()", "path": "linalg#torch.linalg.cholesky", "type": "torch.linalg", "text": "\nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-\nvalued matrices) positive-definite matrix or the Cholesky decompositions for a\nbatch of such matrices. Each decomposition has the form:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.cond()", "path": "linalg#torch.linalg.cond", "type": "torch.linalg", "text": "\nComputes the condition number of a matrix `input`, or of each matrix in a\nbatched `input`, using the matrix norm defined by `p`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.det()", "path": "linalg#torch.linalg.det", "type": "torch.linalg", "text": "\nComputes the determinant of a square matrix `input`, or of each square matrix\nin a batched `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.eigh()", "path": "linalg#torch.linalg.eigh", "type": "torch.linalg", "text": "\nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real\nsymmetric) matrix `input`, or of each such matrix in a batched `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.eigvalsh()", "path": "linalg#torch.linalg.eigvalsh", "type": "torch.linalg", "text": "\nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix\n`input`, or of each such matrix in a batched `input`. The eigenvalues are\nreturned in ascending order.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.inv()", "path": "linalg#torch.linalg.inv", "type": "torch.linalg", "text": "\nComputes the multiplicative inverse matrix of a square matrix `input`, or of\neach square matrix in a batched `input`. The result satisfies the relation:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.matrix_rank()", "path": "linalg#torch.linalg.matrix_rank", "type": "torch.linalg", "text": "\nComputes the numerical rank of a matrix `input`, or of each matrix in a\nbatched `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.norm()", "path": "linalg#torch.linalg.norm", "type": "torch.linalg", "text": "\nReturns the matrix norm or vector norm of a given tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.pinv()", "path": "linalg#torch.linalg.pinv", "type": "torch.linalg", "text": "\nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a\nmatrix `input`, or of each matrix in a batched `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.qr()", "path": "linalg#torch.linalg.qr", "type": "torch.linalg", "text": "\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.slogdet()", "path": "linalg#torch.linalg.slogdet", "type": "torch.linalg", "text": "\nCalculates the sign and natural logarithm of the absolute value of a square\nmatrix\u2019s determinant, or of the absolute values of the determinants of a batch\nof square matrices `input`. The determinant can be computed with `sign *\nexp(logabsdet)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.solve()", "path": "linalg#torch.linalg.solve", "type": "torch.linalg", "text": "\nComputes the solution `x` to the matrix equation `matmul(input, x) = other`\nwith a square matrix, or batches of such matrices, `input` and one or more\nright-hand side vectors `other`. If `input` is batched and `other` is not,\nthen `other` is broadcast to have the same batch dimensions as `input`. The\nresulting tensor has the same shape as the (possibly broadcast) `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.svd()", "path": "linalg#torch.linalg.svd", "type": "torch.linalg", "text": "\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\u201d The singular value decomposition is represented as a\nnamedtuple `(U, S, Vh)`, such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@}\ndiag(S) \\times Vh . If `input` is a batch of tensors, then `U`, `S`, and `Vh`\nare also batched with the same batch dimensions as `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.tensorinv()", "path": "linalg#torch.linalg.tensorinv", "type": "torch.linalg", "text": "\nComputes a tensor `input_inv` such that `tensordot(input_inv, input, ind) ==\nI_n` (inverse tensor equation), where `I_n` is the n-dimensional identity\ntensor and `n` is equal to `input.ndim`. The resulting tensor `input_inv` has\nshape equal to `input.shape[ind:] + input.shape[:ind]`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linalg.tensorsolve()", "path": "linalg#torch.linalg.tensorsolve", "type": "torch.linalg", "text": "\nComputes a tensor `x` such that `tensordot(input, x, dims=x.ndim) = other`.\nThe resulting tensor `x` has the same shape as `input[other.ndim:]`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.linspace()", "path": "generated/torch.linspace#torch.linspace", "type": "torch", "text": "\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from `start` to `end`, inclusive. That is, the value are:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.load()", "path": "generated/torch.load#torch.load", "type": "torch", "text": "\nLoads an object saved with `torch.save()` from a file.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lobpcg()", "path": "generated/torch.lobpcg#torch.lobpcg", "type": "torch", "text": "\nFind the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized eigenvalue problem\nusing matrix-free LOBPCG methods.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.log()", "path": "generated/torch.log#torch.log", "type": "torch", "text": "\nReturns a new tensor with the natural logarithm of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.log10()", "path": "generated/torch.log10#torch.log10", "type": "torch", "text": "\nReturns a new tensor with the logarithm to the base 10 of the elements of\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.log1p()", "path": "generated/torch.log1p#torch.log1p", "type": "torch", "text": "\nReturns a new tensor with the natural logarithm of (1 + `input`).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.log2()", "path": "generated/torch.log2#torch.log2", "type": "torch", "text": "\nReturns a new tensor with the logarithm to the base 2 of the elements of\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logaddexp()", "path": "generated/torch.logaddexp#torch.logaddexp", "type": "torch", "text": "\nLogarithm of the sum of exponentiations of the inputs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logaddexp2()", "path": "generated/torch.logaddexp2#torch.logaddexp2", "type": "torch", "text": "\nLogarithm of the sum of exponentiations of the inputs in base-2.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logcumsumexp()", "path": "generated/torch.logcumsumexp#torch.logcumsumexp", "type": "torch", "text": "\nReturns the logarithm of the cumulative summation of the exponentiation of\nelements of `input` in the dimension `dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logdet()", "path": "generated/torch.logdet#torch.logdet", "type": "torch", "text": "\nCalculates log determinant of a square matrix or batches of square matrices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logical_and()", "path": "generated/torch.logical_and#torch.logical_and", "type": "torch", "text": "\nComputes the element-wise logical AND of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logical_not()", "path": "generated/torch.logical_not#torch.logical_not", "type": "torch", "text": "\nComputes the element-wise logical NOT of the given input tensor. If not\nspecified, the output tensor will have the bool dtype. If the input tensor is\nnot a bool tensor, zeros are treated as `False` and non-zeros are treated as\n`True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logical_or()", "path": "generated/torch.logical_or#torch.logical_or", "type": "torch", "text": "\nComputes the element-wise logical OR of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logical_xor()", "path": "generated/torch.logical_xor#torch.logical_xor", "type": "torch", "text": "\nComputes the element-wise logical XOR of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logit()", "path": "generated/torch.logit#torch.logit", "type": "torch", "text": "\nReturns a new tensor with the logit of the elements of `input`. `input` is\nclamped to [eps, 1 - eps] when eps is not None. When eps is None and `input` <\n0 or `input` > 1, the function will yields NaN.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logspace()", "path": "generated/torch.logspace#torch.logspace", "type": "torch", "text": "\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}} to\nbaseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale\nwith base `base`. That is, the values are:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.logsumexp()", "path": "generated/torch.logsumexp#torch.logsumexp", "type": "torch", "text": "\nReturns the log of summed exponentials of each row of the `input` tensor in\nthe given dimension `dim`. The computation is numerically stabilized.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lstsq()", "path": "generated/torch.lstsq#torch.lstsq", "type": "torch", "text": "\nComputes the solution to the least squares and least norm problems for a full\nrank matrix AA of size (m\u00d7n)(m \\times n) and a matrix BB of size (m\u00d7k)(m\n\\times k) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lt()", "path": "generated/torch.lt#torch.lt", "type": "torch", "text": "\nComputes input<other\\text{input} < \\text{other} element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lu()", "path": "generated/torch.lu#torch.lu", "type": "torch", "text": "\nComputes the LU factorization of a matrix or batches of matrices `A`. Returns\na tuple containing the LU factorization and pivots of `A`. Pivoting is done if\n`pivot` is set to `True`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lu_solve()", "path": "generated/torch.lu_solve#torch.lu_solve", "type": "torch", "text": "\nReturns the LU solve of the linear system Ax=bAx = b using the partially\npivoted LU factorization of A from `torch.lu()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.lu_unpack()", "path": "generated/torch.lu_unpack#torch.lu_unpack", "type": "torch", "text": "\nUnpacks the data and pivots from a LU factorization of a tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.manual_seed()", "path": "generated/torch.manual_seed#torch.manual_seed", "type": "torch", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.masked_select()", "path": "generated/torch.masked_select#torch.masked_select", "type": "torch", "text": "\nReturns a new 1-D tensor which indexes the `input` tensor according to the\nboolean mask `mask` which is a `BoolTensor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.matmul()", "path": "generated/torch.matmul#torch.matmul", "type": "torch", "text": "\nMatrix product of two tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.matrix_exp()", "path": "generated/torch.matrix_exp#torch.matrix_exp", "type": "torch", "text": "\nReturns the matrix exponential. Supports batched input. For a matrix `A`, the\nmatrix exponential is defined as\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.matrix_power()", "path": "generated/torch.matrix_power#torch.matrix_power", "type": "torch", "text": "\nReturns the matrix raised to the power `n` for square matrices. For batch of\nmatrices, each individual matrix is raised to the power `n`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.matrix_rank()", "path": "generated/torch.matrix_rank#torch.matrix_rank", "type": "torch", "text": "\nReturns the numerical rank of a 2-D tensor. The method to compute the matrix\nrank is done using SVD by default. If `symmetric` is `True`, then `input` is\nassumed to be symmetric, and the computation of the rank is done by obtaining\nthe eigenvalues.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.max()", "path": "generated/torch.max#torch.max", "type": "torch", "text": "\nReturns the maximum value of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.maximum()", "path": "generated/torch.maximum#torch.maximum", "type": "torch", "text": "\nComputes the element-wise maximum of `input` and `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.mean()", "path": "generated/torch.mean#torch.mean", "type": "torch", "text": "\nReturns the mean value of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.median()", "path": "generated/torch.median#torch.median", "type": "torch", "text": "\nReturns the median of the values in `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.meshgrid()", "path": "generated/torch.meshgrid#torch.meshgrid", "type": "torch", "text": "\nTake NN tensors, each of which can be either scalar or 1-dimensional vector,\nand create NN N-dimensional grids, where the ii th grid is defined by\nexpanding the ii th input over dimensions defined by other inputs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.min()", "path": "generated/torch.min#torch.min", "type": "torch", "text": "\nReturns the minimum value of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.minimum()", "path": "generated/torch.minimum#torch.minimum", "type": "torch", "text": "\nComputes the element-wise minimum of `input` and `other`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.mm()", "path": "generated/torch.mm#torch.mm", "type": "torch", "text": "\nPerforms a matrix multiplication of the matrices `input` and `mat2`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.mode()", "path": "generated/torch.mode#torch.mode", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the mode value of\neach row of the `input` tensor in the given dimension `dim`, i.e. a value\nwhich appears most often in that row, and `indices` is the index location of\neach mode value found.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.moveaxis()", "path": "generated/torch.moveaxis#torch.moveaxis", "type": "torch", "text": "\nAlias for `torch.movedim()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.movedim()", "path": "generated/torch.movedim#torch.movedim", "type": "torch", "text": "\nMoves the dimension(s) of `input` at the position(s) in `source` to the\nposition(s) in `destination`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.msort()", "path": "generated/torch.msort#torch.msort", "type": "torch", "text": "\nSorts the elements of the `input` tensor along its first dimension in\nascending order by value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.mul()", "path": "generated/torch.mul#torch.mul", "type": "torch", "text": "\nMultiplies each element of the input `input` with the scalar `other` and\nreturns a new resulting tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multinomial()", "path": "generated/torch.multinomial#torch.multinomial", "type": "torch", "text": "\nReturns a tensor where each row contains `num_samples` indices sampled from\nthe multinomial probability distribution located in the corresponding row of\ntensor `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiply()", "path": "generated/torch.multiply#torch.multiply", "type": "torch", "text": "\nAlias for `torch.mul()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing", "path": "multiprocessing", "type": "torch.multiprocessing", "text": "\ntorch.multiprocessing is a wrapper around the native `multiprocessing` module.\nIt registers custom reducers, that use shared memory to provide shared views\non the same data in different processes. Once the tensor/storage is moved to\nshared_memory (see `share_memory_()`), it will be possible to send it to other\nprocesses without making any copies.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing.get_all_sharing_strategies()", "path": "multiprocessing#torch.multiprocessing.get_all_sharing_strategies", "type": "torch.multiprocessing", "text": "\nReturns a set of sharing strategies supported on a current system.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing.get_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.get_sharing_strategy", "type": "torch.multiprocessing", "text": "\nReturns the current strategy for sharing CPU tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing.set_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.set_sharing_strategy", "type": "torch.multiprocessing", "text": "\nSets the strategy for sharing CPU tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing.spawn()", "path": "multiprocessing#torch.multiprocessing.spawn", "type": "torch.multiprocessing", "text": "\nSpawns `nprocs` processes that run `fn` with `args`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing.SpawnContext", "path": "multiprocessing#torch.multiprocessing.SpawnContext", "type": "torch.multiprocessing", "text": "\nReturned by `spawn()` when called with `join=False`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.multiprocessing.SpawnContext.join()", "path": "multiprocessing#torch.multiprocessing.SpawnContext.join", "type": "torch.multiprocessing", "text": "\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.mv()", "path": "generated/torch.mv#torch.mv", "type": "torch", "text": "\nPerforms a matrix-vector product of the matrix `input` and the vector `vec`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.mvlgamma()", "path": "generated/torch.mvlgamma#torch.mvlgamma", "type": "torch", "text": "\nComputes the multivariate log-gamma function) with dimension pp element-wise,\ngiven by\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nanmedian()", "path": "generated/torch.nanmedian#torch.nanmedian", "type": "torch", "text": "\nReturns the median of the values in `input`, ignoring `NaN` values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nanquantile()", "path": "generated/torch.nanquantile#torch.nanquantile", "type": "torch", "text": "\nThis is a variant of `torch.quantile()` that \u201cignores\u201d `NaN` values, computing\nthe quantiles `q` as if `NaN` values in `input` did not exist. If all values\nin a reduced row are `NaN` then the quantiles for that reduction will be\n`NaN`. See the documentation for `torch.quantile()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nansum()", "path": "generated/torch.nansum#torch.nansum", "type": "torch", "text": "\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nan_to_num()", "path": "generated/torch.nan_to_num#torch.nan_to_num", "type": "torch", "text": "\nReplaces `NaN`, positive infinity, and negative infinity values in `input`\nwith the values specified by `nan`, `posinf`, and `neginf`, respectively. By\ndefault, `NaN`s are replaced with zero, positive infinity is replaced with the\ngreatest finite value representable by :attr:`input`\u2019s dtype, and negative\ninfinity is replaced with the least finite value representable by `input`\u2019s\ndtype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.narrow()", "path": "generated/torch.narrow#torch.narrow", "type": "torch", "text": "\nReturns a new tensor that is a narrowed version of `input` tensor. The\ndimension `dim` is input from `start` to `start + length`. The returned tensor\nand `input` tensor share the same underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ne()", "path": "generated/torch.ne#torch.ne", "type": "torch", "text": "\nComputes input\u2260other\\text{input} \\neq \\text{other} element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.neg()", "path": "generated/torch.neg#torch.neg", "type": "torch", "text": "\nReturns a new tensor with the negative of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.negative()", "path": "generated/torch.negative#torch.negative", "type": "torch", "text": "\nAlias for `torch.neg()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nextafter()", "path": "generated/torch.nextafter#torch.nextafter", "type": "torch", "text": "\nReturn the next floating-point value after `input` towards `other`,\nelementwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn", "path": "nn", "type": "torch.nn", "text": "\nThese are the basic building block for graphs\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool1d", "path": "generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d", "type": "torch.nn", "text": "\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool2d", "path": "generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d", "type": "torch.nn", "text": "\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool3d", "path": "generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d", "type": "torch.nn", "text": "\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "torch.nn", "text": "\nEfficient softmax approximation as described in Efficient softmax\napproximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David\nGrangier, and Herv\u00e9 J\u00e9gou.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "torch.nn", "text": "\nComputes log probabilities for all n_classes\\texttt{n\\\\_classes}\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "torch.nn", "text": "\nThis is equivalent to `self.log_pob(input).argmax(dim=1)`, but is more\nefficient in some cases.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool1d", "path": "generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d", "type": "torch.nn", "text": "\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool2d", "path": "generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool3d", "path": "generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d", "type": "torch.nn", "text": "\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AlphaDropout", "path": "generated/torch.nn.alphadropout#torch.nn.AlphaDropout", "type": "torch.nn", "text": "\nApplies Alpha Dropout over the input.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AvgPool1d", "path": "generated/torch.nn.avgpool1d#torch.nn.AvgPool1d", "type": "torch.nn", "text": "\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AvgPool2d", "path": "generated/torch.nn.avgpool2d#torch.nn.AvgPool2d", "type": "torch.nn", "text": "\nApplies a 2D average pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.AvgPool3d", "path": "generated/torch.nn.avgpool3d#torch.nn.AvgPool3d", "type": "torch.nn", "text": "\nApplies a 3D average pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm1d", "path": "generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs\nwith optional additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm2d", "path": "generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm3d", "path": "generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.BCELoss", "path": "generated/torch.nn.bceloss#torch.nn.BCELoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the Binary Cross Entropy between the target\nand the output:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.BCEWithLogitsLoss", "path": "generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss", "type": "torch.nn", "text": "\nThis loss combines a `Sigmoid` layer and the `BCELoss` in one single class.\nThis version is more numerically stable than using a plain `Sigmoid` followed\nby a `BCELoss` as, by combining the operations into one layer, we take\nadvantage of the log-sum-exp trick for numerical stability.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Bilinear", "path": "generated/torch.nn.bilinear#torch.nn.Bilinear", "type": "torch.nn", "text": "\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.CELU", "path": "generated/torch.nn.celu#torch.nn.CELU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ChannelShuffle", "path": "generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle", "type": "torch.nn", "text": "\nDivide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups\nand rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the\noriginal tensor shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad1d", "path": "generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad2d", "path": "generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad3d", "path": "generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Conv1d", "path": "generated/torch.nn.conv1d#torch.nn.Conv1d", "type": "torch.nn", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Conv2d", "path": "generated/torch.nn.conv2d#torch.nn.Conv2d", "type": "torch.nn", "text": "\nApplies a 2D convolution over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Conv3d", "path": "generated/torch.nn.conv3d#torch.nn.Conv3d", "type": "torch.nn", "text": "\nApplies a 3D convolution over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose1d", "path": "generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d", "type": "torch.nn", "text": "\nApplies a 1D transposed convolution operator over an input image composed of\nseveral input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose2d", "path": "generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d", "type": "torch.nn", "text": "\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose3d", "path": "generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d", "type": "torch.nn", "text": "\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes. The transposed convolution operator multiplies each\ninput value element-wise by a learnable kernel, and sums over the outputs from\nall input feature planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.CosineEmbeddingLoss", "path": "generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the loss given input tensors x1x_1 , x2x_2\nand a `Tensor` label yy with values 1 or -1. This is used for measuring\nwhether two inputs are similar or dissimilar, using the cosine distance, and\nis typically used for learning nonlinear embeddings or semi-supervised\nlearning.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.CosineSimilarity", "path": "generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity", "type": "torch.nn", "text": "\nReturns cosine similarity between x1x_1 and x2x_2 , computed along dim.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.CrossEntropyLoss", "path": "generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss", "type": "torch.nn", "text": "\nThis criterion combines `LogSoftmax` and `NLLLoss` in one single class.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.CTCLoss", "path": "generated/torch.nn.ctcloss#torch.nn.CTCLoss", "type": "torch.nn", "text": "\nThe Connectionist Temporal Classification loss.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.DataParallel", "path": "generated/torch.nn.dataparallel#torch.nn.DataParallel", "type": "torch.nn", "text": "\nImplements data parallelism at the module level.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Dropout", "path": "generated/torch.nn.dropout#torch.nn.Dropout", "type": "torch.nn", "text": "\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution. Each channel will\nbe zeroed out independently on every forward call.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Dropout2d", "path": "generated/torch.nn.dropout2d#torch.nn.Dropout2d", "type": "torch.nn", "text": "\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently\non every forward call with probability `p` using samples from a Bernoulli\ndistribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Dropout3d", "path": "generated/torch.nn.dropout3d#torch.nn.Dropout3d", "type": "torch.nn", "text": "\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently\non every forward call with probability `p` using samples from a Bernoulli\ndistribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ELU", "path": "generated/torch.nn.elu#torch.nn.ELU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Embedding", "path": "generated/torch.nn.embedding#torch.nn.Embedding", "type": "torch.nn", "text": "\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Embedding.from_pretrained()", "path": "generated/torch.nn.embedding#torch.nn.Embedding.from_pretrained", "type": "torch.nn", "text": "\nCreates Embedding instance from given 2-dimensional FloatTensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.EmbeddingBag", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag", "type": "torch.nn", "text": "\nComputes sums or means of \u2018bags\u2019 of embeddings, without instantiating the\nintermediate embeddings.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.EmbeddingBag.from_pretrained()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.from_pretrained", "type": "torch.nn", "text": "\nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten", "path": "generated/torch.nn.flatten#torch.nn.Flatten", "type": "torch.nn", "text": "\nFlattens a contiguous range of dims into a tensor. For use with `Sequential`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.add_module()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.apply()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.bfloat16()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.cpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.cuda()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.double()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.eval()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.float()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.half()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.load_state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_buffer()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_forward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_forward_pre_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_full_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_parameter()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.requires_grad_()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.to()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.train()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.type()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.xpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Flatten.zero_grad()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Fold", "path": "generated/torch.nn.fold#torch.nn.Fold", "type": "torch.nn", "text": "\nCombines an array of sliding local blocks into a large containing tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.FractionalMaxPool2d", "path": "generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D fractional max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional", "path": "nn.functional", "type": "torch.nn.functional", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.affine_grid()", "path": "nn.functional#torch.nn.functional.affine_grid", "type": "torch.nn.functional", "text": "\nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine\nmatrices `theta`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.alpha_dropout()", "path": "nn.functional#torch.nn.functional.alpha_dropout", "type": "torch.nn.functional", "text": "\nApplies alpha dropout to the input.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool1d()", "path": "nn.functional#torch.nn.functional.avg_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool2d()", "path": "nn.functional#torch.nn.functional.avg_pool2d", "type": "torch.nn.functional", "text": "\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool3d()", "path": "nn.functional#torch.nn.functional.avg_pool3d", "type": "torch.nn.functional", "text": "\nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions\nby step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output\nfeatures is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input\nplanes}}{sT}\\rfloor .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.batch_norm()", "path": "nn.functional#torch.nn.functional.batch_norm", "type": "torch.nn.functional", "text": "\nApplies Batch Normalization for each channel across a batch of data.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.bilinear()", "path": "nn.functional#torch.nn.functional.bilinear", "type": "torch.nn.functional", "text": "\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.binary_cross_entropy()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy", "type": "torch.nn.functional", "text": "\nFunction that measures the Binary Cross Entropy between the target and the\noutput.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy_with_logits", "type": "torch.nn.functional", "text": "\nFunction that measures Binary Cross Entropy between target and output logits.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.celu()", "path": "nn.functional#torch.nn.functional.celu", "type": "torch.nn.functional", "text": "\nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x)\n= \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.conv1d()", "path": "nn.functional#torch.nn.functional.conv1d", "type": "torch.nn.functional", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.conv2d()", "path": "nn.functional#torch.nn.functional.conv2d", "type": "torch.nn.functional", "text": "\nApplies a 2D convolution over an input image composed of several input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.conv3d()", "path": "nn.functional#torch.nn.functional.conv3d", "type": "torch.nn.functional", "text": "\nApplies a 3D convolution over an input image composed of several input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose1d()", "path": "nn.functional#torch.nn.functional.conv_transpose1d", "type": "torch.nn.functional", "text": "\nApplies a 1D transposed convolution operator over an input signal composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose2d()", "path": "nn.functional#torch.nn.functional.conv_transpose2d", "type": "torch.nn.functional", "text": "\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose3d()", "path": "nn.functional#torch.nn.functional.conv_transpose3d", "type": "torch.nn.functional", "text": "\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.cosine_embedding_loss()", "path": "nn.functional#torch.nn.functional.cosine_embedding_loss", "type": "torch.nn.functional", "text": "\nSee `CosineEmbeddingLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.cosine_similarity()", "path": "nn.functional#torch.nn.functional.cosine_similarity", "type": "torch.nn.functional", "text": "\nReturns cosine similarity between x1 and x2, computed along dim.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.cross_entropy()", "path": "nn.functional#torch.nn.functional.cross_entropy", "type": "torch.nn.functional", "text": "\nThis criterion combines `log_softmax` and `nll_loss` in a single function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.ctc_loss()", "path": "nn.functional#torch.nn.functional.ctc_loss", "type": "torch.nn.functional", "text": "\nThe Connectionist Temporal Classification loss.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout()", "path": "nn.functional#torch.nn.functional.dropout", "type": "torch.nn.functional", "text": "\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout2d()", "path": "nn.functional#torch.nn.functional.dropout2d", "type": "torch.nn.functional", "text": "\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout3d()", "path": "nn.functional#torch.nn.functional.dropout3d", "type": "torch.nn.functional", "text": "\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.elu()", "path": "nn.functional#torch.nn.functional.elu", "type": "torch.nn.functional", "text": "\nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) =\n\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.elu_()", "path": "nn.functional#torch.nn.functional.elu_", "type": "torch.nn.functional", "text": "\nIn-place version of `elu()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.embedding()", "path": "nn.functional#torch.nn.functional.embedding", "type": "torch.nn.functional", "text": "\nA simple lookup table that looks up embeddings in a fixed dictionary and size.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.embedding_bag()", "path": "nn.functional#torch.nn.functional.embedding_bag", "type": "torch.nn.functional", "text": "\nComputes sums, means or maxes of `bags` of embeddings, without instantiating\nthe intermediate embeddings.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.feature_alpha_dropout()", "path": "nn.functional#torch.nn.functional.feature_alpha_dropout", "type": "torch.nn.functional", "text": "\nRandomly masks out entire channels (a channel is a feature map, e.g. the jj\n-th channel of the ii -th sample in the batch input is a tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting\nactivations to zero, as in regular Dropout, the activations are set to the\nnegative saturation value of the SELU activation function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.fold()", "path": "nn.functional#torch.nn.functional.fold", "type": "torch.nn.functional", "text": "\nCombines an array of sliding local blocks into a large containing tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.gelu()", "path": "nn.functional#torch.nn.functional.gelu", "type": "torch.nn.functional", "text": "\nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.glu()", "path": "nn.functional#torch.nn.functional.glu", "type": "torch.nn.functional", "text": "\nThe gated linear unit. Computes:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.grid_sample()", "path": "nn.functional#torch.nn.functional.grid_sample", "type": "torch.nn.functional", "text": "\nGiven an `input` and a flow-field `grid`, computes the `output` using `input`\nvalues and pixel locations from `grid`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.gumbel_softmax()", "path": "nn.functional#torch.nn.functional.gumbel_softmax", "type": "torch.nn.functional", "text": "\nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally\ndiscretizes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.hardshrink()", "path": "nn.functional#torch.nn.functional.hardshrink", "type": "torch.nn.functional", "text": "\nApplies the hard shrinkage function element-wise\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.hardsigmoid()", "path": "nn.functional#torch.nn.functional.hardsigmoid", "type": "torch.nn.functional", "text": "\nApplies the element-wise function\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.hardswish()", "path": "nn.functional#torch.nn.functional.hardswish", "type": "torch.nn.functional", "text": "\nApplies the hardswish function, element-wise, as described in the paper:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.hardtanh()", "path": "nn.functional#torch.nn.functional.hardtanh", "type": "torch.nn.functional", "text": "\nApplies the HardTanh function element-wise. See `Hardtanh` for more details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.hardtanh_()", "path": "nn.functional#torch.nn.functional.hardtanh_", "type": "torch.nn.functional", "text": "\nIn-place version of `hardtanh()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.hinge_embedding_loss()", "path": "nn.functional#torch.nn.functional.hinge_embedding_loss", "type": "torch.nn.functional", "text": "\nSee `HingeEmbeddingLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.instance_norm()", "path": "nn.functional#torch.nn.functional.instance_norm", "type": "torch.nn.functional", "text": "\nApplies Instance Normalization for each channel in each data sample in a\nbatch.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.interpolate()", "path": "nn.functional#torch.nn.functional.interpolate", "type": "torch.nn.functional", "text": "\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.kl_div()", "path": "nn.functional#torch.nn.functional.kl_div", "type": "torch.nn.functional", "text": "\nThe Kullback-Leibler divergence Loss\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.l1_loss()", "path": "nn.functional#torch.nn.functional.l1_loss", "type": "torch.nn.functional", "text": "\nFunction that takes the mean element-wise absolute value difference.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.layer_norm()", "path": "nn.functional#torch.nn.functional.layer_norm", "type": "torch.nn.functional", "text": "\nApplies Layer Normalization for last certain number of dimensions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.leaky_relu()", "path": "nn.functional#torch.nn.functional.leaky_relu", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nLeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0,\nx) + \\text{negative\\\\_slope} * \\min(0, x)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.leaky_relu_()", "path": "nn.functional#torch.nn.functional.leaky_relu_", "type": "torch.nn.functional", "text": "\nIn-place version of `leaky_relu()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.linear()", "path": "nn.functional#torch.nn.functional.linear", "type": "torch.nn.functional", "text": "\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.local_response_norm()", "path": "nn.functional#torch.nn.functional.local_response_norm", "type": "torch.nn.functional", "text": "\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.logsigmoid()", "path": "nn.functional#torch.nn.functional.logsigmoid", "type": "torch.nn.functional", "text": "\nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) =\n\\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.log_softmax()", "path": "nn.functional#torch.nn.functional.log_softmax", "type": "torch.nn.functional", "text": "\nApplies a softmax followed by a logarithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.lp_pool1d()", "path": "nn.functional#torch.nn.functional.lp_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.lp_pool2d()", "path": "nn.functional#torch.nn.functional.lp_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.margin_ranking_loss()", "path": "nn.functional#torch.nn.functional.margin_ranking_loss", "type": "torch.nn.functional", "text": "\nSee `MarginRankingLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool1d()", "path": "nn.functional#torch.nn.functional.max_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool2d()", "path": "nn.functional#torch.nn.functional.max_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool3d()", "path": "nn.functional#torch.nn.functional.max_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool1d()", "path": "nn.functional#torch.nn.functional.max_unpool1d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool1d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool2d()", "path": "nn.functional#torch.nn.functional.max_unpool2d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool2d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool3d()", "path": "nn.functional#torch.nn.functional.max_unpool3d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool3d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.mse_loss()", "path": "nn.functional#torch.nn.functional.mse_loss", "type": "torch.nn.functional", "text": "\nMeasures the element-wise mean squared error.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.multilabel_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_margin_loss", "type": "torch.nn.functional", "text": "\nSee `MultiLabelMarginLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.multilabel_soft_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_soft_margin_loss", "type": "torch.nn.functional", "text": "\nSee `MultiLabelSoftMarginLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.multi_margin_loss()", "path": "nn.functional#torch.nn.functional.multi_margin_loss", "type": "torch.nn.functional", "text": "\nreduce=None, reduction=\u2019mean\u2019) -> Tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.nll_loss()", "path": "nn.functional#torch.nn.functional.nll_loss", "type": "torch.nn.functional", "text": "\nThe negative log likelihood loss.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.normalize()", "path": "nn.functional#torch.nn.functional.normalize", "type": "torch.nn.functional", "text": "\nPerforms LpL_p normalization of inputs over specified dimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.one_hot()", "path": "nn.functional#torch.nn.functional.one_hot", "type": "torch.nn.functional", "text": "\nTakes LongTensor with index values of shape `(*)` and returns a tensor of\nshape `(*, num_classes)` that have zeros everywhere except where the index of\nlast dimension matches the corresponding value of the input tensor, in which\ncase it will be 1.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.pad()", "path": "nn.functional#torch.nn.functional.pad", "type": "torch.nn.functional", "text": "\nPads tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.pairwise_distance()", "path": "nn.functional#torch.nn.functional.pairwise_distance", "type": "torch.nn.functional", "text": "\nSee `torch.nn.PairwiseDistance` for details\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.pdist()", "path": "nn.functional#torch.nn.functional.pdist", "type": "torch.nn.functional", "text": "\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.pixel_shuffle()", "path": "nn.functional#torch.nn.functional.pixel_shuffle", "type": "torch.nn.functional", "text": "\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nthe `upscale_factor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.pixel_unshuffle()", "path": "nn.functional#torch.nn.functional.pixel_unshuffle", "type": "torch.nn.functional", "text": "\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the `downscale_factor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.poisson_nll_loss()", "path": "nn.functional#torch.nn.functional.poisson_nll_loss", "type": "torch.nn.functional", "text": "\nPoisson negative log likelihood loss.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.prelu()", "path": "nn.functional#torch.nn.functional.prelu", "type": "torch.nn.functional", "text": "\nApplies element-wise the function\nPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight}\n* \\min(0,x) where weight is a learnable parameter.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.relu()", "path": "nn.functional#torch.nn.functional.relu", "type": "torch.nn.functional", "text": "\nApplies the rectified linear unit function element-wise. See `ReLU` for more\ndetails.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.relu6()", "path": "nn.functional#torch.nn.functional.relu6", "type": "torch.nn.functional", "text": "\nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) =\n\\min(\\max(0,x), 6) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.relu_()", "path": "nn.functional#torch.nn.functional.relu_", "type": "torch.nn.functional", "text": "\nIn-place version of `relu()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.rrelu()", "path": "nn.functional#torch.nn.functional.rrelu", "type": "torch.nn.functional", "text": "\nRandomized leaky ReLU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.rrelu_()", "path": "nn.functional#torch.nn.functional.rrelu_", "type": "torch.nn.functional", "text": "\nIn-place version of `rrelu()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.selu()", "path": "nn.functional#torch.nn.functional.selu", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nSELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale *\n(\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with\n\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\nand\nscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946\n.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.sigmoid()", "path": "nn.functional#torch.nn.functional.sigmoid", "type": "torch.nn.functional", "text": "\nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) =\n\\frac{1}{1 + \\exp(-x)}\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.silu()", "path": "nn.functional#torch.nn.functional.silu", "type": "torch.nn.functional", "text": "\nApplies the silu function, element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.smooth_l1_loss()", "path": "nn.functional#torch.nn.functional.smooth_l1_loss", "type": "torch.nn.functional", "text": "\nFunction that uses a squared term if the absolute element-wise error falls\nbelow beta and an L1 term otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.softmax()", "path": "nn.functional#torch.nn.functional.softmax", "type": "torch.nn.functional", "text": "\nApplies a softmax function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.softmin()", "path": "nn.functional#torch.nn.functional.softmin", "type": "torch.nn.functional", "text": "\nApplies a softmin function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.softplus()", "path": "nn.functional#torch.nn.functional.softplus", "type": "torch.nn.functional", "text": "\nApplies element-wise, the function\nSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1\n+ \\exp(\\beta * x)) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.softshrink()", "path": "nn.functional#torch.nn.functional.softshrink", "type": "torch.nn.functional", "text": "\nApplies the soft shrinkage function elementwise\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.softsign()", "path": "nn.functional#torch.nn.functional.softsign", "type": "torch.nn.functional", "text": "\nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) =\n\\frac{x}{1 + |x|}\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.soft_margin_loss()", "path": "nn.functional#torch.nn.functional.soft_margin_loss", "type": "torch.nn.functional", "text": "\nSee `SoftMarginLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.tanh()", "path": "nn.functional#torch.nn.functional.tanh", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nTanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) =\n\\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.tanhshrink()", "path": "nn.functional#torch.nn.functional.tanhshrink", "type": "torch.nn.functional", "text": "\nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x -\n\\text{Tanh}(x)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.threshold()", "path": "nn.functional#torch.nn.functional.threshold", "type": "torch.nn.functional", "text": "\nThresholds each element of the input Tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.threshold_()", "path": "nn.functional#torch.nn.functional.threshold_", "type": "torch.nn.functional", "text": "\nIn-place version of `threshold()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.triplet_margin_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_loss", "type": "torch.nn.functional", "text": "\nSee `TripletMarginLoss` for details\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_with_distance_loss", "type": "torch.nn.functional", "text": "\nSee `TripletMarginWithDistanceLoss` for details.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.unfold()", "path": "nn.functional#torch.nn.functional.unfold", "type": "torch.nn.functional", "text": "\nExtracts sliding local blocks from a batched input tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample()", "path": "nn.functional#torch.nn.functional.upsample", "type": "torch.nn.functional", "text": "\nUpsamples the input to either the given `size` or the given `scale_factor`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample_bilinear()", "path": "nn.functional#torch.nn.functional.upsample_bilinear", "type": "torch.nn.functional", "text": "\nUpsamples the input, using bilinear upsampling.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample_nearest()", "path": "nn.functional#torch.nn.functional.upsample_nearest", "type": "torch.nn.functional", "text": "\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.GaussianNLLLoss", "path": "generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss", "type": "torch.nn", "text": "\nGaussian negative log likelihood loss.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.GELU", "path": "generated/torch.nn.gelu#torch.nn.GELU", "type": "torch.nn", "text": "\nApplies the Gaussian Error Linear Units function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.GroupNorm", "path": "generated/torch.nn.groupnorm#torch.nn.GroupNorm", "type": "torch.nn", "text": "\nApplies Group Normalization over a mini-batch of inputs as described in the\npaper Group Normalization\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.GRU", "path": "generated/torch.nn.gru#torch.nn.GRU", "type": "torch.nn", "text": "\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.GRUCell", "path": "generated/torch.nn.grucell#torch.nn.GRUCell", "type": "torch.nn", "text": "\nA gated recurrent unit (GRU) cell\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Hardshrink", "path": "generated/torch.nn.hardshrink#torch.nn.Hardshrink", "type": "torch.nn", "text": "\nApplies the hard shrinkage function element-wise:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Hardsigmoid", "path": "generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Hardswish", "path": "generated/torch.nn.hardswish#torch.nn.Hardswish", "type": "torch.nn", "text": "\nApplies the hardswish function, element-wise, as described in the paper:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Hardtanh", "path": "generated/torch.nn.hardtanh#torch.nn.Hardtanh", "type": "torch.nn", "text": "\nApplies the HardTanh function element-wise\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.HingeEmbeddingLoss", "path": "generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss", "type": "torch.nn", "text": "\nMeasures the loss given an input tensor xx and a labels tensor yy (containing\n1 or -1). This is usually used for measuring whether two inputs are similar or\ndissimilar, e.g. using the L1 pairwise distance as xx , and is typically used\nfor learning nonlinear embeddings or semi-supervised learning.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.identity#torch.nn.Identity", "type": "torch.nn", "text": "\nA placeholder identity operator that is argument-insensitive.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init", "path": "nn.init", "type": "torch.nn.init", "text": "\nReturn the recommended gain value for the given nonlinearity function. The\nvalues are as follows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.calculate_gain()", "path": "nn.init#torch.nn.init.calculate_gain", "type": "torch.nn.init", "text": "\nReturn the recommended gain value for the given nonlinearity function. The\nvalues are as follows:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.constant_()", "path": "nn.init#torch.nn.init.constant_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the value val\\text{val} .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.dirac_()", "path": "nn.init#torch.nn.init.dirac_", "type": "torch.nn.init", "text": "\nFills the {3, 4, 5}-dimensional input `Tensor` with the Dirac delta function.\nPreserves the identity of the inputs in `Convolutional` layers, where as many\ninput channels are preserved as possible. In case of groups>1, each group of\nchannels preserves identity\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.eye_()", "path": "nn.init#torch.nn.init.eye_", "type": "torch.nn.init", "text": "\nFills the 2-dimensional input `Tensor` with the identity matrix. Preserves the\nidentity of the inputs in `Linear` layers, where as many inputs are preserved\nas possible.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.kaiming_normal_()", "path": "nn.init#torch.nn.init.kaiming_normal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a normal distribution. The\nresulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0,\n\\text{std}^2) where\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.kaiming_uniform_()", "path": "nn.init#torch.nn.init.kaiming_uniform_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a uniform distribution. The\nresulting tensor will have values sampled from\nU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.normal_()", "path": "nn.init#torch.nn.init.normal_", "type": "torch.nn.init", "text": "\nFills the input Tensor with values drawn from the normal distribution\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.ones_()", "path": "nn.init#torch.nn.init.ones_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the scalar value `1`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.orthogonal_()", "path": "nn.init#torch.nn.init.orthogonal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with a (semi) orthogonal matrix, as described in\n`Exact solutions to the nonlinear dynamics of learning in deep linear neural\nnetworks` \\- Saxe, A. et al. (2013). The input tensor must have at least 2\ndimensions, and for tensors with more than 2 dimensions the trailing\ndimensions are flattened.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.sparse_()", "path": "nn.init#torch.nn.init.sparse_", "type": "torch.nn.init", "text": "\nFills the 2D input `Tensor` as a sparse matrix, where the non-zero elements\nwill be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as\ndescribed in `Deep learning via Hessian-free optimization` \\- Martens, J.\n(2010).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.uniform_()", "path": "nn.init#torch.nn.init.uniform_", "type": "torch.nn.init", "text": "\nFills the input Tensor with values drawn from the uniform distribution\nU(a,b)\\mathcal{U}(a, b) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.xavier_normal_()", "path": "nn.init#torch.nn.init.xavier_normal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting\ntensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)\nwhere\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.xavier_uniform_()", "path": "nn.init#torch.nn.init.xavier_uniform_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting\ntensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.init.zeros_()", "path": "nn.init#torch.nn.init.zeros_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the scalar value `0`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm1d", "path": "generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 3D input (a mini-batch of 1D inputs with\noptional additional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm2d", "path": "generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm3d", "path": "generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBn1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 1d and Batch Norm 1d\nmodules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBn2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 2d and Batch Norm 2d\nmodules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBnReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and\nReLU modules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBnReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and\nReLU modules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv1d and ReLU modules. During\nquantization this will be replaced with the corresponding fused module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv2d and ReLU modules. During\nquantization this will be replaced with the corresponding fused module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvBn2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": "\nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with\nFakeQuantize modules for weight, used in quantization aware training.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvBnReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": "\nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU,\nattached with FakeQuantize modules for weight, used in quantization aware\ntraining.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": "\nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\nFakeQuantize modules for weight for quantization aware training.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.LinearReLU", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": "\nA LinearReLU module fused from Linear and ReLU modules, attached with\nFakeQuantize modules for weight, used in quantization aware training.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU2d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": "\nA ConvReLU2d module is a fused module of Conv2d and ReLU\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU3d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": "\nA ConvReLU3d module is a fused module of Conv3d and ReLU\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.LinearReLU", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": "\nA LinearReLU module fused from Linear and ReLU modules\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.KLDivLoss", "path": "generated/torch.nn.kldivloss#torch.nn.KLDivLoss", "type": "torch.nn", "text": "\nThe Kullback-Leibler divergence loss measure\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.L1Loss", "path": "generated/torch.nn.l1loss#torch.nn.L1Loss", "type": "torch.nn", "text": "\nCreates a criterion that measures the mean absolute error (MAE) between each\nelement in the input xx and target yy .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LayerNorm", "path": "generated/torch.nn.layernorm#torch.nn.LayerNorm", "type": "torch.nn", "text": "\nApplies Layer Normalization over a mini-batch of inputs as described in the\npaper Layer Normalization\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConv1d", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d", "type": "torch.nn", "text": "\nA `torch.nn.Conv1d` module with lazy initialization of the `in_channels`\nargument of the `Conv1d` that is inferred from the `input.size(1)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConv1d.cls_to_become", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv1d`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConv2d", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d", "type": "torch.nn", "text": "\nA `torch.nn.Conv2d` module with lazy initialization of the `in_channels`\nargument of the `Conv2d` that is inferred from the `input.size(1)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConv2d.cls_to_become", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv2d`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConv3d", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d", "type": "torch.nn", "text": "\nA `torch.nn.Conv3d` module with lazy initialization of the `in_channels`\nargument of the `Conv3d` that is inferred from the `input.size(1)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConv3d.cls_to_become", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv3d`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose1d", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose1d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose1d` that is inferred from the\n`input.size(1)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose1d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose1d`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose2d", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose2d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose2d` that is inferred from the\n`input.size(1)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose2d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose2d`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose3d", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose3d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose3d` that is inferred from the\n`input.size(1)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose3d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose3d`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyLinear", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear", "type": "torch.nn", "text": "\nA `torch.nn.Linear` module with lazy initialization.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LazyLinear.cls_to_become", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear.cls_to_become", "type": "torch.nn", "text": "\nalias of `Linear`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LeakyReLU", "path": "generated/torch.nn.leakyrelu#torch.nn.LeakyReLU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Linear", "path": "generated/torch.nn.linear#torch.nn.Linear", "type": "torch.nn", "text": "\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LocalResponseNorm", "path": "generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm", "type": "torch.nn", "text": "\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LogSigmoid", "path": "generated/torch.nn.logsigmoid#torch.nn.LogSigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LogSoftmax", "path": "generated/torch.nn.logsoftmax#torch.nn.LogSoftmax", "type": "torch.nn", "text": "\nApplies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an\nn-dimensional input Tensor. The LogSoftmax formulation can be simplified as:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LPPool1d", "path": "generated/torch.nn.lppool1d#torch.nn.LPPool1d", "type": "torch.nn", "text": "\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LPPool2d", "path": "generated/torch.nn.lppool2d#torch.nn.LPPool2d", "type": "torch.nn", "text": "\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LSTM", "path": "generated/torch.nn.lstm#torch.nn.LSTM", "type": "torch.nn", "text": "\nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.LSTMCell", "path": "generated/torch.nn.lstmcell#torch.nn.LSTMCell", "type": "torch.nn", "text": "\nA long short-term memory (LSTM) cell.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MarginRankingLoss", "path": "generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D\nmini-batch `Tensors`, and a label 1D mini-batch tensor yy (containing 1 or\n-1).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MaxPool1d", "path": "generated/torch.nn.maxpool1d#torch.nn.MaxPool1d", "type": "torch.nn", "text": "\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MaxPool2d", "path": "generated/torch.nn.maxpool2d#torch.nn.MaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MaxPool3d", "path": "generated/torch.nn.maxpool3d#torch.nn.MaxPool3d", "type": "torch.nn", "text": "\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MaxUnpool1d", "path": "generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d", "type": "torch.nn", "text": "\nComputes a partial inverse of `MaxPool1d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MaxUnpool2d", "path": "generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d", "type": "torch.nn", "text": "\nComputes a partial inverse of `MaxPool2d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MaxUnpool3d", "path": "generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d", "type": "torch.nn", "text": "\nComputes a partial inverse of `MaxPool3d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module", "path": "generated/torch.nn.module#torch.nn.Module", "type": "torch.nn", "text": "\nBase class for all neural network modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.add_module()", "path": "generated/torch.nn.module#torch.nn.Module.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.apply()", "path": "generated/torch.nn.module#torch.nn.Module.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.bfloat16()", "path": "generated/torch.nn.module#torch.nn.Module.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.buffers()", "path": "generated/torch.nn.module#torch.nn.Module.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.children()", "path": "generated/torch.nn.module#torch.nn.Module.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.cpu()", "path": "generated/torch.nn.module#torch.nn.Module.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.cuda()", "path": "generated/torch.nn.module#torch.nn.Module.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.double()", "path": "generated/torch.nn.module#torch.nn.Module.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.dump_patches", "path": "generated/torch.nn.module#torch.nn.Module.dump_patches", "type": "torch.nn", "text": "\nThis allows better BC support for `load_state_dict()`. In `state_dict()`, the\nversion number will be saved as in the attribute `_metadata` of the returned\nstate dict, and thus pickled. `_metadata` is a dictionary with keys that\nfollow the naming convention of state dict. See `_load_from_state_dict` on how\nto use this information in loading.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.eval()", "path": "generated/torch.nn.module#torch.nn.Module.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.extra_repr()", "path": "generated/torch.nn.module#torch.nn.Module.extra_repr", "type": "torch.nn", "text": "\nSet the extra representation of the module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.float()", "path": "generated/torch.nn.module#torch.nn.Module.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.forward()", "path": "generated/torch.nn.module#torch.nn.Module.forward", "type": "torch.nn", "text": "\nDefines the computation performed at every call.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.half()", "path": "generated/torch.nn.module#torch.nn.Module.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.load_state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.modules()", "path": "generated/torch.nn.module#torch.nn.Module.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.named_buffers()", "path": "generated/torch.nn.module#torch.nn.Module.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.named_children()", "path": "generated/torch.nn.module#torch.nn.Module.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.named_modules()", "path": "generated/torch.nn.module#torch.nn.Module.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.named_parameters()", "path": "generated/torch.nn.module#torch.nn.Module.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.parameters()", "path": "generated/torch.nn.module#torch.nn.Module.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.register_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.register_buffer()", "path": "generated/torch.nn.module#torch.nn.Module.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.register_forward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.register_forward_pre_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.register_full_backward_hook()", "path": "generated/torch.nn.module#torch.nn.Module.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.register_parameter()", "path": "generated/torch.nn.module#torch.nn.Module.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.requires_grad_()", "path": "generated/torch.nn.module#torch.nn.Module.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.state_dict()", "path": "generated/torch.nn.module#torch.nn.Module.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.to()", "path": "generated/torch.nn.module#torch.nn.Module.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.train()", "path": "generated/torch.nn.module#torch.nn.Module.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.type()", "path": "generated/torch.nn.module#torch.nn.Module.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.xpu()", "path": "generated/torch.nn.module#torch.nn.Module.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Module.zero_grad()", "path": "generated/torch.nn.module#torch.nn.Module.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict", "type": "torch.nn", "text": "\nHolds submodules in a dictionary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.clear()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.clear", "type": "torch.nn", "text": "\nRemove all items from the ModuleDict.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.items()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.items", "type": "torch.nn", "text": "\nReturn an iterable of the ModuleDict key/value pairs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.keys()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.keys", "type": "torch.nn", "text": "\nReturn an iterable of the ModuleDict keys.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.pop()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.pop", "type": "torch.nn", "text": "\nRemove key from the ModuleDict and return its module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.update()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.update", "type": "torch.nn", "text": "\nUpdate the `ModuleDict` with the key-value pairs from a mapping or an\niterable, overwriting existing keys.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleDict.values()", "path": "generated/torch.nn.moduledict#torch.nn.ModuleDict.values", "type": "torch.nn", "text": "\nReturn an iterable of the ModuleDict values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleList", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList", "type": "torch.nn", "text": "\nHolds submodules in a list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleList.append()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.append", "type": "torch.nn", "text": "\nAppends a given module to the end of the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleList.extend()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.extend", "type": "torch.nn", "text": "\nAppends modules from a Python iterable to the end of the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ModuleList.insert()", "path": "generated/torch.nn.modulelist#torch.nn.ModuleList.insert", "type": "torch.nn", "text": "\nInsert a given module before a given index in the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin", "type": "torch.nn", "text": "\nA mixin for modules that lazily initialize parameters, also known as \u201clazy\nmodules.\u201d\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.has_uninitialized_params", "type": "torch.nn", "text": "\nCheck if a module has parameters that are not initialized\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters()", "path": "generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin.initialize_parameters", "type": "torch.nn", "text": "\nInitialize parameters according to the input batch properties. This adds an\ninterface to isolate parameter initialization from the forward pass when doing\nparameter shape inference.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.modules.module.register_module_backward_hook()", "path": "generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook common to all the modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.modules.module.register_module_forward_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook", "type": "torch.nn", "text": "\nRegisters a global forward hook for all the modules\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.modules.module.register_module_forward_pre_hook()", "path": "generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook common to all modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MSELoss", "path": "generated/torch.nn.mseloss#torch.nn.MSELoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the mean squared error (squared L2 norm)\nbetween each element in the input xx and target yy .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MultiheadAttention", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention", "type": "torch.nn", "text": "\nAllows the model to jointly attend to information from different\nrepresentation subspaces. See Attention Is All You Need\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MultiheadAttention.forward()", "path": "generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention.forward", "type": "torch.nn", "text": "\nattn_mask: if a 2D mask: (L,S)(L, S) where L is the target sequence length, S\nis the source sequence length.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MultiLabelMarginLoss", "path": "generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a multi-class multi-classification hinge\nloss (margin-based loss) between input xx (a 2D mini-batch `Tensor`) and\noutput yy (which is a 2D `Tensor` of target class indices). For each sample in\nthe mini-batch:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MultiLabelSoftMarginLoss", "path": "generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a multi-label one-versus-all loss based on\nmax-entropy, between input xx and target yy of size (N,C)(N, C) . For each\nsample in the minibatch:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.MultiMarginLoss", "path": "generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a multi-class classification hinge loss\n(margin-based loss) between input xx (a 2D mini-batch `Tensor`) and output yy\n(which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq\n\\text{x.size}(1)-1 ):\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.NLLLoss", "path": "generated/torch.nn.nllloss#torch.nn.NLLLoss", "type": "torch.nn", "text": "\nThe negative log likelihood loss. It is useful to train a classification\nproblem with `C` classes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.PairwiseDistance", "path": "generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance", "type": "torch.nn", "text": "\nComputes the batchwise pairwise distance between vectors v1v_1 , v2v_2 using\nthe p-norm:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parallel.data_parallel()", "path": "nn.functional#torch.nn.parallel.data_parallel", "type": "torch.nn.functional", "text": "\nEvaluates module(input) in parallel across the GPUs given in device_ids.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel", "type": "torch.nn", "text": "\nImplements distributed data parallelism that is based on `torch.distributed`\npackage at the module level.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel.join()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.join", "type": "torch.nn", "text": "\nA context manager to be used in conjunction with an instance of\n`torch.nn.parallel.DistributedDataParallel` to be able to train with uneven\ninputs across participating processes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel.no_sync()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.no_sync", "type": "torch.nn", "text": "\nA context manager to disable gradient synchronizations across DDP processes.\nWithin this context, gradients will be accumulated on module variables, which\nwill later be synchronized in the first forward-backward pass exiting the\ncontext.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parallel.DistributedDataParallel.register_comm_hook()", "path": "generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel.register_comm_hook", "type": "torch.nn", "text": "\nRegisters a communication hook which is an enhancement that provides a\nflexible hook to users where they can specify how DDP aggregates gradients\nacross multiple workers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parameter.Parameter", "path": "generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter", "type": "torch.nn", "text": "\nA kind of Tensor that is to be considered a module parameter.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parameter.UninitializedParameter", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter", "type": "torch.nn", "text": "\nA parameter that is not initialized.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.parameter.UninitializedParameter.materialize()", "path": "generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter.materialize", "type": "torch.nn", "text": "\nCreate a Parameter with the same properties of the uninitialized one. Given a\nshape, it materializes a parameter in the same device and with the same\n`dtype` as the current one or the specified ones in the arguments.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict", "type": "torch.nn", "text": "\nHolds parameters in a dictionary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.clear()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.clear", "type": "torch.nn", "text": "\nRemove all items from the ParameterDict.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.items()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.items", "type": "torch.nn", "text": "\nReturn an iterable of the ParameterDict key/value pairs.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.keys()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.keys", "type": "torch.nn", "text": "\nReturn an iterable of the ParameterDict keys.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.pop()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.pop", "type": "torch.nn", "text": "\nRemove key from the ParameterDict and return its parameter.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.update()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.update", "type": "torch.nn", "text": "\nUpdate the `ParameterDict` with the key-value pairs from a mapping or an\niterable, overwriting existing keys.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterDict.values()", "path": "generated/torch.nn.parameterdict#torch.nn.ParameterDict.values", "type": "torch.nn", "text": "\nReturn an iterable of the ParameterDict values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterList", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList", "type": "torch.nn", "text": "\nHolds parameters in a list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterList.append()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.append", "type": "torch.nn", "text": "\nAppends a given parameter at the end of the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ParameterList.extend()", "path": "generated/torch.nn.parameterlist#torch.nn.ParameterList.extend", "type": "torch.nn", "text": "\nAppends parameters from a Python iterable to the end of the list.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.PixelShuffle", "path": "generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle", "type": "torch.nn", "text": "\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nan upscale factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.PixelUnshuffle", "path": "generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle", "type": "torch.nn", "text": "\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.PoissonNLLLoss", "path": "generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss", "type": "torch.nn", "text": "\nNegative log likelihood loss with Poisson distribution of target.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.PReLU", "path": "generated/torch.nn.prelu#torch.nn.PReLU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.qat.Conv2d", "path": "torch.nn.qat#torch.nn.qat.Conv2d", "type": "Quantization", "text": "\nA Conv2d module attached with FakeQuantize modules for weight, used for\nquantization aware training.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.qat.Conv2d.from_float()", "path": "torch.nn.qat#torch.nn.qat.Conv2d.from_float", "type": "Quantization", "text": "\nCreate a qat module from a float module or qparams_dict\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.qat.Linear", "path": "torch.nn.qat#torch.nn.qat.Linear", "type": "Quantization", "text": "\nA linear module attached with FakeQuantize modules for weight, used for\nquantization aware training.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.qat.Linear.from_float()", "path": "torch.nn.qat#torch.nn.qat.Linear.from_float", "type": "Quantization", "text": "\nCreate a qat module from a float module or qparams_dict\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.BatchNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm2d", "type": "Quantization", "text": "\nThis is the quantized version of `BatchNorm2d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.BatchNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.BatchNorm3d", "type": "Quantization", "text": "\nThis is the quantized version of `BatchNorm3d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv1d", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d", "type": "Quantization", "text": "\nApplies a 1D convolution over a quantized input signal composed of several\nquantized input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv1d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": "\nCreates a quantized module from a float module or qparams_dict.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv2d", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d", "type": "Quantization", "text": "\nApplies a 2D convolution over a quantized input signal composed of several\nquantized input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv2d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": "\nCreates a quantized module from a float module or qparams_dict.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv3d", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d", "type": "Quantization", "text": "\nApplies a 3D convolution over a quantized input signal composed of several\nquantized input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Conv3d.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": "\nCreates a quantized module from a float module or qparams_dict.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.DeQuantize", "path": "torch.nn.quantized#torch.nn.quantized.DeQuantize", "type": "Quantization", "text": "\nDequantizes an incoming tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic", "path": "torch.nn.quantized.dynamic", "type": "torch.nn.quantized.dynamic", "text": "\nA dynamic quantized linear module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.GRUCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.GRUCell", "type": "torch.nn.quantized.dynamic", "text": "\nA gated recurrent unit (GRU) cell\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.Linear", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear", "type": "torch.nn.quantized.dynamic", "text": "\nA dynamic quantized linear module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.Linear.from_float()", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.Linear.from_float", "type": "torch.nn.quantized.dynamic", "text": "\nCreate a dynamic quantized module from a float module or qparams_dict\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.LSTM", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTM", "type": "torch.nn.quantized.dynamic", "text": "\nA dynamic quantized LSTM module with floating point tensor as inputs and\noutputs. We adopt the same interface as `torch.nn.LSTM`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.LSTMCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.LSTMCell", "type": "torch.nn.quantized.dynamic", "text": "\nA long short-term memory (LSTM) cell.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.dynamic.RNNCell", "path": "torch.nn.quantized.dynamic#torch.nn.quantized.dynamic.RNNCell", "type": "torch.nn.quantized.dynamic", "text": "\nAn Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell\nmodule with floating point tensor as inputs and outputs. Weights are quantized\nto 8 bits. We adopt the same interface as `torch.nn.RNNCell`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.ELU", "path": "torch.nn.quantized#torch.nn.quantized.ELU", "type": "Quantization", "text": "\nThis is the quantized equivalent of `ELU`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Embedding", "path": "torch.nn.quantized#torch.nn.quantized.Embedding", "type": "Quantization", "text": "\nA quantized Embedding module with quantized packed weights as inputs. We adopt\nthe same interface as `torch.nn.Embedding`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Embedding.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Embedding.from_float", "type": "Quantization", "text": "\nCreate a quantized embedding module from a float module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.EmbeddingBag", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag", "type": "Quantization", "text": "\nA quantized EmbeddingBag module with quantized packed weights as inputs. We\nadopt the same interface as `torch.nn.EmbeddingBag`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for\ndocumentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.EmbeddingBag.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": "\nCreate a quantized embedding_bag module from a float module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.FloatFunctional", "path": "torch.nn.quantized#torch.nn.quantized.FloatFunctional", "type": "Quantization", "text": "\nState collector class for float operations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.adaptive_avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": "\nApplies a 2D adaptive average pooling over a quantized input signal composed\nof several quantized input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.avg_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": "\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.conv1d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv1d", "type": "Quantization", "text": "\nApplies a 1D convolution over a quantized 1D input composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.conv2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv2d", "type": "Quantization", "text": "\nApplies a 2D convolution over a quantized 2D input composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.conv3d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.conv3d", "type": "Quantization", "text": "\nApplies a 3D convolution over a quantized 3D input composed of several input\nplanes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.hardswish()", "path": "torch.nn.quantized#torch.nn.quantized.functional.hardswish", "type": "Quantization", "text": "\nThis is the quantized version of `hardswish()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.interpolate()", "path": "torch.nn.quantized#torch.nn.quantized.functional.interpolate", "type": "Quantization", "text": "\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.linear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.linear", "type": "Quantization", "text": "\nApplies a linear transformation to the incoming quantized data: y=xAT+by =\nxA^T + b . See `Linear`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.max_pool2d()", "path": "torch.nn.quantized#torch.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": "\nApplies a 2D max pooling over a quantized input signal composed of several\nquantized input planes.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.upsample()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample", "type": "Quantization", "text": "\nUpsamples the input to either the given `size` or the given `scale_factor`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.upsample_bilinear()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": "\nUpsamples the input, using bilinear upsampling.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.functional.upsample_nearest()", "path": "torch.nn.quantized#torch.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": "\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.GroupNorm", "path": "torch.nn.quantized#torch.nn.quantized.GroupNorm", "type": "Quantization", "text": "\nThis is the quantized version of `GroupNorm`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Hardswish", "path": "torch.nn.quantized#torch.nn.quantized.Hardswish", "type": "Quantization", "text": "\nThis is the quantized version of `Hardswish`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.InstanceNorm1d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": "\nThis is the quantized version of `InstanceNorm1d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.InstanceNorm2d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": "\nThis is the quantized version of `InstanceNorm2d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.InstanceNorm3d", "path": "torch.nn.quantized#torch.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": "\nThis is the quantized version of `InstanceNorm3d`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.LayerNorm", "path": "torch.nn.quantized#torch.nn.quantized.LayerNorm", "type": "Quantization", "text": "\nThis is the quantized version of `LayerNorm`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Linear", "path": "torch.nn.quantized#torch.nn.quantized.Linear", "type": "Quantization", "text": "\nA quantized linear module with quantized tensor as inputs and outputs. We\nadopt the same interface as `torch.nn.Linear`, please see\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Linear.from_float()", "path": "torch.nn.quantized#torch.nn.quantized.Linear.from_float", "type": "Quantization", "text": "\nCreate a quantized module from a float module or qparams_dict\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.QFunctional", "path": "torch.nn.quantized#torch.nn.quantized.QFunctional", "type": "Quantization", "text": "\nWrapper class for quantized operations.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.Quantize", "path": "torch.nn.quantized#torch.nn.quantized.Quantize", "type": "Quantization", "text": "\nQuantizes an incoming tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.quantized.ReLU6", "path": "torch.nn.quantized#torch.nn.quantized.ReLU6", "type": "Quantization", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReflectionPad1d", "path": "generated/torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d", "type": "torch.nn", "text": "\nPads the input tensor using the reflection of the input boundary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReflectionPad2d", "path": "generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d", "type": "torch.nn", "text": "\nPads the input tensor using the reflection of the input boundary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReLU", "path": "generated/torch.nn.relu#torch.nn.ReLU", "type": "torch.nn", "text": "\nApplies the rectified linear unit function element-wise:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReLU6", "path": "generated/torch.nn.relu6#torch.nn.ReLU6", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReplicationPad1d", "path": "generated/torch.nn.replicationpad1d#torch.nn.ReplicationPad1d", "type": "torch.nn", "text": "\nPads the input tensor using replication of the input boundary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReplicationPad2d", "path": "generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d", "type": "torch.nn", "text": "\nPads the input tensor using replication of the input boundary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ReplicationPad3d", "path": "generated/torch.nn.replicationpad3d#torch.nn.ReplicationPad3d", "type": "torch.nn", "text": "\nPads the input tensor using replication of the input boundary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.RNN", "path": "generated/torch.nn.rnn#torch.nn.RNN", "type": "torch.nn", "text": "\nApplies a multi-layer Elman RNN with tanh\u2061\\tanh or ReLU\\text{ReLU} non-\nlinearity to an input sequence.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.RNNBase", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase", "type": "torch.nn", "text": "\nResets parameter data pointer so that they can use faster code paths.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.RNNBase.flatten_parameters()", "path": "generated/torch.nn.rnnbase#torch.nn.RNNBase.flatten_parameters", "type": "torch.nn", "text": "\nResets parameter data pointer so that they can use faster code paths.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.RNNCell", "path": "generated/torch.nn.rnncell#torch.nn.RNNCell", "type": "torch.nn", "text": "\nAn Elman RNN cell with tanh or ReLU non-linearity.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.RReLU", "path": "generated/torch.nn.rrelu#torch.nn.RReLU", "type": "torch.nn", "text": "\nApplies the randomized leaky rectified liner unit function, element-wise, as\ndescribed in the paper:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.SELU", "path": "generated/torch.nn.selu#torch.nn.SELU", "type": "torch.nn", "text": "\nApplied element-wise, as:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Sequential", "path": "generated/torch.nn.sequential#torch.nn.Sequential", "type": "torch.nn", "text": "\nA sequential container. Modules will be added to it in the order they are\npassed in the constructor. Alternatively, an ordered dict of modules can also\nbe passed in.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Sigmoid", "path": "generated/torch.nn.sigmoid#torch.nn.Sigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.SiLU", "path": "generated/torch.nn.silu#torch.nn.SiLU", "type": "torch.nn", "text": "\nApplies the silu function, element-wise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.SmoothL1Loss", "path": "generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss", "type": "torch.nn", "text": "\nCreates a criterion that uses a squared term if the absolute element-wise\nerror falls below beta and an L1 term otherwise. It is less sensitive to\noutliers than the `torch.nn.MSELoss` and in some cases prevents exploding\ngradients (e.g. see `Fast R-CNN` paper by Ross Girshick). Omitting a scaling\nfactor of `beta`, this loss is also known as the Huber loss:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.SoftMarginLoss", "path": "generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that optimizes a two-class classification logistic loss\nbetween input tensor xx and target tensor yy (containing 1 or -1).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Softmax", "path": "generated/torch.nn.softmax#torch.nn.Softmax", "type": "torch.nn", "text": "\nApplies the Softmax function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range [0,1]\nand sum to 1.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Softmax2d", "path": "generated/torch.nn.softmax2d#torch.nn.Softmax2d", "type": "torch.nn", "text": "\nApplies SoftMax over features to each spatial location.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Softmin", "path": "generated/torch.nn.softmin#torch.nn.Softmin", "type": "torch.nn", "text": "\nApplies the Softmin function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range `[0,\n1]` and sum to 1.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Softplus", "path": "generated/torch.nn.softplus#torch.nn.Softplus", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Softshrink", "path": "generated/torch.nn.softshrink#torch.nn.Softshrink", "type": "torch.nn", "text": "\nApplies the soft shrinkage function elementwise:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Softsign", "path": "generated/torch.nn.softsign#torch.nn.Softsign", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.SyncBatchNorm", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm", "type": "torch.nn", "text": "\nApplies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D\ninputs with additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm()", "path": "generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm.convert_sync_batchnorm", "type": "torch.nn", "text": "\nHelper function to convert all `BatchNorm*D` layers in the model to\n`torch.nn.SyncBatchNorm` layers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Tanh", "path": "generated/torch.nn.tanh#torch.nn.Tanh", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Tanhshrink", "path": "generated/torch.nn.tanhshrink#torch.nn.Tanhshrink", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Threshold", "path": "generated/torch.nn.threshold#torch.nn.Threshold", "type": "torch.nn", "text": "\nThresholds each element of the input Tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Transformer", "path": "generated/torch.nn.transformer#torch.nn.Transformer", "type": "torch.nn", "text": "\nA transformer model. User is able to modify the attributes as needed. The\narchitecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish\nVaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\nIn Advances in Neural Information Processing Systems, pages 6000-6010. Users\ncan build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding\nparameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Transformer.forward()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.forward", "type": "torch.nn", "text": "\nTake in and process masked source/target sequences.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Transformer.generate_square_subsequent_mask()", "path": "generated/torch.nn.transformer#torch.nn.Transformer.generate_square_subsequent_mask", "type": "torch.nn", "text": "\nGenerate a square mask for the sequence. The masked positions are filled with\nfloat(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoder", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder", "type": "torch.nn", "text": "\nTransformerDecoder is a stack of N decoder layers\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoder.forward()", "path": "generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder.forward", "type": "torch.nn", "text": "\nPass the inputs (and mask) through the decoder layer in turn.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoderLayer", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer", "type": "torch.nn", "text": "\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and\nfeedforward network. This standard decoder layer is based on the paper\n\u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n2017. Attention is all you need. In Advances in Neural Information Processing\nSystems, pages 6000-6010. Users may modify or implement in a different way\nduring application.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerDecoderLayer.forward()", "path": "generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer.forward", "type": "torch.nn", "text": "\nPass the inputs (and mask) through the decoder layer.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoder", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder", "type": "torch.nn", "text": "\nTransformerEncoder is a stack of N encoder layers\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoder.forward()", "path": "generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder.forward", "type": "torch.nn", "text": "\nPass the input through the encoder layers in turn.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoderLayer", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer", "type": "torch.nn", "text": "\nTransformerEncoderLayer is made up of self-attn and feedforward network. This\nstandard encoder layer is based on the paper \u201cAttention Is All You Need\u201d.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\nN Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\nIn Advances in Neural Information Processing Systems, pages 6000-6010. Users\nmay modify or implement in a different way during application.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TransformerEncoderLayer.forward()", "path": "generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer.forward", "type": "torch.nn", "text": "\nPass the input through the encoder layer.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TripletMarginLoss", "path": "generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the triplet loss given an input tensors x1x1\n, x2x2 , x3x3 and a margin with a value greater than 00 . This is used for\nmeasuring a relative similarity between samples. A triplet is composed by `a`,\n`p` and `n` (i.e., `anchor`, `positive examples` and `negative examples`\nrespectively). The shapes of all input tensors should be (N,D)(N, D) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.TripletMarginWithDistanceLoss", "path": "generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the triplet loss given input tensors aa , pp\n, and nn (representing anchor, positive, and negative examples, respectively),\nand a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute\nthe relationship between the anchor and positive example (\u201cpositive distance\u201d)\nand the anchor and negative example (\u201cnegative distance\u201d).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten", "type": "torch.nn", "text": "\nUnflattens a tensor dim expanding it to a desired shape. For use with\n`Sequential`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.add_module()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.apply()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.bfloat16()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.cpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.cuda()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.double()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.eval()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.float()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.half()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.load_state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_buffers()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_children()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_modules()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.named_parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.parameters()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_buffer()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_forward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_forward_pre_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_full_backward_hook()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.register_parameter()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.requires_grad_()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.state_dict()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.to()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.train()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.type()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.xpu()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unflatten.zero_grad()", "path": "generated/torch.nn.unflatten#torch.nn.Unflatten.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Unfold", "path": "generated/torch.nn.unfold#torch.nn.Unfold", "type": "torch.nn", "text": "\nExtracts sliding local blocks from a batched input tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.Upsample", "path": "generated/torch.nn.upsample#torch.nn.Upsample", "type": "torch.nn", "text": "\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric)\ndata.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.UpsamplingBilinear2d", "path": "generated/torch.nn.upsamplingbilinear2d#torch.nn.UpsamplingBilinear2d", "type": "torch.nn", "text": "\nApplies a 2D bilinear upsampling to an input signal composed of several input\nchannels.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.UpsamplingNearest2d", "path": "generated/torch.nn.upsamplingnearest2d#torch.nn.UpsamplingNearest2d", "type": "torch.nn", "text": "\nApplies a 2D nearest neighbor upsampling to an input signal composed of\nseveral input channels.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.clip_grad_norm_()", "path": "generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_", "type": "torch.nn", "text": "\nClips gradient norm of an iterable of parameters.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.clip_grad_value_()", "path": "generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_", "type": "torch.nn", "text": "\nClips gradient of an iterable of parameters at specified value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.parameters_to_vector()", "path": "generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector", "type": "torch.nn", "text": "\nConvert parameters to one vector\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod", "type": "torch.nn", "text": "\nAbstract base class for creation of new pruning techniques.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.apply_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.compute_mask()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.compute_mask", "type": "torch.nn", "text": "\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a random mask to apply on top of the `default_mask`\naccording to the specific pruning method recipe.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.prune()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.BasePruningMethod.remove()", "path": "generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.apply()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.apply_mask()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.prune()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.CustomFromMask.remove()", "path": "generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.custom_from_mask()", "path": "generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by applying\nthe pre-computed mask in `mask`. Modifies module in place (and also return the\nmodified module) by: 1) adding a named buffer called `name+'_mask'`\ncorresponding to the binary mask applied to the parameter `name` by the\npruning method. 2) replacing the parameter `name` by its pruned version, while\nthe original (unpruned) parameter is stored in a new parameter named\n`name+'_orig'`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.global_unstructured()", "path": "generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured", "type": "torch.nn", "text": "\nGlobally prunes tensors corresponding to all parameters in `parameters` by\napplying the specified `pruning_method`. Modifies modules in place by: 1)\nadding a named buffer called `name+'_mask'` corresponding to the binary mask\napplied to the parameter `name` by the pruning method. 2) replacing the\nparameter `name` by its pruned version, while the original (unpruned)\nparameter is stored in a new parameter named `name+'_orig'`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity", "type": "torch.nn", "text": "\nUtility pruning method that does not prune any units but generates the pruning\nparametrization with a mask of ones.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.apply()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.apply_mask()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.prune()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.Identity.remove()", "path": "generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.is_pruned()", "path": "generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned", "type": "torch.nn", "text": "\nCheck whether `module` is pruned by looking for `forward_pre_hooks` in its\nmodules that inherit from the `BasePruningMethod`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured", "type": "torch.nn", "text": "\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the\nlowest L1-norm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.apply()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.prune()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.L1Unstructured.remove()", "path": "generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.l1_unstructured()", "path": "generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units with the lowest L1-norm.\nModifies module in place (and also return the modified module) by: 1) adding a\nnamed buffer called `name+'_mask'` corresponding to the binary mask applied to\nthe parameter `name` by the pruning method. 2) replacing the parameter `name`\nby its pruned version, while the original (unpruned) parameter is stored in a\nnew parameter named `name+'_orig'`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured", "type": "torch.nn", "text": "\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.apply()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.compute_mask", "type": "torch.nn", "text": "\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a mask to apply on top of the `default_mask` by zeroing\nout the channels along the specified dim with the lowest Ln-norm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.prune()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.LnStructured.remove()", "path": "generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.ln_structured()", "path": "generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` with the lowest L``n``-norm. Modifies module in place (and also return\nthe modified module) by: 1) adding a named buffer called `name+'_mask'`\ncorresponding to the binary mask applied to the parameter `name` by the\npruning method. 2) replacing the parameter `name` by its pruned version, while\nthe original (unpruned) parameter is stored in a new parameter named\n`name+'_orig'`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer", "type": "torch.nn", "text": "\nContainer holding a sequence of pruning methods for iterative pruning. Keeps\ntrack of the order in which pruning methods are applied and handles combining\nsuccessive pruning calls.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.add_pruning_method()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.add_pruning_method", "type": "torch.nn", "text": "\nAdds a child pruning `method` to the container.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.apply()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.apply_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.compute_mask()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.compute_mask", "type": "torch.nn", "text": "\nApplies the latest `method` by computing the new partial masks and returning\nits combination with the `default_mask`. The new partial mask should be\ncomputed on the entries or channels that were not zeroed out by the\n`default_mask`. Which portions of the tensor `t` the new mask will be\ncalculated from depends on the `PRUNING_TYPE` (handled by the type handler):\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.prune()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.PruningContainer.remove()", "path": "generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured", "type": "torch.nn", "text": "\nPrune entire (currently unpruned) channels in a tensor at random.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.apply()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.compute_mask()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.compute_mask", "type": "torch.nn", "text": "\nComputes and returns a mask for the input tensor `t`. Starting from a base\n`default_mask` (which should be a mask of ones if the tensor has not been\npruned yet), generate a random mask to apply on top of the `default_mask` by\nrandomly zeroing out channels along the specified dim of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.prune()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomStructured.remove()", "path": "generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured", "type": "torch.nn", "text": "\nPrune (currently unpruned) units in a tensor at random.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply", "type": "torch.nn", "text": "\nAdds the forward pre-hook that enables pruning on the fly and the\nreparametrization of a tensor in terms of the original tensor and the pruning\nmask.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.apply_mask()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.apply_mask", "type": "torch.nn", "text": "\nSimply handles the multiplication between the parameter being pruned and the\ngenerated mask. Fetches the mask and the original tensor from the module and\nreturns the pruned version of the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.prune()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.prune", "type": "torch.nn", "text": "\nComputes and returns a pruned version of input tensor `t` according to the\npruning rule specified in `compute_mask()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.RandomUnstructured.remove()", "path": "generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module. The pruned parameter\nnamed `name` remains permanently pruned, and the parameter named\n`name+'_orig'` is removed from the parameter list. Similarly, the buffer named\n`name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.random_structured()", "path": "generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` selected at random. Modifies module in place (and also return the\nmodified module) by: 1) adding a named buffer called `name+'_mask'`\ncorresponding to the binary mask applied to the parameter `name` by the\npruning method. 2) replacing the parameter `name` by its pruned version, while\nthe original (unpruned) parameter is stored in a new parameter named\n`name+'_orig'`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.random_unstructured()", "path": "generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured", "type": "torch.nn", "text": "\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units selected at random.\nModifies module in place (and also return the modified module) by: 1) adding a\nnamed buffer called `name+'_mask'` corresponding to the binary mask applied to\nthe parameter `name` by the pruning method. 2) replacing the parameter `name`\nby its pruned version, while the original (unpruned) parameter is stored in a\nnew parameter named `name+'_orig'`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.prune.remove()", "path": "generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove", "type": "torch.nn", "text": "\nRemoves the pruning reparameterization from a module and the pruning method\nfrom the forward hook. The pruned parameter named `name` remains permanently\npruned, and the parameter named `name+'_orig'` is removed from the parameter\nlist. Similarly, the buffer named `name+'_mask'` is removed from the buffers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.remove_spectral_norm()", "path": "generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm", "type": "torch.nn", "text": "\nRemoves the spectral normalization reparameterization from a module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.remove_weight_norm()", "path": "generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm", "type": "torch.nn", "text": "\nRemoves the weight normalization reparameterization from a module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence", "type": "torch.nn", "text": "\nHolds the data and list of `batch_sizes` of a packed sequence.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.batch_sizes()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.batch_sizes", "type": "torch.nn", "text": "\nAlias for field number 1\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.count()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.count", "type": "torch.nn", "text": "\nReturn number of occurrences of value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.data()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.data", "type": "torch.nn", "text": "\nAlias for field number 0\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.index()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.index", "type": "torch.nn", "text": "\nReturn first index of value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.is_cuda()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_cuda", "type": "torch.nn", "text": "\nReturns true if `self.data` stored on a gpu\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.is_pinned()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.is_pinned", "type": "torch.nn", "text": "\nReturns true if `self.data` stored on in pinned memory\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.sorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.sorted_indices", "type": "torch.nn", "text": "\nAlias for field number 2\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.to()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.to", "type": "torch.nn", "text": "\nPerforms dtype and/or device conversion on `self.data`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices()", "path": "generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence.unsorted_indices", "type": "torch.nn", "text": "\nAlias for field number 3\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pack_padded_sequence()", "path": "generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence", "type": "torch.nn", "text": "\nPacks a Tensor containing padded sequences of variable length.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pack_sequence()", "path": "generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence", "type": "torch.nn", "text": "\nPacks a list of variable length Tensors\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pad_packed_sequence()", "path": "generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence", "type": "torch.nn", "text": "\nPads a packed batch of variable length sequences.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.rnn.pad_sequence()", "path": "generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence", "type": "torch.nn", "text": "\nPad a list of variable length Tensors with `padding_value`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.spectral_norm()", "path": "generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm", "type": "torch.nn", "text": "\nApplies spectral normalization to a parameter in the given module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.vector_to_parameters()", "path": "generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters", "type": "torch.nn", "text": "\nConvert one vector to the parameters\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.utils.weight_norm()", "path": "generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm", "type": "torch.nn", "text": "\nApplies weight normalization to a parameter in the given module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nn.ZeroPad2d", "path": "generated/torch.nn.zeropad2d#torch.nn.ZeroPad2d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with zero.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.nonzero()", "path": "generated/torch.nonzero#torch.nonzero", "type": "torch", "text": "\nNote\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.norm()", "path": "generated/torch.norm#torch.norm", "type": "torch", "text": "\nReturns the matrix norm or vector norm of a given tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.normal()", "path": "generated/torch.normal#torch.normal", "type": "torch", "text": "\nReturns a tensor of random numbers drawn from separate normal distributions\nwhose mean and standard deviation are given.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.not_equal()", "path": "generated/torch.not_equal#torch.not_equal", "type": "torch", "text": "\nAlias for `torch.ne()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.no_grad", "path": "generated/torch.no_grad#torch.no_grad", "type": "torch", "text": "\nContext-manager that disabled gradient calculation.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.numel()", "path": "generated/torch.numel#torch.numel", "type": "torch", "text": "\nReturns the total number of elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ones()", "path": "generated/torch.ones#torch.ones", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `1`, with the shape defined by\nthe variable argument `size`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ones_like()", "path": "generated/torch.ones_like#torch.ones_like", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `1`, with the same size as\n`input`. `torch.ones_like(input)` is equivalent to `torch.ones(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx", "path": "onnx", "type": "torch.onnx", "text": "\nIndexing\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx.export()", "path": "onnx#torch.onnx.export", "type": "torch.onnx", "text": "\nExport a model into ONNX format. This exporter runs your model once in order\nto get a trace of its execution to be exported; at the moment, it supports a\nlimited set of dynamic models (e.g., RNNs.)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx.export_to_pretty_string()", "path": "onnx#torch.onnx.export_to_pretty_string", "type": "torch.onnx", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx.is_in_onnx_export()", "path": "onnx#torch.onnx.is_in_onnx_export", "type": "torch.onnx", "text": "\nCheck whether it\u2019s in the middle of the ONNX export. This function returns\nTrue in the middle of torch.onnx.export(). torch.onnx.export should be\nexecuted with single thread.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx.operators.shape_as_tensor()", "path": "onnx#torch.onnx.operators.shape_as_tensor", "type": "torch.onnx", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx.register_custom_op_symbolic()", "path": "onnx#torch.onnx.register_custom_op_symbolic", "type": "torch.onnx", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.onnx.select_model_mode_for_export()", "path": "onnx#torch.onnx.select_model_mode_for_export", "type": "torch.onnx", "text": "\nA context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019,\nresetting it when we exit the with-block. A no-op if mode is None.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim", "path": "optim", "type": "torch.optim", "text": "\n`torch.optim` is a package implementing various optimization algorithms. Most\ncommonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adadelta", "path": "optim#torch.optim.Adadelta", "type": "torch.optim", "text": "\nImplements Adadelta algorithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adadelta.step()", "path": "optim#torch.optim.Adadelta.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adagrad", "path": "optim#torch.optim.Adagrad", "type": "torch.optim", "text": "\nImplements Adagrad algorithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adagrad.step()", "path": "optim#torch.optim.Adagrad.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adam", "path": "optim#torch.optim.Adam", "type": "torch.optim", "text": "\nImplements Adam algorithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adam.step()", "path": "optim#torch.optim.Adam.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adamax", "path": "optim#torch.optim.Adamax", "type": "torch.optim", "text": "\nImplements Adamax algorithm (a variant of Adam based on infinity norm).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Adamax.step()", "path": "optim#torch.optim.Adamax.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.AdamW", "path": "optim#torch.optim.AdamW", "type": "torch.optim", "text": "\nImplements AdamW algorithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.AdamW.step()", "path": "optim#torch.optim.AdamW.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.ASGD", "path": "optim#torch.optim.ASGD", "type": "torch.optim", "text": "\nImplements Averaged Stochastic Gradient Descent.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.ASGD.step()", "path": "optim#torch.optim.ASGD.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.LBFGS", "path": "optim#torch.optim.LBFGS", "type": "torch.optim", "text": "\nImplements L-BFGS algorithm, heavily inspired by `minFunc\n<https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.LBFGS.step()", "path": "optim#torch.optim.LBFGS.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingLR", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingLR", "type": "torch.optim", "text": "\nSet the learning rate of each parameter group using a cosine annealing\nschedule, where \u03b7max\\eta_{max} is set to the initial lr and TcurT_{cur} is the\nnumber of epochs since the last restart in SGDR:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "type": "torch.optim", "text": "\nSet the learning rate of each parameter group using a cosine annealing\nschedule, where \u03b7max\\eta_{max} is set to the initial lr, TcurT_{cur} is the\nnumber of epochs since the last restart and TiT_{i} is the number of epochs\nbetween two warm restarts in SGDR:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step()", "path": "optim#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step", "type": "torch.optim", "text": "\nStep could be called after every batch update\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CyclicLR", "path": "optim#torch.optim.lr_scheduler.CyclicLR", "type": "torch.optim", "text": "\nSets the learning rate of each parameter group according to cyclical learning\nrate policy (CLR). The policy cycles the learning rate between two boundaries\nwith a constant frequency, as detailed in the paper Cyclical Learning Rates\nfor Training Neural Networks. The distance between the two boundaries can be\nscaled on a per-iteration or per-cycle basis.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.CyclicLR.get_lr()", "path": "optim#torch.optim.lr_scheduler.CyclicLR.get_lr", "type": "torch.optim", "text": "\nCalculates the learning rate at batch index. This function treats\n`self.last_epoch` as the last batch index.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.ExponentialLR", "path": "optim#torch.optim.lr_scheduler.ExponentialLR", "type": "torch.optim", "text": "\nDecays the learning rate of each parameter group by gamma every epoch. When\nlast_epoch=-1, sets initial lr as lr.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.LambdaLR", "path": "optim#torch.optim.lr_scheduler.LambdaLR", "type": "torch.optim", "text": "\nSets the learning rate of each parameter group to the initial lr times a given\nfunction. When last_epoch=-1, sets initial lr as lr.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.LambdaLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.load_state_dict", "type": "torch.optim", "text": "\nLoads the schedulers state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.LambdaLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.LambdaLR.state_dict", "type": "torch.optim", "text": "\nReturns the state of the scheduler as a `dict`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR", "type": "torch.optim", "text": "\nMultiply the learning rate of each parameter group by the factor given in the\nspecified function. When last_epoch=-1, sets initial lr as lr.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict", "type": "torch.optim", "text": "\nLoads the schedulers state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict()", "path": "optim#torch.optim.lr_scheduler.MultiplicativeLR.state_dict", "type": "torch.optim", "text": "\nReturns the state of the scheduler as a `dict`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.MultiStepLR", "path": "optim#torch.optim.lr_scheduler.MultiStepLR", "type": "torch.optim", "text": "\nDecays the learning rate of each parameter group by gamma once the number of\nepoch reaches one of the milestones. Notice that such decay can happen\nsimultaneously with other changes to the learning rate from outside this\nscheduler. When last_epoch=-1, sets initial lr as lr.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.OneCycleLR", "path": "optim#torch.optim.lr_scheduler.OneCycleLR", "type": "torch.optim", "text": "\nSets the learning rate of each parameter group according to the 1cycle\nlearning rate policy. The 1cycle policy anneals the learning rate from an\ninitial learning rate to some maximum learning rate and then from that maximum\nlearning rate to some minimum learning rate much lower than the initial\nlearning rate. This policy was initially described in the paper Super-\nConvergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.ReduceLROnPlateau", "path": "optim#torch.optim.lr_scheduler.ReduceLROnPlateau", "type": "torch.optim", "text": "\nReduce learning rate when a metric has stopped improving. Models often benefit\nfrom reducing the learning rate by a factor of 2-10 once learning stagnates.\nThis scheduler reads a metrics quantity and if no improvement is seen for a\n\u2018patience\u2019 number of epochs, the learning rate is reduced.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.lr_scheduler.StepLR", "path": "optim#torch.optim.lr_scheduler.StepLR", "type": "torch.optim", "text": "\nDecays the learning rate of each parameter group by gamma every step_size\nepochs. Notice that such decay can happen simultaneously with other changes to\nthe learning rate from outside this scheduler. When last_epoch=-1, sets\ninitial lr as lr.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Optimizer", "path": "optim#torch.optim.Optimizer", "type": "torch.optim", "text": "\nBase class for all optimizers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.add_param_group()", "path": "optim#torch.optim.Optimizer.add_param_group", "type": "torch.optim", "text": "\nAdd a param group to the `Optimizer` s `param_groups`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.load_state_dict()", "path": "optim#torch.optim.Optimizer.load_state_dict", "type": "torch.optim", "text": "\nLoads the optimizer state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.state_dict()", "path": "optim#torch.optim.Optimizer.state_dict", "type": "torch.optim", "text": "\nReturns the state of the optimizer as a `dict`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.step()", "path": "optim#torch.optim.Optimizer.step", "type": "torch.optim", "text": "\nPerforms a single optimization step (parameter update).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Optimizer.zero_grad()", "path": "optim#torch.optim.Optimizer.zero_grad", "type": "torch.optim", "text": "\nSets the gradients of all optimized `torch.Tensor` s to zero.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.RMSprop", "path": "optim#torch.optim.RMSprop", "type": "torch.optim", "text": "\nImplements RMSprop algorithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.RMSprop.step()", "path": "optim#torch.optim.RMSprop.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Rprop", "path": "optim#torch.optim.Rprop", "type": "torch.optim", "text": "\nImplements the resilient backpropagation algorithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.Rprop.step()", "path": "optim#torch.optim.Rprop.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.SGD", "path": "optim#torch.optim.SGD", "type": "torch.optim", "text": "\nImplements stochastic gradient descent (optionally with momentum).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.SGD.step()", "path": "optim#torch.optim.SGD.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.SparseAdam", "path": "optim#torch.optim.SparseAdam", "type": "torch.optim", "text": "\nImplements lazy version of Adam algorithm suitable for sparse tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.optim.SparseAdam.step()", "path": "optim#torch.optim.SparseAdam.step", "type": "torch.optim", "text": "\nPerforms a single optimization step.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.orgqr()", "path": "generated/torch.orgqr#torch.orgqr", "type": "torch", "text": "\nComputes the orthogonal matrix `Q` of a QR factorization, from the `(input,\ninput2)` tuple returned by `torch.geqrf()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ormqr()", "path": "generated/torch.ormqr#torch.ormqr", "type": "torch", "text": "\nMultiplies `mat` (given by `input3`) by the orthogonal `Q` matrix of the QR\nfactorization formed by `torch.geqrf()` that is represented by `(a, tau)`\n(given by (`input`, `input2`)).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.outer()", "path": "generated/torch.outer#torch.outer", "type": "torch", "text": "\nOuter product of `input` and `vec2`. If `input` is a vector of size nn and\n`vec2` is a vector of size mm , then `out` must be a matrix of size (n\u00d7m)(n\n\\times m) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides", "path": "torch.overrides", "type": "torch.overrides", "text": "\nThis module exposes various helper functions for the `__torch_function__`\nprotocol. See Extending torch for more detail on the `__torch_function__`\nprotocol.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.get_ignored_functions()", "path": "torch.overrides#torch.overrides.get_ignored_functions", "type": "torch.overrides", "text": "\nReturn public functions that cannot be overridden by `__torch_function__`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.get_overridable_functions()", "path": "torch.overrides#torch.overrides.get_overridable_functions", "type": "torch.overrides", "text": "\nList functions that are overridable via __torch_function__\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.get_testing_overrides()", "path": "torch.overrides#torch.overrides.get_testing_overrides", "type": "torch.overrides", "text": "\nReturn a dict containing dummy overrides for all overridable functions\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.handle_torch_function()", "path": "torch.overrides#torch.overrides.handle_torch_function", "type": "torch.overrides", "text": "\nImplement a function with checks for `__torch_function__` overrides.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.has_torch_function()", "path": "torch.overrides#torch.overrides.has_torch_function", "type": "torch.overrides", "text": "\nCheck for __torch_function__ implementations in the elements of an iterable.\nConsiders exact `Tensor` s and `Parameter` s non-dispatchable. :param\nrelevant_args: Iterable or aguments to check for __torch_function__ methods.\n:type relevant_args: iterable\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.is_tensor_like()", "path": "torch.overrides#torch.overrides.is_tensor_like", "type": "torch.overrides", "text": "\nReturns `True` if the passed-in input is a Tensor-like.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.is_tensor_method_or_property()", "path": "torch.overrides#torch.overrides.is_tensor_method_or_property", "type": "torch.overrides", "text": "\nReturns True if the function passed in is a handler for a method or property\nbelonging to `torch.Tensor`, as passed into `__torch_function__`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.overrides.wrap_torch_function()", "path": "torch.overrides#torch.overrides.wrap_torch_function", "type": "torch.overrides", "text": "\nWraps a given function with `__torch_function__` -related functionality.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.pca_lowrank()", "path": "generated/torch.pca_lowrank#torch.pca_lowrank", "type": "torch", "text": "\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix,\nbatches of such matrices, or sparse matrix.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.pinverse()", "path": "generated/torch.pinverse#torch.pinverse", "type": "torch", "text": "\nCalculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a\n2D tensor. Please look at Moore-Penrose inverse for more details\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.poisson()", "path": "generated/torch.poisson#torch.poisson", "type": "torch", "text": "\nReturns a tensor of the same size as `input` with each element sampled from a\nPoisson distribution with rate parameter given by the corresponding element in\n`input` i.e.,\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.polar()", "path": "generated/torch.polar#torch.polar", "type": "torch", "text": "\nConstructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value `abs` and angle\n`angle`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.polygamma()", "path": "generated/torch.polygamma#torch.polygamma", "type": "torch", "text": "\nComputes the nthn^{th} derivative of the digamma function on `input`. n\u22650n\n\\geq 0 is called the order of the polygamma function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.pow()", "path": "generated/torch.pow#torch.pow", "type": "torch", "text": "\nTakes the power of each element in `input` with `exponent` and returns a\ntensor with the result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.prod()", "path": "generated/torch.prod#torch.prod", "type": "torch", "text": "\nReturns the product of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.promote_types()", "path": "generated/torch.promote_types#torch.promote_types", "type": "torch", "text": "\nReturns the `torch.dtype` with the smallest size and scalar kind that is not\nsmaller nor of lower kind than either `type1` or `type2`. See type promotion\ndocumentation for more information on the type promotion logic.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.qr()", "path": "generated/torch.qr#torch.qr", "type": "torch", "text": "\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantile()", "path": "generated/torch.quantile#torch.quantile", "type": "torch", "text": "\nReturns the q-th quantiles of all elements in the `input` tensor, doing a\nlinear interpolation when the q-th quantile lies between two data points.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization", "path": "torch.quantization", "type": "torch.quantization", "text": "\nThis module implements the functions you call directly to convert your model\nfrom FP32 to quantized form. For example the `prepare()` is used in post\ntraining quantization to prepares your model for the calibration step and\n`convert()` actually converts the weights to int8 and replaces the operations\nwith their quantized counterparts. There are other helper functions for things\nlike quantizing the input to your model and performing critical fusions like\nconv+relu.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.add_observer_()", "path": "torch.quantization#torch.quantization.add_observer_", "type": "torch.quantization", "text": "\nAdd observer for the leaf child of the module.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.add_quant_dequant()", "path": "torch.quantization#torch.quantization.add_quant_dequant", "type": "torch.quantization", "text": "\nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that\nthis function will modify the children of module inplace and it can return a\nnew module which wraps the input module as well.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.convert()", "path": "torch.quantization#torch.quantization.convert", "type": "torch.quantization", "text": "\nConverts submodules in input module to a different module according to\n`mapping` by calling `from_float` method on the target module class. And\nremove qconfig at the end if remove_qconfig is set to True.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.default_eval_fn()", "path": "torch.quantization#torch.quantization.default_eval_fn", "type": "torch.quantization", "text": "\nDefault evaluation function takes a torch.utils.data.Dataset or a list of\ninput Tensors and run the model on the dataset\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.DeQuantStub", "path": "torch.quantization#torch.quantization.DeQuantStub", "type": "torch.quantization", "text": "\nDequantize stub module, before calibration, this is same as identity, this\nwill be swapped as `nnq.DeQuantize` in `convert`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.FakeQuantize", "path": "torch.quantization#torch.quantization.FakeQuantize", "type": "torch.quantization", "text": "\nSimulate the quantize and dequantize operations in training time. The output\nof this module is given by\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.fuse_modules()", "path": "torch.quantization#torch.quantization.fuse_modules", "type": "torch.quantization", "text": "\nFuses a list of modules into a single module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.get_observer_dict()", "path": "torch.quantization#torch.quantization.get_observer_dict", "type": "torch.quantization", "text": "\nTraverse the modules and save all observers into dict. This is mainly used for\nquantization accuracy debug :param mod: the top module we want to save all\nobservers :param prefix: the prefix for the current module :param target_dict:\nthe dictionary used to save all the observers\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.HistogramObserver", "path": "torch.quantization#torch.quantization.HistogramObserver", "type": "torch.quantization", "text": "\nThe module records the running histogram of tensor values along with min/max\nvalues. `calculate_qparams` will calculate scale and zero_point.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.MinMaxObserver", "path": "torch.quantization#torch.quantization.MinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the running\nmin and max values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.MovingAverageMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAverageMinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the moving\naverage of the min and max values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.MovingAveragePerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.MovingAveragePerChannelMinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.NoopObserver", "path": "torch.quantization#torch.quantization.NoopObserver", "type": "torch.quantization", "text": "\nObserver that doesn\u2019t do anything and just passes its configuration to the\nquantized module\u2019s `.from_float()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.ObserverBase", "path": "torch.quantization#torch.quantization.ObserverBase", "type": "torch.quantization", "text": "\nBase observer Module. Any observer implementation should derive from this\nclass.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.ObserverBase.with_args()", "path": "torch.quantization#torch.quantization.ObserverBase.with_args", "type": "torch.quantization", "text": "\nWrapper that allows creation of class factories.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.PerChannelMinMaxObserver", "path": "torch.quantization#torch.quantization.PerChannelMinMaxObserver", "type": "torch.quantization", "text": "\nObserver module for computing the quantization parameters based on the running\nper channel min and max values.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.prepare()", "path": "torch.quantization#torch.quantization.prepare", "type": "torch.quantization", "text": "\nPrepares a copy of the model for quantization calibration or quantization-\naware training.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.prepare_qat()", "path": "torch.quantization#torch.quantization.prepare_qat", "type": "torch.quantization", "text": "\nPrepares a copy of the model for quantization calibration or quantization-\naware training and converts it to quantized version.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.propagate_qconfig_()", "path": "torch.quantization#torch.quantization.propagate_qconfig_", "type": "torch.quantization", "text": "\nPropagate qconfig through the module hierarchy and assign `qconfig` attribute\non each leaf module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.QConfig", "path": "torch.quantization#torch.quantization.QConfig", "type": "torch.quantization", "text": "\nDescribes how to quantize a layer or a part of the network by providing\nsettings (observer classes) for activations and weights respectively.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.QConfigDynamic", "path": "torch.quantization#torch.quantization.QConfigDynamic", "type": "torch.quantization", "text": "\nDescribes how to dynamically quantize a layer or a part of the network by\nproviding settings (observer classes) for weights.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.quantize()", "path": "torch.quantization#torch.quantization.quantize", "type": "torch.quantization", "text": "\nQuantize the input float model with post training static quantization.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.quantize_dynamic()", "path": "torch.quantization#torch.quantization.quantize_dynamic", "type": "torch.quantization", "text": "\nConverts a float model to dynamic (i.e. weights-only) quantized model.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.quantize_qat()", "path": "torch.quantization#torch.quantization.quantize_qat", "type": "torch.quantization", "text": "\nDo quantization aware training and output a quantized model\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.QuantStub", "path": "torch.quantization#torch.quantization.QuantStub", "type": "torch.quantization", "text": "\nQuantize stub module, before calibration, this is same as an observer, it will\nbe swapped as `nnq.Quantize` in `convert`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.QuantWrapper", "path": "torch.quantization#torch.quantization.QuantWrapper", "type": "torch.quantization", "text": "\nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub\nand surround the call to module with call to quant and dequant modules.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.RecordingObserver", "path": "torch.quantization#torch.quantization.RecordingObserver", "type": "torch.quantization", "text": "\nThe module is mainly for debug and records the tensor values during runtime.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantization.swap_module()", "path": "torch.quantization#torch.quantization.swap_module", "type": "torch.quantization", "text": "\nSwaps the module if it has a quantized counterpart and it has an `observer`\nattached.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantize_per_channel()", "path": "generated/torch.quantize_per_channel#torch.quantize_per_channel", "type": "torch", "text": "\nConverts a float tensor to a per-channel quantized tensor with given scales\nand zero points.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quantize_per_tensor()", "path": "generated/torch.quantize_per_tensor#torch.quantize_per_tensor", "type": "torch", "text": "\nConverts a float tensor to a quantized tensor with given scale and zero point.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine", "type": "torch", "text": "\nThe `torch.quasirandom.SobolEngine` is an engine for generating (scrambled)\nSobol sequences. Sobol sequences are an example of low discrepancy quasi-\nrandom sequences.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.draw()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw", "type": "torch", "text": "\nFunction to draw a sequence of `n` points from a Sobol sequence. Note that the\nsamples are dependent on the previous samples. The size of the result is\n(n,dimension)(n, dimension) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.draw_base2()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.draw_base2", "type": "torch", "text": "\nFunction to draw a sequence of `2**m` points from a Sobol sequence. Note that\nthe samples are dependent on the previous samples. The size of the result is\n(2\u2217\u2217m,dimension)(2**m, dimension) .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.fast_forward()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.fast_forward", "type": "torch", "text": "\nFunction to fast-forward the state of the `SobolEngine` by `n` steps. This is\nequivalent to drawing `n` samples without using the samples.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.quasirandom.SobolEngine.reset()", "path": "generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine.reset", "type": "torch", "text": "\nFunction to reset the `SobolEngine` to base state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.rad2deg()", "path": "generated/torch.rad2deg#torch.rad2deg", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` converted from\nangles in radians to degrees.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.rand()", "path": "generated/torch.rand#torch.rand", "type": "torch", "text": "\nReturns a tensor filled with random numbers from a uniform distribution on the\ninterval [0,1)[0, 1)\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.randint()", "path": "generated/torch.randint#torch.randint", "type": "torch", "text": "\nReturns a tensor filled with random integers generated uniformly between `low`\n(inclusive) and `high` (exclusive).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.randint_like()", "path": "generated/torch.randint_like#torch.randint_like", "type": "torch", "text": "\nReturns a tensor with the same shape as Tensor `input` filled with random\nintegers generated uniformly between `low` (inclusive) and `high` (exclusive).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.randn()", "path": "generated/torch.randn#torch.randn", "type": "torch", "text": "\nReturns a tensor filled with random numbers from a normal distribution with\nmean `0` and variance `1` (also called the standard normal distribution).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.randn_like()", "path": "generated/torch.randn_like#torch.randn_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a normal distribution with mean 0 and variance 1.\n`torch.randn_like(input)` is equivalent to `torch.randn(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random", "path": "random", "type": "torch.random", "text": "\nForks the RNG, so that when you return, the RNG is reset to the state that it\nwas previously in.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random.fork_rng()", "path": "random#torch.random.fork_rng", "type": "torch.random", "text": "\nForks the RNG, so that when you return, the RNG is reset to the state that it\nwas previously in.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random.get_rng_state()", "path": "random#torch.random.get_rng_state", "type": "torch.random", "text": "\nReturns the random number generator state as a `torch.ByteTensor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random.initial_seed()", "path": "random#torch.random.initial_seed", "type": "torch.random", "text": "\nReturns the initial seed for generating random numbers as a Python `long`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random.manual_seed()", "path": "random#torch.random.manual_seed", "type": "torch.random", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random.seed()", "path": "random#torch.random.seed", "type": "torch.random", "text": "\nSets the seed for generating random numbers to a non-deterministic random\nnumber. Returns a 64 bit number used to seed the RNG.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.random.set_rng_state()", "path": "random#torch.random.set_rng_state", "type": "torch.random", "text": "\nSets the random number generator state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.randperm()", "path": "generated/torch.randperm#torch.randperm", "type": "torch", "text": "\nReturns a random permutation of integers from `0` to `n - 1`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.rand_like()", "path": "generated/torch.rand_like#torch.rand_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a uniform distribution on the interval [0,1)[0, 1) .\n`torch.rand_like(input)` is equivalent to `torch.rand(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.range()", "path": "generated/torch.range#torch.range", "type": "torch", "text": "\nReturns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rfloor + 1 with values from `start` to `end`\nwith step `step`. Step is the gap between two values in the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.ravel()", "path": "generated/torch.ravel#torch.ravel", "type": "torch", "text": "\nReturn a contiguous flattened tensor. A copy is made only if needed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.real()", "path": "generated/torch.real#torch.real", "type": "torch", "text": "\nReturns a new tensor containing real values of the `self` tensor. The returned\ntensor and `self` share the same underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.reciprocal()", "path": "generated/torch.reciprocal#torch.reciprocal", "type": "torch", "text": "\nReturns a new tensor with the reciprocal of the elements of `input`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.remainder()", "path": "generated/torch.remainder#torch.remainder", "type": "torch", "text": "\nComputes the element-wise remainder of division.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.renorm()", "path": "generated/torch.renorm#torch.renorm", "type": "torch", "text": "\nReturns a tensor where each sub-tensor of `input` along dimension `dim` is\nnormalized such that the `p`-norm of the sub-tensor is lower than the value\n`maxnorm`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.repeat_interleave()", "path": "generated/torch.repeat_interleave#torch.repeat_interleave", "type": "torch", "text": "\nRepeat elements of a tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.reshape()", "path": "generated/torch.reshape#torch.reshape", "type": "torch", "text": "\nReturns a tensor with the same data and number of elements as `input`, but\nwith the specified shape. When possible, the returned tensor will be a view of\n`input`. Otherwise, it will be a copy. Contiguous inputs and inputs with\ncompatible strides can be reshaped without copying, but you should not depend\non the copying vs. viewing behavior.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.result_type()", "path": "generated/torch.result_type#torch.result_type", "type": "torch", "text": "\nReturns the `torch.dtype` that would result from performing an arithmetic\noperation on the provided input tensors. See type promotion documentation for\nmore information on the type promotion logic.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.roll()", "path": "generated/torch.roll#torch.roll", "type": "torch", "text": "\nRoll the tensor along the given dimension(s). Elements that are shifted beyond\nthe last position are re-introduced at the first position. If a dimension is\nnot specified, the tensor will be flattened before rolling and then restored\nto the original shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.rot90()", "path": "generated/torch.rot90#torch.rot90", "type": "torch", "text": "\nRotate a n-D tensor by 90 degrees in the plane specified by dims axis.\nRotation direction is from the first towards the second axis if k > 0, and\nfrom the second towards the first for k < 0.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.round()", "path": "generated/torch.round#torch.round", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` rounded to the\nclosest integer.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.row_stack()", "path": "generated/torch.row_stack#torch.row_stack", "type": "torch", "text": "\nAlias of `torch.vstack()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.rsqrt()", "path": "generated/torch.rsqrt#torch.rsqrt", "type": "torch", "text": "\nReturns a new tensor with the reciprocal of the square-root of each of the\nelements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.save()", "path": "generated/torch.save#torch.save", "type": "torch", "text": "\nSaves an object to a disk file.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.scatter()", "path": "generated/torch.scatter#torch.scatter", "type": "torch", "text": "\nOut-of-place version of `torch.Tensor.scatter_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.scatter_add()", "path": "generated/torch.scatter_add#torch.scatter_add", "type": "torch", "text": "\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.searchsorted()", "path": "generated/torch.searchsorted#torch.searchsorted", "type": "torch", "text": "\nFind the indices from the innermost dimension of `sorted_sequence` such that,\nif the corresponding values in `values` were inserted before the indices, the\norder of the corresponding innermost dimension within `sorted_sequence` would\nbe preserved. Return a new tensor with the same size as `values`. If `right`\nis False (default), then the left boundary of `sorted_sequence` is closed.\nMore formally, the returned index satisfies the following rules:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.seed()", "path": "generated/torch.seed#torch.seed", "type": "torch", "text": "\nSets the seed for generating random numbers to a non-deterministic random\nnumber. Returns a 64 bit number used to seed the RNG.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_default_dtype()", "path": "generated/torch.set_default_dtype#torch.set_default_dtype", "type": "torch", "text": "\nSets the default floating point dtype to `d`. This dtype is:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_default_tensor_type()", "path": "generated/torch.set_default_tensor_type#torch.set_default_tensor_type", "type": "torch", "text": "\nSets the default `torch.Tensor` type to floating point tensor type `t`. This\ntype will also be used as default floating point type for type inference in\n`torch.tensor()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_flush_denormal()", "path": "generated/torch.set_flush_denormal#torch.set_flush_denormal", "type": "torch", "text": "\nDisables denormal floating numbers on CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_grad_enabled", "path": "generated/torch.set_grad_enabled#torch.set_grad_enabled", "type": "torch", "text": "\nContext-manager that sets gradient calculation to on or off.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_num_interop_threads()", "path": "generated/torch.set_num_interop_threads#torch.set_num_interop_threads", "type": "torch", "text": "\nSets the number of threads used for interop parallelism (e.g. in JIT\ninterpreter) on CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_num_threads()", "path": "generated/torch.set_num_threads#torch.set_num_threads", "type": "torch", "text": "\nSets the number of threads used for intraop parallelism on CPU.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_printoptions()", "path": "generated/torch.set_printoptions#torch.set_printoptions", "type": "torch", "text": "\nSet options for printing. Items shamelessly taken from NumPy\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.set_rng_state()", "path": "generated/torch.set_rng_state#torch.set_rng_state", "type": "torch", "text": "\nSets the random number generator state.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sgn()", "path": "generated/torch.sgn#torch.sgn", "type": "torch", "text": "\nFor complex tensors, this function returns a new tensor whose elemants have\nthe same angle as that of the elements of `input` and absolute value 1. For a\nnon-complex tensor, this function returns the signs of the elements of `input`\n(see `torch.sign()`).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sigmoid()", "path": "generated/torch.sigmoid#torch.sigmoid", "type": "torch", "text": "\nReturns a new tensor with the sigmoid of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sign()", "path": "generated/torch.sign#torch.sign", "type": "torch", "text": "\nReturns a new tensor with the signs of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.signbit()", "path": "generated/torch.signbit#torch.signbit", "type": "torch", "text": "\nTests if each element of `input` has its sign bit set (is less than zero) or\nnot.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sin()", "path": "generated/torch.sin#torch.sin", "type": "torch", "text": "\nReturns a new tensor with the sine of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sinc()", "path": "generated/torch.sinc#torch.sinc", "type": "torch", "text": "\nComputes the normalized sinc of `input.`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sinh()", "path": "generated/torch.sinh#torch.sinh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic sine of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.slogdet()", "path": "generated/torch.slogdet#torch.slogdet", "type": "torch", "text": "\nCalculates the sign and log absolute value of the determinant(s) of a square\nmatrix or batches of square matrices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.smm()", "path": "sparse#torch.smm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of the sparse matrix `input` with the dense\nmatrix `mat`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.solve()", "path": "generated/torch.solve#torch.solve", "type": "torch", "text": "\nThis function returns the solution to the system of linear equations\nrepresented by AX=BAX = B and the LU factorization of A, in order as a\nnamedtuple `solution, LU`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sort()", "path": "generated/torch.sort#torch.sort", "type": "torch", "text": "\nSorts the elements of the `input` tensor along a given dimension in ascending\norder by value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse", "path": "sparse", "type": "torch.sparse", "text": "\nPyTorch provides `torch.Tensor` to represent a multi-dimensional array\ncontaining elements of a single data type. By default, array elements are\nstored contiguously in memory leading to efficient implementations of various\narray processing algorithms that relay on the fast access to array elements.\nHowever, there exists an important class of multi-dimensional arrays, so-\ncalled sparse arrays, where the contiguous memory storage of array elements\nturns out to be suboptimal. Sparse arrays have a property of having a vast\nportion of elements being equal to zero which means that a lot of memory as\nwell as processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO, CSR/CSC,\nLIL, etc.) have been developed that are optimized for a particular structure\nof non-zero elements in sparse arrays as well as for specific operations on\nthe arrays.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse.addmm()", "path": "sparse#torch.sparse.addmm", "type": "torch.sparse", "text": "\nThis function does exact same thing as `torch.addmm()` in the forward, except\nthat it supports backward for sparse matrix `mat1`. `mat1` need to have\n`sparse_dim = 2`. Note that the gradients of `mat1` is a coalesced sparse\ntensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse.log_softmax()", "path": "sparse#torch.sparse.log_softmax", "type": "torch.sparse", "text": "\nApplies a softmax function followed by logarithm.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse.mm()", "path": "sparse#torch.sparse.mm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of the sparse matrix `mat1` and the (sparse\nor strided) matrix `mat2`. Similar to `torch.mm()`, If `mat1` is a (n\u00d7m)(n\n\\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p) tensor, out will be a (n\u00d7p)(n\n\\times p) tensor. `mat1` need to have `sparse_dim = 2`. This function also\nsupports backward for both matrices. Note that the gradients of `mat1` is a\ncoalesced sparse tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse.softmax()", "path": "sparse#torch.sparse.softmax", "type": "torch.sparse", "text": "\nApplies a softmax function.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse.sum()", "path": "sparse#torch.sparse.sum", "type": "torch.sparse", "text": "\nReturns the sum of each row of the sparse tensor `input` in the given\ndimensions `dim`. If `dim` is a list of dimensions, reduce over all of them.\nWhen sum over all `sparse_dim`, this method returns a dense tensor instead of\na sparse tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sparse_coo_tensor()", "path": "generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor", "type": "torch", "text": "\nConstructs a sparse tensor in COO(rdinate) format with specified values at the\ngiven `indices`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.split()", "path": "generated/torch.split#torch.split", "type": "torch", "text": "\nSplits the tensor into chunks. Each chunk is a view of the original tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sqrt()", "path": "generated/torch.sqrt#torch.sqrt", "type": "torch", "text": "\nReturns a new tensor with the square-root of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.square()", "path": "generated/torch.square#torch.square", "type": "torch", "text": "\nReturns a new tensor with the square of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.squeeze()", "path": "generated/torch.squeeze#torch.squeeze", "type": "torch", "text": "\nReturns a tensor with all the dimensions of `input` of size `1` removed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sspaddmm()", "path": "sparse#torch.sspaddmm", "type": "torch.sparse", "text": "\nMatrix multiplies a sparse tensor `mat1` with a dense tensor `mat2`, then adds\nthe sparse tensor `input` to the result.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.stack()", "path": "generated/torch.stack#torch.stack", "type": "torch", "text": "\nConcatenates a sequence of tensors along a new dimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.std()", "path": "generated/torch.std#torch.std", "type": "torch", "text": "\nReturns the standard-deviation of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.std_mean()", "path": "generated/torch.std_mean#torch.std_mean", "type": "torch", "text": "\nReturns the standard-deviation and mean of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.stft()", "path": "generated/torch.stft#torch.stft", "type": "torch", "text": "\nShort-time Fourier transform (STFT).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Storage", "path": "storage", "type": "torch.Storage", "text": "\nA `torch.Storage` is a contiguous, one-dimensional array of a single data\ntype.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sub()", "path": "generated/torch.sub#torch.sub", "type": "torch", "text": "\nSubtracts `other`, scaled by `alpha`, from `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.subtract()", "path": "generated/torch.subtract#torch.subtract", "type": "torch", "text": "\nAlias for `torch.sub()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.sum()", "path": "generated/torch.sum#torch.sum", "type": "torch", "text": "\nReturns the sum of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.svd()", "path": "generated/torch.svd#torch.svd", "type": "torch", "text": "\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`. The singular value decomposition is represented as a\nnamedtuple (`U,S,V`), such that `input` = `U` diag(`S`) `V\u1d34`, where `V\u1d34` is\nthe transpose of `V` for the real-valued inputs, or the conjugate transpose of\n`V` for the complex-valued inputs. If `input` is a batch of tensors, then `U`,\n`S`, and `V` are also batched with the same batch dimensions as `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.svd_lowrank()", "path": "generated/torch.svd_lowrank#torch.svd_lowrank", "type": "torch", "text": "\nReturn the singular value decomposition `(U, S, V)` of a matrix, batches of\nmatrices, or a sparse matrix AA such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T\n. In case MM is given, then SVD is computed for the matrix A\u2212MA - M .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.swapaxes()", "path": "generated/torch.swapaxes#torch.swapaxes", "type": "torch", "text": "\nAlias for `torch.transpose()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.swapdims()", "path": "generated/torch.swapdims#torch.swapdims", "type": "torch", "text": "\nAlias for `torch.transpose()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.symeig()", "path": "generated/torch.symeig#torch.symeig", "type": "torch", "text": "\nThis function returns eigenvalues and eigenvectors of a real symmetric matrix\n`input` or a batch of real symmetric matrices, represented by a namedtuple\n(eigenvalues, eigenvectors).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.t()", "path": "generated/torch.t#torch.t", "type": "torch", "text": "\nExpects `input` to be <= 2-D tensor and transposes dimensions 0 and 1.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.take()", "path": "generated/torch.take#torch.take", "type": "torch", "text": "\nReturns a new tensor with the elements of `input` at the given indices. The\ninput tensor is treated as if it were viewed as a 1-D tensor. The result takes\nthe same shape as the indices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tan()", "path": "generated/torch.tan#torch.tan", "type": "torch", "text": "\nReturns a new tensor with the tangent of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tanh()", "path": "generated/torch.tanh#torch.tanh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic tangent of the elements of `input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor", "path": "tensors", "type": "torch.Tensor", "text": "\nA `torch.Tensor` is a multi-dimensional matrix containing elements of a single\ndata type.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tensor()", "path": "generated/torch.tensor#torch.tensor", "type": "torch", "text": "\nConstructs a tensor with `data`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.abs()", "path": "tensors#torch.Tensor.abs", "type": "torch.Tensor", "text": "\nSee `torch.abs()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.absolute()", "path": "tensors#torch.Tensor.absolute", "type": "torch.Tensor", "text": "\nAlias for `abs()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.absolute_()", "path": "tensors#torch.Tensor.absolute_", "type": "torch.Tensor", "text": "\nIn-place version of `absolute()` Alias for `abs_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.abs_()", "path": "tensors#torch.Tensor.abs_", "type": "torch.Tensor", "text": "\nIn-place version of `abs()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.acos()", "path": "tensors#torch.Tensor.acos", "type": "torch.Tensor", "text": "\nSee `torch.acos()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.acosh()", "path": "tensors#torch.Tensor.acosh", "type": "torch.Tensor", "text": "\nSee `torch.acosh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.acosh_()", "path": "tensors#torch.Tensor.acosh_", "type": "torch.Tensor", "text": "\nIn-place version of `acosh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.acos_()", "path": "tensors#torch.Tensor.acos_", "type": "torch.Tensor", "text": "\nIn-place version of `acos()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.add()", "path": "tensors#torch.Tensor.add", "type": "torch.Tensor", "text": "\nAdd a scalar or tensor to `self` tensor. If both `alpha` and `other` are\nspecified, each element of `other` is scaled by `alpha` before being used.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addbmm()", "path": "tensors#torch.Tensor.addbmm", "type": "torch.Tensor", "text": "\nSee `torch.addbmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addbmm_()", "path": "tensors#torch.Tensor.addbmm_", "type": "torch.Tensor", "text": "\nIn-place version of `addbmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addcdiv()", "path": "tensors#torch.Tensor.addcdiv", "type": "torch.Tensor", "text": "\nSee `torch.addcdiv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addcdiv_()", "path": "tensors#torch.Tensor.addcdiv_", "type": "torch.Tensor", "text": "\nIn-place version of `addcdiv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addcmul()", "path": "tensors#torch.Tensor.addcmul", "type": "torch.Tensor", "text": "\nSee `torch.addcmul()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addcmul_()", "path": "tensors#torch.Tensor.addcmul_", "type": "torch.Tensor", "text": "\nIn-place version of `addcmul()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addmm()", "path": "tensors#torch.Tensor.addmm", "type": "torch.Tensor", "text": "\nSee `torch.addmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addmm_()", "path": "tensors#torch.Tensor.addmm_", "type": "torch.Tensor", "text": "\nIn-place version of `addmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addmv()", "path": "tensors#torch.Tensor.addmv", "type": "torch.Tensor", "text": "\nSee `torch.addmv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addmv_()", "path": "tensors#torch.Tensor.addmv_", "type": "torch.Tensor", "text": "\nIn-place version of `addmv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addr()", "path": "tensors#torch.Tensor.addr", "type": "torch.Tensor", "text": "\nSee `torch.addr()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.addr_()", "path": "tensors#torch.Tensor.addr_", "type": "torch.Tensor", "text": "\nIn-place version of `addr()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.add_()", "path": "tensors#torch.Tensor.add_", "type": "torch.Tensor", "text": "\nIn-place version of `add()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.align_as()", "path": "named_tensor#torch.Tensor.align_as", "type": "Named Tensors", "text": "\nPermutes the dimensions of the `self` tensor to match the dimension order in\nthe `other` tensor, adding size-one dims for any new names.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.align_to()", "path": "named_tensor#torch.Tensor.align_to", "type": "Named Tensors", "text": "\nPermutes the dimensions of the `self` tensor to match the order specified in\n`names`, adding size-one dims for any new names.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.all()", "path": "tensors#torch.Tensor.all", "type": "torch.Tensor", "text": "\nSee `torch.all()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.allclose()", "path": "tensors#torch.Tensor.allclose", "type": "torch.Tensor", "text": "\nSee `torch.allclose()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.amax()", "path": "tensors#torch.Tensor.amax", "type": "torch.Tensor", "text": "\nSee `torch.amax()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.amin()", "path": "tensors#torch.Tensor.amin", "type": "torch.Tensor", "text": "\nSee `torch.amin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.angle()", "path": "tensors#torch.Tensor.angle", "type": "torch.Tensor", "text": "\nSee `torch.angle()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.any()", "path": "tensors#torch.Tensor.any", "type": "torch.Tensor", "text": "\nSee `torch.any()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.apply_()", "path": "tensors#torch.Tensor.apply_", "type": "torch.Tensor", "text": "\nApplies the function `callable` to each element in the tensor, replacing each\nelement with the value returned by `callable`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arccos()", "path": "tensors#torch.Tensor.arccos", "type": "torch.Tensor", "text": "\nSee `torch.arccos()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arccosh()", "path": "tensors#torch.Tensor.arccosh", "type": "torch.Tensor", "text": "\nacosh() -> Tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arccosh_()", "path": "tensors#torch.Tensor.arccosh_", "type": "torch.Tensor", "text": "\nacosh_() -> Tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arccos_()", "path": "tensors#torch.Tensor.arccos_", "type": "torch.Tensor", "text": "\nIn-place version of `arccos()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arcsin()", "path": "tensors#torch.Tensor.arcsin", "type": "torch.Tensor", "text": "\nSee `torch.arcsin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arcsinh()", "path": "tensors#torch.Tensor.arcsinh", "type": "torch.Tensor", "text": "\nSee `torch.arcsinh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arcsinh_()", "path": "tensors#torch.Tensor.arcsinh_", "type": "torch.Tensor", "text": "\nIn-place version of `arcsinh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arcsin_()", "path": "tensors#torch.Tensor.arcsin_", "type": "torch.Tensor", "text": "\nIn-place version of `arcsin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arctan()", "path": "tensors#torch.Tensor.arctan", "type": "torch.Tensor", "text": "\nSee `torch.arctan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arctanh()", "path": "tensors#torch.Tensor.arctanh", "type": "torch.Tensor", "text": "\nSee `torch.arctanh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arctanh_()", "path": "tensors#torch.Tensor.arctanh_", "type": "torch.Tensor", "text": "\nIn-place version of `arctanh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.arctan_()", "path": "tensors#torch.Tensor.arctan_", "type": "torch.Tensor", "text": "\nIn-place version of `arctan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.argmax()", "path": "tensors#torch.Tensor.argmax", "type": "torch.Tensor", "text": "\nSee `torch.argmax()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.argmin()", "path": "tensors#torch.Tensor.argmin", "type": "torch.Tensor", "text": "\nSee `torch.argmin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.argsort()", "path": "tensors#torch.Tensor.argsort", "type": "torch.Tensor", "text": "\nSee `torch.argsort()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.asin()", "path": "tensors#torch.Tensor.asin", "type": "torch.Tensor", "text": "\nSee `torch.asin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.asinh()", "path": "tensors#torch.Tensor.asinh", "type": "torch.Tensor", "text": "\nSee `torch.asinh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.asinh_()", "path": "tensors#torch.Tensor.asinh_", "type": "torch.Tensor", "text": "\nIn-place version of `asinh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.asin_()", "path": "tensors#torch.Tensor.asin_", "type": "torch.Tensor", "text": "\nIn-place version of `asin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.as_strided()", "path": "tensors#torch.Tensor.as_strided", "type": "torch.Tensor", "text": "\nSee `torch.as_strided()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.as_subclass()", "path": "tensors#torch.Tensor.as_subclass", "type": "torch.Tensor", "text": "\nMakes a `cls` instance with the same data pointer as `self`. Changes in the\noutput mirror changes in `self`, and the output stays attached to the autograd\ngraph. `cls` must be a subclass of `Tensor`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.atan()", "path": "tensors#torch.Tensor.atan", "type": "torch.Tensor", "text": "\nSee `torch.atan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.atan2()", "path": "tensors#torch.Tensor.atan2", "type": "torch.Tensor", "text": "\nSee `torch.atan2()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.atan2_()", "path": "tensors#torch.Tensor.atan2_", "type": "torch.Tensor", "text": "\nIn-place version of `atan2()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.atanh()", "path": "tensors#torch.Tensor.atanh", "type": "torch.Tensor", "text": "\nSee `torch.atanh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.atanh_()", "path": "tensors#torch.Tensor.atanh_", "type": "torch.Tensor", "text": "\nIn-place version of `atanh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.atan_()", "path": "tensors#torch.Tensor.atan_", "type": "torch.Tensor", "text": "\nIn-place version of `atan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.backward()", "path": "autograd#torch.Tensor.backward", "type": "torch.autograd", "text": "\nComputes the gradient of current tensor w.r.t. graph leaves.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.baddbmm()", "path": "tensors#torch.Tensor.baddbmm", "type": "torch.Tensor", "text": "\nSee `torch.baddbmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.baddbmm_()", "path": "tensors#torch.Tensor.baddbmm_", "type": "torch.Tensor", "text": "\nIn-place version of `baddbmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bernoulli()", "path": "tensors#torch.Tensor.bernoulli", "type": "torch.Tensor", "text": "\nReturns a result tensor where each result[i]\\texttt{result[i]} is\nindependently sampled from\nBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]}) . `self` must have\nfloating point `dtype`, and the result will have the same `dtype`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bernoulli_()", "path": "tensors#torch.Tensor.bernoulli_", "type": "torch.Tensor", "text": "\nFills each location of `self` with an independent sample from\nBernoulli(p)\\text{Bernoulli}(\\texttt{p}) . `self` can have integral `dtype`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bfloat16()", "path": "tensors#torch.Tensor.bfloat16", "type": "torch.Tensor", "text": "\n`self.bfloat16()` is equivalent to `self.to(torch.bfloat16)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bincount()", "path": "tensors#torch.Tensor.bincount", "type": "torch.Tensor", "text": "\nSee `torch.bincount()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_and()", "path": "tensors#torch.Tensor.bitwise_and", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_and()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_and_()", "path": "tensors#torch.Tensor.bitwise_and_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_and()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_not()", "path": "tensors#torch.Tensor.bitwise_not", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_not()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_not_()", "path": "tensors#torch.Tensor.bitwise_not_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_not()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_or()", "path": "tensors#torch.Tensor.bitwise_or", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_or()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_or_()", "path": "tensors#torch.Tensor.bitwise_or_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_or()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_xor()", "path": "tensors#torch.Tensor.bitwise_xor", "type": "torch.Tensor", "text": "\nSee `torch.bitwise_xor()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bitwise_xor_()", "path": "tensors#torch.Tensor.bitwise_xor_", "type": "torch.Tensor", "text": "\nIn-place version of `bitwise_xor()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bmm()", "path": "tensors#torch.Tensor.bmm", "type": "torch.Tensor", "text": "\nSee `torch.bmm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.bool()", "path": "tensors#torch.Tensor.bool", "type": "torch.Tensor", "text": "\n`self.bool()` is equivalent to `self.to(torch.bool)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.broadcast_to()", "path": "tensors#torch.Tensor.broadcast_to", "type": "torch.Tensor", "text": "\nSee `torch.broadcast_to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.byte()", "path": "tensors#torch.Tensor.byte", "type": "torch.Tensor", "text": "\n`self.byte()` is equivalent to `self.to(torch.uint8)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cauchy_()", "path": "tensors#torch.Tensor.cauchy_", "type": "torch.Tensor", "text": "\nFills the tensor with numbers drawn from the Cauchy distribution:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ceil()", "path": "tensors#torch.Tensor.ceil", "type": "torch.Tensor", "text": "\nSee `torch.ceil()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ceil_()", "path": "tensors#torch.Tensor.ceil_", "type": "torch.Tensor", "text": "\nIn-place version of `ceil()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.char()", "path": "tensors#torch.Tensor.char", "type": "torch.Tensor", "text": "\n`self.char()` is equivalent to `self.to(torch.int8)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cholesky()", "path": "tensors#torch.Tensor.cholesky", "type": "torch.Tensor", "text": "\nSee `torch.cholesky()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cholesky_inverse()", "path": "tensors#torch.Tensor.cholesky_inverse", "type": "torch.Tensor", "text": "\nSee `torch.cholesky_inverse()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cholesky_solve()", "path": "tensors#torch.Tensor.cholesky_solve", "type": "torch.Tensor", "text": "\nSee `torch.cholesky_solve()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.chunk()", "path": "tensors#torch.Tensor.chunk", "type": "torch.Tensor", "text": "\nSee `torch.chunk()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.clamp()", "path": "tensors#torch.Tensor.clamp", "type": "torch.Tensor", "text": "\nSee `torch.clamp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.clamp_()", "path": "tensors#torch.Tensor.clamp_", "type": "torch.Tensor", "text": "\nIn-place version of `clamp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.clip()", "path": "tensors#torch.Tensor.clip", "type": "torch.Tensor", "text": "\nAlias for `clamp()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.clip_()", "path": "tensors#torch.Tensor.clip_", "type": "torch.Tensor", "text": "\nAlias for `clamp_()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.clone()", "path": "tensors#torch.Tensor.clone", "type": "torch.Tensor", "text": "\nSee `torch.clone()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.coalesce()", "path": "sparse#torch.Tensor.coalesce", "type": "torch.sparse", "text": "\nReturns a coalesced copy of `self` if `self` is an uncoalesced tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.conj()", "path": "tensors#torch.Tensor.conj", "type": "torch.Tensor", "text": "\nSee `torch.conj()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.contiguous()", "path": "tensors#torch.Tensor.contiguous", "type": "torch.Tensor", "text": "\nReturns a contiguous in memory tensor containing the same data as `self`\ntensor. If `self` tensor is already in the specified memory format, this\nfunction returns the `self` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.copysign()", "path": "tensors#torch.Tensor.copysign", "type": "torch.Tensor", "text": "\nSee `torch.copysign()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.copysign_()", "path": "tensors#torch.Tensor.copysign_", "type": "torch.Tensor", "text": "\nIn-place version of `copysign()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.copy_()", "path": "tensors#torch.Tensor.copy_", "type": "torch.Tensor", "text": "\nCopies the elements from `src` into `self` tensor and returns `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cos()", "path": "tensors#torch.Tensor.cos", "type": "torch.Tensor", "text": "\nSee `torch.cos()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cosh()", "path": "tensors#torch.Tensor.cosh", "type": "torch.Tensor", "text": "\nSee `torch.cosh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cosh_()", "path": "tensors#torch.Tensor.cosh_", "type": "torch.Tensor", "text": "\nIn-place version of `cosh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cos_()", "path": "tensors#torch.Tensor.cos_", "type": "torch.Tensor", "text": "\nIn-place version of `cos()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.count_nonzero()", "path": "tensors#torch.Tensor.count_nonzero", "type": "torch.Tensor", "text": "\nSee `torch.count_nonzero()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cpu()", "path": "tensors#torch.Tensor.cpu", "type": "torch.Tensor", "text": "\nReturns a copy of this object in CPU memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cross()", "path": "tensors#torch.Tensor.cross", "type": "torch.Tensor", "text": "\nSee `torch.cross()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cuda()", "path": "tensors#torch.Tensor.cuda", "type": "torch.Tensor", "text": "\nReturns a copy of this object in CUDA memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cummax()", "path": "tensors#torch.Tensor.cummax", "type": "torch.Tensor", "text": "\nSee `torch.cummax()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cummin()", "path": "tensors#torch.Tensor.cummin", "type": "torch.Tensor", "text": "\nSee `torch.cummin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cumprod()", "path": "tensors#torch.Tensor.cumprod", "type": "torch.Tensor", "text": "\nSee `torch.cumprod()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cumprod_()", "path": "tensors#torch.Tensor.cumprod_", "type": "torch.Tensor", "text": "\nIn-place version of `cumprod()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cumsum()", "path": "tensors#torch.Tensor.cumsum", "type": "torch.Tensor", "text": "\nSee `torch.cumsum()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.cumsum_()", "path": "tensors#torch.Tensor.cumsum_", "type": "torch.Tensor", "text": "\nIn-place version of `cumsum()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.data_ptr()", "path": "tensors#torch.Tensor.data_ptr", "type": "torch.Tensor", "text": "\nReturns the address of the first element of `self` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.deg2rad()", "path": "tensors#torch.Tensor.deg2rad", "type": "torch.Tensor", "text": "\nSee `torch.deg2rad()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.dense_dim()", "path": "sparse#torch.Tensor.dense_dim", "type": "torch.sparse", "text": "\nReturn the number of dense dimensions in a sparse tensor `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.dequantize()", "path": "tensors#torch.Tensor.dequantize", "type": "torch.Tensor", "text": "\nGiven a quantized Tensor, dequantize it and return the dequantized float\nTensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.det()", "path": "tensors#torch.Tensor.det", "type": "torch.Tensor", "text": "\nSee `torch.det()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.detach()", "path": "autograd#torch.Tensor.detach", "type": "torch.autograd", "text": "\nReturns a new Tensor, detached from the current graph.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.detach_()", "path": "autograd#torch.Tensor.detach_", "type": "torch.autograd", "text": "\nDetaches the Tensor from the graph that created it, making it a leaf. Views\ncannot be detached in-place.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.device", "path": "tensors#torch.Tensor.device", "type": "torch.Tensor", "text": "\nIs the `torch.device` where this Tensor is.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.diag()", "path": "tensors#torch.Tensor.diag", "type": "torch.Tensor", "text": "\nSee `torch.diag()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.diagflat()", "path": "tensors#torch.Tensor.diagflat", "type": "torch.Tensor", "text": "\nSee `torch.diagflat()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.diagonal()", "path": "tensors#torch.Tensor.diagonal", "type": "torch.Tensor", "text": "\nSee `torch.diagonal()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.diag_embed()", "path": "tensors#torch.Tensor.diag_embed", "type": "torch.Tensor", "text": "\nSee `torch.diag_embed()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.diff()", "path": "tensors#torch.Tensor.diff", "type": "torch.Tensor", "text": "\nSee `torch.diff()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.digamma()", "path": "tensors#torch.Tensor.digamma", "type": "torch.Tensor", "text": "\nSee `torch.digamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.digamma_()", "path": "tensors#torch.Tensor.digamma_", "type": "torch.Tensor", "text": "\nIn-place version of `digamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.dim()", "path": "tensors#torch.Tensor.dim", "type": "torch.Tensor", "text": "\nReturns the number of dimensions of `self` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.dist()", "path": "tensors#torch.Tensor.dist", "type": "torch.Tensor", "text": "\nSee `torch.dist()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.div()", "path": "tensors#torch.Tensor.div", "type": "torch.Tensor", "text": "\nSee `torch.div()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.divide()", "path": "tensors#torch.Tensor.divide", "type": "torch.Tensor", "text": "\nSee `torch.divide()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.divide_()", "path": "tensors#torch.Tensor.divide_", "type": "torch.Tensor", "text": "\nIn-place version of `divide()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.div_()", "path": "tensors#torch.Tensor.div_", "type": "torch.Tensor", "text": "\nIn-place version of `div()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.dot()", "path": "tensors#torch.Tensor.dot", "type": "torch.Tensor", "text": "\nSee `torch.dot()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.double()", "path": "tensors#torch.Tensor.double", "type": "torch.Tensor", "text": "\n`self.double()` is equivalent to `self.to(torch.float64)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.eig()", "path": "tensors#torch.Tensor.eig", "type": "torch.Tensor", "text": "\nSee `torch.eig()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.element_size()", "path": "tensors#torch.Tensor.element_size", "type": "torch.Tensor", "text": "\nReturns the size in bytes of an individual element.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.eq()", "path": "tensors#torch.Tensor.eq", "type": "torch.Tensor", "text": "\nSee `torch.eq()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.equal()", "path": "tensors#torch.Tensor.equal", "type": "torch.Tensor", "text": "\nSee `torch.equal()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.eq_()", "path": "tensors#torch.Tensor.eq_", "type": "torch.Tensor", "text": "\nIn-place version of `eq()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.erf()", "path": "tensors#torch.Tensor.erf", "type": "torch.Tensor", "text": "\nSee `torch.erf()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.erfc()", "path": "tensors#torch.Tensor.erfc", "type": "torch.Tensor", "text": "\nSee `torch.erfc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.erfc_()", "path": "tensors#torch.Tensor.erfc_", "type": "torch.Tensor", "text": "\nIn-place version of `erfc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.erfinv()", "path": "tensors#torch.Tensor.erfinv", "type": "torch.Tensor", "text": "\nSee `torch.erfinv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.erfinv_()", "path": "tensors#torch.Tensor.erfinv_", "type": "torch.Tensor", "text": "\nIn-place version of `erfinv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.erf_()", "path": "tensors#torch.Tensor.erf_", "type": "torch.Tensor", "text": "\nIn-place version of `erf()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.exp()", "path": "tensors#torch.Tensor.exp", "type": "torch.Tensor", "text": "\nSee `torch.exp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.expand()", "path": "tensors#torch.Tensor.expand", "type": "torch.Tensor", "text": "\nReturns a new view of the `self` tensor with singleton dimensions expanded to\na larger size.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.expand_as()", "path": "tensors#torch.Tensor.expand_as", "type": "torch.Tensor", "text": "\nExpand this tensor to the same size as `other`. `self.expand_as(other)` is\nequivalent to `self.expand(other.size())`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.expm1()", "path": "tensors#torch.Tensor.expm1", "type": "torch.Tensor", "text": "\nSee `torch.expm1()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.expm1_()", "path": "tensors#torch.Tensor.expm1_", "type": "torch.Tensor", "text": "\nIn-place version of `expm1()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.exponential_()", "path": "tensors#torch.Tensor.exponential_", "type": "torch.Tensor", "text": "\nFills `self` tensor with elements drawn from the exponential distribution:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.exp_()", "path": "tensors#torch.Tensor.exp_", "type": "torch.Tensor", "text": "\nIn-place version of `exp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fill_()", "path": "tensors#torch.Tensor.fill_", "type": "torch.Tensor", "text": "\nFills `self` tensor with the specified value.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fill_diagonal_()", "path": "tensors#torch.Tensor.fill_diagonal_", "type": "torch.Tensor", "text": "\nFill the main diagonal of a tensor that has at least 2-dimensions. When\ndims>2, all dimensions of input must be of equal length. This function\nmodifies the input tensor in-place, and returns the input tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fix()", "path": "tensors#torch.Tensor.fix", "type": "torch.Tensor", "text": "\nSee `torch.fix()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fix_()", "path": "tensors#torch.Tensor.fix_", "type": "torch.Tensor", "text": "\nIn-place version of `fix()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.flatten()", "path": "tensors#torch.Tensor.flatten", "type": "torch.Tensor", "text": "\nsee `torch.flatten()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.flip()", "path": "tensors#torch.Tensor.flip", "type": "torch.Tensor", "text": "\nSee `torch.flip()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fliplr()", "path": "tensors#torch.Tensor.fliplr", "type": "torch.Tensor", "text": "\nSee `torch.fliplr()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.flipud()", "path": "tensors#torch.Tensor.flipud", "type": "torch.Tensor", "text": "\nSee `torch.flipud()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.float()", "path": "tensors#torch.Tensor.float", "type": "torch.Tensor", "text": "\n`self.float()` is equivalent to `self.to(torch.float32)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.float_power()", "path": "tensors#torch.Tensor.float_power", "type": "torch.Tensor", "text": "\nSee `torch.float_power()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.float_power_()", "path": "tensors#torch.Tensor.float_power_", "type": "torch.Tensor", "text": "\nIn-place version of `float_power()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.floor()", "path": "tensors#torch.Tensor.floor", "type": "torch.Tensor", "text": "\nSee `torch.floor()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.floor_()", "path": "tensors#torch.Tensor.floor_", "type": "torch.Tensor", "text": "\nIn-place version of `floor()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.floor_divide()", "path": "tensors#torch.Tensor.floor_divide", "type": "torch.Tensor", "text": "\nSee `torch.floor_divide()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.floor_divide_()", "path": "tensors#torch.Tensor.floor_divide_", "type": "torch.Tensor", "text": "\nIn-place version of `floor_divide()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fmax()", "path": "tensors#torch.Tensor.fmax", "type": "torch.Tensor", "text": "\nSee `torch.fmax()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fmin()", "path": "tensors#torch.Tensor.fmin", "type": "torch.Tensor", "text": "\nSee `torch.fmin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fmod()", "path": "tensors#torch.Tensor.fmod", "type": "torch.Tensor", "text": "\nSee `torch.fmod()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.fmod_()", "path": "tensors#torch.Tensor.fmod_", "type": "torch.Tensor", "text": "\nIn-place version of `fmod()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.frac()", "path": "tensors#torch.Tensor.frac", "type": "torch.Tensor", "text": "\nSee `torch.frac()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.frac_()", "path": "tensors#torch.Tensor.frac_", "type": "torch.Tensor", "text": "\nIn-place version of `frac()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.gather()", "path": "tensors#torch.Tensor.gather", "type": "torch.Tensor", "text": "\nSee `torch.gather()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.gcd()", "path": "tensors#torch.Tensor.gcd", "type": "torch.Tensor", "text": "\nSee `torch.gcd()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.gcd_()", "path": "tensors#torch.Tensor.gcd_", "type": "torch.Tensor", "text": "\nIn-place version of `gcd()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ge()", "path": "tensors#torch.Tensor.ge", "type": "torch.Tensor", "text": "\nSee `torch.ge()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.geometric_()", "path": "tensors#torch.Tensor.geometric_", "type": "torch.Tensor", "text": "\nFills `self` tensor with elements drawn from the geometric distribution:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.geqrf()", "path": "tensors#torch.Tensor.geqrf", "type": "torch.Tensor", "text": "\nSee `torch.geqrf()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ger()", "path": "tensors#torch.Tensor.ger", "type": "torch.Tensor", "text": "\nSee `torch.ger()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.get_device()", "path": "tensors#torch.Tensor.get_device", "type": "torch.Tensor", "text": "\nFor CUDA tensors, this function returns the device ordinal of the GPU on which\nthe tensor resides. For CPU tensors, an error is thrown.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ge_()", "path": "tensors#torch.Tensor.ge_", "type": "torch.Tensor", "text": "\nIn-place version of `ge()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.grad", "path": "autograd#torch.Tensor.grad", "type": "torch.autograd", "text": "\nThis attribute is `None` by default and becomes a Tensor the first time a call\nto `backward()` computes gradients for `self`. The attribute will then contain\nthe gradients computed and future calls to `backward()` will accumulate (add)\ngradients into it.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.greater()", "path": "tensors#torch.Tensor.greater", "type": "torch.Tensor", "text": "\nSee `torch.greater()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.greater_()", "path": "tensors#torch.Tensor.greater_", "type": "torch.Tensor", "text": "\nIn-place version of `greater()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.greater_equal()", "path": "tensors#torch.Tensor.greater_equal", "type": "torch.Tensor", "text": "\nSee `torch.greater_equal()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.greater_equal_()", "path": "tensors#torch.Tensor.greater_equal_", "type": "torch.Tensor", "text": "\nIn-place version of `greater_equal()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.gt()", "path": "tensors#torch.Tensor.gt", "type": "torch.Tensor", "text": "\nSee `torch.gt()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.gt_()", "path": "tensors#torch.Tensor.gt_", "type": "torch.Tensor", "text": "\nIn-place version of `gt()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.half()", "path": "tensors#torch.Tensor.half", "type": "torch.Tensor", "text": "\n`self.half()` is equivalent to `self.to(torch.float16)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.hardshrink()", "path": "tensors#torch.Tensor.hardshrink", "type": "torch.Tensor", "text": "\nSee `torch.nn.functional.hardshrink()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.heaviside()", "path": "tensors#torch.Tensor.heaviside", "type": "torch.Tensor", "text": "\nSee `torch.heaviside()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.histc()", "path": "tensors#torch.Tensor.histc", "type": "torch.Tensor", "text": "\nSee `torch.histc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.hypot()", "path": "tensors#torch.Tensor.hypot", "type": "torch.Tensor", "text": "\nSee `torch.hypot()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.hypot_()", "path": "tensors#torch.Tensor.hypot_", "type": "torch.Tensor", "text": "\nIn-place version of `hypot()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.i0()", "path": "tensors#torch.Tensor.i0", "type": "torch.Tensor", "text": "\nSee `torch.i0()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.i0_()", "path": "tensors#torch.Tensor.i0_", "type": "torch.Tensor", "text": "\nIn-place version of `i0()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.igamma()", "path": "tensors#torch.Tensor.igamma", "type": "torch.Tensor", "text": "\nSee `torch.igamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.igammac()", "path": "tensors#torch.Tensor.igammac", "type": "torch.Tensor", "text": "\nSee `torch.igammac()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.igammac_()", "path": "tensors#torch.Tensor.igammac_", "type": "torch.Tensor", "text": "\nIn-place version of `igammac()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.igamma_()", "path": "tensors#torch.Tensor.igamma_", "type": "torch.Tensor", "text": "\nIn-place version of `igamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.imag", "path": "tensors#torch.Tensor.imag", "type": "torch.Tensor", "text": "\nReturns a new tensor containing imaginary values of the `self` tensor. The\nreturned tensor and `self` share the same underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_add()", "path": "tensors#torch.Tensor.index_add", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.index_add_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_add_()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_add_()", "path": "tensors#torch.Tensor.index_add_", "type": "torch.Tensor", "text": "\nAccumulate the elements of `tensor` into the `self` tensor by adding to the\nindices in the order given in `index`. For example, if `dim == 0` and\n`index[i] == j`, then the `i`th row of `tensor` is added to the `j`th row of\n`self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_copy()", "path": "tensors#torch.Tensor.index_copy", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.index_copy_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_copy_()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_copy_()", "path": "tensors#torch.Tensor.index_copy_", "type": "torch.Tensor", "text": "\nCopies the elements of `tensor` into the `self` tensor by selecting the\nindices in the order given in `index`. For example, if `dim == 0` and\n`index[i] == j`, then the `i`th row of `tensor` is copied to the `j`th row of\n`self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_fill()", "path": "tensors#torch.Tensor.index_fill", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.index_fill_()`. `tensor1` corresponds to\n`self` in `torch.Tensor.index_fill_()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_fill_()", "path": "tensors#torch.Tensor.index_fill_", "type": "torch.Tensor", "text": "\nFills the elements of the `self` tensor with value `val` by selecting the\nindices in the order given in `index`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_put()", "path": "tensors#torch.Tensor.index_put", "type": "torch.Tensor", "text": "\nOut-place version of `index_put_()`. `tensor1` corresponds to `self` in\n`torch.Tensor.index_put_()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_put_()", "path": "tensors#torch.Tensor.index_put_", "type": "torch.Tensor", "text": "\nPuts values from the tensor `values` into the tensor `self` using the indices\nspecified in `indices` (which is a tuple of Tensors). The expression\n`tensor.index_put_(indices, values)` is equivalent to `tensor[indices] =\nvalues`. Returns `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.index_select()", "path": "tensors#torch.Tensor.index_select", "type": "torch.Tensor", "text": "\nSee `torch.index_select()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.indices()", "path": "sparse#torch.Tensor.indices", "type": "torch.sparse", "text": "\nReturn the indices tensor of a sparse COO tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.inner()", "path": "tensors#torch.Tensor.inner", "type": "torch.Tensor", "text": "\nSee `torch.inner()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.int()", "path": "tensors#torch.Tensor.int", "type": "torch.Tensor", "text": "\n`self.int()` is equivalent to `self.to(torch.int32)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.int_repr()", "path": "tensors#torch.Tensor.int_repr", "type": "torch.Tensor", "text": "\nGiven a quantized Tensor, `self.int_repr()` returns a CPU Tensor with uint8_t\nas data type that stores the underlying uint8_t values of the given Tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.inverse()", "path": "tensors#torch.Tensor.inverse", "type": "torch.Tensor", "text": "\nSee `torch.inverse()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isclose()", "path": "tensors#torch.Tensor.isclose", "type": "torch.Tensor", "text": "\nSee `torch.isclose()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isfinite()", "path": "tensors#torch.Tensor.isfinite", "type": "torch.Tensor", "text": "\nSee `torch.isfinite()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isinf()", "path": "tensors#torch.Tensor.isinf", "type": "torch.Tensor", "text": "\nSee `torch.isinf()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isnan()", "path": "tensors#torch.Tensor.isnan", "type": "torch.Tensor", "text": "\nSee `torch.isnan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isneginf()", "path": "tensors#torch.Tensor.isneginf", "type": "torch.Tensor", "text": "\nSee `torch.isneginf()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isposinf()", "path": "tensors#torch.Tensor.isposinf", "type": "torch.Tensor", "text": "\nSee `torch.isposinf()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.isreal()", "path": "tensors#torch.Tensor.isreal", "type": "torch.Tensor", "text": "\nSee `torch.isreal()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.istft()", "path": "tensors#torch.Tensor.istft", "type": "torch.Tensor", "text": "\nSee `torch.istft()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_coalesced()", "path": "sparse#torch.Tensor.is_coalesced", "type": "torch.sparse", "text": "\nReturns `True` if `self` is a sparse COO tensor that is coalesced, `False`\notherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_complex()", "path": "tensors#torch.Tensor.is_complex", "type": "torch.Tensor", "text": "\nReturns True if the data type of `self` is a complex data type.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_contiguous()", "path": "tensors#torch.Tensor.is_contiguous", "type": "torch.Tensor", "text": "\nReturns True if `self` tensor is contiguous in memory in the order specified\nby memory format.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_cuda", "path": "tensors#torch.Tensor.is_cuda", "type": "torch.Tensor", "text": "\nIs `True` if the Tensor is stored on the GPU, `False` otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_floating_point()", "path": "tensors#torch.Tensor.is_floating_point", "type": "torch.Tensor", "text": "\nReturns True if the data type of `self` is a floating point data type.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_leaf", "path": "autograd#torch.Tensor.is_leaf", "type": "torch.autograd", "text": "\nAll Tensors that have `requires_grad` which is `False` will be leaf Tensors by\nconvention.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_meta", "path": "tensors#torch.Tensor.is_meta", "type": "torch.Tensor", "text": "\nIs `True` if the Tensor is a meta tensor, `False` otherwise. Meta tensors are\nlike normal tensors, but they carry no data.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_pinned()", "path": "tensors#torch.Tensor.is_pinned", "type": "torch.Tensor", "text": "\nReturns true if this tensor resides in pinned memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_quantized", "path": "tensors#torch.Tensor.is_quantized", "type": "torch.Tensor", "text": "\nIs `True` if the Tensor is quantized, `False` otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_set_to()", "path": "tensors#torch.Tensor.is_set_to", "type": "torch.Tensor", "text": "\nReturns True if both tensors are pointing to the exact same memory (same\nstorage, offset, size and stride).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_shared()", "path": "tensors#torch.Tensor.is_shared", "type": "torch.Tensor", "text": "\nChecks if tensor is in shared memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_signed()", "path": "tensors#torch.Tensor.is_signed", "type": "torch.Tensor", "text": "\nReturns True if the data type of `self` is a signed data type.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.is_sparse", "path": "sparse#torch.Tensor.is_sparse", "type": "torch.sparse", "text": "\nIs `True` if the Tensor uses sparse storage layout, `False` otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.item()", "path": "tensors#torch.Tensor.item", "type": "torch.Tensor", "text": "\nReturns the value of this tensor as a standard Python number. This only works\nfor tensors with one element. For other cases, see `tolist()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.kthvalue()", "path": "tensors#torch.Tensor.kthvalue", "type": "torch.Tensor", "text": "\nSee `torch.kthvalue()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lcm()", "path": "tensors#torch.Tensor.lcm", "type": "torch.Tensor", "text": "\nSee `torch.lcm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lcm_()", "path": "tensors#torch.Tensor.lcm_", "type": "torch.Tensor", "text": "\nIn-place version of `lcm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ldexp()", "path": "tensors#torch.Tensor.ldexp", "type": "torch.Tensor", "text": "\nSee `torch.ldexp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ldexp_()", "path": "tensors#torch.Tensor.ldexp_", "type": "torch.Tensor", "text": "\nIn-place version of `ldexp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.le()", "path": "tensors#torch.Tensor.le", "type": "torch.Tensor", "text": "\nSee `torch.le()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lerp()", "path": "tensors#torch.Tensor.lerp", "type": "torch.Tensor", "text": "\nSee `torch.lerp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lerp_()", "path": "tensors#torch.Tensor.lerp_", "type": "torch.Tensor", "text": "\nIn-place version of `lerp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.less()", "path": "tensors#torch.Tensor.less", "type": "torch.Tensor", "text": "\nlt(other) -> Tensor\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.less_()", "path": "tensors#torch.Tensor.less_", "type": "torch.Tensor", "text": "\nIn-place version of `less()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.less_equal()", "path": "tensors#torch.Tensor.less_equal", "type": "torch.Tensor", "text": "\nSee `torch.less_equal()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.less_equal_()", "path": "tensors#torch.Tensor.less_equal_", "type": "torch.Tensor", "text": "\nIn-place version of `less_equal()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.le_()", "path": "tensors#torch.Tensor.le_", "type": "torch.Tensor", "text": "\nIn-place version of `le()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lgamma()", "path": "tensors#torch.Tensor.lgamma", "type": "torch.Tensor", "text": "\nSee `torch.lgamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lgamma_()", "path": "tensors#torch.Tensor.lgamma_", "type": "torch.Tensor", "text": "\nIn-place version of `lgamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log()", "path": "tensors#torch.Tensor.log", "type": "torch.Tensor", "text": "\nSee `torch.log()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log10()", "path": "tensors#torch.Tensor.log10", "type": "torch.Tensor", "text": "\nSee `torch.log10()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log10_()", "path": "tensors#torch.Tensor.log10_", "type": "torch.Tensor", "text": "\nIn-place version of `log10()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log1p()", "path": "tensors#torch.Tensor.log1p", "type": "torch.Tensor", "text": "\nSee `torch.log1p()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log1p_()", "path": "tensors#torch.Tensor.log1p_", "type": "torch.Tensor", "text": "\nIn-place version of `log1p()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log2()", "path": "tensors#torch.Tensor.log2", "type": "torch.Tensor", "text": "\nSee `torch.log2()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log2_()", "path": "tensors#torch.Tensor.log2_", "type": "torch.Tensor", "text": "\nIn-place version of `log2()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logaddexp()", "path": "tensors#torch.Tensor.logaddexp", "type": "torch.Tensor", "text": "\nSee `torch.logaddexp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logaddexp2()", "path": "tensors#torch.Tensor.logaddexp2", "type": "torch.Tensor", "text": "\nSee `torch.logaddexp2()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logcumsumexp()", "path": "tensors#torch.Tensor.logcumsumexp", "type": "torch.Tensor", "text": "\nSee `torch.logcumsumexp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logdet()", "path": "tensors#torch.Tensor.logdet", "type": "torch.Tensor", "text": "\nSee `torch.logdet()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_and()", "path": "tensors#torch.Tensor.logical_and", "type": "torch.Tensor", "text": "\nSee `torch.logical_and()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_and_()", "path": "tensors#torch.Tensor.logical_and_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_and()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_not()", "path": "tensors#torch.Tensor.logical_not", "type": "torch.Tensor", "text": "\nSee `torch.logical_not()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_not_()", "path": "tensors#torch.Tensor.logical_not_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_not()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_or()", "path": "tensors#torch.Tensor.logical_or", "type": "torch.Tensor", "text": "\nSee `torch.logical_or()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_or_()", "path": "tensors#torch.Tensor.logical_or_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_or()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_xor()", "path": "tensors#torch.Tensor.logical_xor", "type": "torch.Tensor", "text": "\nSee `torch.logical_xor()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logical_xor_()", "path": "tensors#torch.Tensor.logical_xor_", "type": "torch.Tensor", "text": "\nIn-place version of `logical_xor()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logit()", "path": "tensors#torch.Tensor.logit", "type": "torch.Tensor", "text": "\nSee `torch.logit()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logit_()", "path": "tensors#torch.Tensor.logit_", "type": "torch.Tensor", "text": "\nIn-place version of `logit()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.logsumexp()", "path": "tensors#torch.Tensor.logsumexp", "type": "torch.Tensor", "text": "\nSee `torch.logsumexp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log_()", "path": "tensors#torch.Tensor.log_", "type": "torch.Tensor", "text": "\nIn-place version of `log()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.log_normal_()", "path": "tensors#torch.Tensor.log_normal_", "type": "torch.Tensor", "text": "\nFills `self` tensor with numbers samples from the log-normal distribution\nparameterized by the given mean \u03bc\\mu and standard deviation \u03c3\\sigma . Note\nthat `mean` and `std` are the mean and standard deviation of the underlying\nnormal distribution, and not of the returned distribution:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.long()", "path": "tensors#torch.Tensor.long", "type": "torch.Tensor", "text": "\n`self.long()` is equivalent to `self.to(torch.int64)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lstsq()", "path": "tensors#torch.Tensor.lstsq", "type": "torch.Tensor", "text": "\nSee `torch.lstsq()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lt()", "path": "tensors#torch.Tensor.lt", "type": "torch.Tensor", "text": "\nSee `torch.lt()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lt_()", "path": "tensors#torch.Tensor.lt_", "type": "torch.Tensor", "text": "\nIn-place version of `lt()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lu()", "path": "tensors#torch.Tensor.lu", "type": "torch.Tensor", "text": "\nSee `torch.lu()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.lu_solve()", "path": "tensors#torch.Tensor.lu_solve", "type": "torch.Tensor", "text": "\nSee `torch.lu_solve()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.map_()", "path": "tensors#torch.Tensor.map_", "type": "torch.Tensor", "text": "\nApplies `callable` for each element in `self` tensor and the given `tensor`\nand stores the results in `self` tensor. `self` tensor and the given `tensor`\nmust be broadcastable.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.masked_fill()", "path": "tensors#torch.Tensor.masked_fill", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.masked_fill_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.masked_fill_()", "path": "tensors#torch.Tensor.masked_fill_", "type": "torch.Tensor", "text": "\nFills elements of `self` tensor with `value` where `mask` is True. The shape\nof `mask` must be broadcastable with the shape of the underlying tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.masked_scatter()", "path": "tensors#torch.Tensor.masked_scatter", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.masked_scatter_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.masked_scatter_()", "path": "tensors#torch.Tensor.masked_scatter_", "type": "torch.Tensor", "text": "\nCopies elements from `source` into `self` tensor at positions where the `mask`\nis True. The shape of `mask` must be broadcastable with the shape of the\nunderlying tensor. The `source` should have at least as many elements as the\nnumber of ones in `mask`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.masked_select()", "path": "tensors#torch.Tensor.masked_select", "type": "torch.Tensor", "text": "\nSee `torch.masked_select()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.matmul()", "path": "tensors#torch.Tensor.matmul", "type": "torch.Tensor", "text": "\nSee `torch.matmul()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.matrix_exp()", "path": "tensors#torch.Tensor.matrix_exp", "type": "torch.Tensor", "text": "\nSee `torch.matrix_exp()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.matrix_power()", "path": "tensors#torch.Tensor.matrix_power", "type": "torch.Tensor", "text": "\nSee `torch.matrix_power()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.max()", "path": "tensors#torch.Tensor.max", "type": "torch.Tensor", "text": "\nSee `torch.max()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.maximum()", "path": "tensors#torch.Tensor.maximum", "type": "torch.Tensor", "text": "\nSee `torch.maximum()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mean()", "path": "tensors#torch.Tensor.mean", "type": "torch.Tensor", "text": "\nSee `torch.mean()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.median()", "path": "tensors#torch.Tensor.median", "type": "torch.Tensor", "text": "\nSee `torch.median()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.min()", "path": "tensors#torch.Tensor.min", "type": "torch.Tensor", "text": "\nSee `torch.min()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.minimum()", "path": "tensors#torch.Tensor.minimum", "type": "torch.Tensor", "text": "\nSee `torch.minimum()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mm()", "path": "tensors#torch.Tensor.mm", "type": "torch.Tensor", "text": "\nSee `torch.mm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mode()", "path": "tensors#torch.Tensor.mode", "type": "torch.Tensor", "text": "\nSee `torch.mode()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.moveaxis()", "path": "tensors#torch.Tensor.moveaxis", "type": "torch.Tensor", "text": "\nSee `torch.moveaxis()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.movedim()", "path": "tensors#torch.Tensor.movedim", "type": "torch.Tensor", "text": "\nSee `torch.movedim()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.msort()", "path": "tensors#torch.Tensor.msort", "type": "torch.Tensor", "text": "\nSee `torch.msort()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mul()", "path": "tensors#torch.Tensor.mul", "type": "torch.Tensor", "text": "\nSee `torch.mul()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.multinomial()", "path": "tensors#torch.Tensor.multinomial", "type": "torch.Tensor", "text": "\nSee `torch.multinomial()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.multiply()", "path": "tensors#torch.Tensor.multiply", "type": "torch.Tensor", "text": "\nSee `torch.multiply()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.multiply_()", "path": "tensors#torch.Tensor.multiply_", "type": "torch.Tensor", "text": "\nIn-place version of `multiply()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mul_()", "path": "tensors#torch.Tensor.mul_", "type": "torch.Tensor", "text": "\nIn-place version of `mul()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mv()", "path": "tensors#torch.Tensor.mv", "type": "torch.Tensor", "text": "\nSee `torch.mv()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mvlgamma()", "path": "tensors#torch.Tensor.mvlgamma", "type": "torch.Tensor", "text": "\nSee `torch.mvlgamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.mvlgamma_()", "path": "tensors#torch.Tensor.mvlgamma_", "type": "torch.Tensor", "text": "\nIn-place version of `mvlgamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.names", "path": "named_tensor#torch.Tensor.names", "type": "Named Tensors", "text": "\nStores names for each of this tensor\u2019s dimensions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nanmedian()", "path": "tensors#torch.Tensor.nanmedian", "type": "torch.Tensor", "text": "\nSee `torch.nanmedian()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nanquantile()", "path": "tensors#torch.Tensor.nanquantile", "type": "torch.Tensor", "text": "\nSee `torch.nanquantile()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nansum()", "path": "tensors#torch.Tensor.nansum", "type": "torch.Tensor", "text": "\nSee `torch.nansum()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nan_to_num()", "path": "tensors#torch.Tensor.nan_to_num", "type": "torch.Tensor", "text": "\nSee `torch.nan_to_num()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nan_to_num_()", "path": "tensors#torch.Tensor.nan_to_num_", "type": "torch.Tensor", "text": "\nIn-place version of `nan_to_num()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.narrow()", "path": "tensors#torch.Tensor.narrow", "type": "torch.Tensor", "text": "\nSee `torch.narrow()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.narrow_copy()", "path": "tensors#torch.Tensor.narrow_copy", "type": "torch.Tensor", "text": "\nSame as `Tensor.narrow()` except returning a copy rather than shared storage.\nThis is primarily for sparse tensors, which do not have a shared-storage\nnarrow method. Calling ``narrow_copy` with ``dimemsion > self.sparse_dim()``\nwill return a copy with the relevant dense dimension narrowed, and\n``self.shape`` updated accordingly.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ndim", "path": "tensors#torch.Tensor.ndim", "type": "torch.Tensor", "text": "\nAlias for `dim()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ndimension()", "path": "tensors#torch.Tensor.ndimension", "type": "torch.Tensor", "text": "\nAlias for `dim()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ne()", "path": "tensors#torch.Tensor.ne", "type": "torch.Tensor", "text": "\nSee `torch.ne()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.neg()", "path": "tensors#torch.Tensor.neg", "type": "torch.Tensor", "text": "\nSee `torch.neg()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.negative()", "path": "tensors#torch.Tensor.negative", "type": "torch.Tensor", "text": "\nSee `torch.negative()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.negative_()", "path": "tensors#torch.Tensor.negative_", "type": "torch.Tensor", "text": "\nIn-place version of `negative()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.neg_()", "path": "tensors#torch.Tensor.neg_", "type": "torch.Tensor", "text": "\nIn-place version of `neg()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nelement()", "path": "tensors#torch.Tensor.nelement", "type": "torch.Tensor", "text": "\nAlias for `numel()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.new_empty()", "path": "tensors#torch.Tensor.new_empty", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with uninitialized data. By default,\nthe returned Tensor has the same `torch.dtype` and `torch.device` as this\ntensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.new_full()", "path": "tensors#torch.Tensor.new_full", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with `fill_value`. By default, the\nreturned Tensor has the same `torch.dtype` and `torch.device` as this tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.new_ones()", "path": "tensors#torch.Tensor.new_ones", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with `1`. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.new_tensor()", "path": "tensors#torch.Tensor.new_tensor", "type": "torch.Tensor", "text": "\nReturns a new Tensor with `data` as the tensor data. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.new_zeros()", "path": "tensors#torch.Tensor.new_zeros", "type": "torch.Tensor", "text": "\nReturns a Tensor of size `size` filled with `0`. By default, the returned\nTensor has the same `torch.dtype` and `torch.device` as this tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nextafter()", "path": "tensors#torch.Tensor.nextafter", "type": "torch.Tensor", "text": "\nSee `torch.nextafter()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nextafter_()", "path": "tensors#torch.Tensor.nextafter_", "type": "torch.Tensor", "text": "\nIn-place version of `nextafter()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ne_()", "path": "tensors#torch.Tensor.ne_", "type": "torch.Tensor", "text": "\nIn-place version of `ne()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.nonzero()", "path": "tensors#torch.Tensor.nonzero", "type": "torch.Tensor", "text": "\nSee `torch.nonzero()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.norm()", "path": "tensors#torch.Tensor.norm", "type": "torch.Tensor", "text": "\nSee `torch.norm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.normal_()", "path": "tensors#torch.Tensor.normal_", "type": "torch.Tensor", "text": "\nFills `self` tensor with elements samples from the normal distribution\nparameterized by `mean` and `std`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.not_equal()", "path": "tensors#torch.Tensor.not_equal", "type": "torch.Tensor", "text": "\nSee `torch.not_equal()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.not_equal_()", "path": "tensors#torch.Tensor.not_equal_", "type": "torch.Tensor", "text": "\nIn-place version of `not_equal()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.numel()", "path": "tensors#torch.Tensor.numel", "type": "torch.Tensor", "text": "\nSee `torch.numel()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.numpy()", "path": "tensors#torch.Tensor.numpy", "type": "torch.Tensor", "text": "\nReturns `self` tensor as a NumPy `ndarray`. This tensor and the returned\n`ndarray` share the same underlying storage. Changes to `self` tensor will be\nreflected in the `ndarray` and vice versa.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.orgqr()", "path": "tensors#torch.Tensor.orgqr", "type": "torch.Tensor", "text": "\nSee `torch.orgqr()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ormqr()", "path": "tensors#torch.Tensor.ormqr", "type": "torch.Tensor", "text": "\nSee `torch.ormqr()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.outer()", "path": "tensors#torch.Tensor.outer", "type": "torch.Tensor", "text": "\nSee `torch.outer()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.permute()", "path": "tensors#torch.Tensor.permute", "type": "torch.Tensor", "text": "\nReturns a view of the original tensor with its dimensions permuted.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.pinverse()", "path": "tensors#torch.Tensor.pinverse", "type": "torch.Tensor", "text": "\nSee `torch.pinverse()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.pin_memory()", "path": "tensors#torch.Tensor.pin_memory", "type": "torch.Tensor", "text": "\nCopies the tensor to pinned memory, if it\u2019s not already pinned.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.polygamma()", "path": "tensors#torch.Tensor.polygamma", "type": "torch.Tensor", "text": "\nSee `torch.polygamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.polygamma_()", "path": "tensors#torch.Tensor.polygamma_", "type": "torch.Tensor", "text": "\nIn-place version of `polygamma()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.pow()", "path": "tensors#torch.Tensor.pow", "type": "torch.Tensor", "text": "\nSee `torch.pow()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.pow_()", "path": "tensors#torch.Tensor.pow_", "type": "torch.Tensor", "text": "\nIn-place version of `pow()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.prod()", "path": "tensors#torch.Tensor.prod", "type": "torch.Tensor", "text": "\nSee `torch.prod()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.put_()", "path": "tensors#torch.Tensor.put_", "type": "torch.Tensor", "text": "\nCopies the elements from `tensor` into the positions specified by indices. For\nthe purpose of indexing, the `self` tensor is treated as if it were a 1-D\ntensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.qr()", "path": "tensors#torch.Tensor.qr", "type": "torch.Tensor", "text": "\nSee `torch.qr()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.qscheme()", "path": "tensors#torch.Tensor.qscheme", "type": "torch.Tensor", "text": "\nReturns the quantization scheme of a given QTensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.quantile()", "path": "tensors#torch.Tensor.quantile", "type": "torch.Tensor", "text": "\nSee `torch.quantile()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.q_per_channel_axis()", "path": "tensors#torch.Tensor.q_per_channel_axis", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\nthe index of dimension on which per-channel quantization is applied.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.q_per_channel_scales()", "path": "tensors#torch.Tensor.q_per_channel_scales", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\na Tensor of scales of the underlying quantizer. It has the number of elements\nthat matches the corresponding dimensions (from q_per_channel_axis) of the\ntensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.q_per_channel_zero_points()", "path": "tensors#torch.Tensor.q_per_channel_zero_points", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns\na tensor of zero_points of the underlying quantizer. It has the number of\nelements that matches the corresponding dimensions (from q_per_channel_axis)\nof the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.q_scale()", "path": "tensors#torch.Tensor.q_scale", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of\nthe underlying quantizer().\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.q_zero_point()", "path": "tensors#torch.Tensor.q_zero_point", "type": "torch.Tensor", "text": "\nGiven a Tensor quantized by linear(affine) quantization, returns the\nzero_point of the underlying quantizer().\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.rad2deg()", "path": "tensors#torch.Tensor.rad2deg", "type": "torch.Tensor", "text": "\nSee `torch.rad2deg()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.random_()", "path": "tensors#torch.Tensor.random_", "type": "torch.Tensor", "text": "\nFills `self` tensor with numbers sampled from the discrete uniform\ndistribution over `[from, to - 1]`. If not specified, the values are usually\nonly bounded by `self` tensor\u2019s data type. However, for floating point types,\nif unspecified, range will be `[0, 2^mantissa]` to ensure that every value is\nrepresentable. For example, `torch.tensor(1, dtype=torch.double).random_()`\nwill be uniform in `[0, 2^53]`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.ravel()", "path": "tensors#torch.Tensor.ravel", "type": "torch.Tensor", "text": "\nsee `torch.ravel()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.real", "path": "tensors#torch.Tensor.real", "type": "torch.Tensor", "text": "\nReturns a new tensor containing real values of the `self` tensor. The returned\ntensor and `self` share the same underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.reciprocal()", "path": "tensors#torch.Tensor.reciprocal", "type": "torch.Tensor", "text": "\nSee `torch.reciprocal()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.reciprocal_()", "path": "tensors#torch.Tensor.reciprocal_", "type": "torch.Tensor", "text": "\nIn-place version of `reciprocal()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.record_stream()", "path": "tensors#torch.Tensor.record_stream", "type": "torch.Tensor", "text": "\nEnsures that the tensor memory is not reused for another tensor until all\ncurrent work queued on `stream` are complete.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.refine_names()", "path": "named_tensor#torch.Tensor.refine_names", "type": "Named Tensors", "text": "\nRefines the dimension names of `self` according to `names`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.register_hook()", "path": "autograd#torch.Tensor.register_hook", "type": "torch.autograd", "text": "\nRegisters a backward hook.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.remainder()", "path": "tensors#torch.Tensor.remainder", "type": "torch.Tensor", "text": "\nSee `torch.remainder()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.remainder_()", "path": "tensors#torch.Tensor.remainder_", "type": "torch.Tensor", "text": "\nIn-place version of `remainder()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.rename()", "path": "named_tensor#torch.Tensor.rename", "type": "Named Tensors", "text": "\nRenames dimension names of `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.rename_()", "path": "named_tensor#torch.Tensor.rename_", "type": "Named Tensors", "text": "\nIn-place version of `rename()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.renorm()", "path": "tensors#torch.Tensor.renorm", "type": "torch.Tensor", "text": "\nSee `torch.renorm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.renorm_()", "path": "tensors#torch.Tensor.renorm_", "type": "torch.Tensor", "text": "\nIn-place version of `renorm()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.repeat()", "path": "tensors#torch.Tensor.repeat", "type": "torch.Tensor", "text": "\nRepeats this tensor along the specified dimensions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.repeat_interleave()", "path": "tensors#torch.Tensor.repeat_interleave", "type": "torch.Tensor", "text": "\nSee `torch.repeat_interleave()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.requires_grad", "path": "autograd#torch.Tensor.requires_grad", "type": "torch.autograd", "text": "\nIs `True` if gradients need to be computed for this Tensor, `False` otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.requires_grad_()", "path": "tensors#torch.Tensor.requires_grad_", "type": "torch.Tensor", "text": "\nChange if autograd should record operations on this tensor: sets this tensor\u2019s\n`requires_grad` attribute in-place. Returns this tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.reshape()", "path": "tensors#torch.Tensor.reshape", "type": "torch.Tensor", "text": "\nReturns a tensor with the same data and number of elements as `self` but with\nthe specified shape. This method returns a view if `shape` is compatible with\nthe current shape. See `torch.Tensor.view()` on when it is possible to return\na view.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.reshape_as()", "path": "tensors#torch.Tensor.reshape_as", "type": "torch.Tensor", "text": "\nReturns this tensor as the same shape as `other`. `self.reshape_as(other)` is\nequivalent to `self.reshape(other.sizes())`. This method returns a view if\n`other.sizes()` is compatible with the current shape. See\n`torch.Tensor.view()` on when it is possible to return a view.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.resize_()", "path": "tensors#torch.Tensor.resize_", "type": "torch.Tensor", "text": "\nResizes `self` tensor to the specified size. If the number of elements is\nlarger than the current storage size, then the underlying storage is resized\nto fit the new number of elements. If the number of elements is smaller, the\nunderlying storage is not changed. Existing elements are preserved but any new\nmemory is uninitialized.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.resize_as_()", "path": "tensors#torch.Tensor.resize_as_", "type": "torch.Tensor", "text": "\nResizes the `self` tensor to be the same size as the specified `tensor`. This\nis equivalent to `self.resize_(tensor.size())`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.retain_grad()", "path": "autograd#torch.Tensor.retain_grad", "type": "torch.autograd", "text": "\nEnables .grad attribute for non-leaf Tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.roll()", "path": "tensors#torch.Tensor.roll", "type": "torch.Tensor", "text": "\nSee `torch.roll()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.rot90()", "path": "tensors#torch.Tensor.rot90", "type": "torch.Tensor", "text": "\nSee `torch.rot90()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.round()", "path": "tensors#torch.Tensor.round", "type": "torch.Tensor", "text": "\nSee `torch.round()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.round_()", "path": "tensors#torch.Tensor.round_", "type": "torch.Tensor", "text": "\nIn-place version of `round()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.rsqrt()", "path": "tensors#torch.Tensor.rsqrt", "type": "torch.Tensor", "text": "\nSee `torch.rsqrt()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.rsqrt_()", "path": "tensors#torch.Tensor.rsqrt_", "type": "torch.Tensor", "text": "\nIn-place version of `rsqrt()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.scatter()", "path": "tensors#torch.Tensor.scatter", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.scatter_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.scatter_()", "path": "tensors#torch.Tensor.scatter_", "type": "torch.Tensor", "text": "\nWrites all values from the tensor `src` into `self` at the indices specified\nin the `index` tensor. For each value in `src`, its output index is specified\nby its index in `src` for `dimension != dim` and by the corresponding value in\n`index` for `dimension = dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.scatter_add()", "path": "tensors#torch.Tensor.scatter_add", "type": "torch.Tensor", "text": "\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.scatter_add_()", "path": "tensors#torch.Tensor.scatter_add_", "type": "torch.Tensor", "text": "\nAdds all values from the tensor `other` into `self` at the indices specified\nin the `index` tensor in a similar fashion as `scatter_()`. For each value in\n`src`, it is added to an index in `self` which is specified by its index in\n`src` for `dimension != dim` and by the corresponding value in `index` for\n`dimension = dim`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.select()", "path": "tensors#torch.Tensor.select", "type": "torch.Tensor", "text": "\nSlices the `self` tensor along the selected dimension at the given index. This\nfunction returns a view of the original tensor with the given dimension\nremoved.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.set_()", "path": "tensors#torch.Tensor.set_", "type": "torch.Tensor", "text": "\nSets the underlying storage, size, and strides. If `source` is a tensor,\n`self` tensor will share the same storage and have the same size and strides\nas `source`. Changes to elements in one tensor will be reflected in the other.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sgn()", "path": "tensors#torch.Tensor.sgn", "type": "torch.Tensor", "text": "\nSee `torch.sgn()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sgn_()", "path": "tensors#torch.Tensor.sgn_", "type": "torch.Tensor", "text": "\nIn-place version of `sgn()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.share_memory_()", "path": "tensors#torch.Tensor.share_memory_", "type": "torch.Tensor", "text": "\nMoves the underlying storage to shared memory.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.short()", "path": "tensors#torch.Tensor.short", "type": "torch.Tensor", "text": "\n`self.short()` is equivalent to `self.to(torch.int16)`. See `to()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sigmoid()", "path": "tensors#torch.Tensor.sigmoid", "type": "torch.Tensor", "text": "\nSee `torch.sigmoid()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sigmoid_()", "path": "tensors#torch.Tensor.sigmoid_", "type": "torch.Tensor", "text": "\nIn-place version of `sigmoid()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sign()", "path": "tensors#torch.Tensor.sign", "type": "torch.Tensor", "text": "\nSee `torch.sign()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.signbit()", "path": "tensors#torch.Tensor.signbit", "type": "torch.Tensor", "text": "\nSee `torch.signbit()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sign_()", "path": "tensors#torch.Tensor.sign_", "type": "torch.Tensor", "text": "\nIn-place version of `sign()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sin()", "path": "tensors#torch.Tensor.sin", "type": "torch.Tensor", "text": "\nSee `torch.sin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sinc()", "path": "tensors#torch.Tensor.sinc", "type": "torch.Tensor", "text": "\nSee `torch.sinc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sinc_()", "path": "tensors#torch.Tensor.sinc_", "type": "torch.Tensor", "text": "\nIn-place version of `sinc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sinh()", "path": "tensors#torch.Tensor.sinh", "type": "torch.Tensor", "text": "\nSee `torch.sinh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sinh_()", "path": "tensors#torch.Tensor.sinh_", "type": "torch.Tensor", "text": "\nIn-place version of `sinh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sin_()", "path": "tensors#torch.Tensor.sin_", "type": "torch.Tensor", "text": "\nIn-place version of `sin()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.size()", "path": "tensors#torch.Tensor.size", "type": "torch.Tensor", "text": "\nReturns the size of the `self` tensor. The returned value is a subclass of\n`tuple`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.slogdet()", "path": "tensors#torch.Tensor.slogdet", "type": "torch.Tensor", "text": "\nSee `torch.slogdet()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.solve()", "path": "tensors#torch.Tensor.solve", "type": "torch.Tensor", "text": "\nSee `torch.solve()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sort()", "path": "tensors#torch.Tensor.sort", "type": "torch.Tensor", "text": "\nSee `torch.sort()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_dim()", "path": "sparse#torch.Tensor.sparse_dim", "type": "torch.sparse", "text": "\nReturn the number of sparse dimensions in a sparse tensor `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_mask()", "path": "sparse#torch.Tensor.sparse_mask", "type": "torch.sparse", "text": "\nReturns a new sparse tensor with values from a strided tensor `self` filtered\nby the indices of the sparse tensor `mask`. The values of `mask` sparse tensor\nare ignored. `self` and `mask` tensors must have the same shape.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_resize_()", "path": "sparse#torch.Tensor.sparse_resize_", "type": "torch.sparse", "text": "\nResizes `self` sparse tensor to the desired size and the number of sparse and\ndense dimensions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sparse_resize_and_clear_()", "path": "sparse#torch.Tensor.sparse_resize_and_clear_", "type": "torch.sparse", "text": "\nRemoves all specified elements from a sparse tensor `self` and resizes `self`\nto the desired size and the number of sparse and dense dimensions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.split()", "path": "tensors#torch.Tensor.split", "type": "torch.Tensor", "text": "\nSee `torch.split()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sqrt()", "path": "tensors#torch.Tensor.sqrt", "type": "torch.Tensor", "text": "\nSee `torch.sqrt()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sqrt_()", "path": "tensors#torch.Tensor.sqrt_", "type": "torch.Tensor", "text": "\nIn-place version of `sqrt()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.square()", "path": "tensors#torch.Tensor.square", "type": "torch.Tensor", "text": "\nSee `torch.square()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.square_()", "path": "tensors#torch.Tensor.square_", "type": "torch.Tensor", "text": "\nIn-place version of `square()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.squeeze()", "path": "tensors#torch.Tensor.squeeze", "type": "torch.Tensor", "text": "\nSee `torch.squeeze()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.squeeze_()", "path": "tensors#torch.Tensor.squeeze_", "type": "torch.Tensor", "text": "\nIn-place version of `squeeze()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.std()", "path": "tensors#torch.Tensor.std", "type": "torch.Tensor", "text": "\nSee `torch.std()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.stft()", "path": "tensors#torch.Tensor.stft", "type": "torch.Tensor", "text": "\nSee `torch.stft()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.storage()", "path": "tensors#torch.Tensor.storage", "type": "torch.Tensor", "text": "\nReturns the underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.storage_offset()", "path": "tensors#torch.Tensor.storage_offset", "type": "torch.Tensor", "text": "\nReturns `self` tensor\u2019s offset in the underlying storage in terms of number of\nstorage elements (not bytes).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.storage_type()", "path": "tensors#torch.Tensor.storage_type", "type": "torch.Tensor", "text": "\nReturns the type of the underlying storage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.stride()", "path": "tensors#torch.Tensor.stride", "type": "torch.Tensor", "text": "\nReturns the stride of `self` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sub()", "path": "tensors#torch.Tensor.sub", "type": "torch.Tensor", "text": "\nSee `torch.sub()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.subtract()", "path": "tensors#torch.Tensor.subtract", "type": "torch.Tensor", "text": "\nSee `torch.subtract()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.subtract_()", "path": "tensors#torch.Tensor.subtract_", "type": "torch.Tensor", "text": "\nIn-place version of `subtract()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sub_()", "path": "tensors#torch.Tensor.sub_", "type": "torch.Tensor", "text": "\nIn-place version of `sub()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sum()", "path": "tensors#torch.Tensor.sum", "type": "torch.Tensor", "text": "\nSee `torch.sum()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.sum_to_size()", "path": "tensors#torch.Tensor.sum_to_size", "type": "torch.Tensor", "text": "\nSum `this` tensor to `size`. `size` must be broadcastable to `this` tensor\nsize.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.svd()", "path": "tensors#torch.Tensor.svd", "type": "torch.Tensor", "text": "\nSee `torch.svd()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.swapaxes()", "path": "tensors#torch.Tensor.swapaxes", "type": "torch.Tensor", "text": "\nSee `torch.swapaxes()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.swapdims()", "path": "tensors#torch.Tensor.swapdims", "type": "torch.Tensor", "text": "\nSee `torch.swapdims()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.symeig()", "path": "tensors#torch.Tensor.symeig", "type": "torch.Tensor", "text": "\nSee `torch.symeig()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.T", "path": "tensors#torch.Tensor.T", "type": "torch.Tensor", "text": "\nIs this Tensor with its dimensions reversed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.t()", "path": "tensors#torch.Tensor.t", "type": "torch.Tensor", "text": "\nSee `torch.t()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.take()", "path": "tensors#torch.Tensor.take", "type": "torch.Tensor", "text": "\nSee `torch.take()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tan()", "path": "tensors#torch.Tensor.tan", "type": "torch.Tensor", "text": "\nSee `torch.tan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tanh()", "path": "tensors#torch.Tensor.tanh", "type": "torch.Tensor", "text": "\nSee `torch.tanh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tanh_()", "path": "tensors#torch.Tensor.tanh_", "type": "torch.Tensor", "text": "\nIn-place version of `tanh()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tan_()", "path": "tensors#torch.Tensor.tan_", "type": "torch.Tensor", "text": "\nIn-place version of `tan()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tensor_split()", "path": "tensors#torch.Tensor.tensor_split", "type": "torch.Tensor", "text": "\nSee `torch.tensor_split()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tile()", "path": "tensors#torch.Tensor.tile", "type": "torch.Tensor", "text": "\nSee `torch.tile()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.to()", "path": "tensors#torch.Tensor.to", "type": "torch.Tensor", "text": "\nPerforms Tensor dtype and/or device conversion. A `torch.dtype` and\n`torch.device` are inferred from the arguments of `self.to(*args, **kwargs)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tolist()", "path": "tensors#torch.Tensor.tolist", "type": "torch.Tensor", "text": "\nReturns the tensor as a (nested) list. For scalars, a standard Python number\nis returned, just like with `item()`. Tensors are automatically moved to the\nCPU first if necessary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.topk()", "path": "tensors#torch.Tensor.topk", "type": "torch.Tensor", "text": "\nSee `torch.topk()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.to_dense()", "path": "sparse#torch.Tensor.to_dense", "type": "torch.sparse", "text": "\nCreates a strided copy of `self`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.to_mkldnn()", "path": "tensors#torch.Tensor.to_mkldnn", "type": "torch.Tensor", "text": "\nReturns a copy of the tensor in `torch.mkldnn` layout.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.to_sparse()", "path": "sparse#torch.Tensor.to_sparse", "type": "torch.sparse", "text": "\nReturns a sparse copy of the tensor. PyTorch supports sparse tensors in\ncoordinate format.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.trace()", "path": "tensors#torch.Tensor.trace", "type": "torch.Tensor", "text": "\nSee `torch.trace()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.transpose()", "path": "tensors#torch.Tensor.transpose", "type": "torch.Tensor", "text": "\nSee `torch.transpose()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.transpose_()", "path": "tensors#torch.Tensor.transpose_", "type": "torch.Tensor", "text": "\nIn-place version of `transpose()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.triangular_solve()", "path": "tensors#torch.Tensor.triangular_solve", "type": "torch.Tensor", "text": "\nSee `torch.triangular_solve()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tril()", "path": "tensors#torch.Tensor.tril", "type": "torch.Tensor", "text": "\nSee `torch.tril()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.tril_()", "path": "tensors#torch.Tensor.tril_", "type": "torch.Tensor", "text": "\nIn-place version of `tril()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.triu()", "path": "tensors#torch.Tensor.triu", "type": "torch.Tensor", "text": "\nSee `torch.triu()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.triu_()", "path": "tensors#torch.Tensor.triu_", "type": "torch.Tensor", "text": "\nIn-place version of `triu()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.true_divide()", "path": "tensors#torch.Tensor.true_divide", "type": "torch.Tensor", "text": "\nSee `torch.true_divide()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.true_divide_()", "path": "tensors#torch.Tensor.true_divide_", "type": "torch.Tensor", "text": "\nIn-place version of `true_divide_()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.trunc()", "path": "tensors#torch.Tensor.trunc", "type": "torch.Tensor", "text": "\nSee `torch.trunc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.trunc_()", "path": "tensors#torch.Tensor.trunc_", "type": "torch.Tensor", "text": "\nIn-place version of `trunc()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.type()", "path": "tensors#torch.Tensor.type", "type": "torch.Tensor", "text": "\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.type_as()", "path": "tensors#torch.Tensor.type_as", "type": "torch.Tensor", "text": "\nReturns this tensor cast to the type of the given tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.t_()", "path": "tensors#torch.Tensor.t_", "type": "torch.Tensor", "text": "\nIn-place version of `t()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unbind()", "path": "tensors#torch.Tensor.unbind", "type": "torch.Tensor", "text": "\nSee `torch.unbind()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unflatten()", "path": "named_tensor#torch.Tensor.unflatten", "type": "Named Tensors", "text": "\nExpands the dimension `dim` of the `self` tensor over multiple dimensions of\nsizes given by `sizes`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unfold()", "path": "tensors#torch.Tensor.unfold", "type": "torch.Tensor", "text": "\nReturns a view of the original tensor which contains all slices of size `size`\nfrom `self` tensor in the dimension `dimension`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.uniform_()", "path": "tensors#torch.Tensor.uniform_", "type": "torch.Tensor", "text": "\nFills `self` tensor with numbers sampled from the continuous uniform\ndistribution:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unique()", "path": "tensors#torch.Tensor.unique", "type": "torch.Tensor", "text": "\nReturns the unique elements of the input tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unique_consecutive()", "path": "tensors#torch.Tensor.unique_consecutive", "type": "torch.Tensor", "text": "\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unsqueeze()", "path": "tensors#torch.Tensor.unsqueeze", "type": "torch.Tensor", "text": "\nSee `torch.unsqueeze()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.unsqueeze_()", "path": "tensors#torch.Tensor.unsqueeze_", "type": "torch.Tensor", "text": "\nIn-place version of `unsqueeze()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.values()", "path": "sparse#torch.Tensor.values", "type": "torch.sparse", "text": "\nReturn the values tensor of a sparse COO tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.var()", "path": "tensors#torch.Tensor.var", "type": "torch.Tensor", "text": "\nSee `torch.var()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.vdot()", "path": "tensors#torch.Tensor.vdot", "type": "torch.Tensor", "text": "\nSee `torch.vdot()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.view()", "path": "tensors#torch.Tensor.view", "type": "torch.Tensor", "text": "\nReturns a new tensor with the same data as the `self` tensor but of a\ndifferent `shape`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.view_as()", "path": "tensors#torch.Tensor.view_as", "type": "torch.Tensor", "text": "\nView this tensor as the same size as `other`. `self.view_as(other)` is\nequivalent to `self.view(other.size())`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.where()", "path": "tensors#torch.Tensor.where", "type": "torch.Tensor", "text": "\n`self.where(condition, y)` is equivalent to `torch.where(condition, self, y)`.\nSee `torch.where()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.xlogy()", "path": "tensors#torch.Tensor.xlogy", "type": "torch.Tensor", "text": "\nSee `torch.xlogy()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.xlogy_()", "path": "tensors#torch.Tensor.xlogy_", "type": "torch.Tensor", "text": "\nIn-place version of `xlogy()`\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.Tensor.zero_()", "path": "tensors#torch.Tensor.zero_", "type": "torch.Tensor", "text": "\nFills `self` tensor with zeros.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tensordot()", "path": "generated/torch.tensordot#torch.tensordot", "type": "torch", "text": "\nReturns a contraction of a and b over multiple dimensions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tensor_split()", "path": "generated/torch.tensor_split#torch.tensor_split", "type": "torch", "text": "\nSplits a tensor into multiple sub-tensors, all of which are views of `input`,\nalong dimension `dim` according to the indices or number of sections specified\nby `indices_or_sections`. This function is based on NumPy\u2019s\n`numpy.array_split()`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tile()", "path": "generated/torch.tile#torch.tile", "type": "torch", "text": "\nConstructs a tensor by repeating the elements of `input`. The `reps` argument\nspecifies the number of repetitions in each dimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.topk()", "path": "generated/torch.topk#torch.topk", "type": "torch", "text": "\nReturns the `k` largest elements of the given `input` tensor along a given\ndimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.default_generator", "path": "torch#torch.torch.default_generator", "type": "torch", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.device", "path": "tensor_attributes#torch.torch.device", "type": "Tensor Attributes", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.dtype", "path": "tensor_attributes#torch.torch.dtype", "type": "Tensor Attributes", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.finfo", "path": "type_info#torch.torch.finfo", "type": "Type Info", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.iinfo", "path": "type_info#torch.torch.iinfo", "type": "Type Info", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.layout", "path": "tensor_attributes#torch.torch.layout", "type": "Tensor Attributes", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.torch.memory_format", "path": "tensor_attributes#torch.torch.memory_format", "type": "Tensor Attributes", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.trace()", "path": "generated/torch.trace#torch.trace", "type": "torch", "text": "\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.transpose()", "path": "generated/torch.transpose#torch.transpose", "type": "torch", "text": "\nReturns a tensor that is a transposed version of `input`. The given dimensions\n`dim0` and `dim1` are swapped.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.trapz()", "path": "generated/torch.trapz#torch.trapz", "type": "torch", "text": "\nEstimate \u222bydx\\int y\\,dx along `dim`, using the trapezoid rule.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.triangular_solve()", "path": "generated/torch.triangular_solve#torch.triangular_solve", "type": "torch", "text": "\nSolves a system of equations with a triangular coefficient matrix AA and\nmultiple right-hand sides bb .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tril()", "path": "generated/torch.tril#torch.tril", "type": "torch", "text": "\nReturns the lower triangular part of the matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.tril_indices()", "path": "generated/torch.tril_indices#torch.tril_indices", "type": "torch", "text": "\nReturns the indices of the lower triangular part of a `row`-by- `col` matrix\nin a 2-by-N Tensor, where the first row contains row coordinates of all\nindices and the second row contains column coordinates. Indices are ordered\nbased on rows and then columns.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.triu()", "path": "generated/torch.triu#torch.triu", "type": "torch", "text": "\nReturns the upper triangular part of a matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.triu_indices()", "path": "generated/torch.triu_indices#torch.triu_indices", "type": "torch", "text": "\nReturns the indices of the upper triangular part of a `row` by `col` matrix in\na 2-by-N Tensor, where the first row contains row coordinates of all indices\nand the second row contains column coordinates. Indices are ordered based on\nrows and then columns.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.true_divide()", "path": "generated/torch.true_divide#torch.true_divide", "type": "torch", "text": "\nAlias for `torch.div()` with `rounding_mode=None`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.trunc()", "path": "generated/torch.trunc#torch.trunc", "type": "torch", "text": "\nReturns a new tensor with the truncated integer values of the elements of\n`input`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.unbind()", "path": "generated/torch.unbind#torch.unbind", "type": "torch", "text": "\nRemoves a tensor dimension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.unique()", "path": "generated/torch.unique#torch.unique", "type": "torch", "text": "\nReturns the unique elements of the input tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.unique_consecutive()", "path": "generated/torch.unique_consecutive#torch.unique_consecutive", "type": "torch", "text": "\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.unsqueeze()", "path": "generated/torch.unsqueeze#torch.unsqueeze", "type": "torch", "text": "\nReturns a new tensor with a dimension of size one inserted at the specified\nposition.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.use_deterministic_algorithms()", "path": "generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms", "type": "torch", "text": "\nSets whether PyTorch operations must use \u201cdeterministic\u201d algorithms. That is,\nalgorithms which, given the same input, and when run on the same software and\nhardware, always produce the same output. When True, operations will use\ndeterministic algorithms when available, and if only nondeterministic\nalgorithms are available they will throw a :class:RuntimeError when called.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark", "path": "benchmark_utils", "type": "torch.utils.benchmark", "text": "\nHelper class for measuring execution time of PyTorch statements.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats", "type": "torch.utils.benchmark", "text": "\nTop level container for Callgrind results collected by Timer.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.as_standardized()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.as_standardized", "type": "torch.utils.benchmark", "text": "\nStrip library names and some prefixes from function strings.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.counts()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.counts", "type": "torch.utils.benchmark", "text": "\nReturns the total number of instructions executed.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.delta()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.delta", "type": "torch.utils.benchmark", "text": "\nDiff two sets of counts.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.CallgrindStats.stats()", "path": "benchmark_utils#torch.utils.benchmark.CallgrindStats.stats", "type": "torch.utils.benchmark", "text": "\nReturns detailed function counts.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts", "type": "torch.utils.benchmark", "text": "\nContainer for manipulating Callgrind results.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts.denoise()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.denoise", "type": "torch.utils.benchmark", "text": "\nRemove known noisy instructions.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts.filter()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.filter", "type": "torch.utils.benchmark", "text": "\nKeep only the elements where `filter_fn` applied to function name returns\nTrue.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.FunctionCounts.transform()", "path": "benchmark_utils#torch.utils.benchmark.FunctionCounts.transform", "type": "torch.utils.benchmark", "text": "\nApply `map_fn` to all of the function names.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Measurement", "path": "benchmark_utils#torch.utils.benchmark.Measurement", "type": "torch.utils.benchmark", "text": "\nThe result of a Timer measurement.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Measurement.merge()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.merge", "type": "torch.utils.benchmark", "text": "\nConvenience method for merging replicates.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Measurement.significant_figures()", "path": "benchmark_utils#torch.utils.benchmark.Measurement.significant_figures", "type": "torch.utils.benchmark", "text": "\nApproximate significant figure estimate.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer", "path": "benchmark_utils#torch.utils.benchmark.Timer", "type": "torch.utils.benchmark", "text": "\nHelper class for measuring execution time of PyTorch statements.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer.blocked_autorange()", "path": "benchmark_utils#torch.utils.benchmark.Timer.blocked_autorange", "type": "torch.utils.benchmark", "text": "\nMeasure many replicates while keeping timer overhead to a minimum.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer.collect_callgrind()", "path": "benchmark_utils#torch.utils.benchmark.Timer.collect_callgrind", "type": "torch.utils.benchmark", "text": "\nCollect instruction counts using Callgrind.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.benchmark.Timer.timeit()", "path": "benchmark_utils#torch.utils.benchmark.Timer.timeit", "type": "torch.utils.benchmark", "text": "\nMirrors the semantics of timeit.Timer.timeit().\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.bottleneck", "path": "bottleneck", "type": "torch.utils.bottleneck", "text": "\n`torch.utils.bottleneck` is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.checkpoint", "path": "checkpoint", "type": "torch.utils.checkpoint", "text": "\nNote\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.checkpoint.checkpoint()", "path": "checkpoint#torch.utils.checkpoint.checkpoint", "type": "torch.utils.checkpoint", "text": "\nCheckpoint a model or part of the model\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.checkpoint.checkpoint_sequential()", "path": "checkpoint#torch.utils.checkpoint.checkpoint_sequential", "type": "torch.utils.checkpoint", "text": "\nA helper function for checkpointing sequential models.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension", "path": "cpp_extension", "type": "torch.utils.cpp_extension", "text": "\nCreates a `setuptools.Extension` for C++.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.BuildExtension()", "path": "cpp_extension#torch.utils.cpp_extension.BuildExtension", "type": "torch.utils.cpp_extension", "text": "\nA custom `setuptools` build extension .\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.check_compiler_abi_compatibility()", "path": "cpp_extension#torch.utils.cpp_extension.check_compiler_abi_compatibility", "type": "torch.utils.cpp_extension", "text": "\nVerifies that the given compiler is ABI-compatible with PyTorch.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.CppExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CppExtension", "type": "torch.utils.cpp_extension", "text": "\nCreates a `setuptools.Extension` for C++.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.CUDAExtension()", "path": "cpp_extension#torch.utils.cpp_extension.CUDAExtension", "type": "torch.utils.cpp_extension", "text": "\nCreates a `setuptools.Extension` for CUDA/C++.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.include_paths()", "path": "cpp_extension#torch.utils.cpp_extension.include_paths", "type": "torch.utils.cpp_extension", "text": "\nGet the include paths required to build a C++ or CUDA extension.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.is_ninja_available()", "path": "cpp_extension#torch.utils.cpp_extension.is_ninja_available", "type": "torch.utils.cpp_extension", "text": "\nReturns `True` if the ninja build system is available on the system, `False`\notherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.load()", "path": "cpp_extension#torch.utils.cpp_extension.load", "type": "torch.utils.cpp_extension", "text": "\nLoads a PyTorch C++ extension just-in-time (JIT).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.load_inline()", "path": "cpp_extension#torch.utils.cpp_extension.load_inline", "type": "torch.utils.cpp_extension", "text": "\nLoads a PyTorch C++ extension just-in-time (JIT) from string sources.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.cpp_extension.verify_ninja_availability()", "path": "cpp_extension#torch.utils.cpp_extension.verify_ninja_availability", "type": "torch.utils.cpp_extension", "text": "\nRaises `RuntimeError` if ninja build system is not available on the system,\ndoes nothing otherwise.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data", "path": "data", "type": "torch.utils.data", "text": "\nAt the heart of PyTorch data loading utility is the\n`torch.utils.data.DataLoader` class. It represents a Python iterable over a\ndataset, with support for\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.BatchSampler", "path": "data#torch.utils.data.BatchSampler", "type": "torch.utils.data", "text": "\nWraps another sampler to yield a mini-batch of indices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.BufferedShuffleDataset", "path": "data#torch.utils.data.BufferedShuffleDataset", "type": "torch.utils.data", "text": "\nDataset shuffled from the original dataset.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.ChainDataset", "path": "data#torch.utils.data.ChainDataset", "type": "torch.utils.data", "text": "\nDataset for chainning multiple `IterableDataset` s.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.ConcatDataset", "path": "data#torch.utils.data.ConcatDataset", "type": "torch.utils.data", "text": "\nDataset as a concatenation of multiple datasets.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.DataLoader", "path": "data#torch.utils.data.DataLoader", "type": "torch.utils.data", "text": "\nData loader. Combines a dataset and a sampler, and provides an iterable over\nthe given dataset.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.Dataset", "path": "data#torch.utils.data.Dataset", "type": "torch.utils.data", "text": "\nAn abstract class representing a `Dataset`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.distributed.DistributedSampler", "path": "data#torch.utils.data.distributed.DistributedSampler", "type": "torch.utils.data", "text": "\nSampler that restricts data loading to a subset of the dataset.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.get_worker_info()", "path": "data#torch.utils.data.get_worker_info", "type": "torch.utils.data", "text": "\nReturns the information about the current `DataLoader` iterator worker\nprocess.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.IterableDataset", "path": "data#torch.utils.data.IterableDataset", "type": "torch.utils.data", "text": "\nAn iterable Dataset.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.RandomSampler", "path": "data#torch.utils.data.RandomSampler", "type": "torch.utils.data", "text": "\nSamples elements randomly. If without replacement, then sample from a shuffled\ndataset. If with replacement, then user can specify `num_samples` to draw.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.random_split()", "path": "data#torch.utils.data.random_split", "type": "torch.utils.data", "text": "\nRandomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.:\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.Sampler", "path": "data#torch.utils.data.Sampler", "type": "torch.utils.data", "text": "\nBase class for all Samplers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.SequentialSampler", "path": "data#torch.utils.data.SequentialSampler", "type": "torch.utils.data", "text": "\nSamples elements sequentially, always in the same order.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.Subset", "path": "data#torch.utils.data.Subset", "type": "torch.utils.data", "text": "\nSubset of a dataset at specified indices.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.SubsetRandomSampler", "path": "data#torch.utils.data.SubsetRandomSampler", "type": "torch.utils.data", "text": "\nSamples elements randomly from a given list of indices, without replacement.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.TensorDataset", "path": "data#torch.utils.data.TensorDataset", "type": "torch.utils.data", "text": "\nDataset wrapping tensors.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.data.WeightedRandomSampler", "path": "data#torch.utils.data.WeightedRandomSampler", "type": "torch.utils.data", "text": "\nSamples elements from `[0,..,len(weights)-1]` with given probabilities\n(weights).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.dlpack", "path": "dlpack", "type": "torch.utils.dlpack", "text": "\nDecodes a DLPack to a tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.dlpack.from_dlpack()", "path": "dlpack#torch.utils.dlpack.from_dlpack", "type": "torch.utils.dlpack", "text": "\nDecodes a DLPack to a tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.dlpack.to_dlpack()", "path": "dlpack#torch.utils.dlpack.to_dlpack", "type": "torch.utils.dlpack", "text": "\nReturns a DLPack representing the tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.mobile_optimizer", "path": "mobile_optimizer", "type": "torch.utils.mobile_optimizer", "text": "\nWarning\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.mobile_optimizer.optimize_for_mobile()", "path": "mobile_optimizer#torch.utils.mobile_optimizer.optimize_for_mobile", "type": "torch.utils.mobile_optimizer", "text": "\nA new optimized torch script module\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.model_zoo", "path": "model_zoo", "type": "torch.utils.model_zoo", "text": "\nMoved to `torch.hub`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.model_zoo.load_url()", "path": "model_zoo#torch.utils.model_zoo.load_url", "type": "torch.utils.model_zoo", "text": "\nLoads the Torch serialized object at the given URL.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard", "path": "tensorboard", "type": "torch.utils.tensorboard", "text": "\nBefore going further, more details on TensorBoard can be found at\nhttps://www.tensorflow.org/tensorboard/\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter", "type": "torch.utils.tensorboard", "text": "\nWrites entries directly to event files in the log_dir to be consumed by\nTensorBoard.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_audio()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_audio", "type": "torch.utils.tensorboard", "text": "\nAdd audio data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars", "type": "torch.utils.tensorboard", "text": "\nCreate special chart by collecting charts tags in \u2018scalars\u2019. Note that this\nfunction can only be called once for each SummaryWriter() object. Because it\nonly provides metadata to tensorboard, the function can be called before or\nafter the training loop.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_embedding", "type": "torch.utils.tensorboard", "text": "\nAdd embedding projector data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_figure()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_figure", "type": "torch.utils.tensorboard", "text": "\nRender matplotlib figure into an image and add it to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_graph()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_graph", "type": "torch.utils.tensorboard", "text": "\nAdd graph data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_histogram", "type": "torch.utils.tensorboard", "text": "\nAdd histogram to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_hparams", "type": "torch.utils.tensorboard", "text": "\nAdd a set of hyperparameters to be compared in TensorBoard.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_image()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_image", "type": "torch.utils.tensorboard", "text": "\nAdd image data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_images()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_images", "type": "torch.utils.tensorboard", "text": "\nAdd batched image data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_mesh", "type": "torch.utils.tensorboard", "text": "\nAdd meshes or 3D point clouds to TensorBoard. The visualization is based on\nThree.js, so it allows users to interact with the rendered object. Besides the\nbasic definitions such as vertices, faces, users can further provide camera\nparameter, lighting condition, etc. Please check\nhttps://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene\nfor advanced usage.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve", "type": "torch.utils.tensorboard", "text": "\nAdds precision recall curve. Plotting a precision-recall curve lets you\nunderstand your model\u2019s performance under different threshold settings. With\nthis function, you provide the ground truth labeling (T/F) and prediction\nconfidence (usually the output of your model) for each target. The TensorBoard\nUI will let you choose the threshold interactively.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalar", "type": "torch.utils.tensorboard", "text": "\nAdd scalar data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_scalars", "type": "torch.utils.tensorboard", "text": "\nAdds many scalar data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_text()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_text", "type": "torch.utils.tensorboard", "text": "\nAdd text data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.add_video()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.add_video", "type": "torch.utils.tensorboard", "text": "\nAdd video data to summary.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.close()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.close", "type": "torch.utils.tensorboard", "text": "\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.flush()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.flush", "type": "torch.utils.tensorboard", "text": "\nFlushes the event file to disk. Call this method to make sure that all pending\nevents have been written to disk.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.utils.tensorboard.writer.SummaryWriter.__init__()", "path": "tensorboard#torch.utils.tensorboard.writer.SummaryWriter.__init__", "type": "torch.utils.tensorboard", "text": "\nCreates a `SummaryWriter` that will write out events and summaries to the\nevent file.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.vander()", "path": "generated/torch.vander#torch.vander", "type": "torch", "text": "\nGenerates a Vandermonde matrix.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.var()", "path": "generated/torch.var#torch.var", "type": "torch", "text": "\nReturns the variance of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.var_mean()", "path": "generated/torch.var_mean#torch.var_mean", "type": "torch", "text": "\nReturns the variance and mean of all elements in the `input` tensor.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.vdot()", "path": "generated/torch.vdot#torch.vdot", "type": "torch", "text": "\nComputes the dot product of two 1D tensors. The vdot(a, b) function handles\ncomplex numbers differently than dot(a, b). If the first argument is complex,\nthe complex conjugate of the first argument is used for the calculation of the\ndot product.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.view_as_complex()", "path": "generated/torch.view_as_complex#torch.view_as_complex", "type": "torch", "text": "\nReturns a view of `input` as a complex tensor. For an input complex tensor of\n`size` m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , this function returns a new complex\ntensor of `size` m1,m2,\u2026,mim1, m2, \\dots, mi where the last dimension of the\ninput tensor is expected to represent the real and imaginary components of\ncomplex numbers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.view_as_real()", "path": "generated/torch.view_as_real#torch.view_as_real", "type": "torch", "text": "\nReturns a view of `input` as a real tensor. For an input complex tensor of\n`size` m1,m2,\u2026,mim1, m2, \\dots, mi , this function returns a new real tensor\nof size m1,m2,\u2026,mi,2m1, m2, \\dots, mi, 2 , where the last dimension of size 2\nrepresents the real and imaginary components of complex numbers.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.vstack()", "path": "generated/torch.vstack#torch.vstack", "type": "torch", "text": "\nStack tensors in sequence vertically (row wise).\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.where()", "path": "generated/torch.where#torch.where", "type": "torch", "text": "\nReturn a tensor of elements selected from either `x` or `y`, depending on\n`condition`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.xlogy()", "path": "generated/torch.xlogy#torch.xlogy", "type": "torch", "text": "\nComputes `input * log(other)` with the following cases.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.zeros()", "path": "generated/torch.zeros#torch.zeros", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `0`, with the shape defined by\nthe variable argument `size`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch.zeros_like()", "path": "generated/torch.zeros_like#torch.zeros_like", "type": "torch", "text": "\nReturns a tensor filled with the scalar value `0`, with the same size as\n`input`. `torch.zeros_like(input)` is equivalent to `torch.zeros(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}, {"name": "torch._assert()", "path": "generated/torch._assert#torch._assert", "type": "torch", "text": "\nA wrapper around Python\u2019s assert which is symbolically traceable.\n\n  *[LIFO]: last-in, first-out\n  *[FIFO]: first-in, first-out\n\n"}]